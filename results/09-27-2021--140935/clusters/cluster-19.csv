text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Ethical Considerations for Language Modeling within Brain-Computer Interfaces Project Summary Machine learning (ML) and Natural Language Processing (NLP) have the potential to transform communication for patients with neurodegenerative disease through personalized and real-time augmentative and alternative communication (AAC) devices. Individuals with severe communication impairments who can no longer control their daily conversations or participate in previous life roles want AAC devices. And they want them to work – to be reliable, effective, and fast. ML and NLP are emerging as promising tools to bridge current technology and next generation devices for individuals with the most severe speech and physical impairments, like the RSVP Keyboard™, a brain-computer interface (BCI) being developed by the parent grant. BCI systems for communication are referred to as AAC-BCIs. NLP efforts to combine large public data sets with private data sets, such as personal email messages, promise to give individuals with communication impairments their own personalized language models, models that are sufficiently robust to get closer to real-time communication. The focus on getting AAC-BCIs to work with machine learning, however, has led to a critical oversight in the field: an inadequate understanding of why individuals want next-generation devices and what trade-offs they are willing to make for faster and more personalized communication. The turn to ML brings this oversight into sharp relief. Individuals should provide input about the data sets used to construct their personal language models, but this raises important ethical questions about what individuals value, how they understand their identity, and what trade-offs they are willing to make relative to their personalized communication data. The goal of this supplement is to fill this gap in understanding so that researchers can implement ML into next generation AAC-BCI systems in a way that is sensitive to the ethical concerns of future users. There are four components to this ethics supplement: (1) to design a toolbox of ethics vignettes tailored to ethical concerns raised by both BCI communication and ML; (2) to administer monthly vignette-based online ethics surveys to individuals with severe communication impairments due to motor neuron disease (e.g., ALS) (n=25) or movement disorders (e.g., Parkinson's disease) (n=25); (3) to conduct semi-structured vignette-based interviews with individuals with pre- clinical or mild communication impairment due to motor neuron disease (n=10) or movement disorder (n=10). Components (2) and (3) will employ an iterative, parallel mixed-method approach. Trends in Likert-style online responses to ethics vignettes in the severe communication impairment cohort will be used to inform and modify the semi-structured interview prompts asked of the pre-clinical or mild impairment cohort. In parallel, themes emerging from direct content analysis of interviews will be used to refine online survey questions. Results of this iterative, mix-methods approach will be used (4) to outline a framework of core ethical domains and preliminary tools (vignettes and discussion prompts) that AAC-BCI researchers can use to assess ethical concerns while developing and iteratively refining communication technology for personalized language models. Project Narrative The populations of US citizens with severe speech and physical impairments secondary to neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies. Bioethical issues about privacy, agency and identity must be included in technology development and implementation as the parent grant implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Ethical Considerations for Language Modeling within Brain-Computer Interfaces,9929337,R01DC009834,"['Address', 'Administrative Supplement', 'Affect', 'Attention', 'Attitude', 'Augmentative and Alternative Communication', 'Award', 'Bioethical Issues', 'Bioethics', 'Clinical', 'Code', 'Cognitive', 'Communication', 'Communication impairment', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disease', 'Electroencephalography', 'Electronic Mail', 'Encapsulated', 'Engineering', 'Ensure', 'Ethical Analysis', 'Ethical Issues', 'Ethics', 'Foundations', 'Future', 'Goals', 'Home environment', 'Impairment', 'Individual', 'Informed Consent', 'Interview', 'Language', 'Letters', 'Life', 'Link', 'Literature', 'Locked-In Syndrome', 'Machine Learning', 'Medical', 'Medical Technology', 'Methods', 'Modeling', 'Monkeys', 'Motor Neuron Disease', 'Movement', 'Movement Disorders', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Neuromuscular Diseases', 'Oregon', 'Outcome', 'Parents', 'Parkinson Disease', 'Participant', 'Patient advocacy', 'Patients', 'Population', 'Privacy', 'Privatization', 'Public Health', 'Reporting', 'Research Personnel', 'Review Literature', 'Role', 'Secondary to', 'Self-Help Devices', 'Source', 'Speech', 'Structure', 'Surveys', 'System', 'Techniques', 'Technology', 'Time', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'User-Computer Interface', 'Voice', 'Work', 'advocacy organizations', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'cohort', 'communication device', 'computer science', 'design', 'expectation', 'informant', 'neurophysiology', 'next generation', 'novel', 'parent grant', 'pre-clinical', 'recruit', 'research and development', 'response', 'signal processing', 'skills', 'spelling', 'technology development', 'technology validation', 'tool', 'trend', 'uptake']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,153834,-0.012744845416361466
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,9927093,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Comorbidity', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2019,253545,0.012995932628316925
"User-driven Retrospectively Supervised Classification Updating (RESCU) systemfor robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Our milestones for Phase I are as follows:  Milestone 1.1: Extreme Learning Machine with Adaptively Sparse Representation (EASRC) algorithm  successfully implemented and verified  Milestone 1.2: Implementation of Nessa adaptive learning algorithm and smartwatch interface  Milestone 1.3: User Needs and Design Inputs locked as a result of Focus Group testing  Milestone 1.4: Hold a pre-submission meeting with FDA for feedback on device classification and  planned product performance testing  Milestone 1.5: Hold a Scientific Steering Group (SSG) meeting  Milestone 1.6: Convene a Study Monitoring Committee (SMC) and hold an initial meeting to review  clinical plans.  Milestone 1.7: Develop Clinical Study Protocol  Milestone 1.8: Register the study on www.clinicaltrials.gov. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) systemfor robust upper limb prosthesis control,9779227,U44NS108894,"['Activities of Daily Living', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Classification', 'Clinical', 'Clinical Research', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Feedback', 'Focus Groups', 'Freedom', 'Group Meetings', 'Hand', 'Individual', 'Intuition', 'Joints', 'Machine Learning', 'Methods', 'Monitor', 'Ownership', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Protocols documentation', 'Research', 'Signal Transduction', 'Supervision', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Work', 'adaptive learning', 'base', 'clinical translation', 'design', 'empowerment', 'improved', 'learning algorithm', 'meetings', 'myoelectric control', 'novel', 'operation', 'performance tests', 'prosthesis control', 'signal processing', 'smart watch']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2019,79250,0.04663692567799991
"Human and Machine Learning for Customized Control of Assistive Robots PROJECT SUMMARY This application will result in a technological platform that re-empowers persons with severe paralysis, by allowing them to independently control a wide spectrum of robotic actions. Severe paralysis is devastating, and chronic—and reliance on caregivers is persistent. Assistive machines such as wheelchairs and robotic arms offer a groundbreaking path to independence: where control over their environment and interactions is returned to the person.  However, to operate complex machines like robotic arms and hands typically poses a difﬁcult learning challenge and requires complex control signals—and the commercial control interfaces accessible to persons with severe paralysis (e.g. sip-and-puff, switch-based head arrays) are not adequate. As a result, assistive robotic arms remain largely inaccessible to those with severe paralysis—arguably the population who would beneﬁt from them most.  The purpose of the proposed study is to provide people with tetraplegia with the means to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. Control interfaces that generate a unique map from a user's body motions to control signals for a machine offer a customized interaction, however these interfaces have only been used to issue low-dimensional (2-D) control signals whereas more complex machines require higher-dimensional (e.g. 6-D) signals. We propose an approach that leverages robotics autonomy and machine learning in order to aid the end-user in learning how to issue effective higher-dimensional control signals through body motions. Speciﬁcally, initially the human issues a lower- dimensional control signal and robotics autonomy is used to bridge the gap by taking over whatever is not covered by the human's control signal. Help from the robotics autonomy is then progressively scaled back, automatically, to cover fewer and fewer control dimensions as the user becomes more skilled.  The ﬁrst piece to our approach deals with how to extract control signals from the human, using the body-machine interface. The development and optimization of decoding procedures for controlling a robotic arm using residual body motions will be addressed under Speciﬁc Aim 1. The second piece to our approach deals with how to interpret control signals from a human within a paradigm that shares control between the human and robotics autonomy. To identify which shared-control formulations most effectively utilize the human's control signals will be the topic of Speciﬁc Aim 2. The ﬁnal piece to our approach deals with how to adapt the shared-control paradigm so that more control is transferred to the human over time. This adaptation is necessary for the human's learning process, since the goal in the end is for the human to be able to fully control the robotic arm him/herself, and will be assessed under Speciﬁc Aim 3.  At the completion of this project, tetraplegic end-users will be able to operate a robotic arm using their residual body motions, through an interface that both promotes the use of residual body motions (and thus also recovery of motor skill) and adapts with the human as their abilities change over time. By leveraging adaptive robotics autonomy, our application moreover provides a safe mechanism to facilitate learning how to operate the robotic platform. Frequently the more severe a person's motor impairment, the less able they are to operate the very assistive machines, like powered wheelchairs and robotic arms, which might enhance their quality of life. The purpose of the proposed study is to provide people with tetraplegia with the means, through non-invasive technologies, to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. We propose and study an approach that leverages robot machine learning and autonomy in order to facilitate human motor learning of how to operate a robotic arm using a customized interface, in order to make powered assisted manipulation more accessible to people with severe paralysis. 1",Human and Machine Learning for Customized Control of Assistive Robots,9773039,R01EB024058,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Affect', 'Algorithms', 'Back', 'Caregivers', 'Cerebral Palsy', 'Chronic', 'Complex', 'Computers', 'Custom', 'Data', 'Development', 'Devices', 'Dimensions', 'Eating', 'Effectiveness', 'Environment', 'Exercise', 'Formulation', 'Fostering', 'Freedom', 'Future', 'Goals', 'Hand', 'Head', 'Health Benefit', 'Human', 'Learning', 'Left', 'Limb structure', 'Machine Learning', 'Maintenance', 'Maps', 'Mental Depression', 'Mental Health', 'Motion', 'Motivation', 'Motor', 'Motor Skills', 'Movement', 'Multiple Sclerosis', 'Neurologic', 'Paralysed', 'Patients', 'Performance', 'Persons', 'Population', 'Posture', 'Powered wheelchair', 'Procedures', 'Process', 'Quadriplegia', 'Quality of life', 'Rehabilitation therapy', 'Residual state', 'Robot', 'Robotics', 'Self-Help Devices', 'Shoulder', 'Side', 'Signal Transduction', 'Specific qualifier value', 'Spinal cord injured survivor', 'Stroke', 'System', 'Technology', 'Testing', 'Time', 'Wheelchairs', 'Work', 'arm', 'assistive robot', 'base', 'body-machine interface', 'brain machine interface', 'empowerment', 'feeding', 'high dimensionality', 'improved', 'machine learning algorithm', 'motor impairment', 'motor learning', 'motor recovery', 'n-dimensional', 'novel strategies', 'physical conditioning', 'psychologic', 'robot control', 'sensor', 'two-dimensional']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2019,329494,0.0023701158856468778
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,10013405,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'learning algorithm', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'smart watch', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2019,735600,0.04997349847045738
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user's location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user's location by recognizing standard informational signs present in the environment, tracking the user's trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9934891,R01EY029033,"['Adoption', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Environment', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Medical center', 'Process', 'Research', 'Schools', 'System', 'Tactile', 'Time', 'Travel', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'interest', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,105337,0.05292314542209783
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9663319,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416374,0.05424467566540907
"Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time Project Summary Lower limb assistive robotic devices, such as active prosthesis, orthoses, and exoskeletons have the potential to restore function for the millions of Americans who experience mobility challenges due to injury and disability. Since individuals with mobility challenges have an increased energetic cost of transport, the benefit of such assistive devices is commonly assessed via the reduction in the metabolic work rate of the individual who is using the device. Currently, metabolic work rate can only be obtained in a laboratory environment, using breath-by-breath measurements of respiratory gas analysis. To obtain a single steady state data point of metabolic work rate, multiple minutes of data must be collected, since the signals are noisy, sparsely sampled, and dynamically delayed. In addition, the user has to wear a mask and bulky equipment, further restricting the applicability of the method on a larger scale. We propose an improved way to obtain such estimates of metabolic work rate in real-time. Aim 1 will determine salient signal features and characterize the dynamics of sensing metabolic work rate from a variety of physiological sensor signals. Aim 2 will use advanced sensor fusion and machine learning techniques to accurately predict instantaneous energy cost in real-time from multiple physiological signals without relying on a metabolic mask. Aim 3 will use the obtained real-time estimates to optimize push-off timing for an active robotic prosthesis. The resulting methods will enable an automated and continuous evaluation of assistive robotic devices that can be realized outside the laboratory and with simple wearable sensors. This automated evaluation will enable devices, such as active prostheses, orthoses, or exoskeletons, that can self-monitor their performance, optimize their own behavior, and continuously adapt to changing circumstances. This will open up a radically new way of human-robot- interaction for assistive devices. It will greatly increase their clinical viability and enable novel advanced controllers and algorithms that can improve device performance on a subject specific basis. Project Narrative A common way of evaluating assistive robotic devices, such as active prostheses or exoskeletons, is by measuring the reduction in effort that they bring to an individual walking in them. The proposed project will develop ways to perform this evaluation automatically and in real-time by the device itself, which will be used in the future to develop prostheses and exoskeletons that automatically adapt themselves to their users. This project supports the NIH's stated mission of reducing disability by improving patient outcomes with new prosthetic and orthotic devices.",Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time,9668174,R03HD092639,"['Algorithms', 'American', 'Amputation', 'Amputees', 'Ankle', 'Behavior', 'Clinical', 'Data', 'Devices', 'Electromyography', 'Energy Metabolism', 'Environment', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Heart Rate', 'Indirect Calorimetry', 'Individual', 'Injury', 'Laboratories', 'Linear Regressions', 'Lower Extremity', 'Machine Learning', 'Masks', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Noise', 'Orthotic Devices', 'Patient-Focused Outcomes', 'Performance', 'Persons', 'Physiological', 'Population', 'Prosthesis', 'Reference Values', 'Robotics', 'Sampling', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'cost', 'disability', 'exoskeleton', 'experience', 'functional restoration', 'human-robot interaction', 'improved', 'light weight', 'neural network', 'novel', 'respiratory gas', 'robotic device', 'sensor', 'wearable device']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2019,78000,0.028241427923541486
"Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection PROJECT SUMMARY/ABSTRACT  Companies are increasingly marketing mobile technologies as FDA-cleared medical devices, yet we do not know the consequences of these devices on healthcare utilization, cost, and outcomes. Recently, Apple released the Apple Watch Series 4 as an FDA-cleared medical device. The device includes an alert for the presence of atrial fibrillation (AF) and allows anyone to monitor their heart rhythm for the presence of AF. Apple has an enormous global audience, and the number of people who will use this (and other similar devices) to self-diagnose or monitor AF will be substantial. On one hand, the device may allow new diagnoses that result in treatment, improved quality of life, fewer AF related complications. On the other hand, the device may result in false positives in otherwise healthy people, resulting in more testing and treatments with associated harms. In fact, the U.S. Preventative Task Force recommends against routine surveillance for AF in the general population, citing lack of evidence and possible harm. We have an urgent need for a population-based infrastructure to ensure that technologies entering the market as medical devices are beneficial and safe.  The overall goal of this project is to measure the uptake and effect of the Apple Watch 4 release on healthcare utilization among first-time and known AF patients. Dr. Shah is an early stage investigator with a K08 Career Development Award from the NHLBI. As part of the K08, she has developed a detailed cohort of contemporary AF patients, including clinical notes. Along with a team, she will use real world data, as proposed by the FDA, to generate evidence about risks and benefits of consumer-driven AF detection. She will use natural language processing to leverage the notes and identify AF patients who seek care due to the medical device, and evaluate downstream healthcare utilization, such as additional clinic visits, cardioversions, additional remote monitoring, and cost. The goals of this project will be accomplished through the following Specific Aims: 1) Estimate the proportion of first-time AF patient visits attributable to a mobile device before and after FDA clearance of the Apple Watch 4, and characterize device accuracy and downstream healthcare utilization in this population; and 2) Evaluate healthcare utilization patterns among prevalent AF patients who use mobile devices with AF alerts.  In 2017, Apple sold 17.7 million smart watches, in a device market that continues to grow. Extrapolating from prior annual sales and conservatively assuming a 5% increase in users each year, almost 60 million people will have an Apple Watch by the end of 2020 (not accounting for non-Apple devices with similar functionality). Thus, even in this short period of time, uptake will be substantial and warrant immediate feedback. The results of this project will provide preliminary data for a long-term, multicenter study that evaluates the benefits (improved quality of life, fewer strokes) and harms (increased treatment complications, increased cost) of consumer-driven AF detection. PROJECT NARRATIVE The consequence of mobile technologies marketed as medical devices are unknown, including devices that provide alerts for the presence of atrial fibrillation. The goal of this project is to evaluate the benefits and harm associated with consumer-driven atrial fibrillation detection.",Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection,9809717,R03HL148372,"['Adult', 'Advisory Committees', 'Affect', 'Apple', 'Apple watch', 'Arrhythmia', 'Atrial Fibrillation', 'Benefits and Risks', 'Cardiovascular system', 'Caring', 'Case-Control Studies', 'Clinic Visits', 'Clinical', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Electric Countershock', 'Ensure', 'Feedback', 'General Population', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hemorrhage', 'Holter Electrocardiography', 'Infrastructure', 'Interruption', 'Intervention', 'K-Series Research Career Programs', 'Lead', 'Marketing', 'Measures', 'Medical Device', 'Monitor', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Preventive', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sales', 'Series', 'Sinus', 'Stroke', 'Technology', 'Testing', 'Text', 'Time', 'United States Food and Drug Administration', 'Universities', 'Utah', 'Visit', 'base', 'care seeking', 'cohort', 'cost', 'cost outcomes', 'cryptogenic stroke', 'design', 'follow-up', 'handheld mobile device', 'health care service utilization', 'heart rhythm', 'improved', 'mobile computing', 'population based', 'routine screening', 'self diagnosis', 'smart watch', 'uptake']",NHLBI,UNIVERSITY OF UTAH,R03,2019,76250,0.0244805091209884
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9750520,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2019,507856,0.07110318271934964
"Clinic Interactions of a Brain-Computer Interface for Communication ﻿    DESCRIPTION (provided by applicant): The promise of brain-computer interfaces (BCI) for communication is becoming a reality for individuals with severe speech and physical impairments (SSPI) who cannot rely on speech or writing to express themselves. While the majority of research efforts are devoted to technology development to address problems of stability, reliability and/or classification, clinical and behavioral challenges are becoming more apparent as individuals with SSPI and their family/care teams assess the systems during novice or long-term trials. The objective of the RSVP Keyboard(tm) BCI translational research team is to address the clinical challenges raised during functional BCI use with innovative engineering design, thereby enhancing the potential of this novel assistive technology. Four specific aims are proposed: (1) to develop a BCI Communication Application Suite (BCI-CAS) that offers a set of language modules to people with SSPI that can meet their language/literacy skills; (2) to develop improved statistical signal models for personalized feature extraction, artifact/interference handling, and robust, accurate intent evidence extraction from physiologic signals; (3) to develop improved language models and stimulus sequence optimization methods; and (4) to evaluate cognitive variables that affect learning and performance of the BCI-CAS. Five language modules are proposed that rely on a multimodal evidence fusion framework for model-based context-aware optimal intent inference: RSVP Keyboard(tm) generative spelling; RSVP texting; RSVP in-context typing; RSVP in-context icon typing; and binary yes/no responses with SSVEPs. Usability data on the current RSVP Keyboard(tm) and SSVEP system drive all proposed aims. Users select a language module, and the BCI system optimizes performance for each individual based on user adaptation, intent inference, and personalized language modeling. A unique simulation function drives individualization of system parameters. The robustness of the BCI customization efforts are evaluated continually by adults with SSPI and neurotypical controls in an iterative fashion. The effect of three intervention programs that address the cognitive construct of attention (process-specific attention training, mindfulness meditation training and novel stimulus presentations) will be implemented through hypothesis-driven single subject designs. Thirty participants, ages 21 years and older with SSPI will be included in home-based interventions. By measuring information transfer rate (ITR), user satisfaction, and intrinsic user factors, we will identify learning strategies that influence BCI sill acquisition and performance for adults with neurodegenerative or neurodevelopmental conditions. The translational teams include (1) signal processing (Erdogmus); (2) clinical neurophysiology (Oken); (3) natural language processing (Bedrick/Gorman); and (4) assistive technology (Fried-Oken). We continue to rely on a solid Bayesian foundation and theoretical frameworks: ICF disability classification (WHO, 2001), the AAC model of participation (Beukelman & Mirenda, 2013) and the Matching Person to Technology Model (Scherer, 2002). PUBLIC HEALTH RELEVANCE: The populations of patients with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means to interface with communication systems. The BCI Communication Applications Suite is a hybrid brain-computer interface that is an innovative technological advance so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Clinic Interactions of a Brain-Computer Interface for Communication,9650545,R01DC009834,"['21 year old', 'Address', 'Adult', 'Advocate', 'Affect', 'Analysis of Variance', 'Attention', 'Awareness', 'Behavior', 'Behavioral', 'Caring', 'Classification', 'Clinic', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Complex', 'Custom', 'Data', 'Decision Making', 'Dependence', 'Electroencephalography', 'Engineering', 'Environment', 'Event-Related Potentials', 'Family', 'Foundations', 'Heterogeneity', 'Home environment', 'Hybrids', 'Impairment', 'Individual', 'Informed Consent', 'Infrastructure', 'Intercept', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Letters', 'Life', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Movement', 'Natural Language Processing', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Participant', 'Partner Communications', 'Patients', 'Performance', 'Persons', 'Phase', 'Physiological', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Public Health', 'Research', 'Role', 'Running', 'Secondary to', 'Self-Help Devices', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'Stress', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Text Messaging', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'User-Computer Interface', 'Validation', 'Visual', 'Visual evoked cortical potential', 'Vocabulary', 'Writing', 'acronyms', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinically relevant', 'cognitive system', 'computer science', 'cost', 'design', 'disability', 'engineering design', 'experimental study', 'human-in-the-loop', 'improved', 'innovation', 'intervention program', 'learning strategy', 'literacy', 'mindfulness meditation', 'multimodality', 'neurophysiology', 'novel', 'patient population', 'preference', 'public health relevance', 'recruit', 'residence', 'response', 'satisfaction', 'signal processing', 'simulation', 'skills', 'spelling', 'statistics', 'syntax', 'technology development', 'time interval', 'usability', 'vigilance']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,655251,-0.017400114778535956
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9653180,R01EY025332,"['3-Dimensional', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416574,-0.008039153885529013
"Adaptive & Individualized AAC The heterogeneity of the more than 1.3% of Americans who suffer from severe physical impairments (SPIs) preclude the use of common augmentative or alternative communication (AAC) solutions such as manual signs, gestures or dexterous interaction with a touchscreen for communication. While efforts to develop alternative access methods through eye or head tracking have provided some communication advancements for these individuals, all current technologies suffer from the same fundamental limitation: existing AAC devices require patients to conform to generic communication access methods and interfaces rather than the device conforming to the user. Consequently, AAC users are forced to settle for interventions that require excessive training and cognitive workload only to deliver extremely slow information transfer rates (ITRs) and recurrent communication errors that ultimately deprive them of the fundamental human right of communication. To meet this health need, we propose the first smart-AAC system designed using individually adaptive access methods and AAC interfaces to accommodate the unique manifestations of motor impairments specific to each user. Preliminary research by our team of speech researchers at Madonna Rehabilitation Hospital (Communication Center Lab) and Boston University (STEPP Lab), utilizing wearable sensors developed by our group (Altec, Inc) have already demonstrated that metrics based on surface electromyographic (sEMG) and accelerometer measures of muscle activity and movement for head-mediated control can be combined with optimizable AAC interfaces to improve ITRs when compared with traditional unoptimized AAC devices. Leveraging this pilot work, our team is now proposing a Phase I project to demonstrate the proof-of-concept that a single sEMG/IMU hybrid sensor worn on the forehead can provide improvements in ITR and communication accuracy when integrated with an AAC interface that is optimized through machine learning algorithms. The prototype system will be tested and compared to a conventional (non-adaptable) interface in subjects with SPI at a collaborative clinical site. Assistance by our speech and expert-AAC collaborators will ensure that all phases of technology development are patient-centric and usable in the context of clinical care. In Phase II we will build upon this proof-of-concept to design a smart-AAC system with automated optimization software that achieves dynamic learning which adapts to intra-individual changes in function through disease progression or training as well as inter-individual differences in motor impairments for a diverse set of users with spinal cord injury, traumatic brain injury, cerebral palsy, ALS, and other SPIs. The innovation is the first and only AAC technology that combines advancements in wearable-sensor access with interfaces that are autonomously optimized to the user, thereby reducing the resources and training needed to achieve effective person-centric communication in SPI, through improved HMI performance and reduced workload. This project addresses the fundamental mission of NIDCD (National Institute for Deafness and Communication Disorders) to provide a direct means of assisting communication for people with severe physical impairments caused by stroke, high level spinal cord injury, neural degeneration, or neuromuscular disease. Leveraging wearable access technology (which has barely been explored for AAC users), we will develop a first-of-its-kind adaptive tablet interface tailored to individual users through advanced movement classification algorithms. Through these efforts, we aim to provide an improved Human Machine Interface (HMI) that is able to accommodate varying degrees of inter- and intra-subject residual motor function and context dependent impairments to provide individuals with SPI the opportunity for improved societal integration and quality of life.",Adaptive & Individualized AAC,9907832,R43DC018437,"['Accelerometer', 'Address', 'American', 'Boston', 'Cerebral Palsy', 'Child', 'Cognitive', 'Communication', 'Communication Methods', 'Communication impairment', 'Computer software', 'Custom', 'Development', 'Devices', 'Diagnosis', 'Disease Progression', 'Ensure', 'Eye', 'Facial Muscles', 'Fatigue', 'Forehead', 'Gestures', 'Goals', 'Head', 'Head Movements', 'Health', 'Heterogeneity', 'Hospitals', 'Human Rights', 'Hybrids', 'Image', 'Imaging problem', 'Impairment', 'Individual', 'Individual Differences', 'Institutes', 'Intervention', 'Intuition', 'Learning', 'Linguistics', 'Manuals', 'Measures', 'Mediating', 'Methods', 'Mission', 'Motor', 'Motor Manifestations', 'Movement', 'Muscle', 'National Institute on Deafness and Other Communication Disorders', 'Nerve Degeneration', 'Neuromuscular Diseases', 'Patients', 'Pattern', 'Performance', 'Persons', 'Phase', 'Population Heterogeneity', 'Quality of life', 'Recurrence', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Resources', 'Series', 'Signal Transduction', 'Speech', 'Spinal cord injury', 'Stroke', 'Surface', 'System', 'Tablets', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Traumatic Brain Injury', 'United States National Aeronautics and Space Administration', 'Universities', 'User-Computer Interface', 'Variant', 'Work', 'Workload', 'alternative communication', 'base', 'classification algorithm', 'clinical care', 'clinical research site', 'communication device', 'deafness', 'design', 'experimental study', 'improved', 'innovation', 'kinematics', 'machine learning algorithm', 'mathematical model', 'motor impairment', 'novel', 'prototype', 'rehabilitation engineering', 'rehabilitation science', 'sensor', 'sensor technology', 'signal processing', 'technology development', 'touchscreen', 'two-dimensional', 'wearable device']",NIDCD,"ALTEC, INC.",R43,2019,224701,0.004973003920952092
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community—the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit’s gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing. Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9570624,R44AG053890,"['3-Dimensional', 'Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Custom', 'Dependence', 'Devices', 'Electrical Engineering', 'Emotional', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Impairment', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Vision', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'usability']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2019,814735,0.03670470225047894
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,9818350,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2019,641060,-0.020083325032949167
"Independent Exoskeleton-Use through Robust Stand-to-Sit Safety Project Summary/Abstract Innovative Design Labs (IDL) proposes to create a system for the sensing and control of stand-to-sit motions of a wearable bionics suit. Currently 5.6 million people in the US have impaired mobility from a number of different causes. The primary means of mobility for many of these patients is the wheelchair as it has been for most of the last 50 years. Despite all the benefits introduced by widespread use of the wheelchair, it remains a less than ideal mobility solution. Exoskeleton suits have the potential to empower individuals with impaired mobility with an alternative to wheelchairs that allows them to stand up and walk independently within their home and community has the potential to more fully reintegrate these individuals into society while also further improving their health and quality of life. For exoskeletons to gain acceptance in every-day independent home and community use, many control and safety related functionalities still need to be addressed. Our proposal seeks to address one of the gaps in allowing for independent use of exoskeletons in the home and community, namely, functionality to transition from standing to sitting in a safe manner. The proposed work will provide exoskeleton users with the new ability to independently sit down without assistance and confidence in being able to do so without falling and risking possible injuries. It aims to significantly change the way exoskeletons work thereby facilitating their adoption into the market and directly impacting the lives of individuals with disabilities. Project Narrative Exoskeletons can provide patients with SCI, stroke, and other types of impaired mobility access to extended duration, gravity dependent ambulation that can directly combat the risks associated with physical deconditioning. There are benefits for exoskeleton-use to gain widespread acceptance in every-day, independent home and community settings. Currently, exoskeletons are not approved for independent-use because functionalities like transferring from standing-to-sitting requires continuous assistance from caregivers.",Independent Exoskeleton-Use through Robust Stand-to-Sit Safety,9560683,R44AG057267,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area', 'Bionics', 'Caliber', 'Caregivers', 'Chicago', 'Collaborations', 'Communities', 'Computer Vision Systems', 'Country', 'Custom', 'Development', 'Disabled Persons', 'Emerging Technologies', 'Engineering', 'Environment', 'Evaluation', 'Fall injury', 'Feedback', 'Force of Gravity', 'Gait', 'Health', 'Height', 'Home environment', 'Hospitals', 'Imaging technology', 'Impairment', 'Individual', 'Injury', 'Institutes', 'Letters', 'Mechanics', 'Metric System', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Quality of life', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Rest', 'Risk', 'Robotics', 'Safety', 'Scientist', 'Small Business Innovation Research Grant', 'Societies', 'Spinal cord injury patients', 'Stroke', 'Surface', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Validation', 'Walking', 'Wheelchairs', 'Work', 'blind', 'combat', 'community setting', 'critical period', 'deconditioning', 'design', 'exoskeleton', 'experience', 'fall risk', 'falls', 'human study', 'improved', 'innovation', 'member', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'stroke patient']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2019,806285,0.003952135167308302
"Intuitive, complete neural control of tablet computers for communication Project Summary Conventional augmentative and alternative communication (AAC) devices for people with severe speech and motor impairments (SSMI) rely on residual motor function, inherently limiting communication throughput. Commercially available AAC solutions require daily caregiver setup, need frequent recalibration often from a technically savvy caregiver, are often unable to be used in dark lighting conditions, and can encumber or fatigue important remaining physical abilities. Furthermore, for people with progressive motor dysfunction due to amyotrophic lateral sclerosis (ALS), even the most well-designed AAC devices will eventually fail as movements become unreliable. For people with brainstem stroke, ALS, and other disorders causing locked-in syndrome (LIS) or SSMI, brain-computer interfaces (BCIs) hold promise as a method of enabling communication that does not rely upon speech or voluntary movement. In prior NIDCD-supported research, our BrainGate research team provided early proofs of principle of a powerful intracortical brain-computer interface (iBCI) that decodes movement intentions directly from brain activity. This technology has allowed people to control a cursor on a computer screen for communication simply by imagining movements of their own arm. The proposed NIDCD U01 clinical research will further the development and testing of a fully implanted iBCI that could provide robust, intuitive control of industry-grade communication apps for people with LIS or SSMI. By leveraging the ongoing pilot clinical trials of the investigational BrainGate system, we aim to (1) improve the robustness and accuracy of neurally actuated point-and click, in part through the translation of neuronal activity from human premotor and motor cortex, (2) expand the number of input dimensions to tablet computers available via neural activity, allowing intended hand gesture commands to control communication apps on touch-screen tablet computers, and (3) rigorously compare the performance of the investigational BrainGate system to trial participants’ conventional AAC systems with respect to communication competence, information throughput, user preference and outcomes measures. By incorporating the feedback of six individual participants with paralysis, this feasibility trial will optimize a powerful iBCI for communication and will establish the metrics needed for a subsequent pivotal trial of a fully implanted, always-available iBCI communication system for people with SSMI. Project Narrative People with brainstem stroke, advanced amyotrophic lateral sclerosis (ALS, also known as Lou Gehrig’s disease), or other disorders can become unable to move or speak despite being awake and alert. In this project, we seek to further translate knowledge about interpreting brain signals related to movement, and to further develop an intracortical brain-computer interface (iBCI) that could restore rapid and intuitive use of communication apps on tablet computers by people with paralysis.","Intuitive, complete neural control of tablet computers for communication",9729335,U01DC017844,"['Amyotrophic Lateral Sclerosis', 'Augmentative and Alternative Communication', 'BRAIN initiative', 'Brain', 'Brain Stem Infarctions', 'Caregivers', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communication', 'Competence', 'Computer software', 'Computers', 'Data', 'Detection', 'Development', 'Devices', 'Dimensions', 'Disease', 'Family Caregiver', 'Fatigue', 'Feedback', 'Friends', 'Gestures', 'Hand', 'Home environment', 'Human', 'Human Activities', 'Implant', 'Individual', 'Industry', 'Injury', 'Intention', 'Interactive Communication', 'Intuition', 'Investigation', 'Knowledge', 'Language', 'Lighting', 'Location', 'Locked-In Syndrome', 'Methods', 'Modeling', 'Modernization', 'Motor', 'Motor Cortex', 'Movement', 'Mus', 'National Institute on Deafness and Other Communication Disorders', 'Neurons', 'Outcome Measure', 'Paralysed', 'Participant', 'Pattern', 'Performance', 'Research', 'Research Support', 'Residual state', 'Signal Transduction', 'Speech', 'Speed', 'Surveys', 'System', 'Tablet Computer', 'Tablets', 'Technology', 'Testing', 'Time', 'Touch sensation', 'Translating', 'Translations', 'United States National Institutes of Health', 'Wireless Technology', 'arm', 'awake', 'base', 'brain computer interface', 'communication device', 'deep learning', 'design', 'expectation', 'experience', 'feasibility trial', 'first-in-human', 'flexibility', 'improved', 'innovation', 'long short term memory network', 'motor disorder', 'motor impairment', 'neural implant', 'neuroregulation', 'nonhuman primate', 'novel', 'phrases', 'preference', 'relating to nervous system', 'tool', 'touchscreen', 'two-dimensional']",NIDCD,BROWN UNIVERSITY,U01,2019,837287,-0.01750912149873553
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9507909,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Grain', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'cognitive development', 'computerized', 'cost', 'deep learning', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'sensor technology', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2018,203125,0.00954571363088853
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,9622537,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2018,216724,0.04997349847045738
"Human and Machine Learning for Customized Control of Assistive Robots PROJECT SUMMARY This application will result in a technological platform that re-empowers persons with severe paralysis, by allowing them to independently control a wide spectrum of robotic actions. Severe paralysis is devastating, and chronic—and reliance on caregivers is persistent. Assistive machines such as wheelchairs and robotic arms offer a groundbreaking path to independence: where control over their environment and interactions is returned to the person.  However, to operate complex machines like robotic arms and hands typically poses a difﬁcult learning challenge and requires complex control signals—and the commercial control interfaces accessible to persons with severe paralysis (e.g. sip-and-puff, switch-based head arrays) are not adequate. As a result, assistive robotic arms remain largely inaccessible to those with severe paralysis—arguably the population who would beneﬁt from them most.  The purpose of the proposed study is to provide people with tetraplegia with the means to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. Control interfaces that generate a unique map from a user's body motions to control signals for a machine offer a customized interaction, however these interfaces have only been used to issue low-dimensional (2-D) control signals whereas more complex machines require higher-dimensional (e.g. 6-D) signals. We propose an approach that leverages robotics autonomy and machine learning in order to aid the end-user in learning how to issue effective higher-dimensional control signals through body motions. Speciﬁcally, initially the human issues a lower- dimensional control signal and robotics autonomy is used to bridge the gap by taking over whatever is not covered by the human's control signal. Help from the robotics autonomy is then progressively scaled back, automatically, to cover fewer and fewer control dimensions as the user becomes more skilled.  The ﬁrst piece to our approach deals with how to extract control signals from the human, using the body-machine interface. The development and optimization of decoding procedures for controlling a robotic arm using residual body motions will be addressed under Speciﬁc Aim 1. The second piece to our approach deals with how to interpret control signals from a human within a paradigm that shares control between the human and robotics autonomy. To identify which shared-control formulations most effectively utilize the human's control signals will be the topic of Speciﬁc Aim 2. The ﬁnal piece to our approach deals with how to adapt the shared-control paradigm so that more control is transferred to the human over time. This adaptation is necessary for the human's learning process, since the goal in the end is for the human to be able to fully control the robotic arm him/herself, and will be assessed under Speciﬁc Aim 3.  At the completion of this project, tetraplegic end-users will be able to operate a robotic arm using their residual body motions, through an interface that both promotes the use of residual body motions (and thus also recovery of motor skill) and adapts with the human as their abilities change over time. By leveraging adaptive robotics autonomy, our application moreover provides a safe mechanism to facilitate learning how to operate the robotic platform. Frequently the more severe a person's motor impairment, the less able they are to operate the very assistive machines, like powered wheelchairs and robotic arms, which might enhance their quality of life. The purpose of the proposed study is to provide people with tetraplegia with the means, through non-invasive technologies, to control robotic arms with their available body mobility, while concurrently promoting the exercise of available body motions and the maintenance of physical health. We propose and study an approach that leverages robot machine learning and autonomy in order to facilitate human motor learning of how to operate a robotic arm using a customized interface, in order to make powered assisted manipulation more accessible to people with severe paralysis. 1",Human and Machine Learning for Customized Control of Assistive Robots,9448794,R01EB024058,"['3-Dimensional', 'Activities of Daily Living', 'Address', 'Affect', 'Algorithms', 'Back', 'Caregivers', 'Cerebral Palsy', 'Chronic', 'Complex', 'Computers', 'Custom', 'Data', 'Development', 'Devices', 'Dimensions', 'Eating', 'Effectiveness', 'Environment', 'Exercise', 'Formulation', 'Fostering', 'Freedom', 'Future', 'Goals', 'Hand', 'Head', 'Health Benefit', 'Human', 'Learning', 'Left', 'Limb structure', 'Machine Learning', 'Maintenance', 'Maps', 'Mental Depression', 'Mental Health', 'Motion', 'Motivation', 'Motor', 'Motor Skills', 'Movement', 'Multiple Sclerosis', 'Neurologic', 'Paralysed', 'Patients', 'Performance', 'Persons', 'Population', 'Posture', 'Powered wheelchair', 'Procedures', 'Process', 'Quadriplegia', 'Quality of life', 'Rehabilitation therapy', 'Residual state', 'Robot', 'Robotics', 'Self-Help Devices', 'Shoulder', 'Side', 'Signal Transduction', 'Specific qualifier value', 'Spinal cord injured survivor', 'Stroke', 'System', 'Technology', 'Testing', 'Time', 'Wheelchairs', 'Work', 'arm', 'base', 'body-machine interface', 'brain machine interface', 'empowerment', 'feeding', 'high dimensionality', 'improved', 'motor impairment', 'motor learning', 'motor recovery', 'n-dimensional', 'novel strategies', 'physical conditioning', 'psychologic', 'robot control', 'sensor', 'two-dimensional']",NIBIB,REHABILITATION INSTITUTE OF CHICAGO D/B/A SHIRLEY RYAN ABILITYLAB,R01,2018,331851,0.0023701158856468778
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9499823,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Research Infrastructure', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radiofrequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416374,0.05424467566540907
"Wireless Movement Sensing System for People with Severe Disabilities Abstract The ability of people with severe physical impairment to participate in family life, communication, work, or recreation is severely restricted without access to assistive technology (AT). Yet, as disability severity increases, so does the challenge to finding (1) an access movement that a person can perform to control AT and (2) an access technology that can detect the access movement. We propose to create a wireless movement sensing system that can learn a user’s access movement and then recognize that movement in order to wirelessly control assistive devices. Once we complete the technology development, we will measure the sensitivity and specificity of our wireless movement sensing system using the movements of ten people with SPI. We will present our results to AT experts during a focus group session and determine the perceived strengths and weaknesses of the technology. This Phase 1 research is proposed by a multidisciplinary research team consisting of AT engineers, AT clinicians, and a machine learning expert. Project Narrative The proposed movement sensing system adapts to the abilities of people with severe physical impairment and enables them to better control smart devices (e.g., computers, smartphones, etc.).",Wireless Movement Sensing System for People with Severe Disabilities,9554167,R43DC017791,"['Address', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Android', 'Arkansas', 'Beds', 'Bluetooth', 'Brain Stem Infarctions', 'Cellular Phone', 'Cerebral Palsy', 'Communication', 'Computer software', 'Computers', 'Custom', 'Data', 'Devices', 'Digit structure', 'Engineering', 'Etiology', 'Exhibits', 'Family', 'Fingers', 'Focus Groups', 'Generations', 'Hand', 'Hospitals', 'Impairment', 'Interdisciplinary Study', 'Learning', 'Life', 'Machine Learning', 'Measures', 'Movement', 'Output', 'Participant', 'Performance', 'Persons', 'Phase', 'Positioning Attribute', 'Records', 'Recreation', 'Rehabilitation therapy', 'Research', 'Scanning', 'Self-Help Devices', 'Sensitivity and Specificity', 'Severities', 'Speech', 'Spinal cord injury', 'System', 'Tablets', 'Technology', 'Thumb structure', 'Training', 'Universities', 'Wheelchairs', 'Wireless Technology', 'Woman', 'Work', 'cost', 'disability', 'effectiveness measure', 'prototype', 'sensor', 'technology development', 'wearable device']",NIDCD,"INVOTEK, INC.",R43,2018,222217,0.006995464420634117
"Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time Project Summary Lower limb assistive robotic devices, such as active prosthesis, orthoses, and exoskeletons have the potential to restore function for the millions of Americans who experience mobility challenges due to injury and disability. Since individuals with mobility challenges have an increased energetic cost of transport, the benefit of such assistive devices is commonly assessed via the reduction in the metabolic work rate of the individual who is using the device. Currently, metabolic work rate can only be obtained in a laboratory environment, using breath-by-breath measurements of respiratory gas analysis. To obtain a single steady state data point of metabolic work rate, multiple minutes of data must be collected, since the signals are noisy, sparsely sampled, and dynamically delayed. In addition, the user has to wear a mask and bulky equipment, further restricting the applicability of the method on a larger scale. We propose an improved way to obtain such estimates of metabolic work rate in real-time. Aim 1 will determine salient signal features and characterize the dynamics of sensing metabolic work rate from a variety of physiological sensor signals. Aim 2 will use advanced sensor fusion and machine learning techniques to accurately predict instantaneous energy cost in real-time from multiple physiological signals without relying on a metabolic mask. Aim 3 will use the obtained real-time estimates to optimize push-off timing for an active robotic prosthesis. The resulting methods will enable an automated and continuous evaluation of assistive robotic devices that can be realized outside the laboratory and with simple wearable sensors. This automated evaluation will enable devices, such as active prostheses, orthoses, or exoskeletons, that can self-monitor their performance, optimize their own behavior, and continuously adapt to changing circumstances. This will open up a radically new way of human-robot- interaction for assistive devices. It will greatly increase their clinical viability and enable novel advanced controllers and algorithms that can improve device performance on a subject specific basis. Project Narrative A common way of evaluating assistive robotic devices, such as active prostheses or exoskeletons, is by measuring the reduction in effort that they bring to an individual walking in them. The proposed project will develop ways to perform this evaluation automatically and in real-time by the device itself, which will be used in the future to develop prostheses and exoskeletons that automatically adapt themselves to their users. This project supports the NIH's stated mission of reducing disability by improving patient outcomes with new prosthetic and orthotic devices.",Evaluating and Improving Assistive Robotic Devices Continuously and in Real-time,9529742,R03HD092639,"['Algorithms', 'American', 'Amputation', 'Amputees', 'Ankle', 'Behavior', 'Biological Neural Networks', 'Clinical', 'Data', 'Devices', 'Electromyography', 'Energy Metabolism', 'Environment', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Gray unit of radiation dose', 'Heart Rate', 'Indirect Calorimetry', 'Individual', 'Injury', 'Laboratories', 'Linear Regressions', 'Lower Extremity', 'Machine Learning', 'Masks', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Noise', 'Orthotic Devices', 'Patient-Focused Outcomes', 'Performance', 'Persons', 'Physiological', 'Population', 'Prosthesis', 'Reference Values', 'Robotics', 'Sampling', 'Self-Help Devices', 'Shapes', 'Signal Transduction', 'Techniques', 'Time', 'Training', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'cost', 'disability', 'exoskeleton', 'experience', 'functional restoration', 'human-robot interaction', 'improved', 'light weight', 'novel', 'respiratory gas', 'robotic device', 'sensor', 'wearable device']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R03,2018,77959,0.028241427923541486
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9711194,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2018,99914,0.008839711296439953
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9465330,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,503162,0.02232172638259315
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9698505,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,50000,0.02232172638259315
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9644103,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2018,519349,0.07110318271934964
"Clinic Interactions of a Brain-Computer Interface for Communication ﻿    DESCRIPTION (provided by applicant): The promise of brain-computer interfaces (BCI) for communication is becoming a reality for individuals with severe speech and physical impairments (SSPI) who cannot rely on speech or writing to express themselves. While the majority of research efforts are devoted to technology development to address problems of stability, reliability and/or classification, clinical and behavioral challenges are becoming more apparent as individuals with SSPI and their family/care teams assess the systems during novice or long-term trials. The objective of the RSVP Keyboard(tm) BCI translational research team is to address the clinical challenges raised during functional BCI use with innovative engineering design, thereby enhancing the potential of this novel assistive technology. Four specific aims are proposed: (1) to develop a BCI Communication Application Suite (BCI-CAS) that offers a set of language modules to people with SSPI that can meet their language/literacy skills; (2) to develop improved statistical signal models for personalized feature extraction, artifact/interference handling, and robust, accurate intent evidence extraction from physiologic signals; (3) to develop improved language models and stimulus sequence optimization methods; and (4) to evaluate cognitive variables that affect learning and performance of the BCI-CAS. Five language modules are proposed that rely on a multimodal evidence fusion framework for model-based context-aware optimal intent inference: RSVP Keyboard(tm) generative spelling; RSVP texting; RSVP in-context typing; RSVP in-context icon typing; and binary yes/no responses with SSVEPs. Usability data on the current RSVP Keyboard(tm) and SSVEP system drive all proposed aims. Users select a language module, and the BCI system optimizes performance for each individual based on user adaptation, intent inference, and personalized language modeling. A unique simulation function drives individualization of system parameters. The robustness of the BCI customization efforts are evaluated continually by adults with SSPI and neurotypical controls in an iterative fashion. The effect of three intervention programs that address the cognitive construct of attention (process-specific attention training, mindfulness meditation training and novel stimulus presentations) will be implemented through hypothesis-driven single subject designs. Thirty participants, ages 21 years and older with SSPI will be included in home-based interventions. By measuring information transfer rate (ITR), user satisfaction, and intrinsic user factors, we will identify learning strategies that influence BCI sill acquisition and performance for adults with neurodegenerative or neurodevelopmental conditions. The translational teams include (1) signal processing (Erdogmus); (2) clinical neurophysiology (Oken); (3) natural language processing (Bedrick/Gorman); and (4) assistive technology (Fried-Oken). We continue to rely on a solid Bayesian foundation and theoretical frameworks: ICF disability classification (WHO, 2001), the AAC model of participation (Beukelman & Mirenda, 2013) and the Matching Person to Technology Model (Scherer, 2002). PUBLIC HEALTH RELEVANCE: The populations of patients with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means to interface with communication systems. The BCI Communication Applications Suite is a hybrid brain-computer interface that is an innovative technological advance so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Clinic Interactions of a Brain-Computer Interface for Communication,9432500,R01DC009834,"['21 year old', 'Address', 'Adult', 'Advocate', 'Affect', 'Analysis of Variance', 'Attention', 'Awareness', 'Behavior', 'Behavioral', 'Caring', 'Classification', 'Clinic', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Complex', 'Custom', 'Data', 'Decision Making', 'Dependence', 'Electroencephalography', 'Engineering', 'Environment', 'Event-Related Potentials', 'Family', 'Foundations', 'Heterogeneity', 'Home environment', 'Hybrids', 'Impairment', 'Individual', 'Informed Consent', 'Intercept', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Letters', 'Life', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Movement', 'Natural Language Processing', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Participant', 'Partner Communications', 'Patients', 'Performance', 'Persons', 'Phase', 'Physiological', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Public Health', 'Research', 'Research Infrastructure', 'Role', 'Running', 'Secondary to', 'Self-Help Devices', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'Stress', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'User-Computer Interface', 'Validation', 'Visual', 'Visual evoked cortical potential', 'Vocabulary', 'Writing', 'acronyms', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinically relevant', 'cognitive system', 'computer science', 'cost', 'design', 'disability', 'engineering design', 'experimental study', 'human-in-the-loop', 'improved', 'innovation', 'intervention program', 'learning strategy', 'literacy', 'mindfulness meditation', 'multimodality', 'neurophysiology', 'novel', 'patient population', 'preference', 'public health relevance', 'recruit', 'residence', 'response', 'satisfaction', 'signal processing', 'simulation', 'skills', 'spelling', 'statistics', 'syntax', 'technology development', 'time interval', 'usability', 'vigilance']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2018,651980,-0.017400114778535956
"Real-time prediction of marijuana use & effects of use on cognition in the natural environment ABSTRACT Some young adult marijuana (MJ) users report adverse effects of MJ use on cognition that impact daily functioning, with negative consequences such as injury and fatality due to driving while under the influence of MJ. Research on the effects of MJ use on cognition, however, has produced mixed findings. MJ effects on cognition may depend on factors such as history and current severity of marijuana use, time since last MJ use (including possible MJ withdrawal effects), and gender. This R21 aims to address limitations of existing research by (1) starting to develop an algorithm to predict MJ use using smartphone data in regular/heavy MJ users based on “routine” or “habitual use”, and (2) examining effects of MJ use on cognition using smartphone- based cognitive testing in the natural environment. Development of an algorithm to predict MJ use would facilitate systematic assessment of MJ effects on cognitive functioning through more efficient scheduling of smartphone cognitive testing among regular/heavy MJ users in relation to daily routines. Cognitive testing by smartphone in the natural environment is an innovative method that has shown validity, and permits sampling of cognitive functioning within and across days in relation to MJ use. This project will enroll non-treatment seeking young adult (ages 18-25) MJ users from the community, representing “low”, “regular”, and “heavy” MJ use, with 50% female at each level of use. Participants will complete a baseline lab assessment, 30-day data collection using smartphone and wearable devices (e.g., wristband), and a debriefing interview. Piloting will optimize the protocol and methods for compliance. Smartphones will collect continuously sensed data (e.g., geolocation) for input to an algorithm to predict MJ use in regular/heavy MJ users. This R21 will identify which types of data, available through smartphone, provide optimal detection of routines in MJ use among regular/heavy users. Smartphone cognitive testing will be administered at various times during acute MJ intoxication and various naturalistically occurring lengths of MJ abstinence to examine effects of MJ use on selected aspects of cognitive functioning in daily life. Development of an algorithm to predict MJ use in regular/heavy MJ users based on smartphone data could, for example, facilitate real-time assessment of MJ effects on cognition through improved sampling of cognition in relation to acute and non-acute effects of MJ use. This R21 will provide the foundation for a research program that aims to examine MJ effects on cognitive functioning in vivo, and could support the development of just-in-time intervention to reduce MJ use. This R21 aligns with NIDA's strategic goal of determining consequences of drug use, and cross-cutting themes of highlighting real-world relevance of research and leveraging mobile health technologies to reduce drug use. Project Narrative This exploratory project will initiate development of an algorithm to predict marijuana use using data from smartphone and ecological momentary assessment, and will examine effects of marijuana use on cognitive functioning in the natural environment using innovative smartphone-based cognitive tests. Developing an algorithm to predict marijuana use has substantial healthcare applications, specifically for timely intervention to reduce marijuana use. Further, examining effects of marijuana use on cognitive functioning daily life has important implications for determining possible adverse health consequences associated with marijuana use.",Real-time prediction of marijuana use & effects of use on cognition in the natural environment,9456715,R21DA043181,"['Abstinence', 'Acute', 'Address', 'Adverse effects', 'Age', 'Age of Onset', 'Algorithms', 'Attention', 'Automobile Driving', 'Awareness', 'Behavior', 'Cellular Phone', 'Cognition', 'Communities', 'Data', 'Data Collection', 'Detection', 'Development', 'Drug usage', 'Ecological momentary assessment', 'Enrollment', 'Environment', 'Female', 'Foundations', 'Frequencies', 'Gender', 'Geography', 'Goals', 'Health', 'Health Technology', 'Healthcare', 'Heart Rate', 'Hour', 'Injury', 'Intake', 'Intervention', 'Interview', 'Intoxication', 'Length', 'Life', 'Literature', 'Location', 'Machine Learning', 'Marijuana', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Participant', 'Patient Self-Report', 'Performance', 'Positioning Attribute', 'Procedures', 'Protocols documentation', 'Recording of previous events', 'Relaxation', 'Reporting', 'Research', 'Sampling', 'Schedule', 'Severities', 'Short-Term Memory', 'System', 'Testing', 'Text', 'Time', 'Travel', 'Withdrawal', 'addiction', 'age group', 'base', 'cognitive function', 'cognitive task', 'cognitive testing', 'computer science', 'daily functioning', 'data mining', 'improved', 'in vivo', 'innovation', 'mHealth', 'male', 'marijuana use', 'marijuana use disorder', 'marijuana user', 'marijuana withdrawal', 'mobile computing', 'prediction algorithm', 'predictive modeling', 'programs', 'recruit', 'reduce marijuana use', 'wearable device', 'young adult']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2018,196166,-0.0115212618959891
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9438535,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416574,-0.008039153885529013
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9596236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Computer Simulation', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Gray unit of radiation dose', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Rehabilitation Research', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'orthotics', 'powered prosthesis', 'programs', 'prosthesis control', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF TEXAS DALLAS,R01,2018,474602,0.015783816327796248
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9474120,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2018,364257,0.012555630750356858
"Artificial Intelligence in a Mobile Intervention for Depression (AIM) DESCRIPTION (provided by applicant): The primary aim of this proposal is to develop and evaluate the use of state of the art machine learning approaches within a mobile intervention application for the treatment of major depressive disorder (MDD). Machine learning, a branch of artificial intelligence, focuses on the development of algorithms that automatically improve and evolve based on collected data. Machine learning models can learn to detect complex, latent patterns in data and apply such knowledge to decision making in real time. The proposed intervention, called IntelliCare, will use ongoing data collected from the patient and intervention application to continuously adapt intervention content, content form, and motivational messaging to create a highly tailored and user-responsive treatment system. Behavioral intervention technologies (BITs), including web-based and mobile interventions, have been developed and are increasingly being used to treat MDD. BITs are moderately effective in treating depression, particularly when guided by human coaching via email or telephone. However, lack of personalization and inability to adapt to patient needs or preferences, which results in a perceived lack of relevance, contributes to poorer adherence and outcomes. IntelliCare will be designed as a mobile application, but will be accessible via computer web browsers and tablets. The IntelliCare machine learning framework will use individual data obtained from use data (e.g., length of time using a treatment component), embedded sensors in the phone (e.g., GPS), and the user's self-reports (e.g., ""like"" and usefulness ratings of treatment components) to provide a highly tailored intervention that can learn from the patient and adapt intervention and motivational materials to the patient's preferences and state. Low intensity coaching will serve as a backstop to support adherence. This project will contain three phases.  Phase 1 will involve the development of IntelliCare and its optimization through usability testing.  Phase 2 will be a field trial of 200 users who will receive IntelliCare for 12 weeks. The field trial has two aims: first to complete usability testing and optimization of the treatment framework, and second to develop the machine learning models and algorithms. Phase 3 will subject IntelliCare to a double blind, randomized controlled trial, comparing it to MobilCare. MobilCare will be identical to IntelliCare except that it will use standard presentation and presentation, rather than machine learning, to provide treatment and motivational materials. We will recruit half the participants from primary care settings, as this is the de facto site for treatment of depression in the United States, and half through the Internet, which is the main portal to health apps. The application of adaptive machine learning analytics to a mobile intervention has the potential to create a new generation of BITs that could revolutionize the way that such interventions are conceptualized, designed, and deployed. These innovations would have broad consequences and could be extended a broader range of BITS, including web- based interventions, and to other interventions targeting a wide range of health and mental health problems. PUBLIC HEALTH RELEVANCE: This project will create and evaluate IntelliCare, a mobile intervention also accessible by web browser, for depression. IntelliCare will harness modern adaptive machine learning analytics that can learn from a user's activity on the application, embedded phone sensors, and patient report to tailor intervention elements and motivational messaging to the needs and preferences of the user. This unprecedented level of tailoring could revolutionize the way such interventions are conceptualized, designed, and deployed.",Artificial Intelligence in a Mobile Intervention for Depression (AIM),9314333,R01MH100482,"['Adherence', 'Aftercare', 'Algorithms', 'American', 'Android', 'Artificial Intelligence', 'Australia', 'Behavior Therapy', 'Car Phone', 'Cellular Phone', 'Complex', 'Computers', 'Data', 'Decision Making', 'Development', 'Devices', 'Double-Blind Method', 'Electronic Mail', 'Elements', 'Employee Assistance Program (Health Care)', 'Generations', 'Health', 'Healthcare Systems', 'Human', 'Individual', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mediating', 'Mental Depression', 'Mental Health', 'Modeling', 'Modernization', 'Morbidity - disease rate', 'Motivation', 'Online Systems', 'Outcome', 'Participant', 'Patient Care', 'Patient Preferences', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phase', 'Phonation', 'Population', 'Prevalence', 'Primary Health Care', 'Protocols documentation', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recruitment Activity', 'Reporting', 'Secure', 'Severities', 'Site', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Textiles', 'Time', 'United States', 'United States Department of Veterans Affairs', 'base', 'care systems', 'cost', 'depressed patient', 'depressive symptoms', 'design', 'experience', 'improved', 'individualized medicine', 'innovation', 'learning progression', 'mobile application', 'mortality', 'preference', 'primary care setting', 'psychologic', 'psychopharmacologic', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sensor', 'tailored messaging', 'time use', 'tool', 'treatment site', 'trial comparing', 'usability', 'web-accessible', 'web-enabled']",NIMH,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2017,593935,0.023542009785596715
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,9133939,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Awareness', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Conscious', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Foundations', 'GTP-Binding Protein alpha Subunits, Gs', 'Hand functions', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Subconscious', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'robot control', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2017,133916,0.06408292545038635
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9373088,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Cereals', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Darkness', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Technology', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'behavioral study', 'cognitive development', 'computerized', 'cost', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2017,243750,0.00954571363088853
"User-driven fitting of hearing aids and other assistive hearing devices Hearing aids are the principal tool today for ameliorating age-related hearing loss and its significant social, cognitive and functional costs to patients and society at large. However, many individuals who are prescribed hearing aids do not use them at all, or use them only occasionally. Most reasons behind the “hearing aid in the drawer” phenomenon relate to the characteristics of the sound produced, and could, in theory, be addressed with the correct signal processing strategy. The problem persists despite the increased complexity and power of new devices, for three reasons: (a) The hearing aid parameters, as set in the clinic, introduce distortion or render audible many sounds that the hearing impaired user had become accustomed to not hearing. The novelty is often so uncomfortable for the user as to discard the device. (b) The optimum parameters vary depending on the listening task and environment. Under some conditions, a device with parameters designed for a different condition will perform worse than no device at all. (c) The clinical fitting is derived from a non-ideal way to assess auditory function (the pure- tone audiogram). The optimum parameters for the actual impairment may be different from those of the prescribed fitting. Although it is true that the physiological mechanisms make it impossible to process sound so as to completely reverse the effect of sensorineural hearing loss, a device that delivers some benefit at all times is likely to be used all the time. The goal is to develop a hearing aid that can adaptively change its parameters to address the problems above, and will be accomplished with a novel fitting approach that rapidly presents a number of parameter settings to the user and lets the user guide the system toward the optimal settings for each listening situation. This requires the development of machine-learning algorithms to effectively search the parameter space and user interface devices and instructions that are easy for the patient to use. The focus of this Phase I proposal is the development of the algorithms and the adaptive user-driven fitting program, and to compare the proposed fitting with the traditional audiogram-based fitting across measures of functional hearing (ability to recognize speech in noise) and subjective preference. A hearing aid user is often dissatisfied with the sound quality of their device, despite its sophistication and adjustment by a trained audiologist. The problem can be mitigated by letting the user fine-tune the device for maximum comfort in everyday use. We will apply modern machine learning methods to develop a program for efficient user-driven fitting of hearing assistive devices.",User-driven fitting of hearing aids and other assistive hearing devices,9409910,R43DC016251,"['Address', 'Algorithms', 'Audiometry', 'Auditory', 'Back', 'Books', 'Cellular Phone', 'Characteristics', 'Clinic', 'Clinical', 'Cognitive', 'Complex', 'Computer software', 'Development', 'Devices', 'Environment', 'Future', 'Goals', 'Hearing', 'Hearing Aids', 'Human', 'Impairment', 'Individual', 'Instruction', 'Intuition', 'Knowledge', 'Likelihood Functions', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Music', 'Noise', 'Outcome', 'Patients', 'Performance', 'Phase', 'Physiological', 'Presbycusis', 'Process', 'Protocols documentation', 'Psychology', 'Psychophysics', 'Relaxation', 'Reproducibility', 'Self-Help Devices', 'Sensorineural Hearing Loss', 'Societies', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Time', 'Training', 'Update', 'base', 'cohort', 'cost', 'design', 'hearing impairment', 'improved', 'learning strategy', 'models and simulation', 'novel', 'performance tests', 'preference', 'programs', 'response', 'signal processing', 'simulation', 'social', 'sound', 'success', 'theories', 'tool', 'vector']",NIDCD,"CARAWAY SOFTWARE, INC.",R43,2017,224966,0.0019195094652860285
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9312420,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2017,200225,0.008839711296439953
"Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health ﻿    DESCRIPTION (provided by applicant): Longitudinal sensor data collected passively from mobile phones and other wearable sensors will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes. Computers will analyze individual-level data streams to permit unprecedented, individual-level precision in research and intervention. This type of precision medicine enables targeting of science and medicine to a particular individual's genetic makeup, past and current situation, and behavioral health exposures. Mobile phones, smartwatches, and common fitness devices are already capable of generating rich data on behavior, but developing algorithms to interpret that raw data using the latest machine learning algorithms requires practical strategies to annotate large datasets. We propose to develop and test the feasibility and usability of a mobile and online crowdsource-based system for cleaning and annotating behavioral data collected from motion sensors, mobile phones, and other mobile devices. Our goal is to demonstrate how individuals playing mobile and online games - the ""crowd"" - can collectively, affordably, and incrementally clean and add important metadata to raw sensor data that has been passively collected from individuals, similar to that from population-scale surveillance studies (e.g., the National Health and Nutrition Examination Survey (NHANES) and UK Biobank) and those planned for studies such as the White House's Precision Medicine Initiative. The game-playing crowd will thereby dramatically improve the utility of the datasets collected for a variety of scientific studies. We will validate our prototye system on datasets collected from motion monitors used to study physical activity, sedentary behavior, and sleep, but we will demonstrate how the system could be extended for use on the increasingly rich datasets that are being collected with mobile devices and that include not only motion data, but also sensor data on location, light, audio, and person-to-person proximity. We will then refine the system, foster a community of crowd game players interested in citizen science, and release the source code to the system as an open source project so that other researchers can adapt the technique for their own work. PUBLIC HEALTH RELEVANCE: Longitudinal sensor data collected passively from wearable activity monitors and mobile phones will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes and personalize health intervention and research. We propose to develop and test a system that permits typical mobile application game players to help scientists improve this type of data, by adding additional annotations that enrich the data, making it more useful for behavioral science and more amenable to automatic processing. This will help researchers to better understand how individual-level behaviors relate to health outcomes in current research studies that collect personal-level sensor data such as NHANES and the Women's Health Study, and future big data ventures such as the new Precision Medicine Initiative.",Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health,9357585,UH2EB024407,"['Accelerometer', 'Algorithms', 'American', 'Behavior', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Car Phone', 'Classification', 'Cohort Studies', 'Communities', 'Complex', 'Computers', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Science', 'Data Set', 'Devices', 'Environmental Exposure', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Intervention', 'Interview', 'Label', 'Lead', 'Light', 'Location', 'Machine Learning', 'Manuals', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Motion', 'National Health and Nutrition Examination Survey', 'Outcome', 'Output', 'Participant', 'Pathway Analysis', 'Pattern', 'Performance', 'Persons', 'Phonation', 'Physical activity', 'Play', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteomics', 'Research', 'Research Personnel', 'Risk Behaviors', 'Sampling', 'Science', 'Scientist', 'Sleep', 'Source Code', 'Stream', 'System', 'Techniques', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Visualization software', 'Women&apos', 's Health', 'Work', 'annotation  system', 'base', 'behavioral health', 'biobank', 'citizen science', 'crowdsourcing', 'design', 'experimental study', 'fitness', 'genetic makeup', 'handheld mobile device', 'improved', 'innovation', 'insight', 'instrument', 'interest', 'metabolomics', 'mobile application', 'mobile computing', 'novel', 'open source', 'precision medicine', 'prototype', 'public health relevance', 'research study', 'sedentary lifestyle', 'sensor', 'surveillance study', 'temporal measurement', 'tool', 'ubiquitous computing', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,UH2,2017,300665,0.008514080625058668
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability. PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.",Capti Screen Reading Assistant for Goal Directed Web Browsing,9199231,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2017,500000,0.04516549207837182
"Real-time prediction of marijuana use & effects of use on cognition in the natural environment ABSTRACT Some young adult marijuana (MJ) users report adverse effects of MJ use on cognition that impact daily functioning, with negative consequences such as injury and fatality due to driving while under the influence of MJ. Research on the effects of MJ use on cognition, however, has produced mixed findings. MJ effects on cognition may depend on factors such as history and current severity of marijuana use, time since last MJ use (including possible MJ withdrawal effects), and gender. This R21 aims to address limitations of existing research by (1) starting to develop an algorithm to predict MJ use using smartphone data in regular/heavy MJ users based on “routine” or “habitual use”, and (2) examining effects of MJ use on cognition using smartphone- based cognitive testing in the natural environment. Development of an algorithm to predict MJ use would facilitate systematic assessment of MJ effects on cognitive functioning through more efficient scheduling of smartphone cognitive testing among regular/heavy MJ users in relation to daily routines. Cognitive testing by smartphone in the natural environment is an innovative method that has shown validity, and permits sampling of cognitive functioning within and across days in relation to MJ use. This project will enroll non-treatment seeking young adult (ages 18-25) MJ users from the community, representing “low”, “regular”, and “heavy” MJ use, with 50% female at each level of use. Participants will complete a baseline lab assessment, 30-day data collection using smartphone and wearable devices (e.g., wristband), and a debriefing interview. Piloting will optimize the protocol and methods for compliance. Smartphones will collect continuously sensed data (e.g., geolocation) for input to an algorithm to predict MJ use in regular/heavy MJ users. This R21 will identify which types of data, available through smartphone, provide optimal detection of routines in MJ use among regular/heavy users. Smartphone cognitive testing will be administered at various times during acute MJ intoxication and various naturalistically occurring lengths of MJ abstinence to examine effects of MJ use on selected aspects of cognitive functioning in daily life. Development of an algorithm to predict MJ use in regular/heavy MJ users based on smartphone data could, for example, facilitate real-time assessment of MJ effects on cognition through improved sampling of cognition in relation to acute and non-acute effects of MJ use. This R21 will provide the foundation for a research program that aims to examine MJ effects on cognitive functioning in vivo, and could support the development of just-in-time intervention to reduce MJ use. This R21 aligns with NIDA's strategic goal of determining consequences of drug use, and cross-cutting themes of highlighting real-world relevance of research and leveraging mobile health technologies to reduce drug use. Project Narrative This exploratory project will initiate development of an algorithm to predict marijuana use using data from smartphone and ecological momentary assessment, and will examine effects of marijuana use on cognitive functioning in the natural environment using innovative smartphone-based cognitive tests. Developing an algorithm to predict marijuana use has substantial healthcare applications, specifically for timely intervention to reduce marijuana use. Further, examining effects of marijuana use on cognitive functioning daily life has important implications for determining possible adverse health consequences associated with marijuana use.",Real-time prediction of marijuana use & effects of use on cognition in the natural environment,9329948,R21DA043181,"['Abstinence', 'Acute', 'Address', 'Adverse effects', 'Age', 'Age of Onset', 'Algorithms', 'Attention', 'Automobile Driving', 'Awareness', 'Behavior', 'Cellular Phone', 'Cognition', 'Communities', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Drug usage', 'Ecological momentary assessment', 'Enrollment', 'Environment', 'Female', 'Foundations', 'Frequencies', 'Gender', 'Geography', 'Goals', 'Health', 'Health Technology', 'Healthcare', 'Heart Rate', 'Hour', 'Injury', 'Intake', 'Intervention', 'Interview', 'Intoxication', 'Length', 'Life', 'Literature', 'Location', 'Machine Learning', 'Marijuana', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Moods', 'National Institute of Drug Abuse', 'Participant', 'Patient Self-Report', 'Performance', 'Positioning Attribute', 'Procedures', 'Protocols documentation', 'Recording of previous events', 'Recruitment Activity', 'Relaxation', 'Reporting', 'Research', 'Sampling', 'Schedule', 'Severities', 'Short-Term Memory', 'System', 'Testing', 'Text', 'Time', 'Travel', 'Withdrawal', 'addiction', 'age group', 'base', 'cognitive function', 'cognitive task', 'cognitive testing', 'computer science', 'daily functioning', 'data mining', 'improved', 'in vivo', 'innovation', 'mHealth', 'male', 'marijuana use', 'marijuana use disorder', 'marijuana user', 'marijuana withdrawal', 'mobile computing', 'prediction algorithm', 'programs', 'reduce marijuana use', 'young adult']",NIDA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R21,2017,234230,-0.0115212618959891
"Clinic Interactions of a Brain-Computer Interface for Communication ﻿    DESCRIPTION (provided by applicant): The promise of brain-computer interfaces (BCI) for communication is becoming a reality for individuals with severe speech and physical impairments (SSPI) who cannot rely on speech or writing to express themselves. While the majority of research efforts are devoted to technology development to address problems of stability, reliability and/or classification, clinical and behavioral challenges are becoming more apparent as individuals with SSPI and their family/care teams assess the systems during novice or long-term trials. The objective of the RSVP Keyboard(tm) BCI translational research team is to address the clinical challenges raised during functional BCI use with innovative engineering design, thereby enhancing the potential of this novel assistive technology. Four specific aims are proposed: (1) to develop a BCI Communication Application Suite (BCI-CAS) that offers a set of language modules to people with SSPI that can meet their language/literacy skills; (2) to develop improved statistical signal models for personalized feature extraction, artifact/interference handling, and robust, accurate intent evidence extraction from physiologic signals; (3) to develop improved language models and stimulus sequence optimization methods; and (4) to evaluate cognitive variables that affect learning and performance of the BCI-CAS. Five language modules are proposed that rely on a multimodal evidence fusion framework for model-based context-aware optimal intent inference: RSVP Keyboard(tm) generative spelling; RSVP texting; RSVP in-context typing; RSVP in-context icon typing; and binary yes/no responses with SSVEPs. Usability data on the current RSVP Keyboard(tm) and SSVEP system drive all proposed aims. Users select a language module, and the BCI system optimizes performance for each individual based on user adaptation, intent inference, and personalized language modeling. A unique simulation function drives individualization of system parameters. The robustness of the BCI customization efforts are evaluated continually by adults with SSPI and neurotypical controls in an iterative fashion. The effect of three intervention programs that address the cognitive construct of attention (process-specific attention training, mindfulness meditation training and novel stimulus presentations) will be implemented through hypothesis-driven single subject designs. Thirty participants, ages 21 years and older with SSPI will be included in home-based interventions. By measuring information transfer rate (ITR), user satisfaction, and intrinsic user factors, we will identify learning strategies that influence BCI sill acquisition and performance for adults with neurodegenerative or neurodevelopmental conditions. The translational teams include (1) signal processing (Erdogmus); (2) clinical neurophysiology (Oken); (3) natural language processing (Bedrick/Gorman); and (4) assistive technology (Fried-Oken). We continue to rely on a solid Bayesian foundation and theoretical frameworks: ICF disability classification (WHO, 2001), the AAC model of participation (Beukelman & Mirenda, 2013) and the Matching Person to Technology Model (Scherer, 2002). PUBLIC HEALTH RELEVANCE: The populations of patients with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means to interface with communication systems. The BCI Communication Applications Suite is a hybrid brain-computer interface that is an innovative technological advance so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Clinic Interactions of a Brain-Computer Interface for Communication,9233069,R01DC009834,"['21 year old', 'Address', 'Adult', 'Advocate', 'Affect', 'Analysis of Variance', 'Attention', 'Awareness', 'Behavior', 'Behavioral', 'Caring', 'Classification', 'Clinic', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Complex', 'Custom', 'Data', 'Decision Making', 'Dependency', 'Electroencephalography', 'Engineering', 'Environment', 'Event-Related Potentials', 'Family', 'Foundations', 'Heterogeneity', 'Home environment', 'Hybrids', 'Impairment', 'Individual', 'Informed Consent', 'Intercept', 'Intervention', 'Laboratories', 'Language', 'Learning', 'Letters', 'Life', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Movement', 'Natural Language Processing', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'Participant', 'Partner Communications', 'Patients', 'Performance', 'Persons', 'Phase', 'Physiological', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Public Health', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Role', 'Running', 'Secondary to', 'Self-Help Devices', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'Stress', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Validation', 'Visual', 'Visual evoked cortical potential', 'Vocabulary', 'Writing', 'acronyms', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinically relevant', 'cognitive system', 'computer science', 'cost', 'design', 'disability', 'engineering design', 'experimental study', 'human-in-the-loop', 'improved', 'innovation', 'intervention program', 'learning strategy', 'literacy', 'mindfulness meditation', 'multimodality', 'neurophysiology', 'novel', 'patient population', 'preference', 'public health relevance', 'residence', 'response', 'satisfaction', 'signal processing', 'simulation', 'skills', 'spelling', 'statistics', 'syntax', 'technology development', 'time interval', 'usability', 'vigilance']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2017,652111,-0.017400114778535956
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9407137,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Persons', 'Phase', 'Phonation', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2017,159267,0.07110318271934964
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9238777,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2017,416574,-0.008039153885529013
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community—the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit’s gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing. Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9474743,R44AG053890,"['Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Custom', 'Dependence', 'Devices', 'Electrical Engineering', 'Emotional', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Impairment', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Vision', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'rehabilitation technology', 'robot exoskeleton', 'usability']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2017,562410,0.03670470225047894
"Independent Exoskeleton-Use through Robust Stand-to-Sit Safety Project Summary/Abstract Innovative Design Labs (IDL) proposes to create a system for the sensing and control of stand-to-sit motions of a wearable bionics suit. Currently 5.6 million people in the US have impaired mobility from a number of different causes. The primary means of mobility for many of these patients is the wheelchair as it has been for most of the last 50 years. Despite all the benefits introduced by widespread use of the wheelchair, it remains a less than ideal mobility solution. Exoskeleton suits have the potential to empower individuals with impaired mobility with an alternative to wheelchairs that allows them to stand up and walk independently within their home and community has the potential to more fully reintegrate these individuals into society while also further improving their health and quality of life. For exoskeletons to gain acceptance in every-day independent home and community use, many control and safety related functionalities still need to be addressed. Our proposal seeks to address one of the gaps in allowing for independent use of exoskeletons in the home and community, namely, functionality to transition from standing to sitting in a safe manner. The proposed work will provide exoskeleton users with the new ability to independently sit down without assistance and confidence in being able to do so without falling and risking possible injuries. It aims to significantly change the way exoskeletons work thereby facilitating their adoption into the market and directly impacting the lives of individuals with disabilities. Project Narrative Exoskeletons can provide patients with SCI, stroke, and other types of impaired mobility access to extended duration, gravity dependent ambulation that can directly combat the risks associated with physical deconditioning. There are benefits for exoskeleton-use to gain widespread acceptance in every-day, independent home and community settings. Currently, exoskeletons are not approved for independent-use because functionalities like transferring from standing-to-sitting requires continuous assistance from caregivers.",Independent Exoskeleton-Use through Robust Stand-to-Sit Safety,9409715,R44AG057267,"['Activities of Daily Living', 'Address', 'Adoption', 'Algorithmic Software', 'Algorithms', 'Area', 'Bionics', 'Caliber', 'Caregivers', 'Chicago', 'Collaborations', 'Communities', 'Computer Vision Systems', 'Country', 'Custom', 'Development', 'Disabled Persons', 'Emerging Technologies', 'Engineering', 'Environment', 'Evaluation', 'Fall injury', 'Feedback', 'Force of Gravity', 'Gait', 'Health', 'Height', 'Home environment', 'Hospitals', 'Imaging technology', 'Impairment', 'Individual', 'Injury', 'Institutes', 'Letters', 'Mechanics', 'Metric System', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Production', 'Quality of life', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Rest', 'Risk', 'Robotics', 'Safety', 'Scientist', 'Small Business Innovation Research Grant', 'Societies', 'Stroke', 'Surface', 'System', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Validation', 'Walking', 'Wheelchairs', 'Work', 'blind', 'combat', 'community setting', 'critical period', 'deconditioning', 'design', 'exoskeleton', 'experience', 'fall risk', 'falls', 'human study', 'improved', 'innovation', 'member', 'prototype', 'rehabilitation technology', 'robot exoskeleton']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2017,653843,0.003952135167308302
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9292314,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2017,364423,0.012555630750356858
"Artificial Intelligence in a Mobile Intervention for Depression (AIM) DESCRIPTION (provided by applicant): The primary aim of this proposal is to develop and evaluate the use of state of the art machine learning approaches within a mobile intervention application for the treatment of major depressive disorder (MDD). Machine learning, a branch of artificial intelligence, focuses on the development of algorithms that automatically improve and evolve based on collected data. Machine learning models can learn to detect complex, latent patterns in data and apply such knowledge to decision making in real time. The proposed intervention, called IntelliCare, will use ongoing data collected from the patient and intervention application to continuously adapt intervention content, content form, and motivational messaging to create a highly tailored and user-responsive treatment system. Behavioral intervention technologies (BITs), including web-based and mobile interventions, have been developed and are increasingly being used to treat MDD. BITs are moderately effective in treating depression, particularly when guided by human coaching via email or telephone. However, lack of personalization and inability to adapt to patient needs or preferences, which results in a perceived lack of relevance, contributes to poorer adherence and outcomes. IntelliCare will be designed as a mobile application, but will be accessible via computer web browsers and tablets. The IntelliCare machine learning framework will use individual data obtained from use data (e.g., length of time using a treatment component), embedded sensors in the phone (e.g., GPS), and the user's self-reports (e.g., ""like"" and usefulness ratings of treatment components) to provide a highly tailored intervention that can learn from the patient and adapt intervention and motivational materials to the patient's preferences and state. Low intensity coaching will serve as a backstop to support adherence. This project will contain three phases.  Phase 1 will involve the development of IntelliCare and its optimization through usability testing.  Phase 2 will be a field trial of 200 users who will receive IntelliCare for 12 weeks. The field trial has two aims: first to complete usability testing and optimization of the treatment framework, and second to develop the machine learning models and algorithms. Phase 3 will subject IntelliCare to a double blind, randomized controlled trial, comparing it to MobilCare. MobilCare will be identical to IntelliCare except that it will use standard presentation and presentation, rather than machine learning, to provide treatment and motivational materials. We will recruit half the participants from primary care settings, as this is the de facto site for treatment of depression in the United States, and half through the Internet, which is the main portal to health apps. The application of adaptive machine learning analytics to a mobile intervention has the potential to create a new generation of BITs that could revolutionize the way that such interventions are conceptualized, designed, and deployed. These innovations would have broad consequences and could be extended a broader range of BITS, including web- based interventions, and to other interventions targeting a wide range of health and mental health problems. PUBLIC HEALTH RELEVANCE: This project will create and evaluate IntelliCare, a mobile intervention also accessible by web browser, for depression. IntelliCare will harness modern adaptive machine learning analytics that can learn from a user's activity on the application, embedded phone sensors, and patient report to tailor intervention elements and motivational messaging to the needs and preferences of the user. This unprecedented level of tailoring could revolutionize the way such interventions are conceptualized, designed, and deployed.",Artificial Intelligence in a Mobile Intervention for Depression (AIM),9109063,R01MH100482,"['Adherence', 'Aftercare', 'Algorithms', 'American', 'Android', 'Artificial Intelligence', 'Australia', 'Behavior Therapy', 'Car Phone', 'Cellular Phone', 'Complex', 'Computers', 'Data', 'Decision Making', 'Depressed mood', 'Development', 'Devices', 'Double-Blind Method', 'Electronic Mail', 'Elements', 'Employee Assistance Program (Health Care)', 'Generations', 'Health', 'Healthcare Systems', 'Human', 'Individual', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mediating', 'Mental Depression', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Online Systems', 'Outcome', 'Participant', 'Patient Preferences', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phase', 'Population', 'Prevalence', 'Primary Health Care', 'Protocols documentation', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recruitment Activity', 'Reporting', 'Secure', 'Severities', 'Site', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Textiles', 'Time', 'United States', 'United States Department of Veterans Affairs', 'base', 'care systems', 'cost', 'depressive symptoms', 'design', 'effective therapy', 'experience', 'improved', 'individualized medicine', 'innovation', 'meetings', 'mobile application', 'motivational intervention', 'preference', 'primary care setting', 'psychologic', 'psychopharmacologic', 'response', 'satisfaction', 'secondary outcome', 'sensor', 'tailored messaging', 'time use', 'tool', 'treatment site', 'trial comparing', 'usability', 'web-accessible', 'web-enabled']",NIMH,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2016,580914,0.023542009785596715
"Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health ﻿    DESCRIPTION (provided by applicant): Longitudinal sensor data collected passively from mobile phones and other wearable sensors will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes. Computers will analyze individual-level data streams to permit unprecedented, individual-level precision in research and intervention. This type of precision medicine enables targeting of science and medicine to a particular individual's genetic makeup, past and current situation, and behavioral health exposures. Mobile phones, smartwatches, and common fitness devices are already capable of generating rich data on behavior, but developing algorithms to interpret that raw data using the latest machine learning algorithms requires practical strategies to annotate large datasets. We propose to develop and test the feasibility and usability of a mobile and online crowdsource-based system for cleaning and annotating behavioral data collected from motion sensors, mobile phones, and other mobile devices. Our goal is to demonstrate how individuals playing mobile and online games - the ""crowd"" - can collectively, affordably, and incrementally clean and add important metadata to raw sensor data that has been passively collected from individuals, similar to that from population-scale surveillance studies (e.g., the National Health and Nutrition Examination Survey (NHANES) and UK Biobank) and those planned for studies such as the White House's Precision Medicine Initiative. The game-playing crowd will thereby dramatically improve the utility of the datasets collected for a variety of scientific studies. We will validate our prototye system on datasets collected from motion monitors used to study physical activity, sedentary behavior, and sleep, but we will demonstrate how the system could be extended for use on the increasingly rich datasets that are being collected with mobile devices and that include not only motion data, but also sensor data on location, light, audio, and person-to-person proximity. We will then refine the system, foster a community of crowd game players interested in citizen science, and release the source code to the system as an open source project so that other researchers can adapt the technique for their own work.         PUBLIC HEALTH RELEVANCE: Longitudinal sensor data collected passively from wearable activity monitors and mobile phones will transform behavioral science by allowing researchers to use ""big data,"" but at the person-level, to understand how behavior and related environmental exposures impact health outcomes and personalize health intervention and research. We propose to develop and test a system that permits typical mobile application game players to help scientists improve this type of data, by adding additional annotations that enrich the data, making it more useful for behavioral science and more amenable to automatic processing. This will help researchers to better understand how individual-level behaviors relate to health outcomes in current research studies that collect personal-level sensor data such as NHANES and the Women's Health Study, and future big data ventures such as the new Precision Medicine Initiative.        ",Crowd-Sourced Annotation of Longitudinal Sensor Data to Enhance Data-Driven Precision Medicine for Behavioral Health,9078547,UH2EB024407,"['Accelerometer', 'Algorithms', 'American', 'Behavior', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Car Phone', 'Classification', 'Cohort Studies', 'Communities', 'Complex', 'Computers', 'Crowding', 'Data', 'Data Collection', 'Data Quality', 'Data Science', 'Data Set', 'Devices', 'Environmental Exposure', 'Environmental Risk Factor', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Genomics', 'Goals', 'Health', 'Housing', 'Human', 'Imagery', 'Individual', 'Intervention', 'Intervention Studies', 'Interview', 'Label', 'Lead', 'Light', 'Location', 'Machine Learning', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Monitor', 'Motion', 'National Health and Nutrition Examination Survey', 'Outcome', 'Output', 'Participant', 'Pathway Analysis', 'Pattern', 'Performance', 'Persons', 'Physical activity', 'Play', 'Population', 'Precision Medicine Initiative', 'Process', 'Proteomics', 'Research', 'Research Personnel', 'Risk Behaviors', 'Sampling', 'Science', 'Scientist', 'Sleep', 'Source Code', 'Stream', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Women&apos', 's Health', 'Work', 'base', 'behavioral health', 'biobank', 'citizen science', 'crowdsourcing', 'design', 'fitness', 'genetic makeup', 'handheld mobile device', 'improved', 'innovation', 'insight', 'instrument', 'interest', 'learning network', 'meetings', 'metabolomics', 'mobile application', 'mobile computing', 'novel', 'open source', 'precision medicine', 'prototype', 'public health relevance', 'research study', 'sedentary lifestyle', 'sensor', 'surveillance study', 'temporal measurement', 'tool', 'ubiquitous computing', 'usability']",NIBIB,NORTHEASTERN UNIVERSITY,UH2,2016,299438,0.008514080625058668
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability.         PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.        ",Capti Screen Reading Assistant for Goal Directed Web Browsing,9048176,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Learning', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'empowered', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2016,500000,0.04516549207837182
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,9336584,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,28000,-0.03559743506937784
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9136188,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'human-robot interaction', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,274076,0.004767275443187999
"Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition ﻿    DESCRIPTION (provided by applicant): Non-invasive medical devices are fast emerging as powerful tools in providing quantitative, simple to parse data in assessing the health and wellness of users. These devices can provide information feedback to users on their state of health, and can potentially provide valuable real-time insight to doctors on the links between user consumption and activity on their respective health. Recently described epidermal electronics / multifunctional tattoos introduced from our lab and others have demonstrated sensing of biopressure, bioelectricity, analyte concentration, bacteria, and more. These structures potentially represent the evolution of wearable devices as they possess a negligible form factor and conform to any surface (such as skin or teeth), minimizing user impact. These devices also bypass more traditional, ""wearable"" gadgets that often require bulky mechanical fixtures or straps, require complex microfluidic systems or integrated electronics, cannot provide dynamic readout, and cannot be disposed of easily. Dielectric sensors are a class of structures that are able to probe the composition of a biofluid via their impedance spectrum, and can be configured for remote sensing via radio waves. These devices can be composed of isolated, thin film circuits, and are directly amenable to epidermal or tattoo formats. This measurement methodology is inherently tremendously powerful, as it can potentially measure multiple analytes at once by probing multiple resonance peaks, and can potentially be piggybacked onto existing RFID infrastructure for data readout. However, these capabilities have not yet been demonstrated, and under real-world applications dielectric sensors have often proven difficult to use. This is often due to simplistic readout (devices will directly correlate a single metric such s the real value of the impedance at a frequency to a biomarker) and are thus can be sensitive from user to user or to environmental conditions. Herein, the applicant will leverage the expertise and knowledge of the labs of Dr. Omenetto and colleagues to bring practical usability to these dielectric antennas, bridging the gap between laboratory measurement and real-time, and real-world health monitoring applications. The end-goal is to generate disposable, skin and teeth-mounted, dielectric sensors to remotely probe the presence of biomarkers in sweat, saliva, and potentially blood to draw definitive links between user nutrition and their health.         PUBLIC HEALTH RELEVANCE: This proposal seeks to create non-invasive, wireless sensor tattoos to be tagged onto the human body for probing the composition of saliva, sweat, or blood. These wireless sensors would present negligible impediment to the user, and would require next to no upkeep to maintain, potentially facilitating an unprecedented level of subject data collection and dissemination. Such data could yield conclusive links between diet, activity, biomarkers, and public health.            ","Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition",9148070,F32EB021159,"['Architecture', 'Bacteria', 'Behavior', 'Biocompatible Materials', 'Biological', 'Biological Markers', 'Blood', 'Bypass', 'Cells', 'Characteristics', 'Classification', 'Complex', 'Consumption', 'Data', 'Data Analyses', 'Data Collection', 'Detection', 'Development', 'Devices', 'Diet', 'Electronics', 'Evolution', 'Feedback', 'Film', 'Fingerprint', 'Frequencies', 'Goals', 'Health', 'Human body', 'Ions', 'Knowledge', 'Laboratories', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Methodology', 'Microfluidics', 'Monitor', 'Nafion', 'Pattern', 'Performance', 'Principal Component Analysis', 'Public Health', 'Radio', 'Radio Waves', 'Reader', 'Research Infrastructure', 'Running', 'Saliva', 'Silk', 'Skin', 'Sodium', 'Stimulus', 'Structure', 'Surface', 'Sweat', 'Sweating', 'System', 'Tattooing', 'Testing', 'Time', 'Tooth structure', 'Training', 'Urea', 'Validation', 'Wireless Technology', 'World Health', 'bioelectricity', 'design', 'electric impedance', 'enzyme activity', 'flexibility', 'glucose monitor', 'improved', 'insight', 'non-invasive monitor', 'nutrition', 'public health relevance', 'real world application', 'remote sensing', 'response', 'saliva composition', 'sensor', 'tool', 'usability']",NIBIB,TUFTS UNIVERSITY MEDFORD,F32,2016,47190,0.050195705859795804
"Clinic Interactions of a Brain-Computer Interface for Communication ﻿    DESCRIPTION (provided by applicant): The promise of brain-computer interfaces (BCI) for communication is becoming a reality for individuals with severe speech and physical impairments (SSPI) who cannot rely on speech or writing to express themselves. While the majority of research efforts are devoted to technology development to address problems of stability, reliability and/or classification, clinical and behavioral challenges are becoming more apparent as individuals with SSPI and their family/care teams assess the systems during novice or long-term trials. The objective of the RSVP Keyboard(tm) BCI translational research team is to address the clinical challenges raised during functional BCI use with innovative engineering design, thereby enhancing the potential of this novel assistive technology. Four specific aims are proposed: (1) to develop a BCI Communication Application Suite (BCI-CAS) that offers a set of language modules to people with SSPI that can meet their language/literacy skills; (2) to develop improved statistical signal models for personalized feature extraction, artifact/interference handling, and robust, accurate intent evidence extraction from physiologic signals; (3) to develop improved language models and stimulus sequence optimization methods; and (4) to evaluate cognitive variables that affect learning and performance of the BCI-CAS. Five language modules are proposed that rely on a multimodal evidence fusion framework for model-based context-aware optimal intent inference: RSVP Keyboard(tm) generative spelling; RSVP texting; RSVP in-context typing; RSVP in-context icon typing; and binary yes/no responses with SSVEPs. Usability data on the current RSVP Keyboard(tm) and SSVEP system drive all proposed aims. Users select a language module, and the BCI system optimizes performance for each individual based on user adaptation, intent inference, and personalized language modeling. A unique simulation function drives individualization of system parameters. The robustness of the BCI customization efforts are evaluated continually by adults with SSPI and neurotypical controls in an iterative fashion. The effect of three intervention programs that address the cognitive construct of attention (process-specific attention training, mindfulness meditation training and novel stimulus presentations) will be implemented through hypothesis-driven single subject designs. Thirty participants, ages 21 years and older with SSPI will be included in home-based interventions. By measuring information transfer rate (ITR), user satisfaction, and intrinsic user factors, we will identify learning strategies that influence BCI sill acquisition and performance for adults with neurodegenerative or neurodevelopmental conditions. The translational teams include (1) signal processing (Erdogmus); (2) clinical neurophysiology (Oken); (3) natural language processing (Bedrick/Gorman); and (4) assistive technology (Fried-Oken). We continue to rely on a solid Bayesian foundation and theoretical frameworks: ICF disability classification (WHO, 2001), the AAC model of participation (Beukelman & Mirenda, 2013) and the Matching Person to Technology Model (Scherer, 2002). PUBLIC HEALTH RELEVANCE: The populations of patients with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means to interface with communication systems. The BCI Communication Applications Suite is a hybrid brain-computer interface that is an innovative technological advance so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Clinic Interactions of a Brain-Computer Interface for Communication,9038348,R01DC009834,"['21 year old', 'Accounting', 'Address', 'Adult', 'Advocate', 'Affect', 'Analysis of Variance', 'Attention', 'Behavior', 'Behavioral', 'Caring', 'Classification', 'Clinic', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Complex', 'Data', 'Decision Making', 'Dependency', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Foundations', 'Heterogeneity', 'Home environment', 'Hybrids', 'Impairment', 'Individual', 'Informed Consent', 'Intercept', 'Intervention', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Movement', 'Natural Language Processing', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'P300 Event-Related Potentials', 'Participant', 'Partner Communications', 'Patients', 'Performance', 'Persons', 'Physiological', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Public Health', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Role', 'Running', 'Secondary to', 'Self-Help Devices', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'Stress', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Validation', 'Visual', 'Visual evoked cortical potential', 'Vocabulary', 'Writing', 'acronyms', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinically relevant', 'cognitive system', 'computer science', 'cost', 'design', 'disability', 'engineering design', 'human-in-the-loop', 'improved', 'innovation', 'intervention program', 'learning strategy', 'literacy', 'meetings', 'mindfulness meditation', 'neurophysiology', 'novel', 'patient population', 'preference', 'public health relevance', 'research study', 'residence', 'response', 'satisfaction', 'signal processing', 'simulation', 'skills', 'spelling', 'statistics', 'syntax', 'technology development', 'time interval', 'usability', 'vigilance']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2016,652362,-0.017400114778535956
"Enabling access to printed text for blind people via assisted mobile OCR DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality. PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.",Enabling access to printed text for blind people via assisted mobile OCR,8989105,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Augmented Reality', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Health', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2016,230563,0.04825734849028449
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.                ",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9030162,R01EY025332,"['3D Print', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image', 'Information Systems', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'research study', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2016,416574,-0.008039153885529013
"Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility ﻿    DESCRIPTION (provided by applicant): Innovative Design Labs (IDL) proposes to create a system to improve the mobility and control of exoskeletons. Recent research has found that 3.86 million Americans require wheelchairs and the number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk, thus providing a way to more fully reintegrate these individuals into society. Our proposal seeks to address one of the hurdles limiting the widespread adoption of exoskeletons in the home and community-the inability of the user to dynamically control gait parameters. This concept has the potential to significantly change the way exoskeletons work and facilitate their adoption into the market. Hypothesis: We hypothesize that the proposed solution will provide users a practical way to adjust their suit's gait to precisely achieve their navigational goals. Specific Aims: Phase I: 1) Build a prototype and Perform Preliminary Laboratory Testing; 2) Develop and Benchmark Algorithms; and 3) Perform Pilot Human Study of Prototype with Exoskeleton Subjects. Phase II: 1) Develop Customized, Production-Ready Hardware and Firmware 2) Integrate with Exoskeleton Control System; and 3) Perform an evaluation of the system through human study testing.         PUBLIC HEALTH RELEVANCE: Recent research has found that 3.86 million Americans require wheelchairs and that number has been increasing annually by an average annual rate of 5.9% per year. While wheelchairs provide freedom, allowing users to be independent as well as reducing dependence upon others, wheelchair use is not physically or emotionally equivalent to walking and is often thought to limit community participation and thus exacerbate social isolation. Robotic exoskeletons/bionic suits have the potential to enable these individuals to stand up and walk thereby providing a way to more fully reintegrate these individuals into society.        ",Environmental Imaging and Control for Exoskeletons to Improve Safety and Mobility,9140712,R44AG053890,"['Address', 'Adoption', 'Algorithm Design', 'Algorithms', 'American', 'Benchmarking', 'Bionics', 'Caregivers', 'Chicago', 'Clinical', 'Collaborations', 'Communities', 'Community Participation', 'Computational algorithm', 'Computer Vision Systems', 'Crutches', 'Dependence', 'Devices', 'Electrical Engineering', 'Environment', 'Evaluation', 'Exercise', 'Eye', 'Family', 'Feedback', 'Freedom', 'Friends', 'Gait', 'Goals', 'Health', 'Height', 'Home environment', 'Hospitals', 'Human', 'Image', 'Individual', 'Industry', 'Institutes', 'Laboratories', 'Length', 'Location', 'Marketing', 'Medical', 'Methods', 'Motion', 'Patients', 'Performance', 'Phase', 'Population', 'Process', 'Production', 'Quality of life', 'Ramp', 'Rehabilitation Centers', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Safety', 'Small Business Innovation Research Grant', 'Social isolation', 'Societies', 'Software Engineering', 'System', 'Technology', 'Testing', 'Uncertainty', 'Walking', 'Wheelchairs', 'Work', 'commercialization', 'design', 'exoskeleton', 'experience', 'human study', 'image processing', 'improved', 'improved mobility', 'innovation', 'insight', 'member', 'product development', 'prototype', 'public health relevance', 'rehabilitation technology', 'robot exoskeleton', 'usability', 'vision aid']",NIA,"INNOVATIVE DESIGN LABS, INC.",R44,2016,224990,0.036588308079270965
"Artificial Intelligence in a Mobile Intervention for Depression (AIM) DESCRIPTION (provided by applicant): The primary aim of this proposal is to develop and evaluate the use of state of the art machine learning approaches within a mobile intervention application for the treatment of major depressive disorder (MDD). Machine learning, a branch of artificial intelligence, focuses on the development of algorithms that automatically improve and evolve based on collected data. Machine learning models can learn to detect complex, latent patterns in data and apply such knowledge to decision making in real time. The proposed intervention, called IntelliCare, will use ongoing data collected from the patient and intervention application to continuously adapt intervention content, content form, and motivational messaging to create a highly tailored and user-responsive treatment system. Behavioral intervention technologies (BITs), including web-based and mobile interventions, have been developed and are increasingly being used to treat MDD. BITs are moderately effective in treating depression, particularly when guided by human coaching via email or telephone. However, lack of personalization and inability to adapt to patient needs or preferences, which results in a perceived lack of relevance, contributes to poorer adherence and outcomes. IntelliCare will be designed as a mobile application, but will be accessible via computer web browsers and tablets. The IntelliCare machine learning framework will use individual data obtained from use data (e.g., length of time using a treatment component), embedded sensors in the phone (e.g., GPS), and the user's self-reports (e.g., ""like"" and usefulness ratings of treatment components) to provide a highly tailored intervention that can learn from the patient and adapt intervention and motivational materials to the patient's preferences and state. Low intensity coaching will serve as a backstop to support adherence. This project will contain three phases.  Phase 1 will involve the development of IntelliCare and its optimization through usability testing.  Phase 2 will be a field trial of 200 users who will receive IntelliCare for 12 weeks. The field trial has two aims: first to complete usability testing and optimization of the treatment framework, and second to develop the machine learning models and algorithms. Phase 3 will subject IntelliCare to a double blind, randomized controlled trial, comparing it to MobilCare. MobilCare will be identical to IntelliCare except that it will use standard presentation and presentation, rather than machine learning, to provide treatment and motivational materials. We will recruit half the participants from primary care settings, as this is the de facto site for treatment of depression in the United States, and half through the Internet, which is the main portal to health apps. The application of adaptive machine learning analytics to a mobile intervention has the potential to create a new generation of BITs that could revolutionize the way that such interventions are conceptualized, designed, and deployed. These innovations would have broad consequences and could be extended a broader range of BITS, including web- based interventions, and to other interventions targeting a wide range of health and mental health problems. PUBLIC HEALTH RELEVANCE: This project will create and evaluate IntelliCare, a mobile intervention also accessible by web browser, for depression. IntelliCare will harness modern adaptive machine learning analytics that can learn from a user's activity on the application, embedded phone sensors, and patient report to tailor intervention elements and motivational messaging to the needs and preferences of the user. This unprecedented level of tailoring could revolutionize the way such interventions are conceptualized, designed, and deployed.",Artificial Intelligence in a Mobile Intervention for Depression (AIM),8892259,R01MH100482,"['Adherence', 'Aftercare', 'Algorithms', 'American', 'Artificial Intelligence', 'Australia', 'Behavior Therapy', 'Car Phone', 'Cellular Phone', 'Complex', 'Computers', 'Data', 'Decision Making', 'Depressed mood', 'Development', 'Devices', 'Double-Blind Method', 'Electronic Mail', 'Elements', 'Employee Assistance Program (Health Care)', 'Generations', 'Glosso-Sterandryl', 'Health', 'Healthcare Systems', 'Human', 'Individual', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mediating', 'Mental Depression', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Online Systems', 'Outcome', 'Participant', 'Patient Preferences', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phase', 'Population', 'Prevalence', 'Primary Health Care', 'Protocols documentation', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recruitment Activity', 'Reporting', 'Secure', 'Severities', 'Site', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Textiles', 'Time', 'United States', 'United States Department of Veterans Affairs', 'base', 'care systems', 'cost', 'depressive symptoms', 'design', 'effective therapy', 'experience', 'improved', 'individualized medicine', 'innovation', 'meetings', 'mobile application', 'motivational intervention', 'preference', 'primary care setting', 'psychologic', 'psychopharmacologic', 'response', 'satisfaction', 'secondary outcome', 'sensor', 'tailored messaging', 'time use', 'tool', 'treatment site', 'trial comparing', 'usability', 'web-accessible', 'web-enabled']",NIMH,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2015,613425,0.023542009785596715
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8914675,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Health', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2015,136355,0.06408292545038635
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8920573,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,3412,-0.03559743506937784
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9050942,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Solutions', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,280721,0.004767275443187999
"Providing Access to Appliance Displays for Visually Impaired Users DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8916115,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Health', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'contrast enhanced', 'contrast imaging', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2015,368560,0.06851542738673294
"Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition ﻿    DESCRIPTION (provided by applicant): Non-invasive medical devices are fast emerging as powerful tools in providing quantitative, simple to parse data in assessing the health and wellness of users. These devices can provide information feedback to users on their state of health, and can potentially provide valuable real-time insight to doctors on the links between user consumption and activity on their respective health. Recently described epidermal electronics / multifunctional tattoos introduced from our lab and others have demonstrated sensing of biopressure, bioelectricity, analyte concentration, bacteria, and more. These structures potentially represent the evolution of wearable devices as they possess a negligible form factor and conform to any surface (such as skin or teeth), minimizing user impact. These devices also bypass more traditional, ""wearable"" gadgets that often require bulky mechanical fixtures or straps, require complex microfluidic systems or integrated electronics, cannot provide dynamic readout, and cannot be disposed of easily. Dielectric sensors are a class of structures that are able to probe the composition of a biofluid via their impedance spectrum, and can be configured for remote sensing via radio waves. These devices can be composed of isolated, thin film circuits, and are directly amenable to epidermal or tattoo formats. This measurement methodology is inherently tremendously powerful, as it can potentially measure multiple analytes at once by probing multiple resonance peaks, and can potentially be piggybacked onto existing RFID infrastructure for data readout. However, these capabilities have not yet been demonstrated, and under real-world applications dielectric sensors have often proven difficult to use. This is often due to simplistic readout (devices will directly correlate a single metric such s the real value of the impedance at a frequency to a biomarker) and are thus can be sensitive from user to user or to environmental conditions. Herein, the applicant will leverage the expertise and knowledge of the labs of Dr. Omenetto and colleagues to bring practical usability to these dielectric antennas, bridging the gap between laboratory measurement and real-time, and real-world health monitoring applications. The end-goal is to generate disposable, skin and teeth-mounted, dielectric sensors to remotely probe the presence of biomarkers in sweat, saliva, and potentially blood to draw definitive links between user nutrition and their health.         PUBLIC HEALTH RELEVANCE: This proposal seeks to create non-invasive, wireless sensor tattoos to be tagged onto the human body for probing the composition of saliva, sweat, or blood. These wireless sensors would present negligible impediment to the user, and would require next to no upkeep to maintain, potentially facilitating an unprecedented level of subject data collection and dissemination. Such data could yield conclusive links between diet, activity, biomarkers, and public health.            ","Development of epidermal, wireless sensor tattoos for non-invasive monitoring of biofluid composition",8980210,F32EB021159,"['Architecture', 'Bacteria', 'Behavior', 'Biocompatible Materials', 'Biological', 'Biological Markers', 'Blood', 'Bypass', 'Cells', 'Characteristics', 'Classification', 'Complex', 'Consumption', 'Data', 'Data Analyses', 'Data Collection', 'Detection', 'Development', 'Devices', 'Diet', 'Electronics', 'Evolution', 'Feedback', 'Film', 'Fingerprint', 'Frequencies', 'Goals', 'Health', 'Human body', 'Ions', 'Knowledge', 'Laboratories', 'Libraries', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Mechanics', 'Medical Device', 'Methodology', 'Microfluidics', 'Monitor', 'Nafion', 'Pattern', 'Performance', 'Principal Component Analysis', 'Public Health', 'Radio', 'Radio Waves', 'Reader', 'Research Infrastructure', 'Running', 'Saliva', 'Silk', 'Skin', 'Sodium', 'Solutions', 'Stimulus', 'Structure', 'Surface', 'Sweat', 'Sweating', 'System', 'Tattooing', 'Testing', 'Time', 'Tooth structure', 'Training', 'Urea', 'Validation', 'Wireless Technology', 'World Health', 'bioelectricity', 'design', 'electric impedance', 'enzyme activity', 'flexibility', 'glucose monitor', 'improved', 'insight', 'non-invasive monitor', 'nutrition', 'public health relevance', 'real world application', 'remote sensing', 'response', 'saliva composition', 'sensor', 'tool', 'usability']",NIBIB,TUFTS UNIVERSITY MEDFORD,F32,2015,56042,0.050195705859795804
"Clinic Interactions of a Brain-Computer Interface for Communication ﻿    DESCRIPTION (provided by applicant): The promise of brain-computer interfaces (BCI) for communication is becoming a reality for individuals with severe speech and physical impairments (SSPI) who cannot rely on speech or writing to express themselves. While the majority of research efforts are devoted to technology development to address problems of stability, reliability and/or classification, clinical and behavioral challenges are becoming more apparent as individuals with SSPI and their family/care teams assess the systems during novice or long-term trials. The objective of the RSVP Keyboard(tm) BCI translational research team is to address the clinical challenges raised during functional BCI use with innovative engineering design, thereby enhancing the potential of this novel assistive technology. Four specific aims are proposed: (1) to develop a BCI Communication Application Suite (BCI-CAS) that offers a set of language modules to people with SSPI that can meet their language/literacy skills; (2) to develop improved statistical signal models for personalized feature extraction, artifact/interference handling, and robust, accurate intent evidence extraction from physiologic signals; (3) to develop improved language models and stimulus sequence optimization methods; and (4) to evaluate cognitive variables that affect learning and performance of the BCI-CAS. Five language modules are proposed that rely on a multimodal evidence fusion framework for model-based context-aware optimal intent inference: RSVP Keyboard(tm) generative spelling; RSVP texting; RSVP in-context typing; RSVP in-context icon typing; and binary yes/no responses with SSVEPs. Usability data on the current RSVP Keyboard(tm) and SSVEP system drive all proposed aims. Users select a language module, and the BCI system optimizes performance for each individual based on user adaptation, intent inference, and personalized language modeling. A unique simulation function drives individualization of system parameters. The robustness of the BCI customization efforts are evaluated continually by adults with SSPI and neurotypical controls in an iterative fashion. The effect of three intervention programs that address the cognitive construct of attention (process-specific attention training, mindfulness meditation training and novel stimulus presentations) will be implemented through hypothesis-driven single subject designs. Thirty participants, ages 21 years and older with SSPI will be included in home-based interventions. By measuring information transfer rate (ITR), user satisfaction, and intrinsic user factors, we will identify learning strategies that influence BCI sill acquisition and performance for adults with neurodegenerative or neurodevelopmental conditions. The translational teams include (1) signal processing (Erdogmus); (2) clinical neurophysiology (Oken); (3) natural language processing (Bedrick/Gorman); and (4) assistive technology (Fried-Oken). We continue to rely on a solid Bayesian foundation and theoretical frameworks: ICF disability classification (WHO, 2001), the AAC model of participation (Beukelman & Mirenda, 2013) and the Matching Person to Technology Model (Scherer, 2002).         PUBLIC HEALTH RELEVANCE: The populations of patients with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means to interface with communication systems. The BCI Communication Applications Suite is a hybrid brain-computer interface that is an innovative technological advance so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.                ",Clinic Interactions of a Brain-Computer Interface for Communication,8876473,R01DC009834,"['21 year old', 'Accounting', 'Address', 'Adult', 'Advocate', 'Affect', 'Analysis of Variance', 'Attention', 'Behavior', 'Behavioral', 'Caring', 'Classification', 'Clinic', 'Clinical', 'Code', 'Cognitive', 'Collaborations', 'Communication', 'Complex', 'Data', 'Decision Making', 'Dependency', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Foundations', 'Heterogeneity', 'Home environment', 'Human', 'Hybrids', 'Impairment', 'Individual', 'Informed Consent', 'Intercept', 'Intervention', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Movement', 'Natural Language Processing', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Outcome', 'P300 Event-Related Potentials', 'Participant', 'Partner Communications', 'Patients', 'Performance', 'Persons', 'Physiological', 'Procedures', 'Process', 'Production', 'Protocols documentation', 'Public Health', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Role', 'Running', 'Secondary to', 'Self-Help Devices', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Statistical Models', 'Stimulus', 'Stress', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Validation', 'Visual', 'Visual evoked cortical potential', 'Vocabulary', 'Writing', 'acronyms', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinically relevant', 'cognitive system', 'computer science', 'cost', 'design', 'disability', 'engineering design', 'improved', 'innovation', 'intervention program', 'literacy', 'meetings', 'mindfulness meditation', 'neurophysiology', 'novel', 'patient population', 'preference', 'public health relevance', 'research study', 'residence', 'response', 'satisfaction', 'signal processing', 'simulation', 'skills', 'spelling', 'statistics', 'syntax', 'technology development', 'time interval', 'usability', 'vigilance']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2015,665012,-0.017400114778535956
"Enabling access to printed text for blind people via assisted mobile OCR     DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality.         PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.                ",Enabling access to printed text for blind people via assisted mobile OCR,8812658,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Solutions', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'public health relevance', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2015,191510,0.04825734849028449
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8795182,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2015,195445,0.06582806692276959
"Artificial Intelligence in a Mobile Intervention for Depression (AIM)     DESCRIPTION (provided by applicant): The primary aim of this proposal is to develop and evaluate the use of state of the art machine learning approaches within a mobile intervention application for the treatment of major depressive disorder (MDD). Machine learning, a branch of artificial intelligence, focuses on the development of algorithms that automatically improve and evolve based on collected data. Machine learning models can learn to detect complex, latent patterns in data and apply such knowledge to decision making in real time. The proposed intervention, called IntelliCare, will use ongoing data collected from the patient and intervention application to continuously adapt intervention content, content form, and motivational messaging to create a highly tailored and user-responsive treatment system. Behavioral intervention technologies (BITs), including web-based and mobile interventions, have been developed and are increasingly being used to treat MDD. BITs are moderately effective in treating depression, particularly when guided by human coaching via email or telephone. However, lack of personalization and inability to adapt to patient needs or preferences, which results in a perceived lack of relevance, contributes to poorer adherence and outcomes. IntelliCare will be designed as a mobile application, but will be accessible via computer web browsers and tablets. The IntelliCare machine learning framework will use individual data obtained from use data (e.g., length of time using a treatment component), embedded sensors in the phone (e.g., GPS), and the user's self-reports (e.g., ""like"" and usefulness ratings of treatment components) to provide a highly tailored intervention that can learn from the patient and adapt intervention and motivational materials to the patient's preferences and state. Low intensity coaching will serve as a backstop to support adherence. This project will contain three phases.  Phase 1 will involve the development of IntelliCare and its optimization through usability testing.  Phase 2 will be a field trial of 200 users who will receive IntelliCare for 12 weeks. The field trial has two aims: first to complete usability testing and optimization of the treatment framework, and second to develop the machine learning models and algorithms. Phase 3 will subject IntelliCare to a double blind, randomized controlled trial, comparing it to MobilCare. MobilCare will be identical to IntelliCare except that it will use standard presentation and presentation, rather than machine learning, to provide treatment and motivational materials. We will recruit half the participants from primary care settings, as this is the de facto site for treatment of depression in the United States, and half through the Internet, which is the main portal to health apps. The application of adaptive machine learning analytics to a mobile intervention has the potential to create a new generation of BITs that could revolutionize the way that such interventions are conceptualized, designed, and deployed. These innovations would have broad consequences and could be extended a broader range of BITS, including web- based interventions, and to other interventions targeting a wide range of health and mental health problems.            PUBLIC HEALTH RELEVANCE: This project will create and evaluate IntelliCare, a mobile intervention also accessible by web browser, for depression. IntelliCare will harness modern adaptive machine learning analytics that can learn from a user's activity on the application, embedded phone sensors, and patient report to tailor intervention elements and motivational messaging to the needs and preferences of the user. This unprecedented level of tailoring could revolutionize the way such interventions are conceptualized, designed, and deployed.",Artificial Intelligence in a Mobile Intervention for Depression (AIM),8705032,R01MH100482,"['Adherence', 'Aftercare', 'Algorithms', 'American', 'Artificial Intelligence', 'Australia', 'Behavior Therapy', 'Car Phone', 'Complex', 'Computers', 'Data', 'Decision Making', 'Depressed mood', 'Development', 'Devices', 'Double-Blind Method', 'Electronic Mail', 'Elements', 'Employee Assistance Program (Health Care)', 'Generations', 'Glosso-Sterandryl', 'Health', 'Healthcare Systems', 'Human', 'Individual', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mediating', 'Mental Depression', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Online Systems', 'Outcome', 'Participant', 'Patient Preferences', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phase', 'Population', 'Prevalence', 'Primary Health Care', 'Protocols documentation', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recruitment Activity', 'Reporting', 'Secure', 'Severities', 'Site', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Textiles', 'Time', 'United States', 'United States Department of Veterans Affairs', 'base', 'care systems', 'cost', 'depressive symptoms', 'design', 'effective therapy', 'experience', 'improved', 'innovation', 'meetings', 'mobile application', 'motivational intervention', 'preference', 'primary care setting', 'psychologic', 'psychopharmacologic', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sensor', 'tailored messaging', 'time use', 'tool', 'treatment site', 'trial comparing', 'usability', 'web-accessible', 'web-enabled']",NIMH,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2014,626698,0.023542009785596715
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair     DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning.          PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.             ",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8838311,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Build-it', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Reliance', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2014,155663,0.06408292545038635
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8704450,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2014,51689,-0.03559743506937784
"Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices     DESCRIPTION (provided by applicant): The inability to access information on printed signs directly impacts the mobility independence of the over 1.2 million blind persons in the U.S. Many previously proposed technological solutions to this problem either required physical modifications to the environment (talking signs or the placement of coded markers) or required the user to carry around specialized computational equipment, which can be stigmatizing. A recently pursued strategy is to utilize the computational capabilities of smart phones and techniques from computer vision to allow blind persons to read signs at a distance using commercially available, non-stigmatizing, smart- phones. However, despite the fact that sophisticated algorithms exist to recognize and extract sign text from cluttered video input (as evidenced, for example, by mapping services such as Google Maps automatically locating and blurring out only license plate text in street-view maps) current mobile solutions for reading sign text at a distance perform relatively poorly. This poor performance is largely because until recently, smart-phone processors have simply not been able to execute state-of-the-art computer vision text extraction and recognition algorithms at real-time rates, which forced previous mobile sign readers to utilize older, simplistic, less effective algorithms. Next-generation smart-phones run on fundamentally different, hybrid processor architectures (such as the Tegra 4, Snapdragon 800, both released in 2013) with dedicated embedded graphical processing units (GPUs) and multi-core CPUs, which make them ideal for high-performance, vision-heavy computation. In this study, we propose to develop a smart-phone-based system for finding and reading signs at a distance which significantly outperforms previous such readers by implementing state-of-the-art text extraction algorithms on modern smart-phone hybrid GPU/CPU processor architectures. In Phase I, the proposed system will be developed and tested with blind users. In Phase II, feedback from user testing will be integrated into system design and the performance will be improved to permit operation in extremely challenging (such as low light) environments.         PUBLIC HEALTH RELEVANCE: Over 1.2 million people in the US are blind, and lack of safe and independent mobility substantially impacts the quality of life of this population. Printed textual signs, which are ubiquitously used in sighted navigation, are inaccessible to visually impaired persons, and this lack of access to environmental information contributes significantly to the mobility problem. This research would help develop a system whereby blind persons could use commercially available smart-phones to locate and read sign text at a distance.            ",Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices,8779810,R43EY024800,"['Acceleration', 'Access to Information', 'Algorithms', 'Antirrhinum', 'Architecture', 'Back', 'Code', 'Computer Vision Systems', 'Distant', 'Environment', 'Equipment', 'Eye', 'Feedback', 'Hybrids', 'Licensing', 'Light', 'Literature', 'Maps', 'Modification', 'Performance', 'Phase', 'Population', 'Printing', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Research Institute', 'Risk', 'Running', 'SKI gene', 'Self-Help Devices', 'Services', 'Solutions', 'System', 'Techniques', 'Telephone', 'Test Result', 'Testing', 'Text', 'Time', 'Vision', 'Visually Impaired Persons', 'assistive device/technology', 'authority', 'base', 'blind', 'design', 'experience', 'handheld mobile device', 'improved', 'next generation', 'operation', 'phase 1 study', 'public health relevance', 'volunteer']",NEI,"LYNNTECH, INC.",R43,2014,229742,0.048761687843669486
"HHSN261201400054C; TOPIC 308 AUTOMATED COLLECTION, STORAGE, ANALYSIS, AND REPORTING SYSTEMS FOR DIETARY IMAGES 'TITLE: THE MOBILE FOOD INTAKE PHOTO STORAGE & ANALYSIS SYSTEM. PERFORMANCE PERIOD 09/16/ This proposal describes enhancements to Viocare’s Mobile Food Intake Visual and Voice Recognizer (FIVR)  System, a novel combination of innovative technologies including computer vision and speech recognition to  measure dietary intake using a mobile phone. FIVR uses a mobile phone’s camera to capture a short video  of foods to be consumed, which is then verbally-annotated on the mobile phone by the user. These video  and audio files are processed through a real-time backend server speech and image recognition engine for  food recognition and portion size measurement. This project will extend FIVR’s capabilities to analyze more  foods, enhance the analysis and reporting tools, expand system support tools, and develop interfaces to a  diverse set of clinical and research systems. A final evaluation of the FIVR system will be conducted at The  Ohio State University to assess the usability and accuracy of food intake tracking with a group of 100 freeliving  subjects, comparing 4 days of FIVR food intake data to 4 days of 24 hour recalls collected using  ASA24 data. The resulting FIVR product will be a unique food intake tracker that combines selfadministration,  automation (vision), and backend coding to collect food intake records to generate a detailed  nutritional analysis. n/a","HHSN261201400054C; TOPIC 308 AUTOMATED COLLECTION, STORAGE, ANALYSIS, AND REPORTING SYSTEMS FOR DIETARY IMAGES 'TITLE: THE MOBILE FOOD INTAKE PHOTO STORAGE & ANALYSIS SYSTEM. PERFORMANCE PERIOD 09/16/",8947304,61201400054C,"['Architecture', 'Automation', 'Car Phone', 'Clinical Research', 'Code', 'Collection', 'Computer Vision Systems', 'Computerized Medical Record', 'Data', 'Databases', 'Diet', 'Dietary intake', 'Eating', 'Evaluation', 'Food', 'Health', 'Hour', 'Image', 'Individual', 'Location', 'Measurement', 'Measures', 'Methods', 'Nutritional', 'Ohio', 'Output', 'Patients', 'Performance', 'Procedures', 'Process', 'Records', 'Reporting', 'Research Personnel', 'Speech', 'Support System', 'System', 'Systems Analysis', 'Time', 'Universities', 'Vision', 'Visual', 'Voice', 'innovative technologies', 'mobile application', 'novel', 'speech recognition', 'tool', 'usability']",NCI,"VIOCARE, INC.",N44,2014,1000000,0.031763501881898054
"Providing Access to Appliance Displays for Visually Impaired Users     DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8712492,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2014,368560,0.06851542738673294
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a  web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8609036,R44EY020082,"['Advertisements', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2014,357073,0.06582806692276959
"Artificial Intelligence in a Mobile Intervention for Depression (AIM)     DESCRIPTION (provided by applicant): The primary aim of this proposal is to develop and evaluate the use of state of the art machine learning approaches within a mobile intervention application for the treatment of major depressive disorder (MDD). Machine learning, a branch of artificial intelligence, focuses on the development of algorithms that automatically improve and evolve based on collected data. Machine learning models can learn to detect complex, latent patterns in data and apply such knowledge to decision making in real time. The proposed intervention, called IntelliCare, will use ongoing data collected from the patient and intervention application to continuously adapt intervention content, content form, and motivational messaging to create a highly tailored and user-responsive treatment system. Behavioral intervention technologies (BITs), including web-based and mobile interventions, have been developed and are increasingly being used to treat MDD. BITs are moderately effective in treating depression, particularly when guided by human coaching via email or telephone. However, lack of personalization and inability to adapt to patient needs or preferences, which results in a perceived lack of relevance, contributes to poorer adherence and outcomes. IntelliCare will be designed as a mobile application, but will be accessible via computer web browsers and tablets. The IntelliCare machine learning framework will use individual data obtained from use data (e.g., length of time using a treatment component), embedded sensors in the phone (e.g., GPS), and the user's self-reports (e.g., ""like"" and usefulness ratings of treatment components) to provide a highly tailored intervention that can learn from the patient and adapt intervention and motivational materials to the patient's preferences and state. Low intensity coaching will serve as a backstop to support adherence. This project will contain three phases.  Phase 1 will involve the development of IntelliCare and its optimization through usability testing.  Phase 2 will be a field trial of 200 users who will receive IntelliCare for 12 weeks. The field trial has two aims: first to complete usability testing and optimization of the treatment framework, and second to develop the machine learning models and algorithms. Phase 3 will subject IntelliCare to a double blind, randomized controlled trial, comparing it to MobilCare. MobilCare will be identical to IntelliCare except that it will use standard presentation and presentation, rather than machine learning, to provide treatment and motivational materials. We will recruit half the participants from primary care settings, as this is the de facto site for treatment of depression in the United States, and half through the Internet, which is the main portal to health apps. The application of adaptive machine learning analytics to a mobile intervention has the potential to create a new generation of BITs that could revolutionize the way that such interventions are conceptualized, designed, and deployed. These innovations would have broad consequences and could be extended a broader range of BITS, including web- based interventions, and to other interventions targeting a wide range of health and mental health problems.            PUBLIC HEALTH RELEVANCE: This project will create and evaluate IntelliCare, a mobile intervention also accessible by web browser, for depression. IntelliCare will harness modern adaptive machine learning analytics that can learn from a user's activity on the application, embedded phone sensors, and patient report to tailor intervention elements and motivational messaging to the needs and preferences of the user. This unprecedented level of tailoring could revolutionize the way such interventions are conceptualized, designed, and deployed.                   ",Artificial Intelligence in a Mobile Intervention for Depression (AIM),8496352,R01MH100482,"['Adherence', 'Aftercare', 'Algorithms', 'American', 'Artificial Intelligence', 'Australia', 'Behavior Therapy', 'Car Phone', 'Complex', 'Computers', 'Data', 'Decision Making', 'Depressed mood', 'Development', 'Devices', 'Double-Blind Method', 'Electronic Mail', 'Elements', 'Employee Assistance Program (Health Care)', 'Generations', 'Glosso-Sterandryl', 'Health', 'Healthcare Systems', 'Human', 'Individual', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Learning', 'Length', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mediating', 'Mental Depression', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Online Systems', 'Outcome', 'Participant', 'Patient Preferences', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pattern', 'Phase', 'Population', 'Prevalence', 'Primary Health Care', 'Protocols documentation', 'Public Health', 'Randomized Controlled Trials', 'Recommendation', 'Recruitment Activity', 'Reporting', 'Secure', 'Severities', 'Site', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Textiles', 'Time', 'United States', 'United States Department of Veterans Affairs', 'base', 'care systems', 'cost', 'depressive symptoms', 'design', 'effective therapy', 'experience', 'improved', 'innovation', 'meetings', 'motivational intervention', 'preference', 'primary care setting', 'psychologic', 'psychopharmacologic', 'public health relevance', 'response', 'satisfaction', 'secondary outcome', 'sensor', 'tailored messaging', 'time use', 'tool', 'treatment site', 'trial comparing', 'usability', 'web-accessible', 'web-enabled']",NIMH,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2013,636036,0.023542009785596715
"Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation     DESCRIPTION (provided by applicant): Over 3 million emergency intubations are performed in the US every year and failure rates can be as high as 50% (3-5). Success is highly dependent on how frequently the responder performs this life-saving procedure on humans (6). Brio Device, LLC, an airway management medical device company, is addressing the need to decouple the success of the procedure from the experience of the user with their ""smart"" intubation device which integrates anatomic structure recognition algorithms and visual guidance feedback with an articulating stylet. Brio's intubation device is specifically designed fo the needs of emergency responders, such as paramedics, emergency department personnel, code teams in hospitals and military medics, who often arrive at the patient first. The smart intubation device will reduce failure rates by providing the user with visual instruction of the correct path to the trachea as he places the endotracheal tube. The guidance software uses machine learning and computer vision algorithms to recognize the anatomy and determine the path to insert the tube. Ultimately, the intubation device will include both a guidance display on an LCD screen and an optical stylet that has single-axis angulation control of the distal tip. For the purpose of this Phase I study, a laptop or desktop computer will be used for the image processing and the guidance display that accompanies the articulating stylet. The long-term goal is to create a device that is compact, light-weight and portable to suit the needs of ambulances and hospital crash carts.  The hypothesis for this study is that by incorporating a video guidance display with an articulating stylet, inexperienced users will be more successful in correctly placing the endotracheal tube using this device compared to direct laryngoscopy. To achieve this goal, image processing and machine learning algorithms will be developed to recognize key anatomic structures in the airway. Software will also be developed determine the path the tube should follow and to display this information for the user. Finally, the efficacy of the device will be validated in airway simulation mannequins with medical students serving as the inexperienced users. Phase II will focus on integrating the guidance software, articulating optical stylet and display into a portable device with embedded hardware and software contained within the stylet handle. At completion of Phase II, the device will be ready for clinica trials and FDA testing.  Brio will enter the $20 billion airway market with its intubation device. Initial sales will begin with anesthesiologists who are early adopters of new technology to assist with difficult airways. Brio will market its product to ~327,000 clinicians who use intubation devices. The U.S. addressable market for emergency intubation is ~$900M for the 41,000 ambulances and 5,800 emergency departments and hospital code teams.         PUBLIC HEALTH RELEVANCE: In this SBIR Phase I, Brio Device, LLC plans to create and evaluate a device that improves the success rate of emergency intubations by coupling a smart guidance display with a user-controlled single-axis articulating stylet. Emergency intubations are often performed in challenging situations by personnel who do the procedure infrequently. Since failure rates are as high as 50% and approximately 180,000 deaths occur each year from failed pre-hospital intubations, a device is needed to provide visual guidance information to assist the users and increase their success rates in emergency situations.            ",Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation,8453607,R43HL114160,"['Accident and Emergency department', 'Address', 'Algorithms', 'Ambulances', 'Anatomic structures', 'Anatomy', 'Brain Death', 'Brain Injuries', 'Cessation of life', 'Clinical Trials', 'Code', 'Computer Vision Systems', 'Computer software', 'Computers', 'Coupling', 'Critical Care', 'Destinations', 'Devices', 'Distal', 'Emergency Situation', 'Failure', 'Feasibility Studies', 'Feedback', 'Goals', 'Hospitals', 'Human', 'Human Resources', 'Image', 'Imagery', 'Instruction', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Laryngoscopy', 'Left', 'Life', 'Light', 'Location', 'Lung', 'Machine Learning', 'Manikins', 'Marketing', 'Medical Device', 'Medical Students', 'Military Hospitals', 'Military Personnel', 'Optics', 'Outcome', 'Outcome Measure', 'Oxygen', 'Paramedical Personnel', 'Patients', 'Phase', 'Physicians', 'Procedures', 'Resuscitation', 'Sales', 'Small Business Innovation Research Grant', 'Structure', 'System', 'Testing', 'Time', 'Trachea', 'Tube', 'Visual', 'commercial application', 'design', 'endotracheal', 'experience', 'flexibility', 'image processing', 'improved', 'information display', 'laptop', 'light weight', 'new technology', 'novel', 'phase 1 study', 'public health relevance', 'secondary outcome', 'simulation', 'success']",NHLBI,"BRIO DEVICE, LLC",R43,2013,244710,0.03562777894336288
"CRCNS: Hybrid non-invasive brain-machine interfaces for 3D object manipulation Interacting with the physical environment and manipulating objects is an essential part of daily life. This ability is lost in upper-limb amputees as well as patients with spinal cord injury, stroke, ALS and other movement disorders. These people know what they want to do as well as how they would do it if their arms were functional. If such knowledge is decoded and sent to a prosthetic arm (or to the patient's own arm fitted with functional electric stimulators) the lost motor function could be restored. The decoding is unlikely to be perfect however the brain can adapt to an imperfect decoder using real-time feedback. Several groups including ours have recently demonstrated that at least in principle this can be achieved. However, as is often the case in science, the initial work has been done in idealized conditions and its applicability to real-world usage scenarios remains an open question. The goal of this project is to bring movement control brain-machine interfaces (BMIs) closer to helping the people who need them, and at the same time exploit the rich datasets we collect in order to advance our understanding of sensorimotor control and learning. This will be accomplished by creating hybrid BMIs which exploit information from multiple sources, combined with modern algorithms from machine learning and automatic control. RELEVANCE (See instructions): Being able to interact with the physical environment and manipulate objects is an essential part of daily life. Brain-machine interfaces are one way to restore this ability to patients who have lost it. The proposed project will bring brain-machine interfaces closer to helping patients in real-worid object manipulation tasks.  n/a",CRCNS: Hybrid non-invasive brain-machine interfaces for 3D object manipulation,8507287,R01NS073120,"['Address', 'Algorithms', 'Amputees', 'Behavior', 'Brain', 'Communication', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Devices', 'Dose', 'Electrodes', 'Electroencephalography', 'Epilepsy', 'Eye Movements', 'Feedback', 'Goals', 'Hand', 'Head Movements', 'Home environment', 'Hybrids', 'Instruction', 'Knowledge', 'Learning', 'Left', 'Life', 'Lifting', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Activity', 'Movement', 'Movement Disorders', 'Neck', 'Patients', 'Physical environment', 'Positioning Attribute', 'Prosthesis', 'Robot', 'Robotics', 'Scheme', 'Science', 'Shapes', 'Signal Transduction', 'Simulate', 'Source', 'Specific qualifier value', 'Spinal cord injury', 'Stroke', 'Testing', 'Text', 'Time', 'Upper Extremity', 'Work', 'arm', 'brain machine interface', 'design', 'grasp', 'instrument', 'interest', 'kinematics', 'visual feedback']",NINDS,UNIVERSITY OF WASHINGTON,R01,2013,240369,-0.03231257863946077
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.           The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8389864,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2013,499358,0.03906018046560789
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.            ",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8650411,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'high school', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2013,81362,-0.03559743506937784
"Providing Access to Appliance Displays for Visually Impaired Users  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays. This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image. For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast. These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view. Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users. Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures. The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.                ",Providing Access to Appliance Displays for Visually Impaired Users,8579051,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,376082,0.06851542738673294
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.       PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8435501,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,383018,0.03937896271223208
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,8413778,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Award', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2013,655370,0.009343594549921619
"Context Understanding Technology to improve internet accessibility for users with     DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization.         PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.                ",Context Understanding Technology to improve internet accessibility for users with,8459121,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'public health relevance', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2013,371933,0.06582806692276959
"CRCNS: Hybrid non-invasive brain-machine interfaces for 3D object manipulation Interacting with the physical environment and manipulating objects is an essential part of daily life. This ability is lost in upper-limb amputees as well as patients with spinal cord injury, stroke, ALS and other movement disorders. These people know what they want to do as well as how they would do it if their arms were functional. If such knowledge is decoded and sent to a prosthetic arm (or to the patient's own arm fitted with functional electric stimulators) the lost motor function could be restored. The decoding is unlikely to be perfect however the brain can adapt to an imperfect decoder using real-time feedback. Several groups including ours have recently demonstrated that at least in principle this can be achieved. However, as is often the case in science, the initial work has been done in idealized conditions and its applicability to real-world usage scenarios remains an open question. The goal of this project is to bring movement control brain-machine interfaces (BMIs) closer to helping the people who need them, and at the same time exploit the rich datasets we collect in order to advance our understanding of sensorimotor control and learning. This will be accomplished by creating hybrid BMIs which exploit information from multiple sources, combined with modern algorithms from machine learning and automatic control. RELEVANCE (See instructions): Being able to interact with the physical environment and manipulate objects is an essential part of daily life. Brain-machine interfaces are one way to restore this ability to patients who have lost it. The proposed project will bring brain-machine interfaces closer to helping patients in real-worid object manipulation tasks.  n/a",CRCNS: Hybrid non-invasive brain-machine interfaces for 3D object manipulation,8288148,R01NS073120,"['Address', 'Algorithms', 'Amputees', 'Behavior', 'Brain', 'Communication', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Devices', 'Dose', 'Electrodes', 'Electroencephalography', 'Epilepsy', 'Eye Movements', 'Feedback', 'Goals', 'Hand', 'Head Movements', 'Home environment', 'Hybrids', 'Instruction', 'Knowledge', 'Learning', 'Left', 'Life', 'Lifting', 'Machine Learning', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Methodology', 'Methods', 'Modeling', 'Motor', 'Motor Activity', 'Movement', 'Movement Disorders', 'Neck', 'Patients', 'Physical environment', 'Positioning Attribute', 'Prosthesis', 'Robot', 'Robotics', 'Scheme', 'Science', 'Shapes', 'Signal Transduction', 'Simulate', 'Source', 'Specific qualifier value', 'Spinal cord injury', 'Stroke', 'Testing', 'Text', 'Time', 'Upper Extremity', 'Work', 'arm', 'brain machine interface', 'design', 'grasp', 'instrument', 'interest', 'kinematics', 'visual feedback']",NINDS,UNIVERSITY OF WASHINGTON,R01,2012,250764,-0.03231257863946077
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.      PUBLIC HEALTH RELEVANCE:    The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.                 The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8198847,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2012,499358,0.030927098709400646
"OTHER FUNCTIONS SBIR TOPIC 308, PHASE I: THE MOBILE FOOD INTAKE PHOTO STORAGE AN This proposal describes plans to enhance Viocare¿s Mobile Food Intake Visual and Voice Recognizer (FIVR) System. FIVR, an active Genes, Environment and Health Initiative (GEI) project, is a novel combination of innovative technologies including computer vision and speech recognition to measure dietary intake using a mobile phone. Version 1 of FIVR uses a mobile phone¿s embedded camera to capture a short video of food to be consumed. The food to be eaten is annotated verbally on the mobile phone by the user. These video and audio files are sent to a backend server for real-time food recognition and portion size measurement through speech recognition and image analysis. This project will develop specifications to extend FIVR¿s capabilities to standardize, store, and analyze more diverse food images, such as 3D photos; to collect other food data; to enhance the analysis tools; and for interfaces to a variety of clinical/research systems. The FIVR Version 2 functional prototype will be developed to use 3D dietary images as input. A final evaluation of the FIVR V2 prototype will be conducted to assess the accuracy and feasibility of the 3D image diet capture with a group of 9 subjects in a controlled feeding study. n/a","OTHER FUNCTIONS SBIR TOPIC 308, PHASE I: THE MOBILE FOOD INTAKE PHOTO STORAGE AN",8554263,61201200042C,"['Car Phone', 'Clinical Research', 'Computer Vision Systems', 'Data', 'Diet', 'Dietary intake', 'Documentation', 'Eating', 'Environment', 'Evaluation', 'Food', 'Genes', 'Health', 'Image', 'Image Analysis', 'Measurement', 'Measures', 'Phase', 'Reporting', 'Small Business Innovation Research Grant', 'System', 'Three-Dimensional Image', 'Time', 'Visual', 'Voice', 'feeding', 'innovative technologies', 'novel', 'prototype', 'speech recognition', 'tool']",NCI,"VIOCARE, INC.",N43,2012,200000,0.025852433670781237
"Vision Without Sight: Exploring the Environment with a Portable Camera  Vision without Sight: Exploring the Environment with a Portable Camera Project Summary As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his colaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specificaly examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.  Vision without Sight: Exploring the Environment with a Portable Camera Project Narrative The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cel phones but are typicaly designed for normaly sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population",Vision Without Sight: Exploring the Environment with a Portable Camera,8334623,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Telephone', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2012,229834,0.06340334458286452
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,8465025,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Award', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2012,40000,0.009343594549921619
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,8213637,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Award', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2012,699362,0.009343594549921619
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              Public Health Relevance The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8227997,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2012,403177,0.05415508693783395
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8473426,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2012,407051,0.0645998653219146
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,8142000,R01EY016093,"['Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Peripheral', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,1141143,0.03368653051525982
"NLP for Augmentative and Alternative Communication in Adults    DESCRIPTION (provided by applicant):  This proposal relates to the technology of Augmentative and Alternative Communication (AAC).  The research, to be developed over the three-year course of this project, relates to increasing communication speed for adult users of typing-based AAC devices. The proposed method has commonalities both with chatter bots and more sophisticated automated question answering systems. In particular, we propose to develop a program that will mine a very large database of stored interactions for sentences that are similar to the sentence currently being uttered by the interlocutor, and propose a set of plausible responses for the AAC user. The outcome of this research will be a system that improves over the current state of the art in whole utterance approaches in AAC, making use of sophisticated natural language processing techniques.    Through this research and its practical application to helping real people with real communications needs, as well as coursework, seminars, participation in the AAC and disabilities community in Portland, OR, and intensive one-on-one meetings with his mentor Dr. Melanie Fried-Oken, the PI will accrue substantial clinical experience in AAC, and will gain a deep understanding of how technology can be used to help people.      PUBLIC HEALTH RELEVANCE: The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.              The proposed project will develop a research program in the field of Augmentative and Alternative Communication. The program proposes to improve the throughput of AAC devices for conversation by modeling dialogue context for literate adult users. Specifically, we will use corpus-based techniques from question answering and chatter bots to select appropriate utterances in response to utterances from interlocutors.            ",NLP for Augmentative and Alternative Communication in Adults,8189460,K25DC011308,"['Address', 'Adult', 'Area', 'Clinical', 'Communication', 'Communication Aids for Disabled', 'Communication Disability', 'Communities', 'Computers', 'Data', 'Databases', 'Devices', 'Environment', 'Food', 'Generations', 'Hobbies', 'Interview', 'Length', 'Measures', 'Mentors', 'Methods', 'Mining', 'Modeling', 'Modification', 'Names', 'Natural Language Processing', 'Oregon', 'Outcomes Research', 'Participant', 'Play', 'Questionnaires', 'Recruitment Activity', 'Relative (related person)', 'Research', 'Restaurants', 'Role', 'Savings', 'Self Assessment', 'Simulate', 'Source', 'Speed', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Travel', 'Universities', 'alternative communication', 'base', 'efficacy testing', 'experience', 'improved', 'literate', 'meetings', 'movie', 'novel', 'practical application', 'programs', 'response', 'satisfaction', 'speech recognition', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,K25,2011,162459,0.02183188370408149
"Micro-environment Glasses as a Treatment for CVS Computer Vision Syndrome (CVS) refers to a collection of eye problems associated with computer use, and  about three-quarters of computer users have it. Conservative estimates indicate that over $2 billion is  currently spent on examinations and special eyewear for CVS treatment. The most common symptoms of  CVS include: eyestrain or eye fatigue, dry eyes, burning eyes, sensitivity to light, and blurred vision. Non-  ocular symptoms include headaches, pain in the shoulders, neck, or back. As diverse as the symptoms are,  they may be related and can be subdivided into to three potential pathophysiological causes:   1) Ocular Surface Mechanisms  2) Accommodative Mechanisms  3) Extra-Ocular Mechanisms  There is a significant gap in the fund of knowledge regarding the diagnosis of this disease. In the near-term,  we plan to focus on the ocular surface category of disorders as a cause of CVS, identify clinical conditions  associated with this syndrome and develop a treatment that addresses this cause. In phase t, we propose to:  ¿Clinically define CVS by observing the incidence of ocular surface abnormalities in symptomatic subjects  and compare them with an age and sex matched non-symptomatic control population  ¿Develop specialized micro-environment glasses to combat CVS symptoms  ¿Study the efficacy of micro-environment glasses in symptomatic and control populations  ¿Critically evaluate viability of CVS micro-environment glasses as a commercial product using both statistical   methods and subjective questionnaires n/a",Micro-environment Glasses as a Treatment for CVS,8203808,R41EY015023,"['Address', 'Age', 'Asthenopia', 'Back', 'Blurred vision', 'Categories', 'Clinical', 'Collection', 'Computer Vision Systems', 'Computers', 'Devices', 'Disease', 'Environment', 'Eye', 'Eye Burns', 'Funding', 'Glass', 'Headache', 'Incidence', 'Knowledge', 'Light', 'Neck', 'Pain', 'Phase', 'Population Control', 'Process', 'Questionnaires', 'Shoulder', 'Statistical Methods', 'Symptoms', 'Syndrome', 'combat', 'disease diagnosis', 'effective therapy', 'eye dryness', 'improved', 'ocular surface', 'sex']",NEI,"SEEFIT, INC.",R41,2011,47724,-0.0001804892632262877
"Vision Without Sight: Exploring the Environment with a Portable Camera    DESCRIPTION (provided by applicant): As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his collaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specific examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.      PUBLIC HEALTH RELEVANCE: The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population              The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population            ",Vision Without Sight: Exploring the Environment with a Portable Camera,8097202,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2011,205070,0.06316664438450044
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,8020057,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Award', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2011,698439,0.009343594549921619
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8042468,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2011,403177,0.05415508693783395
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8133823,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2011,776548,0.0645998653219146
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8109271,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2011,205267,-0.019691421156817455
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7904837,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,1196495,0.03368653051525982
"A mobile Enabling Technology to promote adherence to behavioral therapy    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (06) Enabling Technologies, 06-DA-105: Improving health through ICT/mobile technologies. The ultimate goal of this research is to fundamentally change the ways in which behavioral interventions are delivered. We propose an innovative mobile Enabling Technology-iHeal-that recognizes stressors that threaten a patient's recovery and then delivers evidence-based interventions exactly at the moment of greatest need. Our objective is to determine, within subjects, the extent to which physiologic and affective changes detected by iHeal are predictive, within subjects, of posttraumatic stress or drug cues. The study team has considerable expertise in technology development and in assessment of behavioral interventions in co-occurring disorders. We will study 25 subjects drawn from an existing SAMHSA-funded investigation that utilizes intense case management to monitor progression of PTSD and substance abuse in returning combat veterans. Our proposed investigation will share interventions with the SMAHSA study that are based upon a blending of Motivational Interviewing and Cognitive Behavioral Therapy approaches for PTSD and substance abuse. Specific aims: 1) To evaluate the accuracy with which iHeal characterizes physiological and affective phenomena as acute stress reactions related to PTSD and environmental drug cues; and 2) To evaluate the effect of Motivational Interviewing-based interventions on acute stress reactions related to PTSD and environmental drug cues. This initial proposal is extremely innovative. The proposed iHeal device will employ cutting-edge wireless technology to link wearable sensors to personal mobile computing platforms (e.g., iPhone). This linkage will allow iHeal to detect co-occurring biological and behavioral processes, while embedded computing in the mobile platform permits iHeal to deliver evidence-based empathetic interventions at the opportune moment. iHeal can learn to intervene in ways that are most effective for the user, including scripted text-based dialogues modeled after brief interventions; use of motivating images or messages from loved ones; playing a meaningful song; or contacting a counselor at the moment of greatest need. Ultimately, a wearable wireless device that anticipates stressors and intervenes at a likely transition to risky activities has enormous potential in a variety of social, behavioral, and biomedical research enterprises. Importantly, iHeal has immediate commercial applications that will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises. iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.               Project Narrative iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.",A mobile Enabling Technology to promote adherence to behavioral therapy,7941740,RC1DA028428,"['Acceleration', 'Acute', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Affect', 'Affective', 'Afghanistan', 'Area', 'Artificial Intelligence', 'Arts', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Biological', 'Biomedical Research', 'Case Management', 'Chronic', 'Clinical', 'Cognitive Therapy', 'Communication', 'Computer software', 'Cues', 'Data', 'Devices', 'Disease', 'Drug usage', 'Effectiveness of Interventions', 'Electrical Engineering', 'Enrollment', 'Environment', 'Evidence based intervention', 'Feasibility Studies', 'Feedback', 'Funding', 'Galvanic Skin Response', 'Goals', 'Growth', 'Health', 'Hour', 'Image', 'Intervention', 'Investigation', 'Iraq', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Occupations', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Physiologic Monitoring', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Study', 'Post-Traumatic Stress Disorders', 'Process', 'Professional counselor', 'Public Health', 'Recovery', 'Recruitment Activity', 'Research', 'Risk Behaviors', 'Services', 'Stress', 'Substance abuse problem', 'Technology', 'Telecommunications', 'Text', 'Time', 'United States Substance Abuse and Mental Health Services Administration', 'Veterans', 'Wireless Technology', 'acute stress', 'acute traumatic stress disorder', 'base', 'biomedical Computer science', 'brief intervention', 'combat', 'commercial application', 'cost effectiveness', 'disorder later incidence prevention', 'evidence base', 'experience', 'follow-up', 'improved', 'in vivo', 'innovation', 'instrument', 'interest', 'loved ones', 'motivational enhancement therapy', 'new technology', 'novel', 'response', 'sensor', 'social', 'stressor', 'study characteristics', 'substance abuse treatment', 'technology development']",NIDA,UNIV OF MASSACHUSETTS MED SCH WORCESTER,RC1,2010,496273,0.018813392074828822
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones    DESCRIPTION (provided by applicant):   The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration. By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wireless accelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at the hip, etc.). Optimal but practical configurations of accelerometers will be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permitting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured.          n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7895863,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Hip region structure', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2010,534121,0.03929625079747907
"Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually There are more than 10 million blind and visually impaired people living in America today. Recent technology developments in computer vision, digital cameras, and portable computers make it possible to assist these individuals by developing camera-based products that combine computer vision technology with other existing products.  Although a number of reading assistants have been designed specifically for people who are blind or visually impaired, reading text from complex backgrounds or non-flat surfaces is very challenging and has not yet been successfully addressed. Many everyday tasks involve these challenging conditions, such as reading instructions on vending machines, titles of books aligned on a shelf, instructions on medicine bottles or labels on soup cans.  This proposal focuses on the development of new computer vision algorithms to recognize text from complex backgrounds: 1) from backgrounds with multiple different colors (e.g .. the titles of books lined up on a shelf) and 2) from non-flat surfaces (e.g .. labels on medicine bottles or soup cans). The newly developed computer vision techniques will be integrated with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed by a portable computer (PDA or cell phone), while the speech display will be outputted via mini speakers, earphones, or Bluetooth device. A practical reading system prototype will be produced to read text from complex backgrounds and non-flat surfaces. The system will be cost-effective since it requires only a head mounted camera (<US$100 for 1M resolution), a wearable computer (<US$300), and two mini-speakers or earphones. The price of ""ReadIRlS"" [74] OCR software is under $150 and the ""TextAloud"" speech synthesis software is about $30 [75].  This project will be executed over two years at the City College of New York (CCNY) and Lighthouse International, New York. CCNY, located in the Harlem neighborhood of New York City, is designated as both a Minority Institution and a Hispanic-serving Institution (37% Hispanic and 27% African American). Lighthouse International is a leading non-profit organization dedicated to preserving vision and to providing critically needed vision and rehabilitation services to help people of all ages overcome the challenges of vision loss. During the two years, we will 1) develop new algorithms to recognize text from backgrounds with multiple different colors; 2) develop new algorithms to recognize text from non-flat surfaces; and 3) develop a cost-effective prototype reading system for blind users by integrating with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. The effectiveness of the prototype and algorithms will be evaluated by people with normal vision and people with vision impairment. A database of text on complex backgrounds (multiple colors and non-flat surfaces) will be created for algorithm and system evaluation. The database will be made available to research communities in the areas of computer vision and vision rehabilitation science. In summary, this effort will provide a research-based foundation to inform the design of next generation reading assistants for blind persons, as well as produce a practical prototype to help the blind user read text from complex backgrounds in real-world environments. PROJECT NARRATIVE  The goal of the proposed research is to develop new computer vision algorithms for camera-based text recognition from complex backgrounds and non-flat surfaces, as well as produce a practical reading system prototype in combination with off-the-shelf  optical character recognition (OCR) and speech-synthesis software products, to help blind or visually impaired people read instructions on vending machines, titles of books aligned on a shelf, labels on medicine bottles or soup cans, etc. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed in realtime through a portable computer, such as a mini laptop or a personal digital assistant (PDA). The speech display will be outputted via mini speakers, earphones, or Bluetooth device.",Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually,7977496,R21EY020990,"['Address', 'African American', 'Age', 'Algorithms', 'Americas', 'Area', 'Blindness', 'Books', 'Cellular Phone', 'Cities', 'Color', 'Communities', 'Complex', 'Computer Systems Development', 'Computer Vision Systems', 'Computer software', 'Computers', 'Databases', 'Development', 'Devices', 'Effectiveness', 'Environment', 'Evaluation', 'Event', 'Facial Expression Recognition', 'Foundations', 'Goals', 'Grant', 'Head', 'Hispanics', 'Image', 'Impairment', 'Individual', 'Institution', 'Instruction', 'International', 'Label', 'Letters', 'Life', 'Mails', 'Marketing', 'Medicine', 'Methods', 'Minority', 'Neighborhoods', 'New York', 'New York City', 'Nonprofit Organizations', 'Output', 'Personal Digital Assistant', 'Price', 'Printing', 'Reading', 'Rehabilitation therapy', 'Research', 'Research Project Grants', 'Resolution', 'Running', 'Scientist', 'Shapes', 'Solutions', 'Speech', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Thick', 'Time', 'United States National Institutes of Health', 'Vertebral column', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Writing', 'base', 'blind', 'college', 'computer generated', 'computer human interaction', 'cost', 'design', 'digital', 'experience', 'laptop', 'next generation', 'optical character recognition', 'prototype', 'rehabilitation science', 'rehabilitation service', 'research and development', 'sunglasses', 'technology development', 'visual information']",NEI,CITY COLLEGE OF NEW YORK,R21,2010,190000,0.05174035113335556
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7799708,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,427932,0.075376458640378
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    IQ Engines' mobile visual search technology will enable the visually impaired to access real-time information about physical objects using their mobile phone camera. The mobile phone provides a visually-driven hyperlink between the physical and digital world: point the camera at an object and get information (for example product information or navigation information). The mobile phone camera is a powerful yet underutilized tool for the visually impaired. Our proposal has two specific aims. Working directly with the visually impaired community, we will build a prototype mobile visual search application that meets their accessibility and use requirements. Our second aim is to improve upon the state of the art for 3D object recognition. We will investigate a novel combination of sparse image representation, feature matching algorithm, and geometric verification in order to advance the performance of 3D object matching. While state-of-the-art image intelligence is robust enough to enable rapid and accurate image search of flat feature-rich objects, current computer vision pales in comparison to the abilities of biological vision systems to recognize 3-dimensional objects. Our underlying goal is to bring inspiration from recent advances in theoretical neuroscience and apply them to image and video search solutions.      PUBLIC HEALTH RELEVANCE:    Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.           Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.",Mobile Search for the Visually Impaired,7909025,R43EY019790,"['3-Dimensional', 'Algorithms', 'Arts', 'Biological', 'Breathing', 'Car Phone', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Databases', 'Feedback', 'Funding', 'Future', 'Goals', 'Image', 'Intelligence', 'Internet', 'Letters', 'Modeling', 'Neurosciences', 'Ocular Prosthesis', 'Performance', 'Research', 'Solutions', 'Speech Synthesizers', 'System', 'Technology', 'Text', 'Time', 'Vision', 'Visual', 'Visual Aid', 'Visual impairment', 'Work', 'base', 'blind', 'cost', 'digital', 'improved', 'meetings', 'novel', 'object recognition', 'prototype', 'public health relevance', 'technology development', 'tool', 'visual search']",NEI,"IQ ENGINES, INC.",R43,2010,138770,-0.031016032679952114
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7911722,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'operation', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,423145,0.055100002728401674
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,7743573,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical and Translational Science Awards', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2010,724428,0.009343594549921619
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,7913126,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Businesses', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Methods', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Project Grants', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'design', 'digital', 'experience', 'falls', 'innovation', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition']",NEI,BLINDSIGHT CORPORATION,R44,2010,656703,0.0645998653219146
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),8136874,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,125017,0.047657950178760004
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),8143048,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,76123,0.047657950178760004
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7876805,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,676574,0.047657950178760004
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8101448,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Arts', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2010,187999,-0.019691421156817455
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7668573,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,1187062,0.03368653051525982
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7922310,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,152260,0.03368653051525982
"A mobile Enabling Technology to promote adherence to behavioral therapy    DESCRIPTION (provided by applicant): This application addresses broad Challenge Area (06) Enabling Technologies, 06-DA-105: Improving health through ICT/mobile technologies. The ultimate goal of this research is to fundamentally change the ways in which behavioral interventions are delivered. We propose an innovative mobile Enabling Technology-iHeal-that recognizes stressors that threaten a patient's recovery and then delivers evidence-based interventions exactly at the moment of greatest need. Our objective is to determine, within subjects, the extent to which physiologic and affective changes detected by iHeal are predictive, within subjects, of posttraumatic stress or drug cues. The study team has considerable expertise in technology development and in assessment of behavioral interventions in co-occurring disorders. We will study 25 subjects drawn from an existing SAMHSA-funded investigation that utilizes intense case management to monitor progression of PTSD and substance abuse in returning combat veterans. Our proposed investigation will share interventions with the SMAHSA study that are based upon a blending of Motivational Interviewing and Cognitive Behavioral Therapy approaches for PTSD and substance abuse. Specific aims: 1) To evaluate the accuracy with which iHeal characterizes physiological and affective phenomena as acute stress reactions related to PTSD and environmental drug cues; and 2) To evaluate the effect of Motivational Interviewing-based interventions on acute stress reactions related to PTSD and environmental drug cues. This initial proposal is extremely innovative. The proposed iHeal device will employ cutting-edge wireless technology to link wearable sensors to personal mobile computing platforms (e.g., iPhone). This linkage will allow iHeal to detect co-occurring biological and behavioral processes, while embedded computing in the mobile platform permits iHeal to deliver evidence-based empathetic interventions at the opportune moment. iHeal can learn to intervene in ways that are most effective for the user, including scripted text-based dialogues modeled after brief interventions; use of motivating images or messages from loved ones; playing a meaningful song; or contacting a counselor at the moment of greatest need. Ultimately, a wearable wireless device that anticipates stressors and intervenes at a likely transition to risky activities has enormous potential in a variety of social, behavioral, and biomedical research enterprises. Importantly, iHeal has immediate commercial applications that will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises. iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.               Project Narrative iHeal is an innovative device that uses wearable sensors to detect pulse, skin conductance, and acceleration; the sensor array links wirelessly to an iPhone which has an app that identifies changes in the user's physiology. Changes consistent with acute stress from PTSD exacerbations or drug use cues generate an empathetic conversation between the iPhone and the user, who enters real-time data on social/behavioral/environmental contexts. The iPhone (which tracks time and GPS data) uses predictive software to anticipate upcoming stressors and helps the user avoid them. The public health significance of this proposal is 1) iHeal will detect co-occurring biological and behavioral processes in real time; 2) it will discern undiscovered behavioral states; 3) it will predict a behavior of interest; and 4) it will deliver empathetic interventions to the user at the opportune moment for intervention. Because it is based on the union of existing technology and has immediate commercial applications, iHeal will encourage job growth in behavioral science, biomedical, computer science, telecommunication, and electrical engineering enterprises.",A mobile Enabling Technology to promote adherence to behavioral therapy,7820117,RC1DA028428,"['Acceleration', 'Acute', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Affect', 'Affective', 'Afghanistan', 'Area', 'Artificial Intelligence', 'Arts', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Sciences', 'Biological', 'Biomedical Research', 'Case Management', 'Chronic', 'Clinical', 'Communication', 'Computer software', 'Cues', 'Data', 'Devices', 'Disease', 'Drug usage', 'Effectiveness of Interventions', 'Electrical Engineering', 'Enrollment', 'Environment', 'Evidence based intervention', 'Feasibility Studies', 'Feedback', 'Funding', 'Galvanic Skin Response', 'Goals', 'Growth', 'Health', 'Hour', 'Image', 'Intervention', 'Investigation', 'Iraq', 'Lead', 'Learning', 'Link', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Occupations', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Physiologic Monitoring', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Population Study', 'Post-Traumatic Stress Disorders', 'Process', 'Professional counselor', 'Public Health', 'Recovery', 'Recruitment Activity', 'Research', 'Risk Behaviors', 'Services', 'Stress', 'Substance abuse problem', 'Technology', 'Telecommunications', 'Text', 'Time', 'United States Substance Abuse and Mental Health Services Administration', 'Veterans', 'Wireless Technology', 'acute stress', 'acute traumatic stress disorder', 'base', 'biomedical Computer science', 'brief intervention', 'cognitive behavior therapy', 'combat', 'commercial application', 'cost effectiveness', 'disorder later incidence prevention', 'evidence base', 'experience', 'follow-up', 'improved', 'in vivo', 'innovation', 'instrument', 'interest', 'loved ones', 'motivational enhancement therapy', 'new technology', 'novel', 'response', 'sensor', 'social', 'stressor', 'study characteristics', 'technology development']",NIDA,UNIV OF MASSACHUSETTS MED SCH WORCESTER,RC1,2009,499381,0.018813392074828822
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones    DESCRIPTION (provided by applicant):   The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration. By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wireless accelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at the hip, etc.). Optimal but practical configurations of accelerometers will be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permitting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured.          n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7915037,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Hip region structure', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2009,116402,0.03929625079747907
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones    DESCRIPTION (provided by applicant):   The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration. By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wireless accelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at the hip, etc.). Optimal but practical configurations of accelerometers will be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permitting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured.          n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7620388,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Hip region structure', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2009,654534,0.03929625079747907
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7589644,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,426946,0.075376458640378
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7373002,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Operative Surgical Procedures', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,417177,0.055100002728401674
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7681076,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,346662,0.01668294589825974
"Translational refinement of adaptive communication system for locked-in patients    DESCRIPTION (provided by applicant): The proliferation of brain-computer interface (BCI) technology promises locked-in patients potential ways to communicate successfully. Most BCI systems either involve selection from among a set of simultaneously presented stimuli, requiring extensive control of the interface; or use binary stimulus selection mechanisms that fail to achieve high communication rates because of slow intent detection or a fixed (context independent) ordering of stimuli. We propose a new interface using binary selection of text input via rapid serial visual presentation of natural language components. Individuals with severe speech and physical impairments (SSPI) resulting from acquired neurological disorders (amyotrophic lateral sclerosis, brainstem stroke, Parkinson's disease, multiple sclerosis, spinal cord injury) and neurodevelopmental disorders (cerebral palsy, muscular dystrophy) drive the proposed research. Four laboratories form an alliance for this translational research project: basic research (Erdogmus, engineering; Roark, computer science and natural language processing), and clinical research (Oken, neurology/neurophysiology; Fried-Oken, augmentative communication/neurogenic communication disorders). Our aims are (1) to develop an innovative EEG-based BCI that achieves increased communication rates with fewer errors and greater satisfaction for the target SSPI populations; (2) to iteratively refine the system in the laboratory with user feedback from healthy subjects and expert LIS users of marketed AAC systems; (3) to evaluate the performance of the system within the natural clinical settings of SSPI patients. The innovative BCI is the RSVP Keyboard with three essential features: (1) rapid serial visual presentation (RSVP) of linguistic components ranging from letters to words to phrases; (2) a detection mechanism that employs multichannel electroencephalography (EEG) and/or other suitable response mechanisms that can reliably indicate the binary intent of the user and adapt based on individualized neurophysiologic data of the user; and (3) an open-vocabulary natural language model with a capability for accurate predictions of upcoming text. Theoretical framework is based on a solid Bayesian foundation; clinical usability is based on the WHO ICF (WHO, 2001) and an Augmentative and Alternative Communication (AAC) model of participation. Rigorous experimental scrutiny in both clinical laboratory and natural settings will be obtained with able-bodied subjects and SSPI patients. Measures of learning rate, speed of message production, error rate and user satisfaction for different iterations of the RSVP keyboard will be obtained using an hypothesis-driven crossover design for 36 healthy subjects, and alternating treatment randomization design for 40 patients with SSPI. Descriptions of the motor, cognitive, and language skills of LIS patients using the novel system in their natural environments will inform clinical guidelines and functional device adaptations to better individualize treatment for children and adults with SSPI. The collaborative nature of the proposed translational research is expected to yield new knowledge for both BCI development and clinical AAC use.    Relevance: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.              Public health relevance statement: The populations of patients with locked-in syndrome are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily care giving if they had faster, more reliable means to interface with communication systems. The RSVP keyboard and proposed language models are innovative technological discoveries that are being applied to clinical augmentative communication tools so that patients and their families can participate in daily activities and advocate for improvements in standard clinical care. The proposed project stresses the translation of basic computer science into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Translational refinement of adaptive communication system for locked-in patients,7570367,R01DC009834,"['Address', 'Adult', 'Advocate', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Base of the Brain', 'Basic Science', 'Brain', 'Brain Stem Infarctions', 'Cerebral Palsy', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Research', 'Clinical and Translational Science Awards', 'Cognitive', 'Collaborations', 'Communication', 'Communication Aids for Disabled', 'Communication Methods', 'Communication Tools', 'Computers', 'Crossover Design', 'Data', 'Decision Making', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Engineering', 'Environment', 'Family', 'Feedback', 'Foundations', 'Funding', 'Generations', 'Guidelines', 'Human Resources', 'Impairment', 'Individual', 'Individuation', 'Informed Consent', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Letters', 'Life', 'Linguistics', 'Locked-In Syndrome', 'Marketing', 'Measures', 'Medical', 'Medical Technology', 'Modeling', 'Motor', 'Movement', 'Multiple Sclerosis', 'Muscular Dystrophies', 'Natural Language Processing', 'Nature', 'Neurodevelopmental Disorder', 'Neurogenic Communication Disorders', 'Neurologist', 'Neurology', 'Oregon', 'Outcome Measure', 'Parkinson Disease', 'Pathologist', 'Patients', 'Pattern Recognition', 'Performance', 'Population', 'Production', 'Public Health', 'Randomized', 'Research', 'Research Institute', 'Research Personnel', 'Research Project Grants', 'Scientist', 'Sensory', 'Series', 'Signal Transduction', 'Solid', 'Speech', 'Speed', 'Spinal cord injury', 'Stimulus', 'Stress', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translational Research', 'Translations', 'Uncertainty', 'United States National Institutes of Health', 'Visual', 'Vocabulary', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'computer science', 'computerized data processing', 'design', 'improved', 'innovation', 'literate', 'natural language', 'nervous system disorder', 'neurophysiology', 'novel', 'patient population', 'phrases', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'satisfaction', 'skills', 'therapy design', 'usability']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2009,708748,0.009343594549921619
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7915039,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2009,168580,0.047657950178760004
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7643324,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2009,815277,0.047657950178760004
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7904674,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Theoretical model', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'information processing', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,92207,-0.010710456440584135
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7500697,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,1146026,0.03368653051525982
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones    DESCRIPTION (provided by applicant):   The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration. By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wireless accelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at the hip, etc.). Optimal but practical configurations of accelerometers will be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permitting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured.          n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7489819,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Hip region structure', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2008,637604,0.03929625079747907
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7446299,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Count', 'Custom', 'Daily', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Public Health', 'Range', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Standards of Weights and Measures', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'desire', 'image processing', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,421791,0.075376458640378
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7490458,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'Standards of Weights and Measures', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'day', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,335542,0.01668294589825974
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7486800,R44EY014487,"['Algorithms', 'Arts', 'Cataract', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Macular degeneration', 'Marketing', 'Melissa', 'Optics', 'Persons', 'Phase', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2008,434041,0.013198552682979954
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7665248,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2008,80289,0.047657950178760004
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7489821,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2008,1042528,0.047657950178760004
"Webcam Interface for Audio/touch Graphics Access by Blind People    DESCRIPTION (provided by applicant):  The goal of this project is to develop a compact inexpensive alternative to the bulky expensive touchpads now required by blind people for audio/touch access to graphical information. Audio/touch is known to provide excellent access to computer-literate blind people as well as people with dyslexia or other severe print disabilities. Preparing Audio/touch materials was very expensive until ViewPlus introduced the IVEO Scalable Vector Graphic (SVG) Authoring/conversion software in 2005. IVEO permits virtually any graphical information to be created or converted/imported easily to a well- structured highly accessible SVG format. Tactile copy was also very expensive before 2000 when ViewPlus introduced the Tiger embossing Windows printers that ""print"" by embossing. The new ViewPlus Emprint printer/embossers emboss and also print color images, creating color tactile images particularly useful for people with dyslexia and a number of other print disabilities. An audio/touch user reads an IVEO SVG graphic using the free IVEO Viewer, a tactile copy of the image, and a touchpad. The user places the tactile graphic on the touchpad and presses a point of interest. The touchpad communicates the position of that point back to the computer, and the IVEO Viewer speaks the appropriate information. Tactile text made from mainstream graphics has a distinctive pattern. When a user presses, that text is spoken by the IVEO Viewer. When the user presses a graphic object having a SVG title within the file, that title will be spoken. Objects may also have arbitrarily long description fields that can be spoken and browsed. All spoken information can be displayed on an attached braille display if desired. Graphical information is ubiquitous today, but almost none is accessible to blind people. Government agencies, libraries, companies, and agencies serving people with disabilities could easily send highly accessible IVEO graphics files and tactile graphic copies to clients with disabilities, but there is a ""chicken and egg"" dilemma that must be overcome before they are likely to do so. Few blind people have a touchpad (which cost $500 or more), so few could use that information. The specific aim of this Phase I proposal is to develop an affordable webcam-based prototype as an alternative to touchpads. It is based on an inexpensive webcam that is focused on the graphic and follows a finger. A touchpad press is emulated in this prototype by pressing some computer key with the other hand. This project could be the key to bringing accessible graphics to all blind computer users and is clearly of interest to NEI whose mission statement includes mental health and quality of life of blind people. PUBLIC HEALTH RELEVANCE:  This proposal is relevant to the mission of the National Eye Institute, because it could be the key to making nearly all graphical information easily accessible to people who are blind or have other severe print disabilities. Graphical information is ubiquitous in the world today but is not presently accessible to blind people except through expensive and time-consuming conversion by trained transcribers. Making all graphical information accessible would have an obviously highly beneficial direct effect on education and professional opportunities, mental health, and quality of life of blind people. Mental health and quality of life issues for blind people are parts of the mission of the National Eye Institute.          n/a",Webcam Interface for Audio/touch Graphics Access by Blind People,7480812,R43EY018973,"['Back', 'Braille Display', 'Businesses', 'Chickens', 'Client', 'Color', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consultations', 'Development', 'Devices', 'Disabled Persons', 'Dyslexia', 'Event', 'Fingers', 'Goals', 'Government Agencies', 'Hand', 'Home environment', 'Image', 'Information Systems', 'Institution', 'Internet', 'Libraries', 'Link', 'Mainstreaming', 'Marketing', 'Mental Health', 'Methods', 'Mission', 'Modeling', 'Mus', 'National Eye Institute', 'Numbers', 'Oregon', 'Pattern', 'Phase', 'Positioning Attribute', 'Printing', 'Professional Education', 'Public Health', 'Publications', 'Quality of life', 'Range', 'Reading', 'Site', 'Structure', 'Structure of nail of finger', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Tigers', 'Time', 'Title', 'Today', 'Touch sensation', 'Training', 'Universities', 'Visual', 'Visually Impaired Persons', 'base', 'blind', 'braille', 'cost', 'desire', 'digital', 'disability', 'egg', 'interest', 'literate', 'print disabilities', 'programs', 'prototype', 'research and development', 'tool', 'touchpad', 'vector']",NEI,"VIEWPLUS TECHNOLOGIES, INC.",R43,2008,100001,0.0074371585833209975
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7477498,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2008,365248,0.0347104704599304
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7477064,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Condition', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Experimental Models', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,250129,-0.010710456440584135
"Computer Vision Methods for the Real Time Assessment of Dietary Intake    DESCRIPTION (provided by applicant): Obesity is a leading cause of preventable death and disability in the U.S. Self- monitoring of all foods and beverages consumed is central to weight loss and maintenance efforts; however, this places a heavy burden on the user. These same burdens also impede nutritional research. The proposed research is for the testing of a semi-automated, objective, near real-time computer vision and pattern recognition approach to the measurement of dietary intake. In the proposed product, cell phone pictures of meals and snacks will be analyzed by software in an attempt to automatically recognize as many items as possible. A small number of intelligent yes/no questions will help provide additional information when necessary in order to meet the accuracy demands of the target application. Following identification of the items, the software will estimate the portion sizes of all identified items. The experiments comprising this Phase I SBIR are (a) extract the most informative sets of features using a large number of food and beverage items taken from an existing database of real world meal images, (b) compare the accuracy of candidate pattern recognition approaches to identify items based on the extracted features, (c) identify the most feasible algorithms for estimating portion size, and (d) test usability and user acceptance with a simulated version of the product. Phase II will (a) apply the approach to a greater variety of food and beverage items, (b) improve automated analysis, and (c) compare the approach to existing assessment instruments. This research will extend defense- and security-related technologies to the assessment and treatment of obesity.          n/a",Computer Vision Methods for the Real Time Assessment of Dietary Intake,7405586,R43CA124265,"['Address', 'Adherence', 'Algorithms', 'Area', 'Behavior', 'Behavioral', 'Biological Neural Networks', 'Biometry', 'Body Weight decreased', 'Calculi', 'Cellular Phone', 'Cessation of life', 'Class', 'Coin', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Decision Trees', 'Diabetic Diet', 'Diet Records', 'Dietary intake', 'Disease', 'Eating', 'Eating Behavior', 'Face', 'Feedback', 'Fingerprint', 'Food', 'Food and Beverages', 'Goals', 'Habits', 'Health', 'Image', 'Individual', 'Information Theory', 'Intake', 'Iris', 'Life', 'Life Style', 'Lighting', 'Machine Learning', 'Maintenance', 'Marketing', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Numbers', 'Nutritional', 'Nutritionist', 'Obesity', 'Obesity associated disease', 'Pattern Recognition', 'Phase', 'Placement', 'Principal Investigator', 'Public Health', 'Research', 'Research Personnel', 'Security', 'Shapes', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Three-Dimensional Image', 'Time', 'Training', 'Treatment Protocols', 'United States', 'Wing', 'base', 'design', 'digital imaging', 'disability', 'improved', 'innovation', 'instrument', 'interest', 'obesity treatment', 'research study', 'size', 'usability']",NCI,"MEDIABALANCE, INC.",R43,2007,191710,0.045807260786948
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7172503,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2007,1159531,0.03368653051525982
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration .By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wirelessaccelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at thehip, etc.). Optimal but practical configurations of accelerometerswill be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permiting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured. n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7492399,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2007,315472,0.03929625079747907
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration .By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wirelessaccelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at thehip, etc.). Optimal but practical configurations of accelerometerswill be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permiting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured. n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7490202,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2007,3000,0.03929625079747907
"Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones    DESCRIPTION (provided by applicant):   The primary aim of this U01 project is the technical development, deployment, and evaluation of hardware and software technology that will enable population-scale, longitudinal measurement of physical activity using common mobile phones. Mobile phones available in Asia and soon in the U.S. already include internal accelerometers and low-power wireless communication capabilities. This study will investigate how to use these computing devices for accurate measurement of physical activity type, intensity and bout duration. By exploiting consumer expenditures on phones that many Americans will purchase, maintain, and carry, it may be possible to run large-scale studies where the physical activity of hundreds of thousands of participants is measured and remotely monitored for months or years at an affordable cost. Wireless accelerometers designed at MIT will be redesigned so that they can send data to common mobile phones available in 2011. Laboratory testing using the current version of the sensors will be used to compare the relative information gain that can be obtained by combining the phone accelerometer data and data obtained by wearing one or more wireless sensors on different convenient body locations (e.g., in a watch or bag, on a shoe, at the hip, etc.). Optimal but practical configurations of accelerometers will be determined so that software running on the mobile phone can automatically detect specific physical activities such as brisk walking, running, cycling, climbing stairs, sweeping, playing sports, etc. Technical challenges that will be addressed by the sensor and software design include, (1) obtaining practical battery life, (2) acquiring physical activity data at high temporal resolution, (3) enabling person-specific customization of the detection algorithms, (4) addressing practical end-user concerns about ergonomics, comfort, and social acceptability, (5) permitting real-time and low-cost remote monitoring and maintenance for studies with hundreds of thousands of phone users, and (4) enabling use of other off-the-shelf sensor devices, such as heart rate monitors, as they become available. A participatory design process will be employed to develop strategies for obtaining longitudinal compliance from typical phone users. After two rounds of iterative technical development, each with laboratory validation conducted at Stanford, the technology will be deployed with 50 typical phone users for 10 months. Validity relative to self report, acceptability, and longitudinal compliance will be measured.          n/a",Enabling Population-Scale Physical Activity Measurement on Common Mobile Phones,7340826,U01HL091737,"['Address', 'Adult', 'Algorithms', 'American', 'Asia', 'Car Phone', 'Communication', 'Computer software', 'Data', 'Data Collection', 'Detection', 'Development', 'Devices', 'Enrollment', 'Evaluation', 'Expenditure', 'Frequencies', 'Funding', 'Goals', 'Heart Rate', 'Hip region structure', 'Individual', 'Internet', 'Laboratories', 'Life', 'Location', 'Machine Learning', 'Maintenance', 'Marketing', 'Measurement', 'Measures', 'Monitor', 'Motion', 'Participant', 'Patient Self-Report', 'Persons', 'Physical activity', 'Physical activity scale', 'Play', 'Population', 'Process', 'Relative (related person)', 'Research', 'Research Personnel', 'Resolution', 'Running', 'Sampling', 'Shoes', 'Side', 'Software Design', 'Sports', 'System', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Walking', 'Wireless Technology', 'cost', 'design', 'ergonomics', 'open source', 'prototype', 'remote sensor', 'sensor', 'social', 'software development', 'software systems']",NHLBI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,U01,2007,681046,0.03929625079747907
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7298706,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'Standards of Weights and Measures', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'day', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2007,338243,0.01668294589825974
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7295688,R21EY017003,"['Access to Information', 'Address', 'Algorithms', 'Auditory', 'Bar Codes', 'Canes', 'Canis familiaris', 'Cellular Phone', 'Clutterings', 'Cognitive', 'Color', 'Complement component C1s', 'Computer Vision Systems', 'Computer software', 'Condition', 'Consultations', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Elderly', 'Employment', 'Environment', 'Exhibits', 'Feedback', 'Future', 'Goals', 'Home environment', 'Housing', 'Image', 'Individual', 'Instruction', 'Label', 'Localized', 'Location', 'Modality', 'Museums', 'Paper', 'Pattern', 'Persons', 'Population', 'Printing', 'Psychoacoustics', 'Quality of life', 'Range', 'Rate', 'Reading', 'Research', 'Running', 'Scanning', 'Semantics', 'Shapes', 'Source', 'Speech', 'Standards of Weights and Measures', 'Stress', 'System', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'age group', 'base', 'blind', 'braille', 'concept', 'cost', 'design', 'interest', 'legally blind', 'meter', 'novel', 'optical character recognition', 'programs', 'prototype', 'research study', 'size', 'skills', 'sound', 'success', 'symposium', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2007,193398,0.049878723790243655
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7327116,R44EY014487,"['Algorithms', 'Arts', 'Back', 'Cataract', 'Clutterings', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Lighting', 'Location', 'Macular degeneration', 'Marketing', 'Melissa', 'Motion', 'Optics', 'Peripheral', 'Persons', 'Phase', 'Reading', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'monocular', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2007,448477,0.013198552682979954
"Smart Wheelchair Component System    DESCRIPTION (provided by applicant):  Independent mobility is critical to individuals of any age. While the needs of many individuals with disabilities can be satisfied with power wheelchairs, some members of the disabled community find it difficult or impossible to operate a standard power wheelchair. This population includes, but is not limited to, individuals with low vision, visual field neglect, spasticity, tremors, or cognitive deficits. The goal of this project is to develop a set of components that can be added to standard power wheelchairs to convert them into ""smart"" wheelchairs which can assist the user in navigation and obstacle avoidance. During Phase I, a prototype of the Smart Wheelchair Component System (SWCS) was developed from a laptop computer and a collection of sonar, infrared and bump sensors. The evaluation activities performed during Phase I demonstrated that the system is compatible with multiple brands of wheelchairs, can accept both continuous and switch-based input, and can support front-, mid-, and rear-wheel drive wheelchairs. During Phase II, we propose to refine the system hardware and software; replace the laptop computer with an embedded microprocessor; fabricate enclosures for the system components; and develop tools to support clinicians in installing and configuring the system. The system will be evaluated in tests involving potential users, clinicians, and wheelchair design standards. The final product will be a market-ready modular system which can be attached to a variety of standard power wheelchairs. This product has the potential to increase the independence and quality of life of many wheelchair users and potential wheelchair users whose disabilities limit their capacity for independent wheelchair navigation.       n/a",Smart Wheelchair Component System,7237214,R44HD040023,"['Adult', 'Age', 'Child', 'Client', 'Cognitive deficits', 'Collection', 'Communities', 'Compatible', 'Computer Vision Systems', 'Computer software', 'Computers', 'Condition', 'Destinations', 'Development', 'Disabled Persons', 'Disadvantaged', 'Documentation', 'Equipment', 'Evaluation', 'Future', 'Goals', 'Individual', 'Joystick', 'Laboratories', 'Learning', 'Location', 'Locomotion', 'Manufacturer Name', 'Marketing', 'Methods', 'Microprocessor', 'Numbers', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Powered wheelchair', 'Production', 'Quality of life', 'Range', 'Relative (related person)', 'Research Personnel', 'Robot', 'Self Perception', 'Standards of Weights and Measures', 'System', 'Technology', 'Testing', 'Touch sensation', 'Travel', 'Tremor', 'Visual Fields', 'Visual impairment', 'Wheelchairs', 'Work', 'base', 'data acquisition', 'design', 'disability', 'laptop', 'member', 'neglect', 'peer', 'prototype', 'sensor', 'sonar', 'tool']",NICHD,AT SCIENCES,R44,2007,387828,0.040682129835177876
"Mobile Food Intake Visualization and Voice Recognize (FIVR) Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives. n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7490204,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2007,3000,0.047657950178760004
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7340845,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2007,1039742,0.047657950178760004
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7326673,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2007,386674,0.0347104704599304
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7271209,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Condition', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Experimental Models', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2007,240641,-0.010710456440584135
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,7004518,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2006,457462,0.0656804960094268
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7143942,R21EY017003,"['clinical research', 'computers', 'reading', 'semantics', 'touch', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,224601,0.049878723790243655
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,7096566,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,214738,0.0316929143634878
"MobileEye OCR for the Visually Impaired    DESCRIPTION (provided by applicant): In this SBIR we propose to demonstrate the technical feasibility of Mobile OCR, a portable software system which makes use of existing personal devices to provide access to textual materials for the elderly or the visually impaired. The system will help these low vision individuals with basic daily activities, such as shopping, preparing meals, taking medication, and reading traffic signs. It will step beyond our proposed MobileEyes vision enhancement system to apply cutting edge recognition technology for mobile devices. The system will use common camera phone hardware to capture and enhance textual information, perform Optical Character Recognition (OCR) and provide audio or visual feedback. Our research will focus on implementing and integrating new vision enhancement and analysis techniques on limited resource mobile devices. Specifically, we will develop algorithms for detection and rectification of text on planes and generalized cylinders subject to perspective distortions, implement more robust and efficient algorithms and systems for stabilization and enhancement of text blocks, provide mobile OCR on complex textured backgrounds, and implement these techniques on small devices across a variety of platforms. The recognized text will be presented through Text-to-Speech (TTS), or displayed on the device with enhanced quality which can be easily read by low vision users. Phase I will focus on demonstrating the technical feasibility of our approach, and will incorporate a performance measurement methodology to quantitatively evaluate progress and evaluate our system against other approaches. In comparison to existing vision enhancement devices, such as magnifying glasses, telescopes, and text reading devices such as scanner-based OCR, our solution has several advantages: 1) it makes use of a single, portable device (camera cell phone) that is commonly available and typically already carried for its telecommunications capabilities; 2) it can be used selectively by users so they will not be overwhelmed by irrelevant information; and 3) it can be integrated directly with other applications for specialized tasks. Our research results will impact the millions of low-vision individuals and the blind, as well as vision and computer vision researchers. Our team is uniquely qualified to explore the feasibility of extending visual applications to these devices, and provide a platform for integrating future vision algorithms.         n/a",MobileEye OCR for the Visually Impaired,7053650,R43EY017216,"['reading', 'solutions', 'vision']",NEI,"APPLIED MEDIA ANALYSIS, LLC",R43,2006,104935,0.0662119827018268
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6995047,R43EY014487,"['artificial intelligence', 'biomedical equipment development', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection', 'digital imaging', 'functional ability', 'human subject', 'image processing', 'medical rehabilitation related tag', 'patient oriented research', 'portable biomedical equipment', 'questionnaires', 'vision aid', 'vision disorders', 'visual fields', 'visual perception', 'visual threshold', 'visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2005,144106,0.05265326048231854
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6832762,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2005,461157,0.0656804960094268
"FINGER COORDINATION IN ELDERLY Impairment of hand function with age is a major disabling factor limiting many activities of daily living.  A decrease in the hand dexterity may lead to a loss of independence and require external care during most everyday activities.  Our present understanding of the functioning of the human hand and changes in its function with age is limited.  This makes it imperative to perform studies directed at improved understanding of the hand function and its deterioration with age. Series of studies on control subjects and preliminary findings in a small group of elderly subjects have allowed us to formulate two major hypotheses: (a) Aging is associated with selective impairment of intrinsic hand muscles; and (b) Aging is associated with an increase in force deficit during multi-finger tasks.  The following research program has been elaborated for an analysis of possible contribution of these age-related changes to the loss of hand dexterity with age: (1) Confirmation of the preliminary conclusions using a larger subject population and a wider spectrum of tasks; (2) Analysis of the effects of these changes on crucial features of multi-finger synergies such as minimization of secondary moments and patterns of variability of individual finger forces (error compensation among fingers); (3) Analysis of possible changes in finger and hand coordination during more common actions such as gripping and bi-manual object handling; and (4) Analysis of correlations between changes in the indices of finger coordination and activities of daily living that involve hand function. Seven experiments on elderly and control subjects are suggested that include force production by sets of fingers in gripping and pressing tasks, self-imposed perturbations, and bi-manual object handling.  The research will combine noninvasive behavioral, biomechanical, electrophysiological, and modeling methods, as well as questionnaires.  In one experiment, the results will be compared with indices of the activities of daily living. Disprovable predictions are made for each experiment.  We believe that supporting or disproving the two main Hypotheses is important not only for a better understanding of age-related changes in hand muscle coordination but also for development of new rehabilitation approaches to hand function.  n/a",FINGER COORDINATION IN ELDERLY,6940629,R01AG018751,"['aging', 'artificial intelligence', 'body movement', 'clinical research', 'computer system design /evaluation', 'hand', 'human old age (65+)', 'human subject', 'psychomotor function']",NIA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2005,231673,-0.00018758579903522092
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,6920594,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2005,255198,0.0316929143634878
"Just in Time Information for Exercise Adoption DESCRIPTION (provided by applicant):     A novel personal digital assistant (PDA)-based system that can automatically detect bouts of moderate or greater walking and deliver health behavior change information to users to increase their levels of physical activity will be developed and evaluated. The system will be an extension of work already performed by the investigators, and will incorporate a validated wireless motion sensor, pattern classification software to identify bouts of walking, and a personified, relational user interface designed to maintain engagement and trust in the tailored behavior change information delivered to users over multiple interactions. The system will be designed to be worn and used continuously by free-living populations and provide users with health behavior change information at the moment it is needed.      Users will interact with the PDA via a simulated face-to-face conversation with the animated relational agent, and will conduct a daily progress review and goal-setting session at which time they will schedule specific times they intend to walk on the following day (bouts of 10 minutes or more of moderate or greater intensity). If they complete a scheduled walk, the agent provides immediate social reinforcement. If they fail to initiate a walk at a scheduled time the agent engages them in a problem-solving session in which it attempts to help them overcome the specific obstacle to exercise they are experiencing.       In the proposed effort the components of the PDA-based system will be developed, integrated and tested, and a randomized pilot study conducted to: 1) evaluate the efficacy of the PDA-based behavior change intervention for increasing walking; 2) evaluate the effect of timeliness of health behavior change information on walking (time of need vs. retrospective); and 3) compare the efficacy of the personified user interface with that of a text-based interface for delivering health behavior change information on a PDA.      The proposed work will make significant contributions to several areas within the science of medical informatics, extending and integrating work in knowledge representation, bio-signal analysis, natural language processing, and consumer health informatics. This research will advance our understanding of the role of time in health behavior change, and result in a model of the temporal relationships among sensor data, user behavior, user goals, and the delivery of health information intended to change behavior. n/a",Just in Time Information for Exercise Adoption,7127056,R21LM008553,[' '],NLM,NORTHEASTERN UNIVERSITY,R21,2005,105924,0.030461156901530707
"Just in Time Information for Exercise Adoption DESCRIPTION (provided by applicant):     A novel personal digital assistant (PDA)-based system that can automatically detect bouts of moderate or greater walking and deliver health behavior change information to users to increase their levels of physical activity will be developed and evaluated. The system will be an extension of work already performed by the investigators, and will incorporate a validated wireless motion sensor, pattern classification software to identify bouts of walking, and a personified, relational user interface designed to maintain engagement and trust in the tailored behavior change information delivered to users over multiple interactions. The system will be designed to be worn and used continuously by free-living populations and provide users with health behavior change information at the moment it is needed.      Users will interact with the PDA via a simulated face-to-face conversation with the animated relational agent, and will conduct a daily progress review and goal-setting session at which time they will schedule specific times they intend to walk on the following day (bouts of 10 minutes or more of moderate or greater intensity). If they complete a scheduled walk, the agent provides immediate social reinforcement. If they fail to initiate a walk at a scheduled time the agent engages them in a problem-solving session in which it attempts to help them overcome the specific obstacle to exercise they are experiencing.       In the proposed effort the components of the PDA-based system will be developed, integrated and tested, and a randomized pilot study conducted to: 1) evaluate the efficacy of the PDA-based behavior change intervention for increasing walking; 2) evaluate the effect of timeliness of health behavior change information on walking (time of need vs. retrospective); and 3) compare the efficacy of the personified user interface with that of a text-based interface for delivering health behavior change information on a PDA.      The proposed work will make significant contributions to several areas within the science of medical informatics, extending and integrating work in knowledge representation, bio-signal analysis, natural language processing, and consumer health informatics. This research will advance our understanding of the role of time in health behavior change, and result in a model of the temporal relationships among sensor data, user behavior, user goals, and the delivery of health information intended to change behavior. n/a",Just in Time Information for Exercise Adoption,6852192,R21LM008553,"['actigraphy', 'behavioral /social science research tag', 'biomedical equipment development', 'body physical activity', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'exercise', 'health behavior', 'health education', 'human subject', 'human therapy evaluation', 'information display', 'miniature biomedical equipment', 'patient monitoring device', 'personal computers', 'personal log /diary']",NLM,BOSTON MEDICAL CENTER,R21,2005,79484,0.030461156901530707
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6910762,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2005,611743,-0.011626451976446201
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6739928,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2004,462571,0.0656804960094268
"FINGER COORDINATION IN ELDERLY Impairment of hand function with age is a major disabling factor limiting many activities of daily living.  A decrease in the hand dexterity may lead to a loss of independence and require external care during most everyday activities.  Our present understanding of the functioning of the human hand and changes in its function with age is limited.  This makes it imperative to perform studies directed at improved understanding of the hand function and its deterioration with age. Series of studies on control subjects and preliminary findings in a small group of elderly subjects have allowed us to formulate two major hypotheses: (a) Aging is associated with selective impairment of intrinsic hand muscles; and (b) Aging is associated with an increase in force deficit during multi-finger tasks.  The following research program has been elaborated for an analysis of possible contribution of these age-related changes to the loss of hand dexterity with age: (1) Confirmation of the preliminary conclusions using a larger subject population and a wider spectrum of tasks; (2) Analysis of the effects of these changes on crucial features of multi-finger synergies such as minimization of secondary moments and patterns of variability of individual finger forces (error compensation among fingers); (3) Analysis of possible changes in finger and hand coordination during more common actions such as gripping and bi-manual object handling; and (4) Analysis of correlations between changes in the indices of finger coordination and activities of daily living that involve hand function. Seven experiments on elderly and control subjects are suggested that include force production by sets of fingers in gripping and pressing tasks, self-imposed perturbations, and bi-manual object handling.  The research will combine noninvasive behavioral, biomechanical, electrophysiological, and modeling methods, as well as questionnaires.  In one experiment, the results will be compared with indices of the activities of daily living. Disprovable predictions are made for each experiment.  We believe that supporting or disproving the two main Hypotheses is important not only for a better understanding of age-related changes in hand muscle coordination but also for development of new rehabilitation approaches to hand function.  n/a",FINGER COORDINATION IN ELDERLY,6785858,R01AG018751,"['aging', 'artificial intelligence', 'body movement', 'clinical research', 'computer system design /evaluation', 'hand', 'human old age (65+)', 'human subject', 'psychomotor function']",NIA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2004,231788,-0.00018758579903522092
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6801171,R01EY013875,"['blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer program /software', 'cues', 'human subject', 'reading', 'vision aid', 'vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,329706,0.06899487448021248
"Micro-environment Glasses as a Treatment for CVS DESCRIPTION (provided by applicant) Computer Vision Syndrome (CVS) refers to a collection of eye problems associated with computer use, and about three-quarters of computer users have it. Conservative estimates indicate that over $2 billion is currently spent on examinations and special eyewear for CVS treatment. The most common symptoms of CVS include: eyestrain or eye fatigue, dry eyes, burning eyes, sensitivity to light, and blurred vision. Non-ocular symptoms include headaches, pain in the shoulders, neck, or back. As diverse as the symptoms are, they may be related and can be subdivided into to three potential pathophysiological causes:   1) Ocular Surface Mechanisms   2) Accommodative Mechanisms   3) Extra-Ocular Mechanisms   There is a significant gap in the fund of knowledge regarding the diagnosis of this disease. In the near-term, we plan to focus on the ocular surface category of disorders as a cause of CVS, identify clinical conditions associated with this syndrome and develop a treatment that addresses this cause. In phase 1, we propose to:   Clinically define CVS by observing the incidence of ocular surface abnormalities in symptomatic subjects and compare them with an age and sex matched non-symptomatic control population   Develop specialized micro-environment glasses to combat CVS symptoms   Study the efficacy of micro-environment glasses in symptomatic and control populations   Critically evaluate viability of CVS micro-environment glasses as a commercial product      using both statistical methods and subjective questionnaires.            n/a",Micro-environment Glasses as a Treatment for CVS,6792878,R41EY015023,"['age difference', 'bioengineering /biomedical engineering', 'biomedical equipment development', 'clinical biomedical equipment', 'clinical research', 'computers', 'data collection methodology /evaluation', 'eye disorder diagnosis', 'gender difference', 'human subject', 'keratoconjunctivitis sicca', 'occupational health /safety', 'portable biomedical equipment', 'questionnaires', 'syndrome', 'vision aid', 'vision disorders', 'visual photosensitivity', 'work site']",NEI,"SEEFIT, INC.",R41,2004,100000,-0.0008566269309297831
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6766751,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2004,593925,-0.011626451976446201
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6710523,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,139234,0.05265326048231854
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6665322,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,245656,0.05265326048231854
"FINGER COORDINATION IN ELDERLY Impairment of hand function with age is a major disabling factor limiting many activities of daily living.  A decrease in the hand dexterity may lead to a loss of independence and require external care during most everyday activities.  Our present understanding of the functioning of the human hand and changes in its function with age is limited.  This makes it imperative to perform studies directed at improved understanding of the hand function and its deterioration with age. Series of studies on control subjects and preliminary findings in a small group of elderly subjects have allowed us to formulate two major hypotheses: (a) Aging is associated with selective impairment of intrinsic hand muscles; and (b) Aging is associated with an increase in force deficit during multi-finger tasks.  The following research program has been elaborated for an analysis of possible contribution of these age-related changes to the loss of hand dexterity with age: (1) Confirmation of the preliminary conclusions using a larger subject population and a wider spectrum of tasks; (2) Analysis of the effects of these changes on crucial features of multi-finger synergies such as minimization of secondary moments and patterns of variability of individual finger forces (error compensation among fingers); (3) Analysis of possible changes in finger and hand coordination during more common actions such as gripping and bi-manual object handling; and (4) Analysis of correlations between changes in the indices of finger coordination and activities of daily living that involve hand function. Seven experiments on elderly and control subjects are suggested that include force production by sets of fingers in gripping and pressing tasks, self-imposed perturbations, and bi-manual object handling.  The research will combine noninvasive behavioral, biomechanical, electrophysiological, and modeling methods, as well as questionnaires.  In one experiment, the results will be compared with indices of the activities of daily living. Disprovable predictions are made for each experiment.  We believe that supporting or disproving the two main Hypotheses is important not only for a better understanding of age-related changes in hand muscle coordination but also for development of new rehabilitation approaches to hand function.  n/a",FINGER COORDINATION IN ELDERLY,6637831,R01AG018751,"['aging', ' artificial intelligence', ' body movement', ' clinical research', ' computer system design /evaluation', ' hand', ' human old age (65+)', ' human subject', ' psychomotor function']",NIA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2003,231899,-0.00018758579903522092
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6666671,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,327524,0.06899487448021248
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6635595,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2003,576626,-0.011626451976446201
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6580977,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2002,246164,0.05265326048231854
"FINGER COORDINATION IN ELDERLY Impairment of hand function with age is a major disabling factor limiting many activities of daily living.  A decrease in the hand dexterity may lead to a loss of independence and require external care during most everyday activities.  Our present understanding of the functioning of the human hand and changes in its function with age is limited.  This makes it imperative to perform studies directed at improved understanding of the hand function and its deterioration with age. Series of studies on control subjects and preliminary findings in a small group of elderly subjects have allowed us to formulate two major hypotheses: (a) Aging is associated with selective impairment of intrinsic hand muscles; and (b) Aging is associated with an increase in force deficit during multi-finger tasks.  The following research program has been elaborated for an analysis of possible contribution of these age-related changes to the loss of hand dexterity with age: (1) Confirmation of the preliminary conclusions using a larger subject population and a wider spectrum of tasks; (2) Analysis of the effects of these changes on crucial features of multi-finger synergies such as minimization of secondary moments and patterns of variability of individual finger forces (error compensation among fingers); (3) Analysis of possible changes in finger and hand coordination during more common actions such as gripping and bi-manual object handling; and (4) Analysis of correlations between changes in the indices of finger coordination and activities of daily living that involve hand function. Seven experiments on elderly and control subjects are suggested that include force production by sets of fingers in gripping and pressing tasks, self-imposed perturbations, and bi-manual object handling.  The research will combine noninvasive behavioral, biomechanical, electrophysiological, and modeling methods, as well as questionnaires.  In one experiment, the results will be compared with indices of the activities of daily living. Disprovable predictions are made for each experiment.  We believe that supporting or disproving the two main Hypotheses is important not only for a better understanding of age-related changes in hand muscle coordination but also for development of new rehabilitation approaches to hand function.  n/a",FINGER COORDINATION IN ELDERLY,6533901,R01AG018751,"['aging', ' artificial intelligence', ' body movement', ' clinical research', ' computer system design /evaluation', ' hand', ' human old age (65+)', ' human subject', ' psychomotor function']",NIA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2002,232006,-0.00018758579903522092
"COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION There are approximately 2.5 million people in the US who are speech impaired to the extent that it is considered a functional limitation.  Today, many people with severe communication disabilities lack access to electronic and even printed material, have a lack of opportunity for interaction and opportunity for self-advocacy, and experience isolation.  Providing accessibility to wireless voice, data and Internet communications directly from the device is of tremendous importance to people with severe communication disabilities. The primary objective under the Phase I grant was to investigate the potential of word-level disambiguation technology for text generation on a communication aid to meet the needs of many individuals requiring augmentative and alternative communication (AAC).  Phase I findings validated T9 technology as a viable method for text generation and also AAC device users want to access wireless voice, data and Internet communications directly from the device.  The specific aims of Phase II are to: investigate hardware platforms for AAC device host candidates, develop Windows software modules for T9 and AAC, develop portable AAC resources, integrate wireless voice and data communications, and conduct usability testing of the product as it develops. PROPOSED COMMERCIAL APPLICATION The outcome of Phase II will be a communication aid device for production in Phase III that is based on commercially available hardware with minimal custom AAC hardware support.  The goal is that the software platform developed in Phase II will be ported to existing low-cost hardware as much as possible to: make use of newly developed platforms, provide more choice to AAC device users, provide more flexibility in user interface, and reduce overall device costs to the end user when commercially manufactured.  n/a",COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION,6569890,R44RR013191,"['Internet', ' artificial intelligence', ' biomedical equipment development', ' clinical biomedical equipment', ' clinical research', ' communication disorder aid', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' human subject', ' online computer', ' vocabulary', ' voice']",NCRR,"MADENTEC (USA), INC.",R44,2002,139082,0.03861662630489331
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6547549,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2002,338540,0.06899487448021248
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6518379,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2002,559831,-0.011626451976446201
"FINGER COORDINATION IN ELDERLY Impairment of hand function with age is a major disabling factor limiting many activities of daily living.  A decrease in the hand dexterity may lead to a loss of independence and require external care during most everyday activities.  Our present understanding of the functioning of the human hand and changes in its function with age is limited.  This makes it imperative to perform studies directed at improved understanding of the hand function and its deterioration with age. Series of studies on control subjects and preliminary findings in a small group of elderly subjects have allowed us to formulate two major hypotheses: (a) Aging is associated with selective impairment of intrinsic hand muscles; and (b) Aging is associated with an increase in force deficit during multi-finger tasks.  The following research program has been elaborated for an analysis of possible contribution of these age-related changes to the loss of hand dexterity with age: (1) Confirmation of the preliminary conclusions using a larger subject population and a wider spectrum of tasks; (2) Analysis of the effects of these changes on crucial features of multi-finger synergies such as minimization of secondary moments and patterns of variability of individual finger forces (error compensation among fingers); (3) Analysis of possible changes in finger and hand coordination during more common actions such as gripping and bi-manual object handling; and (4) Analysis of correlations between changes in the indices of finger coordination and activities of daily living that involve hand function. Seven experiments on elderly and control subjects are suggested that include force production by sets of fingers in gripping and pressing tasks, self-imposed perturbations, and bi-manual object handling.  The research will combine noninvasive behavioral, biomechanical, electrophysiological, and modeling methods, as well as questionnaires.  In one experiment, the results will be compared with indices of the activities of daily living. Disprovable predictions are made for each experiment.  We believe that supporting or disproving the two main Hypotheses is important not only for a better understanding of age-related changes in hand muscle coordination but also for development of new rehabilitation approaches to hand function.  n/a",FINGER COORDINATION IN ELDERLY,6369619,R01AG018751,"['aging', ' artificial intelligence', ' body movement', ' clinical research', ' computer system design /evaluation', ' hand', ' human old age (65+)', ' human subject', ' psychomotor function']",NIA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2001,308867,-0.00018758579903522092
"Advanced Vision Intervention Algorithm(AVIA)   Description (from the investigator's abstract): The objective of this                application is to implement an iterative, nine-step advanced vision                  intervention algorithm (AVIA) in software to optimize the predictability of          virtually any current or anticipated customized human vision intervention            method. The software program will use the investigator's Visual Optics class         library, as well as new software for the ray transfer element, database              analysis routines, and the ray tracing surface optimization algorithm. The           program will allow, but not require, exam data from commercially available           ophthalmic instruments such as corneal topography and wavefront aberration for       input in the optical modeling of an individual's eye. This algorithm is, to the      investigator's knowledge, the only formal framework designed specifically to         optimize the predictability of surgical and non-surgical correction methods. It      is not only a technological innovation in its own right, it also makes the most      of the current and future vision correction methods to which it is applied.          PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                     n/a",Advanced Vision Intervention Algorithm(AVIA),6403968,R43EY013666,"['artificial intelligence', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' eye surgery', ' laser therapy', ' ophthalmoscopy', ' statistics /biometry', ' vision disorders']",NEI,"SARVER AND ASSOCIATES, INC.",R43,2001,99785,-0.003773062345645529
"COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION There are approximately 2.5 million people in the US who are speech impaired to the extent that it is considered a functional limitation.  Today, many people with severe communication disabilities lack access to electronic and even printed material, have a lack of opportunity for interaction and opportunity for self-advocacy, and experience isolation.  Providing accessibility to wireless voice, data and Internet communications directly from the device is of tremendous importance to people with severe communication disabilities. The primary objective under the Phase I grant was to investigate the potential of word-level disambiguation technology for text generation on a communication aid to meet the needs of many individuals requiring augmentative and alternative communication (AAC).  Phase I findings validated T9 technology as a viable method for text generation and also AAC device users want to access wireless voice, data and Internet communications directly from the device.  The specific aims of Phase II are to: investigate hardware platforms for AAC device host candidates, develop Windows software modules for T9 and AAC, develop portable AAC resources, integrate wireless voice and data communications, and conduct usability testing of the product as it develops. PROPOSED COMMERCIAL APPLICATION The outcome of Phase II will be a communication aid device for production in Phase III that is based on commercially available hardware with minimal custom AAC hardware support.  The goal is that the software platform developed in Phase II will be ported to existing low-cost hardware as much as possible to: make use of newly developed platforms, provide more choice to AAC device users, provide more flexibility in user interface, and reduce overall device costs to the end user when commercially manufactured.  n/a",COMMUNICATION AID UTILIZING WORD LEVEL DISAMBIGUATION,6188611,R44RR013191,"['Internet', ' artificial intelligence', ' biomedical equipment development', ' clinical biomedical equipment', ' clinical research', ' communication disorder aid', ' computer program /software', ' computer system design /evaluation', ' computer system hardware', ' human subject', ' online computer', ' vocabulary', ' voice']",NCRR,"MADENTEC (USA), INC.",R44,2001,316991,0.03861662630489331
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6346620,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2001,527694,-0.011626451976446201
"SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS In this Phase l proposal we plan to develop and test a new vision technology to locat and read general informational signs (street names, building directories, office door plates) and location and directional signs (EXIT, Information, aisle signs in supermarkets). To strengthen feasibility, we will target a restricted class of signs: those consisting primarily of one- color text on a different one-color background, and whose shape falls within a prescribed set. The intended market is for people who are blind or whose sight is impaired and hence cannot read these signs unaided. Our approach makes extensive use of recently developed computer vision recognition algorithms. We also make use of the Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for determining what the potential users will require from such a system. The ultimate goal, for Phase II, is to build and test a highly portable PC- based device implementing this vision technology using a CCD camera as input and a voice-generator as output. The user would scan/point the device at a scene and it would locate and read one or more signs. Given the pace of increase in power and decrease in size of computing devices, a hand-held Sign-Finder system may be plausible to build entirely with commercial, off-the-shelf hardware in two to three years. PROPOSED COMMERCIAL APPLICATION: The potential utility to blind and visually impaired individuals is great; a commercial product could have a market potential of 500,000.  n/a",SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS,2720318,R43EY011821,"['artificial intelligence', ' blind aid', ' charge coupled device camera', ' computer graphics /printing', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' information display', ' portable biomedical equipment', ' symbolism', ' technology /technique development', ' vision aid']",NEI,BLINDSIGHT CORPORATION,R43,2000,100000,0.05747680724988155
"A computer vision toolbox for computational analysis of nonverbal social communication PROJECT SUMMARY We will develop novel computer vision tools to reliably and precisely measure nonverbal social communication through quantifying communicative facial and bodily expressions. Our tools will be designed and developed in order to maximize their usability by non-engineer behavioral scientists, filling the enormous gap between engineering advances and their clinical accessibility. Significance: Social interaction inherently relies on perception and production of coordinated face and body expressions. Indeed, atypical face and body movements are observed in many disorders, impacting social interaction and communication. Traditional systems for quantifying nonverbal communication (e.g., FACS, BAP) require extensive training and coding time. Their tedious coding requirements drastically limits their scalability and reproducibility. While an extensive literature exists on advanced computer vision and machine learning techniques for face and body analysis, there is no well-established method commonly used in mental health community to quantify production of facial and bodily expressions or efficiently capture individual differences in nonverbal communication in general. As a part of this proposal, we will develop a computer vision toolbox including tools that are both highly granular and highly scalable, to allow for measurement of complex social behavior in large and heterogeneous populations. Approach: Our team will develop tools that provide granular metrics of nonverbal social behavior, including localized face and body kinematics, characteristics of elicited expressions, and imitation performance. Our tools will facilitate measurement of social communication both within a person and between people, to allow for assessment of individual social communication cues as well as those that occur within bidirectional social contexts. Preliminary Data: We have developed and applied novel computer vision tools to assess: (1) diversity of mouth motion during conversational speech (effect size d=1.0 in differentiating young adults with and without autism during a brief natural conversation), (2) interpersonal facial coordination (91% accuracy in classifying autism diagnosis in young adults during a brief natural conversation, replicated in an independent child sample), and (3) body action imitation (85% accuracy in classifying autism diagnosis based on body imitation performance). As apart of current proposal, we will develop more generic methods that can be used in normative and clinical samples. Aims. In Aim 1, we will develop tools to automatically quantify fine-grained face movements and their coordination during facial expression production; in Aim 2, we will develop tools to quantify body joint kinematics and their coordination during bodily expression production; in Aim 3, we will demonstrate the tools’ ability to yield dimensional metrics using machine learning. Impact: Our approach is designed for fast and rigorous assessment of nonverbal social communication, providing a scalable solution to measure individual variability, within a dimensional and transdiagnostic framework. PROJECT NARRATIVE This project develops novel tools for measuring nonverbal social communication as manifested through facial and bodily expressions. Using advanced computer vision and machine learning methodologies, we will quantify humans’ communicative social behavior. The results of this project will impact public health by facilitating a rich characterization of normative development of social functioning, providing access to precise phenotypic information for neuroscience and genetics studies, and by measuring subtle individual differences to determine whether some interventions or treatments work better than others.",A computer vision toolbox for computational analysis of nonverbal social communication,9946780,R01MH122599,"['Adolescent', 'Age', 'Area', 'Behavior', 'Behavioral', 'Behavioral Research', 'Behavioral Sciences', 'Characteristics', 'Child', 'Clinical', 'Code', 'Communication', 'Community Health', 'Complex', 'Computational Technique', 'Computer Analysis', 'Computer Vision Systems', 'Cues', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Disease', 'Educational Materials', 'Engineering', 'Expression Profiling', 'Face', 'Facial Expression', 'Genetic study', 'Goals', 'Gold', 'Grain', 'Grant', 'Human', 'Individual', 'Individual Differences', 'Intervention', 'Joints', 'Literature', 'Machine Learning', 'Measurement', 'Measures', 'Mental Health', 'Methodology', 'Methods', 'Motion', 'Movement', 'Nature', 'Neurologic', 'Neurosciences', 'Nonverbal Communication', 'Oral cavity', 'Participant', 'Perception', 'Performance', 'Persons', 'Phenotype', 'Population Heterogeneity', 'Production', 'Public Health', 'Publishing', 'Reproducibility', 'Research', 'Research Personnel', 'Sampling', 'Science', 'Scientist', 'Sex Differences', 'Social Behavior', 'Social Development', 'Social Environment', 'Social Functioning', 'Social Interaction', 'Speech', 'System', 'Techniques', 'Teenagers', 'Time', 'Training', 'Translations', 'Validation', 'Work', 'analysis pipeline', 'autism spectrum disorder', 'automated algorithm', 'base', 'behavior measurement', 'behavioral health', 'clinical application', 'computerized tools', 'design', 'individual variation', 'interest', 'kinematics', 'novel', 'open source', 'sex', 'social', 'social communication', 'tool', 'usability', 'young adult']",NIMH,CHILDREN'S HOSP OF PHILADELPHIA,R01,2020,150000,-0.0008606099697264902
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,10022332,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'comorbidity', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2020,243885,0.012995932628316925
"SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder  Intellectual Merit: This project will for the first time provide the fundamental tools to integrate unique multimodal data toward screening, diagnosis, and intervention in eating disorders, with an initial focus on children with ARFID and related developmental and health disorders. This work is critical for enriching the understanding of healthy development and for broadening the foundations of behavioral data science. ARFID ·motivates the development of new computer vision and data analysis tools critical for the analysis of multidimensional behavioral data. The main aims are: 1. Develop and user individualized and integrated continuous facial affect coding from videos to discern affective motivations for food avoidance, critical due to the unique sensory aspects of eating disorders, and resulting from active stimulation via friendly and carefully designed images/videos and real food presentation; 2. Use data analysis and machine learning to derive sensory profiles based on patterns of food consumption and preference from existing unique datasets of selective eaters; and 3. Translate the tools developed in Aims 1 and 2 into the clinic and home to assess the capacity of these tools to define a threshold of clinically significant food avoidance, to detect change in acceptability of food with repeated presentations, and to examine and modify the accuracy of our food suggestion algorithms. Broader Impacts: The impact of this application comprises two broad domains. First is the derivation of processes, tools, and strategies to analyze very disparate data across multiple levels of analysis and to codify those strategies to inform similar future work, in particular incorporating automatic behavioral coding. Second is the exploitation of these tools to address questions about the emergence of healthy/unhealthy food selectivity across the lifespan, including recommendation delivery via apps and at-home recordings. The health impact of even partial success in this project is very broad and significant. Undergraduate students will be involved in this project via the 6-weeks summer research program at the Information Initiative at Duke, a center dedicated to the fundamentals of data science and its applications; via the co-Pl's research lab devoted to eating disorders; and via the Pl's project dedicated to training undergraduate students to address eating disorders of their friends via an anonymous app. Outreach and dissemination will follow the broad use of the developed app, both in the clinic and the general population, including the Pl's connections with low-income and under-represented bi-lingual preK. RELEVANCE (See instructions): Eating disorders are potentially life-threatening mental illnesses affecting the general population; -90% of individuals never receive treatment, in part due to lack of awareness and access. Individuals with eating disorders experience a diminished quality of life, high mental and physical illness comorbidities, and an existence marked by profound loneliness and isolation. Combining expertise in eating disorders with computer vision and machine learning, we bring for the first time data science to this health challenge. PROJECT/PERFORMANCE S1TE(S) (If addItIonal space Is needed use Project/Performance Stte Format Page) n/a",SCH: INT: Computational Tools for Avoidaint/Restrictive Food Intake Disorder ,10228145,R01MH122370,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anxiety', 'Assessment tool', 'Attention', 'Awareness', 'Behavior Therapy', 'Behavioral', 'Caregivers', 'Child', 'Childhood', 'Clinic', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Depressed mood', 'Derivation procedure', 'Development', 'Diagnosis', 'Diet', 'Disease', 'Distress', 'Eating', 'Eating Disorders', 'Emotional', 'Emotions', 'Evolution', 'Exposure to', 'Face', 'Family', 'Food', 'Food Patterns', 'Food Preferences', 'Foundations', 'Friends', 'Fright', 'Future', 'General Population', 'Goals', 'Health', 'Health Personnel', 'Home environment', 'Image', 'Impairment', 'Individual', 'Industry', 'Instruction', 'Intervention', 'Life', 'Link', 'Literature', 'Loneliness', 'Longevity', 'Low income', 'Machine Learning', 'Maps', 'Mathematics', 'Measures', 'Mental disorders', 'Monitor', 'Motion', 'Motivation', 'Parents', 'Performance', 'Phenotype', 'Primary Health Care', 'Process', 'Psyche structure', 'Quality of life', 'Reaction', 'Recommendation', 'Research', 'Scientist', 'Sensory', 'Severities', 'Smell Perception', 'Standardization', 'Structure', 'Suggestion', 'System', 'Taste aversion', 'Time', 'Training', 'Translating', 'Uncertainty', 'Work', 'analytical tool', 'base', 'behavior change', 'clinically significant', 'comorbidity', 'computerized tools', 'design', 'experience', 'food avoidance', 'food consumption', 'gaze', 'improved', 'indexing', 'mathematical algorithm', 'multimodal data', 'novel', 'outreach', 'precision medicine', 'preference', 'programs', 'relating to nervous system', 'response', 'screening', 'success', 'summer research', 'tool', 'undergraduate student', 'wasting', 'willingness']",NIMH,DUKE UNIVERSITY,R01,2020,59206,0.012995932628316925
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9899994,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,416374,0.05424467566540907
"Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit SUMMARY  Many of the estimated four million adults in the U.S. with severe speech and physical impairments (SSPI) resulting from neurodevelopmental or neurodegenerative diseases cannot rely on current assistive technologies (AT) for communication. During a single day, or as their disease progresses, they may transition from one access technology to another due to fatigue, medications, changing physical status, or progressive motor dysfunction. There are currently no clinical or AT solutions that adapt to the multiple, dynamic access needs of these individuals, leaving many people poorly served. This competitive renewal, called BCI-FIT (Brain Computer Interface-Functional Implementation Toolkit) adds to our innovative multidisciplinary translational research conducted over the past 11 years for the advancement of science related to non-invasive BCIs for communication for these clinical populations. BCI-FIT relies on active inference and transfer learning to customize a completely adaptive intent estimation classifier to each user's multiple modality signals in real-time. The BCI-FIT acronym has many implications: our BCI fits to each user's brain signals; to the environment, offering relevant personal language; to the user's internal states, adjusting signals based on drowsiness, medications, physical and cognitive abilities; and to users' learning patterns from BCI introduction to expert use.  Three specific aims are proposed: (1) Develop and evaluate methods for optimizing system and user performance with on-line, robust adaptation of multi-modal signal models. (2) Develop and evaluate methods for efficient user intent inference through active querying. (3) Integrate language interaction and letter/word supplementation as input modalities in real-time BCI use. Four single case experimental research designs will evaluate both user performance and technology performance for functional communication with 35 participants with SSPI in the community, and 30 healthy controls for preliminary testing. The same dependent variables will be tested in all experiments: typing accuracy (correct character selections divided by total character selections), information transfer rate (ITR), typing speed (correct characters/minute), and user experience (UX) questionnaire responses about comfort, workload, and satisfaction. Our goal is to establish individualized recommendations for each user based on a combination of clinical and machine expertise. The clinical expertise plus user feedback added to active sensor fusion and reinforcement learning for intent inference will produce optimized multi-modal BCIs for each end-user that can adjust to short- and long-term fluctuating function. Our research is conducted by four sub-teams who have collaborated successfully to implement translational science: Electrical/computer engineering; Neurophysiology and systems science; Natural language processing; and Clinical rehabilitation. The project is grounded in solid machine learning approaches with models of participatory action research and AAC participation. This project will improve technologies and BCI technical capabilities, demonstrate BCI implementation paradigms and clinical guidelines for people with severe disabilities. PROJECT NARRATIVE The populations of US citizens with severe speech and physical impairments secondary to neurodevelopmental and neurodegenerative diseases are increasing as medical technologies advance and successfully support life. These individuals with limited to no movement could potentially contribute to their medical decision making, informed consent, and daily caregiving if they had faster, more reliable means that adapt to their best access methods in communication technologies, as proposed in BCI-FIT. This project implements the translation of basic computer science and engineering into clinical care, supporting the proposed NIH Roadmap and public health initiatives.",Optimizing BCI-FIT: Brain Computer Interface - Functional Implementation Toolkit,10044301,R01DC009834,"['Adult', 'Attention', 'Behavioral', 'Brain', 'Calibration', 'Clinical', 'Clinical Data', 'Clinical Sciences', 'Clinical assessments', 'Cognition', 'Cognitive', 'Communication', 'Communities', 'Computers', 'Custom', 'Data', 'Decision Making', 'Disease', 'Drowsiness', 'Electroencephalography', 'Engineering', 'Environment', 'Eye Movements', 'Fatigue', 'Feedback', 'Goals', 'Guidelines', 'Head Movements', 'Impairment', 'Individual', 'Informed Consent', 'Knowledge', 'Language', 'Learning', 'Letters', 'Life', 'Locked-In Syndrome', 'Machine Learning', 'Measures', 'Medical', 'Medical Technology', 'Methods', 'Modality', 'Modeling', 'Motor Skills', 'Movement', 'Muscle', 'Natural Language Processing', 'Neurodegenerative Disorders', 'Participant', 'Partner Communications', 'Pattern', 'Performance', 'Pharmaceutical Preparations', 'Policies', 'Population', 'Protocols documentation', 'Psychological Transfer', 'Psychological reinforcement', 'Public Health', 'Questionnaires', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Design', 'Role', 'Science', 'Secondary to', 'Self-Help Devices', 'Sensory', 'Signal Transduction', 'Solid', 'Source', 'Speech', 'Speed', 'Supplementation', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Vocabulary', 'Workload', 'acronyms', 'alternative communication', 'base', 'brain computer interface', 'caregiving', 'clinical care', 'clinical implementation', 'cognitive ability', 'community based participatory research', 'computer science', 'disability', 'experience', 'experimental study', 'improved', 'innovation', 'learning strategy', 'motor disorder', 'multidisciplinary', 'multimodality', 'neurophysiology', 'phrases', 'residence', 'response', 'satisfaction', 'sensor', 'signal processing', 'simulation', 'spelling', 'theories', 'visual tracking']",NIDCD,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,929399,0.027901282620676385
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Abstract COVID-19 has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. Accordingly, the goal of this COVID Supplement, which builds on and expands the work being conducted by the parent grant, is to develop a COVID map tool that provides fully accessible, non-visual access to maps. This tool will allow visually impaired persons to explore maps and preview routes from the comfort of their home, allowing them to plan their travel along safer, less congested routes using crowdedness data. In addition, the tool will present county-by-county COVID incidence data in a fully accessible form, which will inform their travel plans over greater distances. Thus, this project will give visually impaired persons the tools and confidence to undertake safer, more independent travel. Health Relevance The COVID-19 pandemic has an especially severe impact on people with significant vision impairments or blindness. The need for social distancing and reduced touching of one’s surroundings has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. These travel limitations may have adverse impacts on their physical and mental health. The proposed research would result in a new software tool that could greatly increase the confidence of the approximately 10 million Americans with significant vision impairments or blindness to undertake safe, independent travel.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,10220178,R01EY029033,"['American', 'Blindness', 'COVID-19', 'COVID-19 pandemic', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'County', 'Crowding', 'Data', 'Destinations', 'Development', 'Ensure', 'Evaluation', 'Exercise', 'Goals', 'Health', 'Home environment', 'Incidence', 'Internet', 'Knowledge', 'Leisures', 'Maps', 'Mental Health', 'Pharmacy facility', 'Process', 'Publications', 'Research', 'Route', 'Running', 'Social Distance', 'Software Tools', 'System', 'Tablets', 'Tactile', 'Target Populations', 'Touch sensation', 'Travel', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'blind', 'braille', 'coronavirus disease', 'design', 'outreach', 'pandemic disease', 'parent grant', 'physical conditioning', 'software development', 'symposium', 'tool', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,406525,0.029537598732065712
"User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control ABSTRACT Approximately 41,000 individuals live with upper-limb loss (loss of at least one hand) in the US. Fortunately, prosthetic devices have advanced considerably in the past decades with the development of dexterous, anthropomorphic hands. However, potentially the most promising used control strategy, myoelectric control, lacks a correspondingly high-level of performance and hence the use of dexterous hands remains highly limited. The need for a complete overhaul in upper limb prosthesis control is well highlighted by the abandonment rates of myoelectric devices, which can reach up to 40% in the case of trans-humeral amputees. The area of research that has received the most focus over the past decade has been “pattern recognition,” which is a signal processing based control method that uses multi-channel surface electromyography as the control input. While pattern recognition provides intuitive operation of multiple prosthetic degrees of freedom, it lacks robustness and requires frequent, often daily calibration. Thus, it has not yet achieved the desired clinical acceptance. Our team proposes clinical translation of a novel highly adaptive upper limb prosthesis control system that incorporates two major advances: 1) machine learning (robust classification by implementing a non-boundary based algorithm), and 2) training by retrospectively incorporating user data from activities of daily living (ADL). The proposed system will enable machine intelligence with user input for prosthesis control. Our work is organized as follows: Phase I: (a) First, we will implement a fundamentally new machine intelligence technique, Extreme Learning Machine with Adaptive Sparse Representation Classification (EASRC), that is more resilient to untrained noisy conditions that users may encounter in the real-world and requires less data than traditional myoelectric signal processing. (b) In parallel, we will implement an adaptive learning algorithm, Nessa, which allows users to relabel misclassified data recorded during use and then update the EASRC classifier to adapt to any major extrinsic or intrinsic changes in the signals. Taken together, EASRC and Nessa comprise the Retrospectively Supervised Classification Updating (RESCU) system. Once, the RESCU implementation is complete, we will optimize the system through a joint effort with Johns Hopkins University, and complete an iterative benchtop RESCU evaluation with a focus group of 3 amputee subjects and their prosthetists. Phase II: Verification and validation of RESCU will be completed, culminating in third-party validation testing and certification. Finally, we will complete a clinical assessment including self-reporting subjective measures, and real-world usage metrics in a long-term clinical study. PROJECT NARRATIVE In this project, we aim to empower the user by bringing them into the control loop of their prosthesis and improve the stability of their control strategy over time. Specifically, we implement to a robust classifier, an adaptive learning algorithm, and a smartwatch interface, which allows the user to teach their device when it misunderstands the commands that the user is sending to control the prosthesis. This will result in improved control without cumbersome or time-consuming effort on the part of the user and, more importantly, we hope that it will give the user a greater sense of empowerment and ownership over their prosthesis.",User-driven Retrospectively Supervised Classification Updating (RESCU) system for robust upper limb prosthesis control,10078697,U44NS108894,"['Activities of Daily Living', 'Adoption', 'Algorithms', 'Amputees', 'Area', 'Artificial Intelligence', 'Award', 'Calibration', 'Certification', 'Classification', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Communication', 'Consumption', 'Data', 'Development', 'Devices', 'Electromyography', 'Evaluation', 'Focus Groups', 'Freedom', 'Goals', 'Hand', 'Individual', 'Intuition', 'Joints', 'Label', 'Limb Prosthesis', 'Machine Learning', 'Measures', 'Methods', 'Outcome', 'Ownership', 'Parents', 'Patient Self-Report', 'Pattern Recognition', 'Performance', 'Phase', 'Prosthesis', 'Research', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Supervision', 'Surface', 'Surveys', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Update', 'Upper Extremity', 'Validation', 'Work', 'adaptive learning', 'base', 'clinical translation', 'empowerment', 'functional improvement', 'improved', 'innovation', 'intelligent algorithm', 'learning algorithm', 'myoelectric control', 'novel', 'operation', 'programs', 'prospective', 'prosthesis control', 'satisfaction', 'signal processing', 'smart watch', 'verification and validation']",NINDS,"INFINITE BIOMEDICAL TECHNOLOGIES, LLC",U44,2020,64079,0.04997349847045738
"Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection PROJECT SUMMARY/ABSTRACT  Companies are increasingly marketing mobile technologies as FDA-cleared medical devices, yet we do not know the consequences of these devices on healthcare utilization, cost, and outcomes. Recently, Apple released the Apple Watch Series 4 as an FDA-cleared medical device. The device includes an alert for the presence of atrial fibrillation (AF) and allows anyone to monitor their heart rhythm for the presence of AF. Apple has an enormous global audience, and the number of people who will use this (and other similar devices) to self-diagnose or monitor AF will be substantial. On one hand, the device may allow new diagnoses that result in treatment, improved quality of life, fewer AF related complications. On the other hand, the device may result in false positives in otherwise healthy people, resulting in more testing and treatments with associated harms. In fact, the U.S. Preventative Task Force recommends against routine surveillance for AF in the general population, citing lack of evidence and possible harm. We have an urgent need for a population-based infrastructure to ensure that technologies entering the market as medical devices are beneficial and safe.  The overall goal of this project is to measure the uptake and effect of the Apple Watch 4 release on healthcare utilization among first-time and known AF patients. Dr. Shah is an early stage investigator with a K08 Career Development Award from the NHLBI. As part of the K08, she has developed a detailed cohort of contemporary AF patients, including clinical notes. Along with a team, she will use real world data, as proposed by the FDA, to generate evidence about risks and benefits of consumer-driven AF detection. She will use natural language processing to leverage the notes and identify AF patients who seek care due to the medical device, and evaluate downstream healthcare utilization, such as additional clinic visits, cardioversions, additional remote monitoring, and cost. The goals of this project will be accomplished through the following Specific Aims: 1) Estimate the proportion of first-time AF patient visits attributable to a mobile device before and after FDA clearance of the Apple Watch 4, and characterize device accuracy and downstream healthcare utilization in this population; and 2) Evaluate healthcare utilization patterns among prevalent AF patients who use mobile devices with AF alerts.  In 2017, Apple sold 17.7 million smart watches, in a device market that continues to grow. Extrapolating from prior annual sales and conservatively assuming a 5% increase in users each year, almost 60 million people will have an Apple Watch by the end of 2020 (not accounting for non-Apple devices with similar functionality). Thus, even in this short period of time, uptake will be substantial and warrant immediate feedback. The results of this project will provide preliminary data for a long-term, multicenter study that evaluates the benefits (improved quality of life, fewer strokes) and harms (increased treatment complications, increased cost) of consumer-driven AF detection. PROJECT NARRATIVE The consequence of mobile technologies marketed as medical devices are unknown, including devices that provide alerts for the presence of atrial fibrillation. The goal of this project is to evaluate the benefits and harm associated with consumer-driven atrial fibrillation detection.",Healthcare Impact of Consumer-Driven Atrial Fibrillation Detection,9980996,R03HL148372,"['Adult', 'Advisory Committees', 'Affect', 'Apple', 'Apple watch', 'Arrhythmia', 'Atrial Fibrillation', 'Benefits and Risks', 'Cardiovascular system', 'Caring', 'Case-Control Studies', 'Clinic Visits', 'Clinical', 'Data', 'Detection', 'Devices', 'Diagnosis', 'Diagnostic', 'Electric Countershock', 'Ensure', 'Feedback', 'General Population', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Hemorrhage', 'Holter Electrocardiography', 'Infrastructure', 'Interruption', 'Intervention', 'K-Series Research Career Programs', 'Lead', 'Marketing', 'Measures', 'Medical Device', 'Monitor', 'Multicenter Studies', 'National Heart, Lung, and Blood Institute', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevalence', 'Preventive', 'Quality of life', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sales', 'Series', 'Sinus', 'Stroke', 'Technology', 'Testing', 'Text', 'Time', 'United States Food and Drug Administration', 'Universities', 'Utah', 'Visit', 'base', 'care seeking', 'cohort', 'cost', 'cost outcomes', 'cryptogenic stroke', 'design', 'follow-up', 'handheld mobile device', 'health care service utilization', 'heart rhythm', 'improved', 'mobile computing', 'population based', 'routine screening', 'self diagnosis', 'smart watch', 'uptake']",NHLBI,UNIVERSITY OF UTAH,R03,2020,76250,0.0244805091209884
"Development of a Wheelchair Maintenance Alert Application for Elderly Wheelchair Users Project Summary Elderly wheelchair users experience wheelchair breakdowns every 2-3 months in low- and middle-income countries (LMICs) and rural areas of high-income countries. One in three breakdowns leads to adverse physical, social, psychosocial and economic consequences to wheelchair users which increases the public health and personal burden. Preventative wheelchair maintenance has been found to reduce the frequency of wheelchair breakdowns by ten-fold, but compliance with maintenance recommendations is extremely low because they are generic and not reflective of how and where the wheelchair is being used. To address this issue, we are developing a low-cost, scalable maintenance application that leverages artificial intelligence tools to provide maintenance recommendations tailored to how a wheelchair is used. The availability of low-cost technology and widespread use of smartphones by the elderly and people with disabilities in LMICs has led us to develop a smartphone application called WheelTrak that measures wheelchair wear as a function of usage in community. Based on the wear factors, the application produces a Wheelchair Wear Index (WWI) that is representative of wear of critical wheelchair parts that are prone to breakdown. Once a WWI threshold is reached, maintenance is required, and the application notifies the user and/or caregiver who can conduct maintenance to avoid breakdowns and related health consequences. We will conduct a data collection study in collaboration with our wheelchair industry partner – UCP Wheels in El Salvador – and characterize the WWI for the elderly by tracking wear factors which include user’s travel distance, ground shocks and surface vibrations using WheelTrak and a wheel sensor. Based on the trained WWI algorithm, a preventative maintenance schedule will be developed for older adults that can be employed through WheelTrak for maintenance reminders. Semi-structured interviews will be conducted to evaluate the usability of the application and gather barriers to maintenance. User feedback will assist us in improving WheelTrak for greater user satisfaction and compliance with maintenance, and addressing any personal or logistical challenges that elderly users and their caregivers or family members may face with conducting maintenance activities in LMICs. Findings from the proposed studies in this application will assist us in planning future studies to investigate the WWI-enabled WheelTrak tool as an intervention to prevent or reduce breakdowns and health consequences with the elderly in LMICs. Preventative maintenance of wheelchairs is necessary to reduce frequent wheelchair breakdowns and corresponding health consequences experienced by the elderly in adverse environments which are commonly present in low- and middle-income countries (LMICs). WheelTrak is a smartphone application that measures real-time wheelchair wear in the community during use and triggers preventative maintenance reminders. In this study, we are modelling the application algorithm and collecting user and caregiver feedback to transform WheelTrak into a maintenance intervention tool for elderly wheelchair users in LMICs.",Development of a Wheelchair Maintenance Alert Application for Elderly Wheelchair Users,10095020,R03AG069836,"['Activities of Daily Living', 'Address', 'Adult', 'Algorithms', 'Artificial Intelligence', 'Beds', 'Caregivers', 'Cause of Death', 'Cellular Phone', 'Clinic', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Collection', 'Development', 'Devices', 'Disabled Persons', 'Economics', 'El Salvador', 'Elderly', 'Environment', 'Face', 'Failure', 'Family member', 'Feedback', 'Frequencies', 'Future', 'Goals', 'Health', 'Hospitalization', 'Income', 'Injury', 'Intervention', 'Intervention Studies', 'Interview', 'Life Style', 'Logistics', 'Long-Term Care', 'Maintenance', 'Manual wheelchair', 'Measures', 'Mental Depression', 'Modeling', 'Monitor', 'Names', 'Notification', 'Pattern', 'Personal Satisfaction', 'Population', 'Preventive', 'Provider', 'Public Health', 'Recommendation', 'Risk', 'Schedule', 'Services', 'Shock', 'Societies', 'Structure', 'Study models', 'Surface', 'Technology', 'Time', 'Training', 'Travel', 'User Compliance', 'Wheelchairs', 'aged', 'base', 'cohort', 'cost', 'decubitus ulcer', 'disability', 'evidence base', 'experience', 'improved', 'indexing', 'industry partner', 'low and middle-income countries', 'prevent', 'prospective', 'psychosocial', 'rural area', 'satisfaction', 'sensor', 'sensor technology', 'smartphone Application', 'social', 'tool', 'usability', 'vibration']",NIA,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R03,2020,75877,0.007647739340332866
"BehaviorSight: Privacy enhancing wearable system to detect health risk behaviors in real-time. Project Summary/Abstract Health-risk behaviors, such as overeating, smoking, consuming alcohol, and not adhering to medication, are responsible for increases in morbidity and mortality. To track and intervene during these health-risk behaviors, clinicians traditionally rely on self-reports. However, self-reports are inaccurate and biased. Therefore, we cannot use self-reports to validate health-risk behaviors in free-living conditions. Thus, an automated technique for validating health-risk behaviors is extremely necessary. With the growth and popularity of wearable devices (e.g., smartwatches), automatic monitoring of physical activity is possible. However, the devices often do not provide any visual confirmation, making it challenging to verify activities performed in free-living conditions. Cameras can capture point-of-view videos and can thus be used as a wearable device to capture videos for visual confirmation of activities, including health-risk behaviors. Such recordings can help us better understand health-risk behaviors. Additionally, video information can be automatically processed to confirm and validate health-risk behaviors. Recording videos of sensitive content and bystanders is associated with privacy and ethical concerns. Currently there is no privacy-preserving camera that can automatically detect health-risk behaviors, and most people are unwilling to wear cameras without raising privacy concerns. In addition to privacy concerns, people prefer wearables that are unobtrusive and small and that do not require frequent charging. Thus, a privacy-preserving, unobtrusive wearable camera would increase wearability. Infrared (IR) sensor arrays have the potential to provide independent temperature readings, which allows determining whether an object is near or far. The IR sensor array can help record only the wearer and objects near the wearer, while filtering out distant objects. IR sensor arrays have a small power footprint, thus providing longer battery life. Our project aims to develop a privacy-conscious, unobtrusive, wearable, behavior-detection platform that will make it possible to detect and intervene upon health-risk behaviors in real time. In this project, we will (1) develop the wearable behavior-detection device that allows visual confirmation without burdening the wearer. The device will augment RGB camera data with IR sensor array data for privacy-conscious recording and automatic behavior detection. (2) We will test various designs to determine a user's acceptability to wear the device. Then, we will test various image processing techniques and machine learning algorithms to determine the best algorithm for detecting health-risk behaviors. (3) We will incorporate the best-performing behavior-detection algorithm so that it can run on the developed wearable device. With a behavior-detection algorithm running on an acceptable wearable device, the ability to detect health-risk behaviors in real time will become a reality. Ultimately, our wearable device will allow researchers to test and apply appropriate behavioral interventions in real time, rather than relying on self-reports, whenever health-risk behaviors occur. Project Narrative Several activities involving hand-to-mouth gestures (e.g., overeating, smoking, consuming alcohol, or non- adherence to medication) are associated with health-risk behaviors that are a leading cause of preventable deaths. Being able to automatically monitor health-risk behaviors using wearable video cameras will improve our understanding of these behaviors and will ultimately enable us to design effective methods to intervene when they occur. We will develop BehaviorSight, a privacy-conscious, unobtrusive, wearable, behavior-detection device that will allow future researchers and behavioral scientists to use the device for monitoring numerous everyday health-risk behaviors in free-living settings.",BehaviorSight: Privacy enhancing wearable system to detect health risk behaviors in real-time.,10043674,R21EB030305,"['Address', 'Alcohol consumption', 'Algorithms', 'Behavior', 'Behavior Therapy', 'Behavior monitoring', 'Behavioral', 'Bluetooth', 'Cellular Phone', 'Charge', 'Chest', 'Clinical', 'Communication', 'Conscious', 'Data', 'Detection', 'Devices', 'Dietitian', 'Disease', 'Distant', 'Eating', 'Ensure', 'Ethics', 'Future', 'Gestures', 'Goals', 'Grant', 'Growth', 'Hand', 'Health', 'Hyperphagia', 'Intervention', 'Laboratory Study', 'Learning', 'Life', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Notification', 'Obesity', 'Oral cavity', 'Participant', 'Patient Self-Report', 'Pharmaceutical Preparations', 'Physical activity', 'Privacy', 'Process', 'Reading', 'Records', 'Research', 'Research Personnel', 'Resources', 'Risk Behaviors', 'Running', 'Science', 'Scientist', 'Smoke', 'Smoking', 'Substance abuse problem', 'System', 'Techniques', 'Technology', 'Temperature', 'Testing', 'Time', 'Video Recording', 'Visual', 'cost', 'data privacy', 'design', 'drinking', 'image processing', 'improved', 'light weight', 'machine learning algorithm', 'medication nonadherence', 'miniaturize', 'monitoring device', 'mortality', 'multimodality', 'novel', 'prevent', 'preventable death', 'privacy preservation', 'response', 'sensor', 'smart watch', 'wearable device', 'wearable sensor technology', 'willingness']",NIBIB,NORTHWESTERN UNIVERSITY AT CHICAGO,R21,2020,606713,-0.013861521621618226
"Development of assistive self-care robot technologies for people with disabilities Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee Overview We propose to develop a design space framework and co-design methodology for the development of assistive self-care robot technologies that are informed by the social model of disability. Our model of assistive robots in the domain of self-care considers an individual's social and environmental context, coping processes and other factors that can affect independent functioning. Our design methods utilize embedded sensing to intelligently respond to these con- siderations. We speciﬁcally focus on assistive feeding tasks, proposing a formalism that enables a robotic system to feed a person with upper-extremity disability. Our guiding principle is that human-level interaction is feasible only if the robot itself relies on human-level semantics. We im- plement this principle by relying on data to learn and develop object-dependent control policies and timing models for acquiring and transferring a bite to a user at a proper time. The system's ob- server detects world states and arbitrator invokes different control policies based on these states. The tangible result will be an intelligent assistive feeding robot whose performance can generalize to different activities, adapt to user preferences, and recover from failures. Objectives and Relevance to NIH A design framework for assistive robots would provide for- malisms that let us address the fundamental challenge of designing robots that are responsive to context of use and support assisted self-care in a variety of social settings. We combine method- ologies from human-robot interaction, cognitive science, machine learning, robotics and haptics with user studies and our formalism to address the following research questions: (Q1) Mechanics of Feeding-Control Policies: How can control policies be designed for dexterous non-prehensile manipu- lation of deformable objects such as food? (Q2) Social Aspects of Feeding-Bite Timing: How should an assistive feeding robot decide the right timing for feeding a user? (Q3) Human-in-the-Loop: How can human-directed feedback be added into the loop for an autonomous assistive feeding system?  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks. This can in turn help them increase their independence and autonomy making eating easier and more enjoyable. While we presently focus on this spe- ciﬁc application, the tools and insights we gain can generalize to the ﬁelds of robotic assistance and human-robot interaction across other activities of daily living and instrumental activities of daily living. Thus, our work is clearly motivated by the intent to improve the quality of health and life of the aging population and is very relevant to the theme of NIH. 1 Towards Autonomy in Daily Living: A Formalism for Intelligent Assistive Feeding Systems  Applicant PI, Tapomayukh Bhattacharjee  The proposed work will allow users with upper-arm disabilities to use this system for intelli- gent assistance with daily feeding tasks, potentially increasing their independence and autonomy making eating easier and more enjoyable. The long-term promise of this research is to have robots in society that are able to seamlessly and ﬂuently perform complex manipulation tasks in dynamic human environments in real homes which could impact individuals with other disabilities as well as able-bodied individuals. Through improved access to independent living and customizing to the unique needs and preferences of users, the results of this project can positively impact mil- lions of people worldwide, especially given the vast variability in our target population by being transformational in the scalability of assistive robotics for self-care. 1",Development of assistive self-care robot technologies for people with disabilities,9907705,F32HD101192,"['Activities of Daily Living', 'Address', 'Affect', 'Aging', 'Bathing', 'Bite', 'Caregiver Burden', 'Caring', 'Child', 'Cognitive Science', 'Communities', 'Complex', 'Cues', 'Custom', 'Data', 'Development', 'Disabled Persons', 'Eating', 'Emotional', 'Environment', 'Expert Systems', 'Failure', 'Family', 'Feedback', 'Food', 'Generations', 'Health', 'Home environment', 'Human', 'Improve Access', 'Independent Living', 'Individual', 'Intelligence', 'Learning', 'Life', 'Machine Learning', 'Mechanics', 'Mental Depression', 'Methodology', 'Methods', 'Modeling', 'Panthera leo', 'Parents', 'Performance', 'Persons', 'Play', 'Policies', 'Population', 'Process', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Role', 'Self Care', 'Semantics', 'Societies', 'Sterile coverings', 'System', 'Target Populations', 'Taxonomy', 'Technology', 'Time', 'Tweens', 'United States National Institutes of Health', 'Upper Extremity', 'Upper arm', 'Work', 'aging population', 'assistive robot', 'base', 'care recipients', 'coping', 'design', 'disability', 'experience', 'experimental study', 'feeding', 'haptics', 'human subject', 'human-in-the-loop', 'human-robot interaction', 'improved', 'insight', 'instrumental activity of daily living', 'intergenerational', 'kinematics', 'patient oriented', 'peer', 'preference', 'robot assistance', 'robotic system', 'social', 'social model', 'tool']",NICHD,UNIVERSITY OF WASHINGTON,F32,2020,65310,0.029669103063150997
"Gaze-contingent computer screen magnification control for people with low vision ! Project Summary This application describes proposed research with the goal of facilitating use of a computer screen magnifier by people with low vision. Screen magnification is a well-established, popular technology for access of onscreen content. Its main shortcoming is that it requires the user to continuously control, with the mouse or trackpad, the location of the focus of magnification, in order to ensure that the magnified content of interest is within the screen viewport. This tedious process may be time-consuming and ineffective. For example, the simple task of reading the news on a web site requires continuous horizontal scrolling, which affects the experience of using this otherwise very beneficial technology, and may discourage its use, especially by those with poor manual coordination.  We propose to develop a software system that enables hands-free control of a screen magnifier. This system will rely on the user’s eye gaze (measured by a regular IR-based tracker, or from analysis of the images in a camera embedded in the screen) to update the location of the focus of magnification as desired. This research is inspired by preliminary work, which showed promising results with two simple gaze-based control algorithms, tested on three individuals with low vision.  This project will be a collaboration between the Department of Computer Science and Engineering at UC Santa Cruz (PI: Manduchi, Co-I: Prado) and the School of Optometry at UC Berkeley (PI: Chung). Dr. Legge from the Department of Psychology at U. Minnesota will participate as a consultant. Two human subjects studies are planned. In Study 1 with 80 low vision subjects from four different categories of visual impairment, we will investigate the failure rate of a commercial gaze tracker (Aim 1), and will record mouse tracks, gaze tracks, and images from the subjects while performing a number of tasks using two modalities of screen magnification (Aim 2). In Study 2, with the same number of subjects, we will repeat the Study 1 experiment, but using a gaze-based controller trained from the data collected in Study 1, and individually tunable for best performance (Aim 3). In addition, we will experiment with an appearance-based gaze tracker that uses images from the screen camera, thereby removing the need for specialized gaze tracking hardware, as well as with a computer tablet form factor (Aim 4). We expect that reading speed and error rate using our gaze-based controller will be no worse than using mouse-based control. If successful, this study will show that the convenience of hands-free control offered by the proposed system comes at no additional cost in terms of individual performance at the considered tasks. ! ! Project Narrative People with low vision often use screen magnification software to read on a computer screen. Since a magnifier expands the screen content beyond the physical size of the screen (the “viewport”), it is necessary to move the content using the mouse so that the portion of interest falls within the viewport. This project will facilitate use of a screen magnifier by means of a new software system that relies on the user’s own gaze to control scrolling when reading with magnification. !",Gaze-contingent computer screen magnification control for people with low vision,10053172,R01EY030952,"['Affect', 'Age', 'Algorithms', 'Appearance', 'Apple', 'Behavior Control', 'Benchmarking', 'Blindness', 'Categories', 'Collaborations', 'Communication', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consumption', 'Correlation Studies', 'Data', 'Data Set', 'Desktop Video', 'Engineering', 'Ensure', 'Eye', 'Face', 'Failure', 'Funding', 'Glass', 'Goals', 'Hand', 'Image', 'Individual', 'Learning', 'Location', 'Magic', 'Manuals', 'Measures', 'Minnesota', 'Modality', 'Mus', 'Operating System', 'Optometry', 'Performance', 'Peripheral', 'Process', 'Psychological reinforcement', 'Psychology', 'Reader', 'Reading', 'Research', 'Resort', 'Role', 'Schools', 'Science', 'Speech', 'Speed', 'Structure', 'Study Subject', 'System', 'Tablet Computer', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'algorithm development', 'algorithm training', 'base', 'computer science', 'control trial', 'cost', 'data acquisition', 'design', 'experience', 'experimental study', 'falls', 'gaze', 'human subject', 'interest', 'motor control', 'news', 'recurrent neural network', 'sample fixation', 'software systems', 'tool', 'web page', 'web site']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R01,2020,350753,0.02504991876184457
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,10019559,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2020,662134,-0.020083325032949167
"Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs PROJECT ABSTRACT Above-knee amputees often struggle to perform the varying activities of daily life with conventional prostheses. Emerging powered knee-ankle prostheses have motors that can restore normative biomechanics, but these devices are limited to a small set of pre-defined activities that must be tuned to the user by technical experts over several hours. The overall goal of this project is to model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning. The universal use of different task-specific controllers in current powered legs is a direct consequence of the prevailing paradigm for viewing human locomotion as a discrete set of activities. There is a fundamental gap in knowledge about how to analyze, model, and control continuously varying locomotion, which greatly limits the adaptability and agility of powered prostheses. The central hypothesis of this project is that continuously varying activities can be represented by a single mathematical model based on measureable physical quantities called task variables. The proposed project will be scientifically significant to understanding how humans continuously adapt to varying activities and environments, technologically significant to the design of agile, user-synchronized powered prosthetic legs, and clinically significant to the adoption of powered knee-ankle prostheses for improved community ambulation. The proposed model of human locomotion will enable new prosthetic strategies for controlling and adapting to the environment, which aligns with the missions of the NICHD/NCMRR Devices and Technology Development program area and the NIBIB Mathematical Modeling, Simulation, and Analysis program. The innovation of this work is encompassed in 1) a continuous paradigm for variable locomotor activities that challenges the existing discrete paradigm, 2) a unified task control methodology that drastically improves the agility of powered prosthetic legs, and 3) a partially automated tuning process that significantly reduces the time and technical expertise required to configure powered knee- ankle prostheses. This continuous task paradigm will provide new methods and models for studying human locomotion across tasks and task transitions. This innovation will address a key roadblock in control technology that currently restricts powered legs to a small set of activities that do not generalize well across users. The adaptability of the proposed control paradigm across users and activities will transform the prosthetics field with a new generation of “plug-and-play” powered legs for community ambulation. PROJECT NARRATIVE The proposed research is relevant to public health because the clinical application of variable-activity powered prosthetic legs can significantly improve community mobility and therefore quality of life for nearly a million American amputees. Recently developed powered knee-ankle prostheses are limited to a small set of pre- defined activities that require several hours of expert tuning for each user. This project will model and control human locomotion over continuously varying tasks for the design of agile, powered prostheses that require little to no tuning, which aligns with the missions of the Devices and Technology Development program area of the NICHD National Center for Medical Rehabilitation Research and the Mathematical Modeling, Simulation, and Analysis program of the NIBIB.",Controlling Locomotion over Continuously Varying Activities for Agile Powered Prosthetic Legs,9925236,R01HD094772,"['Address', 'Adoption', 'American', 'Amputees', 'Ankle', 'Area', 'Artificial Leg', 'Biomechanics', 'Clinical', 'Communities', 'Data', 'Degree program', 'Device or Instrument Development', 'Devices', 'Doctor of Philosophy', 'Electrical Engineering', 'Environment', 'Gait', 'Gait speed', 'Generations', 'Goals', 'Hand', 'Home environment', 'Hour', 'Human', 'Human body', 'Joints', 'Knee', 'Knowledge', 'Lead', 'Leg', 'Life', 'Locomotion', 'Lower Extremity', 'Machine Learning', 'Mathematical Model Simulation', 'Measurable', 'Measures', 'Mechanics', 'Medical center', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motor', 'Motor Activity', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'Orthotic Devices', 'Outcome', 'Phase', 'Play', 'Process', 'Program Development', 'Prosthesis', 'Public Health', 'Quality of life', 'Research', 'Research Personnel', 'Running', 'Sampling', 'Speed', 'Spinal cord injury', 'Stroke', 'Study models', 'System', 'Technical Expertise', 'Technology', 'Time', 'United States National Institutes of Health', 'Walking', 'Work', 'base', 'clinical application', 'clinically significant', 'design', 'exoskeleton', 'experience', 'human data', 'human model', 'improved', 'innovation', 'kinematics', 'mathematical model', 'multidisciplinary', 'powered prosthesis', 'programs', 'prosthesis control', 'rehabilitation research', 'robot control', 'sensor', 'success', 'technology development', 'temporal measurement', 'trend']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,446707,0.015783816327796248

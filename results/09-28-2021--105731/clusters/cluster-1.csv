text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9783881,U01EB029373,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIBIB,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2019,611358,0.02069136529176344
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9666926,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2019,242725,0.00864324943971247
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,0.056249219120396
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9774249,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2019,332952,0.04930739218467366
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales – precisely the scales at which these processes occur – microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9699440,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'machine learning algorithm', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2019,186101,0.06267758257783938
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9658531,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2019,479215,0.010230319840489638
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9751141,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2019,1186500,0.03349182712941097
"Optimizing imputation for diverse populations in a distributed framework The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.  ",Optimizing imputation for diverse populations in a distributed framework,10016895,U01HG009080,"['Algorithms', 'Biomedical Research', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data Analyses', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genomics', 'Goals', 'Health', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methods', 'Minority', 'Modeling', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resource Development', 'Resources', 'Role', 'Scientist', 'Secure', 'Statistical Methods', 'Structural Protein', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'analysis pipeline', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'improved', 'large-scale database', 'meetings', 'novel', 'online resource', 'prediction algorithm', 'programs', 'protein structure', 'racial diversity', 'rare variant', 'statistics', 'tool', 'web server']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U01,2019,422665,0.04334100955306772
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9789914,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2019,1500000,0.023158671956817763
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9761554,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Structural Genes', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'bioinformatics tool', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'human pathogen', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2019,344107,0.025109976801364994
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9626416,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Infrastructure', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2019,2000000,0.025853178847588157
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,9685931,U01HG009080,"['Algorithms', 'Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Privatization', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Risk', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Structural Protein', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'analysis pipeline', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'online resource', 'patient privacy', 'phenome', 'prediction algorithm', 'predictive modeling', 'preservation', 'programs', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'simulation', 'statistics', 'tool', 'tool development', 'web server', 'web-based tool']",NHGRI,STANFORD UNIVERSITY,U01,2019,276777,0.0445636893265572
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,9995640,U01HG009080,"['Algorithms', 'Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Privatization', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Risk', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Structural Protein', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'analysis pipeline', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'online resource', 'patient privacy', 'phenome', 'prediction algorithm', 'predictive modeling', 'preservation', 'programs', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'simulation', 'statistics', 'tool', 'tool development', 'web server', 'web-based tool']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U01,2019,728322,0.0445636893265572
"Integrating multidimensional genomic data to discover clinically-relevant predictive models The goal of this NIH Pathway to Independence award is to provide Dr. Brittany Lasseigne with an extensive training program to prepare her to be an effective independent investigator who uses computational genomics to study complex human diseases. We propose a formal one-year training and mentoring program in genomics, computer science, statistics, and career development to build on her 8+ years of hands-on training, followed by a three-year structured and independent research program. Research will focus on the integration of multidimensional genomic data sets in the context of complex human diseases. A critical barrier in genomic research is the complexity of data integration: the ability to leverage overlapping and unique information captured by different genomic assays would improve our understanding of data integration and generate clinically relevant genomic signatures. To meet this need, we propose to integrate a combination of genomic data we generated with public data to (1) infer genomic instability signatures from different data types, (2) improve clinically relevant phenotype prediction by building multi-omics machine learning classifiers and reducing phenotype heterogeneity, and (3) create a cloud-enabled R package and associated Shiny application to accelerate future research. The proposed work will advance our understanding of data integration, allow inference of genomic instabilities across data sets, and generate high performance classifiers for assessing clinically relevant phenotypes in both cancer and psychiatric disease using frameworks that will be broadly applicable across other complex diseases. It will also facilitate prioritization of experiments in future studies by informing on the orthogonality of genomic assays, thereby allowing more efficient study designs to capture as much information as possible within a given sample size or scope of experimentation. Collectively, this additional training will allow Dr. Lasseigne to develop new multidimensional data integration approaches and translational questions applicable across complex diseases when independent. Dr. Richard Myers (HudsonAlpha) and Dr. Gregory Cooper (HudsonAlpha), leaders in applying genetics and genomics to complex human diseases, and an Advisory Committee of additional experts including Dr. Barbara Wold (Caltech), Dr. Eddy Yang (UAB), and Dr. Timothy Reddy (Duke), will provide mentoring throughout this award. The mentored phase will take place at the HudsonAlpha Institute for Biotechnology, an ideal environment for this training with extensive translational science collaborations, expert faculty and staff, and state-of-the art computational and laboratory resources devoted to genomics. This combination will maximize Dr. Lasseigne's training program, facilitating her transition to an independent, tenure-track investigator at a university with a strong commitment to data-driven approaches to complex human disease research, i.e. strong genomics research programs with clinical collaborators, ideally at, or affiliated with, an academic medical center. Project Narrative The major outcome of this project will be a scientist with the necessary research, mentoring, teaching, and career development training to run an independent research program in computational genomics. The research proposed will apply novel strategies to further develop integrative machine learning analyses of multidimensional genomic data, discover clinically relevant predictive models, and create computational tools to accelerate future research.",Integrating multidimensional genomic data to discover clinically-relevant predictive models,9901758,R00HG009678,"['Academic Medical Centers', 'Advisory Committees', 'Award', 'Bioconductor', 'Biological', 'Biological Assay', 'Biotechnology', 'Budgets', 'Cancer Etiology', 'Cell Proliferation', 'Cells', 'Characteristics', 'Chemotherapy-Oncologic Procedure', 'Chromosomal Instability', 'Chromosomes', 'Classification', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Copy Number Polymorphism', 'Coupling', 'CpG Island Methylator Phenotype', 'DNA', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Educational process of instructing', 'Environment', 'Faculty', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomic Instability', 'Genomics', 'Goals', 'Heterogeneity', 'Imagery', 'Individual', 'Institutes', 'Instruction', 'Laboratories', 'Lasso', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mental disorders', 'Mentors', 'Methodology', 'Methylation', 'MicroRNAs', 'Microsatellite Instability', 'Modeling', 'Molecular Profiling', 'Neurons', 'Outcome', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Signal Transduction', 'Standardization', 'Structure', 'Systems Biology', 'Techniques', 'Testing', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Variant', 'Work', 'Yang', 'biomarker performance', 'career development', 'clinically relevant', 'computer science', 'computerized tools', 'data integration', 'data reduction', 'experimental study', 'genomic data', 'genomic signature', 'human disease', 'improved', 'insight', 'multidimensional data', 'multiple omics', 'novel strategies', 'predictive modeling', 'programs', 'promoter', 'protein metabolite', 'response', 'single cell sequencing', 'statistics', 'tenure track', 'tool']",NHGRI,UNIVERSITY OF ALABAMA AT BIRMINGHAM,R00,2019,249000,0.08208810549771806
"Scalable tools to effectively translate genomic discoveries into the clinic PROJECT SUMMARY We are in the midst of a genomic revolution; more than 250,000 human genomes have been sequenced, generating over a petabase of genomic data. While these new data hold great promise to impact health, there is a disconnect between genomic discovery and clinical care. Providers frequently misinterpret genomic information, patients often don't understand their own test results, and genomic information about disease risk is infrequently shared between patients and family members. Importantly, ineffective communication and data misinterpretation has devastating consequences- including unnecessary organ removal, missed disease prevention opportunities, and premature death. We are addressing these genomic care gaps by developing and testing tools that optimize the integration of whole-exome and whole-genome sequencing (WES, WGS) for general clinical practice. My vision for improving genomic medicine is based on my work within multidisciplinary consortia and addresses the National Human Genome Research Institute's priority research area of improving the effectiveness of healthcare. In the proposed work we will test the effectiveness of a multilevel genomic e-Health intervention in cancer (Aim 1). Our intervention 1) educates physicians and patients about genomics, 2) enables direct-to-patient return-of- results, 3) provides physicians with patient-specific results and resources for interpretation, and 4) facilitates sharing of genomic results within families. We hypothesize that intervention use will result in higher rates of uptake of high-quality, genetically guided care. We will test our hypothesis in a randomized controlled trial among academic and community physicians who use WES for their patients. Next, we will use an iterative process, with stakeholder engagement, to adapt and pilot test our tool for Spanish and Mandarin speaking patients and for patients who have diabetes (Aim 2). Finally, we will create and assess new, moderated, social networks as a platform for genomic information sharing (Aim 3). Our hypothesis is that providers, patients and family members will engage with the genomic information sharing social networks and find them to be highly useful. Our general approach includes 1) creating the secure social networks, 2) integrating the networks into our e-Health intervention, and 3) using complementary methods, such as interviews and natural language processing, to assess stakeholders' network-related attitudes and network information quality. If successful, we will be well positioned to widely disseminate our e-Health tools. In sum, this work stands to transform how people obtain, process and share genomic information in the context of clinical care. Our tools reconceive genetic communication to allow for multi-directional flow of information, connects multiple stakeholders with one another, and integrates high-quality dynamic web-based resources to improve genomic care. In creating and deploying tools that both respond to and leverage the complexities of our information environment, we intend to transform genomic research and clinical practice. PROJECT NARRATIVE/ RELEVANCE OF PROJECT TO RESEARCH AND PUBLIC HEALTH Widespread utilization of genomic sequencing in medicine creates an urgent need to educate providers and patients. Currently, providers frequently misinterpret genomic information and patients often don't understand their own test results. In order to address this critical need, we propose to design and test multiple e-Health communication tools that will help providers and patients to better understand genomic data, lead to higher quality patient care, and facilitate genomic information sharing within families.",Scalable tools to effectively translate genomic discoveries into the clinic,9815293,R35HG010721,"['Address', 'Area', 'Attitude', 'Caring', 'Cessation of life', 'Clinic', 'Communication', 'Communication Tools', 'Community Physician', 'Data', 'Diabetes Mellitus', 'Effectiveness', 'Environment', 'Excision', 'Family', 'Family member', 'Genetic', 'Genome', 'Genomic medicine', 'Genomics', 'Health', 'Healthcare', 'Human Genome', 'Information Networks', 'Intervention', 'Interview', 'Lead', 'Malignant Neoplasms', 'Medicine', 'Methods', 'National Human Genome Research Institute', 'Natural Language Processing', 'Organ', 'Patient Care', 'Patients', 'Physicians', 'Positioning Attribute', 'Process', 'Provider', 'Public Health', 'Randomized Controlled Trials', 'Research', 'Research Priority', 'Resources', 'Secure', 'Social Network', 'Sum', 'Test Result', 'Testing', 'Translating', 'Vision', 'Work', 'base', 'clinical care', 'clinical practice', 'design', 'disorder prevention', 'disorder risk', 'eHealth', 'exome', 'genome sequencing', 'genomic data', 'genomic platform', 'improved', 'multidisciplinary', 'online resource', 'premature', 'tool', 'uptake', 'whole genome']",NHGRI,BECKMAN RESEARCH INSTITUTE/CITY OF HOPE,R35,2019,556256,0.10166019455093214
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9724498,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2019,67704,0.055102561203800436
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9693291,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2019,397125,0.025751572558715952
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9765970,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Simulation', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Imagery', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2019,433604,0.06757966858378074
"TOOLS FOR NORMALIZING AND INTERPRETING THE CLINICAL ACTIONABILITY OF GENOMIC VARIANTS PROJECT SUMMARY/ABSTRACT The availability of high-throughput, low-cost sequencing has transformed the landscape of biomedical research by dramatically expanding our capacity to interrogate the sequence of the human genome. Consequently, there has been an explosion of biomedical literature describing the role of specific genomic variants and their impact on human diseases. These advances are bringing sequencing into the clinic to shape clinical practice from the patient’s genomic content, a paradigm colloquially referred to as genomic or precision medicine. There remain many obstacles to fully realizing our potential in the era of precision medicine. Among them is a recognized need for robust, well-engineered systems that provide knowledge about genomic variants and their role in disease. Ideally, such systems would provide a comprehensive summary of all knowledge that is relevant to the patient’s unique genomic content. An early bottleneck to realizing precision medicine was that, despite the substantial literature and several established knowledgebases that define interactions between drugs and genes, querying across them was extremely challenging. In response to this need, the Drug-Gene Interaction database (DGIdb, dgidb.org) was developed. Through a combination of automated processing and manual curation, drug-gene interaction information was collected, structured, and connected (normalized) from these diverse sources of data and entered into a database with a user-friendly search interface and an application programming interface (API). However, linking drug and drug-gene interaction concepts across resources remains an extremely challenging task, and aggregated drug-gene interactions are also challenging to represent in a way that highlights the utility of the collected knowledge for precision medicine efforts. This proposal seeks to improve our ability to normalize and interpret drug-gene interactions corresponding to patient genomic variants. We will achieve this goal through two specific aims. First, the DGIdb normalization routines will be improved through incorporation of new content and features. Among these, the DGIdb will support collections of drugs, including combination therapies and drug classes. Also, the DGIdb will have new community submission and curation features, allowing users to incorporate new knowledge into the database. Second, the Variant Interpretation Aggregator database (VIAdb) will be created to normalize knowledge across several disparate sources focused on the clinical interpretations of genomic variants. The VIAdb will operate as a stand-alone web tool and API and will behave as a source of relevant interpretations to DGIdb. Finally, we will develop techniques for automated identification of drug-gene interactions and variant interpretation consensus to assist community curation efforts. If successful, this research will improve breadth and consistency of variant interpretations and drug-gene interactions for precision medicine efforts. PROJECT NARRATIVE This research will improve our ability to interpret genomic variations in human patients in support of precision medicine efforts. Specifically, it will provide web-based tools for identifying potential therapies that specifically target the patient’s individual genes or variants, and an assessment of the clinical actionability of those drugs.",TOOLS FOR NORMALIZING AND INTERPRETING THE CLINICAL ACTIONABILITY OF GENOMIC VARIANTS,9744231,K99HG010157,"['Biological Markers', 'Biomedical Research', 'Client', 'Clinic', 'Clinical', 'Clinical Trials', 'Clinical assessments', 'Cohort Studies', 'Collection', 'Combined Modality Therapy', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Disease', 'Engineering', 'Explosion', 'Genes', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Imagery', 'Internet', 'Knowledge', 'Level of Evidence', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Procedures', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Role', 'Services', 'Shapes', 'Side', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'The Vanderbilt-Ingram Cancer Center at the Vanderbilt University', 'Variant', 'Work', 'application programming interface', 'base', 'cancer type', 'clinical practice', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'design', 'drug discovery', 'drug resource', 'gene interaction', 'genetic variant', 'genomic variation', 'human disease', 'improved', 'individual patient', 'knowledge base', 'learning strategy', 'molecular marker', 'novel', 'novel therapeutics', 'open source', 'precision medicine', 'profiles in patients', 'programs', 'response', 'therapeutic biomarker', 'tool', 'tumor', 'user-friendly', 'web app', 'web interface', 'web-based tool']",NHGRI,WASHINGTON UNIVERSITY,K99,2019,115001,0.04035525324432359
"Organizational and Cultural Dynamics in Genomics Companies: Industry Engagement in Navigating Social and Ethical Issues This K99/R00 Award is designed to generate scholarship and interventions to guide genomics companies towards more just practices. It does so through a five-year training and research project, which investigates perspectives from members of the genomics industry, and leverages them to inform normative analyses and identify feasible paths towards concrete change. The project addresses issues of price, access and industrial control, with a focus on the ethics of profit and social responsibility. Through the proposed training program, the project prepares the candidate as an independent scholar whose research program links empirical and normative research with practical interventions to positively influence ethics and justice in private sector genomics. The first phase of research (conducted during the K phase, alongside training activities) is a comparative study examining social and cultural factors involved in perceptions of ethical issues among members of industry. The comparative study focuses on three biotech hubs in the US (the Bay Area, San Diego, and Boston), as well as an additional site in South Africa (Cape Town). The South African site helps place investigation of the US industry in light of a shifting global industry, and especially South Africa’s policy focus on genomics for population health (as well as its role in the H3Africa initiative). The study also considers industry subsector and individual company culture, in examining influences on industry approaches to social and ethical issues. During its R phase, the project then involves a systematic normative analysis of profit and social obligation in the genomics industry, based on key theories from bioethics and business ethics. The project will then feed this scholarly analysis back to industry stakeholders; using a Delphi study, it allows members of industry to use results of the normative analysis in strategizing interventions with the greatest likelihood of influencing the genomics industry in ethically desirable ways. This project addresses the NHGRI ELSI research priorities of genomic equity and social justice, as well as genomics and public health. Its career development plan focuses on training in normative analysis and Delphi methods, as well as content training in genomics and entrepreneurship. The training will be conducted at Johns Hopkins University, where the school’s preeminent bioethics center houses world experts in qualitative and quantitative empirical bioethics, alongside the University’s leading research and training in the biomedical sciences and their application in industry. The integrated research and training plan will prepare the candidate as an independent ELSI scholar with a rigorous research program focused on empirical and normative analysis of the business of biomedicine and genomics, and engagement with industry stakeholders. This Pathway to Independence Award (K99/R00) investigates and leverages perspectives from members of the health-related private sector genomics industry, to develop guidance for improving approaches to social and ethical issues in the industry. It does so through in-depth qualitative analysis (interviews, cases studies, comparative analysis), scholarly normative analysis (drawing on theories from bioethics and business ethics), and a Delphi process of iterative questionnaires with industry stakeholders, aimed at strategizing concrete change regarding social obligations of the industry. The research and training program is designed to contribute to a more just distribution of the benefits of genomic technologies.",Organizational and Cultural Dynamics in Genomics Companies: Industry Engagement in Navigating Social and Ethical Issues,9720322,K99HG010499,"['Address', 'Agreement', 'American', 'Area', 'Attention', 'Award', 'Back', 'Bioethics', 'Bioethics Consultants', 'Biotechnology', 'Boston', 'Businesses', 'California', 'Case Study', 'Comparative Study', 'Complement', 'Corporate Ethics', 'DNA', 'Decision Making', 'Delphi Study', 'Development', 'Development Plans', 'Diagnostic', 'Empirical Research', 'Employee', 'Entrepreneurship', 'Ethical Issues', 'Ethics', 'Face', 'Genetic', 'Genomic approach', 'Genomics', 'Goals', 'Government', 'Health', 'Health Technology', 'India', 'Individual', 'Industrialization', 'Industry', 'Inequality', 'Intervention', 'Interview', 'Investigation', 'Justice', 'Licensing', 'Light', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pathway interactions', 'Patients', 'Perception', 'Phase', 'Play', 'Policies', 'Price', 'Privacy', 'Private Enterprises', 'Private Sector', 'Privatization', 'Process', 'Public Health', 'Questionnaires', 'Research', 'Research Priority', 'Research Project Grants', 'Research Training', 'Role', 'Scholarship', 'Schools', 'Science', 'Shapes', 'Silicon', 'Site', 'Social Justice', 'Social Obligations', 'Social Responsibility', 'South Africa', 'South African', 'Strategic Planning', 'Structure', 'Technology', 'Therapeutic', 'Training', 'Training Activity', 'Training Programs', 'Universities', 'Work', 'base', 'career development', 'comparative', 'curative treatments', 'design', 'drug development', 'experience', 'feeding', 'gene therapy', 'genome editing', 'improved', 'individual response', 'innovation', 'insight', 'member', 'nebula', 'next generation sequencing', 'novel diagnostics', 'novel therapeutics', 'operation', 'population health', 'programs', 'research and development', 'social', 'theories', 'training project']",NHGRI,JOHNS HOPKINS UNIVERSITY,K99,2019,128484,0.031121367090132288
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9693289,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'analysis pipeline', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2019,864186,0.043457165064557726
"Development of dictyBase, an online informatics resource PROJECT SUMMARY dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species. A community resource, widely supported by the research community, dictyBase contains gold standard expert literature curation of genes, functional annotations using the Gene Ontology and a wide range of genomic resources. Dictyostelium is widely used to study cellular processes such as cell motility, chemotaxis, signal transduction, cellular response to drugs, and host-pathogen interactions. Dictyostelium's genome contains significant orthologs of vertebrate, yeast and microbial genes, attracting researchers interested in a wide variety of biological topics including human disease, multicellular differentiation and comparative genomics. dictyBase enables researchers to search, view and download up-to-date genomic, functional and technical information. It is also widely used by teachers/instructors due to the wealth of available teaching materials and research protocols. Dictyostelium investigators depend on dictyBase as their primary community resource, where help from dictyBase staff (dictyBase help line) or from other users (Dicty ListServ, moderated by dictyBase) is available. We are in the final stages of deploying our completely new technology stack. By the end of this year dictyBase will be run entirely as a cloud-based application. This propoal seeks support to continue operating and expanding this important community resource. Our goals for this proposal are: (Aim 1) To continue (a) expert curation by dictyBase curators and enable (b) Community curation leveraging our strong relationship with the community. We will use additional sequence data to (c) update the AX4 reference genome sequence and improve the efficiency of curation by using (d) Deep learning-based linking of papers to genes prioritizing them for further analysis and curation. (Aim 2) We will improve dictyBase utility and usability by implementing (a) Bulk annotation methods for importing large-scale data sets using both (i) a web interface and (ii) a script/command line method. (b) We will add 10 additional Dictyostelid genomes using automated methods to annotate them. We will improve usability by implementing a (c) concurrent blast search with a new user interface and integrate this with the JBrowse display. (Aim 3) To expand the data and increase the richness of annotations available in dictyBase we will implement mechanisms to capture, store and display: (a) additional context to GO annotations (i) using existing GO extensions and (ii) annotating and displaying biological pathways using GO CAM models; (b) integrate and display genome wide insertion mutant information for over 20 thousand insertional mutants; and (c) develop a graphical display of spatial expression data using Dictyostelium anatomy ontology terms (i) by adding a track in JBrowse for genes annotated with spatial / anatomy expression terms, and (ii) creating a graphical display of these annotations via our Circos-based dashboard tool. As other data sets become available we will add them to dictyBase and develop methods to display the data and make it searchable. PROJECT NARRATIVE dictyBase is the model organism database (MOD) for the eukaryote Dictyostelium discoideum and related species, Dictyostelium is widely used for research in the biomedical, genetic, and environmental domains. The database uses the genome of Dictyostelium to organize biological knowledge developed using this experimental system, and dictyBase is manually curated and up-to-date with current literature. This application proposes capturing new types of data and providing tools to search and visualize that data.","Development of dictyBase, an online informatics resource",9738586,R01GM064426,"['Anatomy', 'Animals', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Cell physiology', 'Chemotaxis', 'Code', 'Collaborations', 'Communities', 'DNA sequencing', 'Data', 'Data Display', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Dictyostelium', 'Dictyostelium discoideum', 'Disease', 'Engineering', 'Eukaryota', 'FAIR principles', 'Funding', 'Gene Proteins', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Gold', 'Information Resources', 'Investments', 'Knowledge', 'Link', 'Literature', 'Manuals', 'Methods', 'Modeling', 'Names', 'Nomenclature', 'Ontology', 'Orthologous Gene', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phenotype', 'Plants', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Research Support', 'Resource Informatics', 'Resources', 'Running', 'Signal Transduction', 'Site', 'Students', 'Supervision', 'System', 'Teaching Materials', 'United States National Institutes of Health', 'Update', 'Work', 'Yeasts', 'analytical tool', 'base', 'cell motility', 'cloud based', 'comparative genomics', 'contig', 'dashboard', 'data warehouse', 'deep learning', 'experimental study', 'genome annotation', 'genome-wide', 'human disease', 'improved', 'instructor', 'interest', 'microbial', 'model organisms databases', 'mutant', 'new technology', 'novel', 'pathogen', 'reference genome', 'response', 'teacher', 'tool', 'usability', 'web interface']",NIGMS,NORTHWESTERN UNIVERSITY AT CHICAGO,R01,2019,506287,0.014220625543002512
"Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders Project Summary/Abstract  In the field of Alzheimer’s and related disorder, there has been very little work focusing on imaging genomics biomarker approaches, despite considerable promise. In part this is due to the fact that most studies have fo- cused on candidate gene approaches or those that do not capitalize on capturing (and amplifying) small effects spread across many sites. Even for genome wide studies, the vast majority of imaging genomic studies still rely on massive univariate analyses. The use of multivariate approaches provides a powerful tool for analyzing the data in the context of genomic and connectomic networks (i.e. weighted combinations of voxels and genetic variables). It is clear that imaging and genomic data are high dimensional and include complex relationships that are poorly understood. Multivariate data fusion models that have been proposed to date typically suffer from two key limitations: 1) they require the data dimensionality to match (i.e. 4D fMRI data has to be reduced to 1D to match with the 1D genomic data, and 2) models typically assume linear relationships despite evidence of non- linearity in brain imaging and genomic data. New methods are needed that can handle data that has mixed temporal dimensionality, e.g., single nucleotide polymorphisms (SNPs) do not change over time, brain structure changes slowly over time, while fMRI changes rapidly over time. Secondly, methods that can handle complex relationships, such as groups of networks that are tightly coupled or nonlinear relationships in the data. To ad- dress these challenges, we introduce a new framework called flexible subspace analysis (FSA) that can auto- matically identify subspaces (groupings of unimodal or multimodal components) in joint multimodal data. Our approach leverages the interpretability of source separation approaches and can include additional flexibility by allowing for a combination of shallow and ‘deep’ subspaces, thus leveraging the power of deep learning. We will apply the developed models to a large longitudinal dataset of individuals at various stages of cognitive impair- ment and dementia. Using follow-up outcomes data we will evaluate the predictive accuracy of a joint analysis compared to a unimodal analysis, as well as its ability to characterize various clinical subtypes including those driven by vascular effects including subcortical ischemic vascular dementia versus those that are more neuro- degenerative. We will evaluate the single subject predictive power of these profiles in independent data to max- imize generalization. All methods and results will be shared with the community. The combination of advanced algorithmic approach plus the large N data promises to advance our understanding of Alzheimer’s and related disorders in addition to providing new tools that can be widely applied to other studies of complex disease. 3 Project Narrative  It is clear that multimodal data fusion provides benefits over unimodal analysis, however existing approaches typically require the data to have matched dimensionality, leading to a loss of information. In addition, most models assume linear relationships, despite strong evidence of nonlinear relationships in the data. We propose to develop new flexible models to capture multi-scale brain imaging and genomics data which we will use to study a large data set of individuals with Alzheimer’s disease and Alzheimer’s disease related disorders. 2",Flexible multivariate models for linking multi-scale connectome and genome data in Alzheimer's disease and related disorders,9826772,RF1AG063153,"['3-Dimensional', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Behavior', 'Benchmarking', 'Biological', 'Blood Vessels', 'Brain', 'Brain imaging', 'Brain region', 'Candidate Disease Gene', 'Categories', 'Classification', 'Communities', 'Complex', 'Coupled', 'Data', 'Data Analyses', 'Data Set', 'Dementia', 'Diagnostic', 'Dimensions', 'Disease', 'Evaluation', 'Functional Magnetic Resonance Imaging', 'Future', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Image', 'Impaired cognition', 'Individual', 'Joints', 'Lead', 'Linear Models', 'Link', 'Magnetic Resonance Imaging', 'Meta-Analysis', 'Methods', 'Modality', 'Modeling', 'Motivation', 'Nerve Degeneration', 'Neurobiology', 'Noise', 'Outcome', 'Pathway interactions', 'Pattern', 'Research Personnel', 'Rest', 'Sampling', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Structure', 'Subgroup', 'Time', 'Vascular Dementia', 'Work', 'base', 'blind', 'clinical subtypes', 'connectome', 'data anonymization', 'data warehouse', 'deep learning', 'flexibility', 'follow-up', 'functional genomics', 'genome-wide analysis', 'genomic biomarker', 'genomic data', 'longitudinal dataset', 'mild cognitive impairment', 'multidimensional data', 'multimodal data', 'multimodality', 'neurobehavioral', 'novel', 'patient subsets', 'statistics', 'structural genomics', 'subcortical ischemic vascular disease', 'tool', 'user friendly software', 'white matter damage']",NIA,GEORGIA STATE UNIVERSITY,RF1,2019,3319889,-0.004760978971756499
"Discovering Novel Structural Genomic Rearrangements Using Deep Neural Networks Abstract Accurately detecting structural variation in the genome is a challenging task. Many approaches have been developed over the last few decades, yet it is estimated that tens of thousands of variants are still being missed in a given sample. Many of these variants are missed due to the limitations of using short-read sequencing to identify large variants. Although many of these missed variants are located within complex regions of the genome, it has been shown that some still have clinical relevance making their discovery important. New platforms have been developed for sequencing the genome using long-reads and show promise for overcoming many of these limitations creating the ability to identify the full spectrum of simple and complex structural variants. Because this technology is relatively young, new computational approaches to support the analysis of long-read sequencing data can aid in the discovery of these variants which are still being missed. In addition to detecting novel variation in samples with long-read sequencing data, computational approaches can be developed to leverage these novel variant calls to reanalyze the hundreds of thousands of short-read datasets currently available. In this proposal, we plan to develop new computational approaches to identify novel structural variation in the genome. In Aim 1, we will apply a recurrence approach to analyze long read sequencing datasets utilizing deep neural networks. In Aim 2, we will develop a tool to derive profiles of structural variants predicted in long- reads which can be used to identify and genotype structural variants calls in short read data-sets. Together, these approaches will allow researchers to accurately characterize structural variation in both long and short- read datasets. Narrative Structural variation has been implicated in numerous human diseases but there are still tens of thousands of variants being overlooked in the genome. The proposed research aims to detect novel variation by developing new computational tools to analyze data generated by state-of-the-art sequencing methods. These tools will aid in the discovery of variants associated with human health.",Discovering Novel Structural Genomic Rearrangements Using Deep Neural Networks,9755117,F31HG010569,"['Affect', 'Algorithms', 'Benchmarking', 'Biological Sciences', 'Categories', 'Complex', 'Computing Methodologies', 'DNA', 'DNA Resequencing', 'DNA Sequence Rearrangement', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Future', 'Genome', 'Genotype', 'Haplotypes', 'Health', 'Human', 'Human Genome', 'Image', 'Image Analysis', 'Label', 'Methods', 'Molecular', 'Molecular Computations', 'Pattern', 'Process', 'Recurrence', 'Repetitive Sequence', 'Research', 'Research Personnel', 'Sampling', 'Structure', 'Techniques', 'Technology', 'Training', 'Validation', 'Variant', 'base', 'clinically relevant', 'comparative', 'computerized tools', 'cost', 'deep learning', 'deep neural network', 'design', 'genome sequencing', 'human disease', 'insertion/deletion mutation', 'new technology', 'novel', 'reference genome', 'structural genomics', 'tool']",NHGRI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,F31,2019,37153,0.016576208793395993
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9902000,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Infrastructure', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2019,730148,0.07409919767373506
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9753389,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Autosomal dominant nocturnal frontal lobe epilepsy\xa0', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'informatics\xa0tool', 'phenotypic data', 'rare variant', 'sample collection']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2019,60925,-0.014531244734719603
"The Enzymatic Reader Project Summary At this point in time, it is generally understood and agreed upon that single-molecule sequencing (SMS) is the future of genomics, transcriptomics, epigenomics, and epitranscriptomics due to its significant advantages over other technologies and methods. However, in order for these advantages to be fully realized, and for SMS to become the “gold standard” sequencing approach, significant issues and hurdles must be solved and overcome. During this program, Electronic BioSciences, Inc. (EBS) aims to demonstrate a completely new and enabling SMS method that will possess the ability to directly and correctly identify individual nucleotides, including chemically modified nucleotides. During this project, we will both demonstrate the ability of this entirely new sequencing approach to sequence DNA with high accuracy (directly comparing the obtained accuracy, throughput, error mechanisms and associated rates to other SMS approaches) and correctly identify (and sequence) 5-methylcytosine (5mC) and its derivatives, at the single molecule level. At the conclusion of this Phase I project, we will have successfully demonstrated an entirely new and dramatically improved SMS approach, and reduced the associated risks involved with its full future commercial developments. There is a current need within the field of next generation sequencing (NGS) or so called third generation sequencing (TGS) for new, enabling instrumentation that is capable of high-accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats. The entirely new SMS methodology that will be developed during this project will overcome known hurdles and limitations of currently available NGS, TGS, and SMS technologies, resulting in technology that is cost-efficient, highly accurate, easy to setup and utilize, capable of de novo sequencing and modified base calling, and yields highly simplistic data for easy analysis and post possessing. Through significant advancements made during this program, this resulting technology will revolutionize the use of the genome and epigenome, radically change standard R&D and clinical practices, and greatly advance clinical diagnostics, prognostics, and therapeutic decision making. Project Narrative The novel single-molecule sequencing (SMS) technology developed during this project will enable high- accuracy, direct, native DNA sequencing, including the ability to correctly identify canonical and modified bases, homopolymer stretches, and sequence repeats via a cost-efficient and easy-to-use methodology. The impact of these advances in SMS will eventually enable wide-scale, routine clinical care and diagnostics toward advanced precision medicine, not just R&D. The performance and accessibility of such technology will transform the understanding and application of genomics and epigenomics, the associated clinical practices, that ability to provide precision clinical diagnostics, prognostics, and therapeutic decision making for improved public healthcare and wellbeing.",The Enzymatic Reader,9677956,R43HG010427,"['Biological', 'Biological Sciences', 'Caliber', 'Chemicals', 'Chemistry', 'Church', 'Complex', 'DNA Primers', 'DNA Sequence', 'DNA polymerase A', 'DNA sequencing', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Decision Making', 'Development', 'Devices', 'Disadvantaged', 'Drops', 'Electrodes', 'Enzymes', 'Evaluation', 'Future', 'Genome', 'Genomics', 'Goals', 'Gold', 'Healthcare', 'Individual', 'Ions', 'Label', 'Length', 'Lipid Bilayers', 'Logistics', 'Methodology', 'Methods', 'Motor', 'Movement', 'Noise', 'Nucleotides', 'Performance', 'Personal Satisfaction', 'Phase', 'Polymerase', 'Polymers', 'Preparation', 'Process', 'Proteins', 'RNA', 'Reader', 'Reading Frames', 'Reproducibility', 'Risk', 'Sampling', 'Side', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Software Tools', 'Speed', 'Stretching', 'System', 'Technology', 'Therapeutic', 'Third Generation Sequencing', 'Time', 'base', 'clinical care', 'clinical diagnostics', 'clinical practice', 'cost', 'cost efficient', 'electric field', 'epigenome', 'epigenomics', 'epitranscriptomics', 'improved', 'instrumentation', 'machine learning algorithm', 'nanopore', 'next generation sequencing', 'novel', 'precision medicine', 'prevent', 'prognostic', 'programs', 'research and development', 'single molecule', 'solid state', 'transcriptomics']",NHGRI,"ELECTRONIC BIOSCIENCES, INC.",R43,2019,247611,0.018817549168212017
"EHR-based Genomic Discovery and Implementation PROJECT SUMMARY Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III, we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders–familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)–we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PROJECT NARRATIVE Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders – familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) – we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9694811,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Familial Hypercholesterolemia', 'Familial colorectal cancer', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Penetrance', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Therapeutic Intervention', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'actionable mutation', 'adverse drug reaction', 'base', 'biobank', 'clinical decision support', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel strategies', 'phenotypic data', 'pleiotropism', 'point of care', 'psychosocial', 'rare variant', 'recruit', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2019,826601,0.04563565981179367
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9894990,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health Care Costs', 'Health system', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Care Costs', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'care costs', 'clinical care', 'clinical decision support', 'clinical practice', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'education resources', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'patient portal', 'polyposis', 'practice setting', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2019,707923,0.05932586713238901
"Computational tools for regulome mapping using single-cell genomic data Project Summary Understanding how genes' activities are controlled is crucial for elucidating the basic operating rules of biology and molecular mechanisms of diseases. Recent innovations in single-cell genomic technologies have opened the door to analyzing a variety of functional genomic features in individual cells. These technologies enable scientists to systematically discover unknown cell subpopulations in complex tissue and disease samples, and allow them to reconstruct a sample's gene regulatory landscape at an unprecedented cellular resolution. Despite these promising developments, many challenges still exist and must be overcome before one can fully decode gene regulation at the single-cell resolution. In particular, current technologies lack the ability to accurately measure the activity of each individual cis-regulatory element (CRE) in a single cell. They also cannot measure all functional genomic data types in the same cell. Moreover, the prevalent technical biases and noises in single-cell genomic data make computational analysis non-trivial. With rapid growth of data, lack of computational tools for data analysis has become a rate-limiting factor for effective applications of single-cell genomic technologies.  The objective of this proposal is to develop computational and statistical methods and software tools for mapping and analyzing gene regulatory landscape using single-cell genomic data. Our Aim 1 addresses the challenge of accurately measuring CRE activities in single cells using single-cell regulome data. Regulome, deﬁned as the activities of all cis-regulatory elements in a genome, contains crucial information for understanding gene regulation. The state-of-the-art technologies for mapping regulome in a single cell produce sparse data that cannot accurately measure activities of individual CREs. We will develop a new computational framework to allow more accurate analysis of individual CREs' activities in single cells using sparse data. Our Aim 2 addresses the challenge of collecting multiple functional genomic data types in the same cell. We will develop a method that uses single-cell RNA sequencing (scRNA-seq), the most widely used single-cell functional genomic technology, to predict cells' regulatory landscape. Since most scRNA-seq datasets do not have accompanying single-cell data for other -omics data types, our method will also signiﬁcantly expand the utility and increase the value of scRNA- seq experiments. Our Aim 3 addresses the challenge of integrating different data types generated by different single-cell genomic technologies from different cells. We will develop a method to align single-cell RNA-seq and single-cell regulome data to generate an integrated map of transcriptome and regulome.  Upon completion of this proposal, we will deliver our methods through open-source software tools. These tools will be widely useful for analyzing and integrating single-cell regulome and transcriptome data. By addressing several major challenges in single-cell genomics, our new methods and tools will help unleash the full potential of single-cell genomic technologies for studying gene regulation. As such, they can have a major impact on advancing our understanding of both basic biology and human diseases. Project Narrative Understanding how genes' activities are controlled at single-cell resolution is crucial for studying human diseases. This proposal will develop a coordinated set of computational and statistical methods and software tools for mapping and analyzing gene regulatory programs using single-cell genomic data. These methods and tools will allow scientists to more accurately and comprehensively reconstruct gene regulatory landscape of individual cells in complex tissue and disease samples, and they can have a major impact on advancing our understanding of both basic biology and human diseases.",Computational tools for regulome mapping using single-cell genomic data,9649896,R01HG010889,"['Address', 'Atlases', 'Behavior', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Cells', 'Cellular Assay', 'Chromatin', 'Complex', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Emerging Technologies', 'Foundations', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genomics', 'Human', 'Immune system', 'Individual', 'Knowledge', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Modality', 'Molecular', 'Multiomic Data', 'Noise', 'Organ', 'Phase', 'Population', 'Regulator Genes', 'Regulatory Element', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Software Tools', 'Statistical Methods', 'Stem Cell Development', 'System', 'Technology', 'Therapeutic', 'Tissues', 'Training', 'Transposase', 'base', 'computer framework', 'computerized tools', 'epigenome', 'experimental study', 'functional genomics', 'genomic data', 'histone modification', 'human disease', 'innovation', 'multiple omics', 'novel strategies', 'open source', 'predictive modeling', 'programs', 'rapid growth', 'single cell analysis', 'single cell technology', 'single-cell RNA sequencing', 'supervised learning', 'tool', 'transcriptome', 'transcriptome sequencing', 'user-friendly']",NHGRI,JOHNS HOPKINS UNIVERSITY,R01,2019,409375,-0.022871361330505907
"Center for Sub-Cellular Genomics A cell is a highly complex system with distributed molecular physiologies in structured sub- cellular compartments whose interplay with the nuclear genome determine the functional characteristics of the cell. A classic example of distributed genomic processes is found in neurons. Learning and memory requires modulation of individual synapses through RNA localization, localized translation, and localized metabolites such as those from dendritic mitochondria. Dendrites of neurons integrate distributed synaptic signals into both electrical and nuclear transcriptional response. Dysfunction of these distributed genomic functions in neurons can result in a broad spectrum of neuropsychiatric diseases such as bipolar and depressive disorders, autism, among others. Understanding complex genomic interactions within a single cell requires new technologies: we need nano-scale ability to make genome-wide measurements at highly localized compartments and to effect highly localized functional genomic manipulations, especially in live tissues. To address this need, we propose to establish a Center for Sub-Cellular Genomics using neurons as model systems. The center will develop new optical and nanotechnology approaches to isolate sub-cellular scale components for genomic, metabolomics, and lipidomic analyses. The center will also develop new mass spectrometry methods, molecular biology methods, and informatics models to create a platform technology for sub-cellular genomics. Many human diseases, especially neuropsychiatric diseases, can be traced to dysfunction of organelles and other sub-cellular components. This project will create novel technologies to study function and dysfunction of sub-cellular processes and apply them to study of neurons.",Center for Sub-Cellular Genomics,9772520,RM1HG010023,"['Address', 'Alzheimer&apos', 's Disease', 'Attention', 'Automobile Driving', 'Awareness', 'Biological Assay', 'Biological Models', 'Bipolar Disorder', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Cellular biology', 'Characteristics', 'Chromatin', 'Communities', 'Complex', 'Data', 'Dendrites', 'Depressive disorder', 'Devices', 'Disease', 'Education and Outreach', 'Educational workshop', 'Functional disorder', 'Genetic Transcription', 'Genome', 'Genomics', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Informatics', 'Institution', 'Investigation', 'Learning', 'Liquid substance', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Memory', 'Mental Depression', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Molecular Biology', 'Motivation', 'Nanotechnology', 'Neurodegenerative Disorders', 'Neurons', 'Nuclear', 'Optical Methods', 'Optics', 'Organelles', 'Output', 'Pathway interactions', 'Phenotype', 'Philadelphia', 'Physiology', 'Process', 'Proteins', 'RNA', 'Regulation', 'Research Project Grants', 'Resolution', 'Risk', 'Rodent', 'Role', 'Schizophrenia', 'Signal Transduction', 'Structure', 'Synapses', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Traumatic Brain Injury', 'Variant', 'Work', 'autism spectrum disorder', 'base', 'deep learning', 'epigenomics', 'exosome', 'experience', 'flexibility', 'functional genomics', 'genome-wide', 'human disease', 'informatics\xa0tool', 'macromolecule', 'metabolomics', 'multimodality', 'nanoscale', 'neuropsychiatric disorder', 'new technology', 'novel', 'outreach', 'response', 'subcellular targeting', 'technology development', 'technology validation', 'transcriptome', 'welfare', 'whole genome']",NHGRI,UNIVERSITY OF PENNSYLVANIA,RM1,2019,500000,0.03585550551534488
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9552183,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'deep learning', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2018,297725,0.041871624651898955
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell- based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off- target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,9678132,U01HL145793,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'learning strategy', 'next generation', 'novel', 'novel therapeutics', 'response', 'safety testing', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NHLBI,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2018,651251,0.020392076295167597
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9617314,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2018,249000,0.00864324943971247
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9566212,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'clinical decision support', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2018,1069680,0.04641831173071665
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9547454,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2018,332952,0.05174906684872359
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,0.056249219120396
"Selective Whole Genome Amplification - Enabling Microbial Population Genomics Microbial population genetic research has been crucial for understanding pathogen dynamics, virulence, host specificity, and many other topics; in many cases uncovering unexpected and transformative biological processes. However, conventional population genetic analyses are limited by the quantity of sequence data from each sample. The temporal, spatial, and evolutionary resolution of techniques that rely on single gene sequences or multi-locus sequence typing are often insufficient to study biological processes on fine scales, precisely the scales at which many evolutionary and mechanistic process occur. Population genomics offers a vast quantity of sequence information for inferring evolutionary and ecological processes on very fine spatial and temporal scales, inferences that are critical to understanding and eventually controlling many infectious diseases. The promise of population genomics is tempered, however, by difficulties in isolating and preparing microbes for next-generation sequencing. We have developed the selective whole genome amplification (SWGA) technology to sequence microbial genomes from complex biological specimens without relying on labor-intensive laboratory culture, even if the focal microbial genome constitutes only a miniscule fraction of the natural sample. The primary hindrance to popular adoption of SWGA for microbial genomic studies is not its effectiveness in producing samples suitable for next-generation sequencing but in the upfront investment needed to develop an effective protocol to amplify the genome of a specific microbial species. Identifying an SWGA protocol that consistently results in selective and even amplification across the target genome is currently hindered by computationally-inefficient software that can evaluate a very limited set of the potentially effective solutions. Further, this software uses marginally-effective optimality criteria as there is currently only a limited understanding of the true criteria that result in highly-selective and even amplification of a target genome. As a result, SWGA protocol development is currently costly in both time and resources. A primary goal of the proposed research is to identify the criteria that result in optimal SWGA by analyzing next- generation sequencing data with advanced machine learning techniques. These optimality criteria will be integrated into a freely-available, computationally-efficient swga development program that will reduce the upfront investment in SWGA protocol development, thus allowing researchers to address medically- and biologically-important questions in any microbial species. In the near term, this project will also generate effective SWGA protocols for four microbial species which can be used immediately to address fundamental questions in evolutionary biology, disease progression, and emerging infectious disease dynamics. From a global disease perspective, this work is imperative as the majority of microbial species cannot easily be cultured and are in danger of becoming bystanders in the genomics revolution that is currently elucidating evolutionary processes and molecular mechanisms in cultivable microbial species. Addressing many of the major outstanding questions about pathogen evolution will require analyses of populations of microbial genomes. Although population genomic studies would provide the analytical resolution to investigate evolutionary and mechanistic processes on fine spatial and temporal scales – precisely the scales at which these processes occur – microbial population genomic research is currently hindered by the practicalities of obtaining sufficient quantities of genomes to analyze. We propose to develop an innovative, cost-effective, practical, and publically-available technology to collect sufficient quantities of microbial genomic DNA necessary for next-generation microbial genome sequencing.",Selective Whole Genome Amplification - Enabling Microbial Population Genomics,9507167,R21AI137433,"['Address', 'Adoption', 'Affect', 'Algorithms', 'Biological', 'Biological Process', 'Biology', 'Characteristics', 'Communicable Diseases', 'Complex', 'Computer software', 'Coupling', 'DNA', 'Data', 'Development', 'Disease', 'Disease Progression', 'Effectiveness', 'Emerging Communicable Diseases', 'Evolution', 'Foundations', 'Genes', 'Genetic Research', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Health', 'Human', 'Investigation', 'Investments', 'Laboratory culture', 'Machine Learning', 'Medical', 'Metaphor', 'Methods', 'Microbe', 'Microbial Genome Sequencing', 'Microsatellite Repeats', 'Molecular', 'Organism', 'Population', 'Population Analysis', 'Population Genetics', 'Process', 'Program Development', 'Protocols documentation', 'Recording of previous events', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resources', 'Sampling', 'Shapes', 'Specificity', 'Specimen', 'System', 'Techniques', 'Technology', 'Time', 'Virulence', 'Work', 'cost', 'cost effective', 'design', 'genetic analysis', 'genetic approach', 'host-microbe interactions', 'improved', 'innovation', 'microbial', 'microbial genome', 'next generation', 'next generation sequencing', 'novel', 'pathogen', 'prevent', 'protocol development', 'vector', 'whole genome']",NIAID,UNIVERSITY OF PENNSYLVANIA,R21,2018,242837,0.06267758257783938
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9486266,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2018,479215,0.010230319840489638
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9593406,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Screening', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'virtual']",NHGRI,STANFORD UNIVERSITY,U01,2018,1420000,0.023158671956817763
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9562959,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2018,1186500,0.03349182712941097
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9501001,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'pathogen', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2018,359810,0.025109976801364994
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9420662,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2018,2000000,0.025853178847588157
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,9462637,U01HG009080,"['Algorithms', 'Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Privatization', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Risk', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'online resource', 'patient privacy', 'phenome', 'prediction algorithm', 'predictive modeling', 'programs', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'simulation', 'statistics', 'tool', 'tool development', 'web-based tool']",NHGRI,STANFORD UNIVERSITY,U01,2018,921743,0.0445636893265572
"Integrating multidimensional genomic data to discover clinically-relevant predictive models The goal of this NIH Pathway to Independence award is to provide Dr. Brittany Lasseigne with an extensive training program to prepare her to be an effective independent investigator who uses computational genomics to study complex human diseases. We propose a formal one-year training and mentoring program in genomics, computer science, statistics, and career development to build on her 8+ years of hands-on training, followed by a three-year structured and independent research program. Research will focus on the integration of multidimensional genomic data sets in the context of complex human diseases. A critical barrier in genomic research is the complexity of data integration: the ability to leverage overlapping and unique information captured by different genomic assays would improve our understanding of data integration and generate clinically relevant genomic signatures. To meet this need, we propose to integrate a combination of genomic data we generated with public data to (1) infer genomic instability signatures from different data types, (2) improve clinically relevant phenotype prediction by building multi-omics machine learning classifiers and reducing phenotype heterogeneity, and (3) create a cloud-enabled R package and associated Shiny application to accelerate future research. The proposed work will advance our understanding of data integration, allow inference of genomic instabilities across data sets, and generate high performance classifiers for assessing clinically relevant phenotypes in both cancer and psychiatric disease using frameworks that will be broadly applicable across other complex diseases. It will also facilitate prioritization of experiments in future studies by informing on the orthogonality of genomic assays, thereby allowing more efficient study designs to capture as much information as possible within a given sample size or scope of experimentation. Collectively, this additional training will allow Dr. Lasseigne to develop new multidimensional data integration approaches and translational questions applicable across complex diseases when independent. Dr. Richard Myers (HudsonAlpha) and Dr. Gregory Cooper (HudsonAlpha), leaders in applying genetics and genomics to complex human diseases, and an Advisory Committee of additional experts including Dr. Barbara Wold (Caltech), Dr. Eddy Yang (UAB), and Dr. Timothy Reddy (Duke), will provide mentoring throughout this award. The mentored phase will take place at the HudsonAlpha Institute for Biotechnology, an ideal environment for this training with extensive translational science collaborations, expert faculty and staff, and state-of-the art computational and laboratory resources devoted to genomics. This combination will maximize Dr. Lasseigne's training program, facilitating her transition to an independent, tenure-track investigator at a university with a strong commitment to data-driven approaches to complex human disease research, i.e. strong genomics research programs with clinical collaborators, ideally at, or affiliated with, an academic medical center. Project Narrative The major outcome of this project will be a scientist with the necessary research, mentoring, teaching, and career development training to run an independent research program in computational genomics. The research proposed will apply novel strategies to further develop integrative machine learning analyses of multidimensional genomic data, discover clinically relevant predictive models, and create computational tools to accelerate future research.",Integrating multidimensional genomic data to discover clinically-relevant predictive models,9526698,K99HG009678,"['Academic Medical Centers', 'Advisory Committees', 'Award', 'Bioconductor', 'Biological', 'Biological Assay', 'Biotechnology', 'Budgets', 'Cancer Etiology', 'Cell Proliferation', 'Cells', 'Characteristics', 'Chemotherapy-Oncologic Procedure', 'Chromosomal Instability', 'Chromosomes', 'Classification', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Copy Number Polymorphism', 'Coupling', 'CpG Island Methylator Phenotype', 'DNA', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Educational process of instructing', 'Environment', 'Faculty', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomic Instability', 'Genomics', 'Goals', 'Heterogeneity', 'Imagery', 'Individual', 'Institutes', 'Instruction', 'Laboratories', 'Lasso', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mental disorders', 'Mentors', 'Methodology', 'Methylation', 'MicroRNAs', 'Microsatellite Instability', 'Modeling', 'Molecular Profiling', 'Neurons', 'Outcome', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Signal Transduction', 'Standardization', 'Structure', 'Systems Biology', 'Techniques', 'Testing', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Variant', 'Work', 'Yang', 'biomarker performance', 'career development', 'clinically relevant', 'computer science', 'computerized tools', 'data integration', 'data reduction', 'experimental study', 'genomic data', 'genomic signature', 'human disease', 'improved', 'insight', 'multiple omics', 'novel strategies', 'predictive modeling', 'programs', 'promoter', 'protein metabolite', 'response', 'single cell sequencing', 'statistics', 'tenure track', 'tool']",NHGRI,HUDSON-ALPHA INSTITUTE FOR BIOTECHNOLOGY,K99,2018,112117,0.08208810549771806
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9545035,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2018,67704,0.055102561203800436
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9498252,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'cancer therapy', 'computerized data processing', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",R01,2018,158992,0.025751572558715952
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9483341,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2018,864186,0.043457165064557726
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9515974,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health care facility', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'implementation strategy', 'innovation', 'inpatient service', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2018,871212,0.07409919767373506
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9416160,R00HG008171,"['Advisory Committees', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Code', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'cancer drug resistance', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'targeted nucleases', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2018,242325,0.06376824856669158
"Epi25 Clinical Phenotyping R03 PROJECT SUMMARY Clinical genetic data suggests that specific categories of epilepsy have genetic contributors, and there may be some overlap between categories. The Epi25 Collaborative was formed among more than 40 cohorts from around the world to sequence as many as 25,000 genomes or exomes. As of 2017, the collaborative has sequenced more than 13,000 exomes and clinical data has been collected for more than 8,000 cases. This project will complete the collection and review of the clinical data for each sample in the Epi25 collection to facilitate the translation of genomic and clinical discoveries into improved care for patients. The clinical and genomic data from Epi25 will be a global resource, shared with the research community for years to come. Epi25's governance structure, membership, and other information are available online at www.epi-25.org. In this project, clinical data is entered by contributors into Red Cap forms or uploaded directly into the Epi25 database. The clinical data is then checked by a computer algorithm that looks for key eligibility criteria for each participant. Errors and missing data are sent to the Phenotyping Coordinator to review and resolve, with the help of the contributing site. PROJECT NARRATIVE In 2014, collaborators from around the world created the Epi25 Collaborative to exome sequence as many as 25,000 patients with epilepsy. The collaborative has more than 6,200 exomes generated in year 2016, an additional 7,500 on sequencers in 2017, and more than 1,000 ready for sequencing in 2018. This project will review and correct errors for the descriptive epilepsy data for each sample sequenced in Epi25, to reveal the genetic underpinnings of common epilepsies.",Epi25 Clinical Phenotyping R03,9584612,R03NS108145,"['Absence Epilepsy', 'Artificial Intelligence', 'Autosomal Dominant Partial Epilepsy with Auditory Features', 'Categories', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Computational algorithm', 'Data', 'Data Discovery', 'Databases', 'Eligibility Determination', 'Epilepsy', 'Ethnic Origin', 'Family', 'Frontal Lobe Epilepsy', 'Genes', 'Genetic', 'Genetic Databases', 'Genetic Determinism', 'Genetic Translation', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Hand', 'Informatics', 'International', 'Juvenile Myoclonic Epilepsy', 'Major Depressive Disorder', 'Medical Genetics', 'Methods', 'Neurodevelopmental Disorder', 'Partial Epilepsies', 'Participant', 'Patient Care', 'Patients', 'Pattern', 'Phenotype', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Site', 'Standardization', 'Structure', 'Syndrome', 'Temporal Lobe Epilepsy', 'Testing', 'Translations', 'Twin Studies', 'Variant', 'autism spectrum disorder', 'clinical phenotype', 'cohort', 'dravet syndrome', 'exome', 'genomic data', 'improved', 'phenotypic data', 'rare variant', 'sample collection', 'tool']",NINDS,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R03,2018,75911,-0.014531244734719603
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9502299,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Electronic Medical Records and Genomics Network', 'Familial Hypercholesterolemia', 'Familial colorectal cancer', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Therapeutic Intervention', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'actionable mutation', 'base', 'biobank', 'clinical decision support', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel strategies', 'phenotypic data', 'pleiotropism', 'point of care', 'psychosocial', 'public health relevance', 'rare variant', 'recruit', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2018,831652,0.04790996571088306
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9491866,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronic Medical Records and Genomics Network', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genetic Diseases', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health Care Costs', 'Health system', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Care Costs', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'care costs', 'clinical care', 'clinical decision support', 'clinical practice', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'patient portal', 'polyposis', 'practice setting', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2018,840228,0.05932586713238901
"Center for Sub-Cellular Genomics A cell is a highly complex system with distributed molecular physiologies in structured sub- cellular compartments whose interplay with the nuclear genome determine the functional characteristics of the cell. A classic example of distributed genomic processes is found in neurons. Learning and memory requires modulation of individual synapses through RNA localization, localized translation, and localized metabolites such as those from dendritic mitochondria. Dendrites of neurons integrate distributed synaptic signals into both electrical and nuclear transcriptional response. Dysfunction of these distributed genomic functions in neurons can result in a broad spectrum of neuropsychiatric diseases such as bipolar and depressive disorders, autism, among others. Understanding complex genomic interactions within a single cell requires new technologies: we need nano-scale ability to make genome-wide measurements at highly localized compartments and to effect highly localized functional genomic manipulations, especially in live tissues. To address this need, we propose to establish a Center for Sub-Cellular Genomics using neurons as model systems. The center will develop new optical and nanotechnology approaches to isolate sub-cellular scale components for genomic, metabolomics, and lipidomic analyses. The center will also develop new mass spectrometry methods, molecular biology methods, and informatics models to create a platform technology for sub-cellular genomics. Many human diseases, especially neuropsychiatric diseases, can be traced to dysfunction of organelles and other sub-cellular components. This project will create novel technologies to study function and dysfunction of sub-cellular processes and apply them to study of neurons.",Center for Sub-Cellular Genomics,9488697,RM1HG010023,"['Address', 'Alzheimer&apos', 's Disease', 'Attention', 'Autistic Disorder', 'Automobile Driving', 'Awareness', 'Biological Assay', 'Biological Models', 'Bipolar Disorder', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Cellular biology', 'Characteristics', 'Chromatin', 'Communities', 'Complex', 'Data', 'Dendrites', 'Depressive disorder', 'Devices', 'Disease', 'Education and Outreach', 'Educational workshop', 'Functional disorder', 'Genetic Transcription', 'Genome', 'Genomics', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Informatics', 'Institution', 'Investigation', 'Learning', 'Liquid substance', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Memory', 'Mental Depression', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Molecular Biology', 'Motivation', 'Nanotechnology', 'Neurodegenerative Disorders', 'Neurons', 'Nuclear', 'Optical Methods', 'Optics', 'Organelles', 'Output', 'Pathway interactions', 'Phenotype', 'Philadelphia', 'Physiology', 'Process', 'Proteins', 'RNA', 'Regulation', 'Research Project Grants', 'Resolution', 'Risk', 'Rodent', 'Role', 'Schizophrenia', 'Signal Transduction', 'Structure', 'Synapses', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Traumatic Brain Injury', 'Variant', 'Work', 'autism spectrum disorder', 'base', 'deep learning', 'epigenomics', 'exosome', 'experience', 'flexibility', 'functional genomics', 'genome-wide', 'human disease', 'macromolecule', 'metabolomics', 'multimodality', 'nanoscale', 'neuropsychiatric disorder', 'new technology', 'novel', 'outreach', 'response', 'subcellular targeting', 'technology development', 'technology validation', 'tool', 'transcriptome', 'welfare', 'whole genome']",NHGRI,UNIVERSITY OF PENNSYLVANIA,RM1,2018,1000000,0.03585550551534488
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9328097,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Algorithms', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA sequencing', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'STEM research', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Time Series Analysis', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'disorder risk', 'fitness', 'flexibility', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2017,298655,0.041871624651898955
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9333402,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Supervision', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2017,37785,0.00864324943971247
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9466681,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2017,1086725,0.04641831173071665
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed un- derstanding of admixture is essential for e ective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on pheno- type are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simpli ed models at the risk of inaccurate inferences. This pro- posal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to lever- age this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate power- ful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as e ectively as homogeneous populations. The rst step in obtaining a thorough understanding of admixture is a principled and scalable statis- tical framework to infer ne-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop e ective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major im- pact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to eciently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human pheno- types, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on ne-scale genomic struc- ture and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can bene t from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9382936,R35GM125055,"['Admixture', 'Age', 'Alleles', 'Architecture', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Machine Learning', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'insight', 'novel', 'reference genome', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2017,225087,0.05174906684872359
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,0.056249219120396
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9344966,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Molecular Profiling', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2017,1186500,0.03349182712941097
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,9268656,U01HG009080,"['Algorithms', 'Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Privatization', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Risk', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'online resource', 'patient privacy', 'phenome', 'prediction algorithm', 'programs', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'simulation', 'statistics', 'tool', 'tool development', 'web-based tool']",NHGRI,STANFORD UNIVERSITY,U01,2017,923476,0.0445636893265572
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9248178,U24HG009446,"['ATAC-seq', 'Alleles', 'Alpha Cell', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Standardization', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data integration', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'member', 'mouse genome', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2017,2000000,0.025853178847588157
"Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.",Pilot for Creating Reproducible Workflows Using Docker Containers for NIH Commons,9275674,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Docking', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,119777,0.01778466749226319
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the world, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9301573,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Intuition', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Privatization', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Transact', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome-wide', 'genome-wide analysis', 'genomic data', 'hackathon', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'online resource', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'webinar', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2017,216422,0.028063051225502608
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,9528959,U41HG007497,"['Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Computational algorithm', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Phenotype', 'Population', 'Process', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'experimental study', 'genetic analysis', 'genetic variant', 'improved', 'integration site', 'method development', 'novel', 'tool']",NHGRI,JACKSON LABORATORY,U41,2017,1986604,0.04935890152728127
"CSHL Computational and Comparative Genomics Course The Cold Spring Harbor Laboratory proposes to continue a course entitled “Computational and Comparative Genomics”, to be held in the Fall of 2017 – 2019. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases that they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. NARRATIVE The Computational & Comparative Genomics, a 9 day course, is designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9357752,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genome', 'Home environment', 'Institution', 'International', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Research Training', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Training Programs', 'Universities', 'Update', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2017,62304,0.055102561203800436
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9250803,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'experimental study', 'genomic data', 'hazard', 'high dimensionality', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'predictive modeling', 'prevent', 'public health relevance', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2017,66026,0.04886688530656791
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9275537,U01HG009086,"['Accounting', 'Affect', 'Architecture', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2017,864186,0.043457165064557726
"OMOP information model for eMERGE phenotyping ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",OMOP information model for eMERGE phenotyping,9481767,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'information model', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,100000,0.0657399476938201
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9285815,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Familial disease', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Geography', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome Study', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic association', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'tool', 'trait', 'translational genomics', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2017,871212,0.07409919767373506
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9258454,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Individual', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'RNA library', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'experimental study', 'functional genomics', 'genetic element', 'genome analysis', 'genome editing', 'genome-wide', 'genomic predictors', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2017,244439,0.06376824856669158
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9282529,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Clinical', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hereditary Disease', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Recruitment Activity', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Therapeutic Intervention', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'base', 'biobank', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel strategies', 'phenotypic data', 'pleiotropism', 'point of care', 'psychosocial', 'public health relevance', 'rare variant', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2017,840645,0.04790996571088306
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9481916,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Clinical', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hereditary Disease', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Recruitment Activity', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Therapeutic Intervention', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'base', 'biobank', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel strategies', 'phenotypic data', 'pleiotropism', 'point of care', 'psychosocial', 'public health relevance', 'rare variant', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2017,98698,0.04790996571088306
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9564312,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medical Libraries', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2017,54232,0.05932586713238901
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9551116,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Digital Libraries', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Intuition', 'Laboratories', 'Leadership', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medical Libraries', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'National Human Genome Research Institute', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Records', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical schools', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'online resource', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION RESEARCH INSTITUTE,U01,2017,842722,0.05932586713238901
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale. PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,9145232,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Health', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic data', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'stem', 'tool', 'whole genome']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2016,303092,0.041871624651898955
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,9015770,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'genomic data', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2016,71329,0.03047524288943212
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9180486,K99HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'abstracting', 'base', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'learning strategy', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'tool']",NHGRI,"RUTGERS, THE STATE UNIV OF N.J.",K99,2016,83411,0.00864324943971247
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,9005867,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'mobile computing', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2016,1271107,0.046313509243879565
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,9268117,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost effectiveness', 'cost efficient', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'genomic data', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2016,1378926,0.04702421667713641
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators.         PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.              ",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,9132536,U01HG009080,"['Algorithms', 'Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Maps', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Online Systems', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Risk', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'cohort', 'disorder control', 'empowered', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'patient privacy', 'phenome', 'prediction algorithm', 'programs', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'simulation', 'statistics', 'tool', 'tool development']",NHGRI,STANFORD UNIVERSITY,U01,2016,974394,0.0445636893265572
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9096856,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,2201640,0.02894662536068666
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9288931,U54GM114838,"['Actinomyces Infections', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'big biomedical data', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'community building', 'data mining', 'design', 'drug discovery', 'field study', 'gene interaction', 'genome sequencing', 'genome-wide', 'genomic data', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'phenotypic data', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2016,224176,0.02894662536068666
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,9103177,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'learning strategy', 'meetings', 'population based', 'programs', 'whole genome']",NHGRI,STANFORD UNIVERSITY,R01,2016,300000,0.061990748459355184
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,9208745,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'genetic variant', 'genome sequencing', 'improved', 'integration site', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2016,2868077,0.04935890152728127
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,9041640,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'genomic data', 'hazard', 'human genomics', 'improved', 'individual patient', 'loss of function', 'novel', 'patient biomarkers', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'survival outcome', 'theories', 'treatment response', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2016,255295,0.04886688530656791
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,9097763,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'Course Content', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'laboratory experience', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2016,52816,0.05545032223131529
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community.         PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.            ","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",9132585,U01HG009086,"['Accounting', 'Affect', 'Architecture', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic screening method', 'Genetic study', 'Genome', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Learning', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Validation', 'Variant', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'functional genomics', 'genetic variant', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'research study', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2016,854333,0.043457165064557726
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9248725,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,98294,0.07409919767373506
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality. PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results",EMR-Linked Biobank for Translational Genomics,9134825,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomic medicine', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'genotyped patients', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'personalized health care', 'phase 3 study', 'phenome', 'population based', 'programs', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2016,873141,0.07409919767373506
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,8974432,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,"BROAD INSTITUTE, INC.",K99,2016,25020,0.06376824856669158
"Genome engineering tools for functional screening of non-coding elements DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root). PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.",Genome engineering tools for functional screening of non-coding elements,9242250,R00HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'CRISPR library', 'CRISPR screen', 'CRISPR/Cas technology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Health', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'laboratory experience', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool', 'whole genome']",NHGRI,NEW YORK GENOME CENTER,R00,2016,246031,0.06376824856669158
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9134844,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2016,996374,0.05932586713238901
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome. PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",9358802,U01HG008657,"['Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Community of Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Herpes zoster disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'genomic data', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2016,72000,0.05932586713238901
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9134797,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Clinical', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Electronics', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hereditary Disease', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Recruitment Activity', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'base', 'biobank', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'electronic data', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel', 'novel strategies', 'novel therapeutic intervention', 'pleiotropism', 'point of care', 'psychosocial', 'rare variant', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2016,849369,0.04790996571088306
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site. PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.",EHR-based Genomic Discovery and Implementation,9336097,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Clinical', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Electronics', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomic medicine', 'Genomics', 'Genotype', 'Health', 'Hereditary Disease', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Recruitment Activity', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'base', 'biobank', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'electronic data', 'genetic variant', 'genomic data', 'health care disparity', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel', 'novel strategies', 'novel therapeutic intervention', 'pleiotropism', 'point of care', 'psychosocial', 'rare variant', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2016,233600,0.04790996571088306
"Mathematical Models and Statistical Methods for Large-Scale Population Genomics ﻿    DESCRIPTION (provided by applicant):     Technological advances in DNA sequencing have dramatically increased the availability of genomic variation data over the past few years. This development offers a powerful window into understanding the genetic basis of human biology and disease risk. To facilitate achieving this goal, it is crucial to develop efficient analytical methods that will allow researchers to more fuly utilize the information in genomic data and consider more complex models than previously possible. The central goal of this project is to tackle this important challenge, by carrying out te following Specific Aims: In Aim 1, we will develop efficient inference tools for whole-genome population genomic analysis by extending our ongoing work on coalescent hidden Markov models and apply them to large-scale data. The methods we develop will enable researchers to analyze large samples under general demographic models involving multiple populations with population splits, migration, and admixture, as well as variable effective population sizes and temporal samples (ancient DNA). Multi-locus full-likelihood computation is often prohibitive in most population genetic models with high complexity. To address this problem, we will develop in Aim 2 a novel likelihood-free inference framework for population genomic analysis by applying a highly active area of machine learning research called deep learning. We will apply the method to various parameter estimation and classification problems in population genomics, particularly joint inference of selection and demography. In addition to carrying out technical research, we will develop a useful software package that will allow researchers from the population genomics community to utilize deep learning in their own research. It is becoming increasingly more popular to utilize time-series genetic variation data at the whole-genome scale to infer allele frequency changes over a time course. This development creates new opportunities to identify genomic regions under selective pressure and to estimate their associated fitness parameters. In Aim 3, we will develop new statistical methods to take full advantage of this novel data source at both short and long evolutionary timescales. Specifically, we will develop and apply efficient statistical inference methods for analyzing time-series genomic variation data from experimental evolution and ancient DNA samples. Useful open-source software will be developed for each specific aim. The novel methods developed in this project will help to analyze and interpret genetic variation data at the whole-genome scale.         PUBLIC HEALTH RELEVANCE:     This project will develop several novel statistical methods for analyzing and interpreting human genetic variation data at the whole-genome scale. The computational tools stemming from this research will enable efficient and accurate inference under complex population genetic models, thereby broadly facilitating research efforts to understand the genetic basis of human biology and disease risk.                ",Mathematical Models and Statistical Methods for Large-Scale Population Genomics,8887722,R01GM094402,"['Accounting', 'Address', 'Admixture', 'Affect', 'Age', 'Alleles', 'Area', 'Classification', 'Communities', 'Complex', 'Computer software', 'DNA', 'DNA Resequencing', 'DNA Sequence', 'Data', 'Data Sources', 'Demography', 'Development', 'Diffusion', 'Event', 'Evolution', 'Gene Frequency', 'Genetic', 'Genetic Models', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Human Biology', 'Human Genetics', 'Individual', 'Joints', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'Mutation', 'Phase', 'Physiologic pulse', 'Population', 'Population Genetics', 'Population Sizes', 'Recording of previous events', 'Research', 'Research Personnel', 'Sampling', 'Series', 'Site', 'Statistical Methods', 'Technology', 'Time', 'Trees', 'Uncertainty', 'Work', 'analytical method', 'base', 'computer based statistical methods', 'computerized tools', 'coping', 'disorder risk', 'fitness', 'genetic analysis', 'genetic selection', 'genome-wide', 'genomic variation', 'human disease', 'interest', 'markov model', 'mathematical model', 'migration', 'novel', 'open source', 'pressure', 'public health relevance', 'stem', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2015,303504,0.041871624651898955
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8825472,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis', 'transcriptome sequencing']",NCI,BROWN UNIVERSITY,R01,2015,71329,0.03047524288943212
"Informatics Tools for High-Throughput Sequences Data Analysis DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK. The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.",Informatics Tools for High-Throughput Sequences Data Analysis,8788050,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2015,967608,0.05095728320693667
"Reactome: An Open Knowledgebase of Human Pathways DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community. RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.",Reactome: An Open Knowledgebase of Human Pathways,8840984,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2015,1243799,0.046313509243879565
"EDAC: ENCODE Data Analysis Center DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health. RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8889700,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2015,2005492,0.04702421667713641
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research. PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",9147033,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Health', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,489338,0.02894662536068666
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8935854,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'collaborative environment', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2015,2116462,0.02894662536068666
"Statistical and computational analysis in whole genome sequencing studies. DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges. PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.",Statistical and computational analysis in whole genome sequencing studies.,8930750,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Health', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs']",NHGRI,STANFORD UNIVERSITY,R01,2015,292499,0.061990748459355184
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8920443,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'deletion detection', 'design', 'genetic variant', 'genome sequencing', 'improved', 'integration site', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2015,2663757,0.04935890152728127
"Human-Specific Gain and Loss of Function DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university. PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.",Human-Specific Gain and Loss of Function,8796200,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomic Segment', 'Genomic approach', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2015,54194,0.04593950634386109
"Heterogeneous and Robust Survival Analysis in Genomic Studies DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed. PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.",Heterogeneous and Robust Survival Analysis in Genomic Studies,8858662,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'personalized genomic medicine', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2015,248912,0.04886688530656791
"CSHL Computational and Comparative Genomics Course DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions. PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.",CSHL Computational and Comparative Genomics Course,8898177,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Health', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2015,52816,0.05545032223131529
"EMR-Linked Biobank for Translational Genomics ﻿    DESCRIPTION (provided by applicant): Medical care informed by genomic information is beginning to move into clinical practice. The Electronic Medical Records and Genomics (eMERGE) network through its initial phases has provided much of the groundwork for this transformation. The Geisinger Health System project, ""EMR-Linked Biobank for Translational Genomics"" intends to build on the knowledge and experience from eMERGE phase II to accelerate discovery and implementation while expanding our understanding of the sociocultural implications of genomics in medicine. We will accomplish this goal through three specific aims: 1) Use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery in the proposed disorders: familial hypercholesterolemia and chronic rhinosinusitis, 2) Develop and test approaches for implementation of genomic information in clinical practice, 3) Explore, develop and implement novel approaches for family-centered communication around clinically relevant genomic results. We currently have over 60,000 patients broadly consented for research with a large and increasing proportion consented for return of results and deposition in the electronic health record. Over 18,000 patients are genotyped on high density platforms. Our two proposed phenotypes, familial hypercholesterolemia (FH) and chronic rhinosinusitis (CRS) were chosen because both conditions have a significant public health impact in the United States, but they are also ideally suited to the specific aims of the project. They provide opportunities for innovation and extension of current eMERGE methods. While many of these innovations will take advantage of the sequencing done as part of the project, there are several other areas emphasized in the funding opportunity that will broaden the scope of eMERGE research. One of the areas of emphasis for eMERGE III is exploring the familial return of actionable results. FH is well suited to this, as the current clinical recommendation is cascade testing of family members for all diagnosed patients. Currently this relies on the patient to contact at risk family members, but this is less than optimal. We will explore this issue using qualitative and quantitative methods and use the results to design and test novel family communication strategies. Gene-environment interactions play an important role in the development and severity of disease. These are very difficult to study. We propose novel approaches that leverage the assets of Geisinger Health System and the eMERGE Network to develop and apply methods to extend existing projects that study the impact of environment on CRS. This would include the first large scale environment-wide association studies (EWAS). Finally, we propose to lead efforts to apply the tools of economic modeling and analysis to eMERGE projects to begin to quantify the value of implementation of genomic medicine in the US healthcare system. These proposed innovations will magnify the already significant impact that the eMERGE program has had in moving genomic medicine from a dream to a reality.         PUBLIC HEALTH RELEVANCE: Through this application GHS seeks to continue its participation in the eMERGE Network for Phase III - Study Investigators U01 (RFA-HG-14-025) funding opportunity. We propose 3 specific aims: 1) use existing biospecimens, genotype and sequence data and EMR-generated phenotypes for discovery and validation of gene-phenotype associations; 2) develop and test approaches for implementation of genomic information in clinical practice; develop and implement novel approaches for family-centered communication around clinically relevant genomic results                ",EMR-Linked Biobank for Translational Genomics,8968014,U01HG008679,"['Adult', 'Algorithms', 'Ambulatory Care', 'Area', 'Attitude', 'Candidate Disease Gene', 'Caring', 'Catchment Area', 'Child', 'Clinical', 'Communication', 'Computerized Medical Record', 'Consent', 'County', 'Cystic Fibrosis Transmembrane Conductance Regulator', 'Data', 'Deposition', 'Development', 'Diagnosis', 'Disease', 'Dreams', 'Economic Models', 'Ecosystem', 'Electronic Health Record', 'Ensure', 'Environment', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Funding Opportunities', 'Generations', 'Genes', 'Genomics', 'Genotype', 'Goals', 'Group Practice', 'Health', 'Health Insurance', 'Health system', 'Healthcare', 'Healthcare Systems', 'Individual', 'Information Systems', 'Inpatients', 'Institute of Medicine (U.S.)', 'Integrated Health Care Systems', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical', 'Medicine', 'Methods', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Pennsylvania', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Population', 'Process', 'Prognostic Factor', 'Provider', 'Public Health', 'Recommendation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Risk', 'Role', 'Rural', 'Rural Population', 'Safety', 'Severity of illness', 'Site', 'Strategic Planning', 'System', 'Techniques', 'Testing', 'Treatment outcome', 'United States', 'Validation', 'Variant', 'base', 'biobank', 'case finding', 'chronic rhinosinusitis', 'clinical care', 'clinical practice', 'clinically relevant', 'density', 'design', 'epidemiology study', 'experience', 'gene environment interaction', 'genetic epidemiology', 'genetic variant', 'implementation research', 'innovation', 'interest', 'meetings', 'novel', 'novel strategies', 'phase 3 study', 'phenome', 'population based', 'programs', 'public health relevance', 'screening', 'systems research', 'tool', 'trait', 'treatment response']",NHGRI,GEISINGER CLINIC,U01,2015,899776,0.07409919767373506
"Genome engineering tools for functional screening of non-coding elements     DESCRIPTION (provided by applicant): A major goal since the completion of the Human Genome Project has been to understand all functional elements in the human genome and the role they play in normal biological processes and disease. To that end, large pooled libraries of RNA interference (RNAi) reagents have been developed for genome-wide loss-of-function screens but have been hindered by 3 problems: 1) the incompleteness of protein depletion inherent in partial knock-down; 2) off-target effects from the seed sequence; and 3) genetic elements that are not transcribed are inaccessible to manipulation. Genome engineering using precisely targeted nucleases has emerged as an innovative technology to modify the genome and causally interrogate the role of different functional elements. Recently, I developed a new technology for functional genomic screening using the RNA- guided CRISPR/Cas9 nuclease (Shalem*, Sanjana*, et al., Science, 2014). Since CRISPR works on the DNA level, it is possible to manipulate non-coding elements that are inaccessible to RNAi. The research goal of this proposal is to develop new biological tools and analysis techniques for functional annotation of non-coding elements using pooled CRISPR screens.  Mentored phase: First, I plan to develop and optimize high-throughput CRISPR non-coding mutagenesis libraries targeting introns, UTRs, promoters, non-coding RNAs, and intergenic regions to enable screening at high-resolution with megabase-scale coverage. Next, I will validate functional non-coding elements and use this large dataset to find unifying principles of how non-coding elements regulate gene expression. Independent phase: I plan to develop a novel CRISPR architecture for tiled deletion screens capable of deleting many segments over entire chromosomes or even entire genomes. With this technology and the increased screening throughput it enables, I will be able to develop a long-term independent research program in several possible directions, including further genome biology, personalized functional genomics, and predictive diagnostics for drug-genome interactions.  The two primary areas of training needed to help me succeed in my research goals are 1) CRISPR technology development (mentor: Dr. Feng Zhang) and 2) knowledge of human genetics and non-coding variation (mentor: Dr. David Altshuler). Each mentor is an established expert in these fields. My career development plan integrates additional laboratory training, specialized tutorials in human genetics from world experts, local and national presentations of my research, and courses in scientific writing, grantsmanship and job search strategies. To assist with science- and career-related decisions, I have assembled an Advisory Committee with a team of established, senior genomics experts: Drs. Eric Lander, Steven Hyman, and David Root. The Broad Institute is an ideal environment: All Mentors and Advisors are located in one building and there are facilities for high-throughput functional screening in th RNAi Platform (Director: Dr. Root).         PUBLIC HEALTH RELEVANCE: This project seeks to transform our understanding of the human genome by developing a new kind of functional assay capable of directly editing the genome and analyzing how this genome editing impacts the growth, development, and drug resistance of human cells. The remarkable feature of this assay is its high capacity, which can test thousands of genome variations in a single experiment. This research will also improve our understanding of which parts of the genome are essential to life and which parts of the genome might be responsible for the proliferation of cancer cells.                ",Genome engineering tools for functional screening of non-coding elements,8804084,K99HG008171,"['Advisory Committees', 'Antineoplastic Agents', 'Architecture', 'Area', 'Beryllium', 'Biological', 'Biological Assay', 'Biological Models', 'Biological Process', 'Biology', 'Cells', 'Chromosomes', 'Chromosomes, Human, Pair 21', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Code', 'DNA', 'Data', 'Data Set', 'Development Plans', 'Diagnostic', 'Disease', 'Drug resistance', 'Elements', 'Environment', 'Gene Expression', 'Gene Targeting', 'Genes', 'Genetic Variation', 'Genome', 'Genome engineering', 'Genomics', 'Genotype', 'Goals', 'Growth and Development function', 'Guide RNA', 'Human', 'Human Genetics', 'Human Genome', 'Human Genome Project', 'Indium', 'Institutes', 'Intercistronic Region', 'Introns', 'Knock-out', 'Knowledge', 'Laboratories', 'Libraries', 'Life', 'Machine Learning', 'Mentors', 'Modeling', 'Modification', 'Mutagenesis', 'Mutation', 'National Human Genome Research Institute', 'Nature', 'Occupations', 'Paper', 'Pathway interactions', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Plant Roots', 'Play', 'Positioning Attribute', 'Postdoctoral Fellow', 'Proteins', 'RNA Interference', 'Reagent', 'Relative (related person)', 'Repetitive Sequence', 'Research', 'Resolution', 'Role', 'Science', 'Seeds', 'Stem cells', 'Subfamily lentivirinae', 'Techniques', 'Technology', 'Testing', 'Training', 'Untranslated RNA', 'Untranslated Regions', 'Variant', 'Work', 'Writing', 'cancer cell', 'career', 'career development', 'clinically relevant', 'deletion library', 'design', 'experience', 'functional genomics', 'genetic element', 'genome editing', 'genome-wide', 'improved', 'innovative technologies', 'insertion/deletion mutation', 'knock-down', 'loss of function', 'loss of function mutation', 'new technology', 'novel', 'nuclease', 'overexpression', 'programs', 'promoter', 'public health relevance', 'repaired', 'research study', 'scaffold', 'screening', 'small hairpin RNA', 'technology development', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",K99,2015,99937,0.06376824856669158
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8840551,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'genetic variant', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2015,219004,0.06636736513904233
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,8836569,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,2687363,0.07150784231277946
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9132876,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,413294,0.07150784231277946
"Genomic Database for the Yeast Saccharomyces DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements. Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,",Genomic Database for the Yeast Saccharomyces,9133491,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2015,115911,0.07150784231277946
"EHR-based Genomic Discovery and Implementation ﻿    DESCRIPTION (provided by applicant): Electronic health record (EHR)-linked biobanks are uniquely positioned for genomic discovery and for implementing genomic medicine to improve patient care. In eMERGE I, we leveraged an EHR-linked biobank, high-density genotyping data, and electronic phenotyping algorithms to discover 29 genetic loci associated with cardiovascular traits. In eMERGE II we began implementing genomic medicine by conducting an EHR-based randomized clinical trial of disclosing genomic risk of coronary heart disease and incorporating pharmacogenomic information in the EHR with linkage to clinical decision support. In eMERGE III; we propose to build on our prior work to conduct a genomic medicine implementation project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders-familial hypercholesterolemia (FH) and familial colorectal cancer (CRC)-we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will obtain informed consent from 3000 participants of Mayo Clinic biobanks in Rochester MN and Phoenix AZ who have moderate to severe hypercholesterolemia or colon polyps. DNA will be sent for CLIA-certified targeted sequencing of 100 disease genes. We will use state-of-the-art methods to classify variant pathogenicity, finalize actionable variants for return, examine near-term outcomes, economic implications and behavioral and psychosocial consequences of such return. We will also conduct genomic discovery leveraging a network-wide data set of ~25,000 individuals with sequence data and an existing network-wide data set of ~50,000 individuals with high-density genotype data linked to the EHR. We will exploit the unique potential of the EHR to assess pleiotropy using novel approaches. In partnership with Mountain Park Health Center, a primary care practice that serves a minority population of Mexican Americans in Phoenix AZ, we will contribute high-density genotyping and phenotype data on 1000 Hispanics for genomic discovery and pilot mechanisms for implementing genomic medicine at this site.         PUBLIC HEALTH RELEVANCE: Electronic health record (EHR)-linked biobanks are uniquely positioned to discover genetic variants relevant to human health and to implement genomic medicine to improve patient care. We propose to build on prior work to implement a genomic medicine project to establish mechanisms for return of actionable findings from targeted sequencing of 100 disease-relevant genes. Focusing on two common genetic disorders - familial hypercholesterolemia (FH) and familial colorectal cancer (CRC) - we will begin to translate genomic discovery and implementation efforts in eMERGE to impact public health. We will expand our efforts to identify genetic variants relevant to human health by leveraging available network-wide data sets.                ",EHR-based Genomic Discovery and Implementation,8967774,U01HG006379,"['Address', 'Algorithms', 'Behavioral', 'Bioinformatics', 'CLIA certified sequencing', 'Cardiovascular system', 'Clinic', 'Clinical', 'Colonic Polyps', 'Colorectal Cancer', 'Coronary heart disease', 'DNA', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Electronic Health Record', 'Electronics', 'Familial Hypercholesterolemia', 'Family', 'Family member', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Health', 'Healthcare', 'Hereditary Disease', 'Hispanics', 'Human', 'Individual', 'Informed Consent', 'Insurance Coverage', 'Link', 'Lipids', 'Low income', 'Medical Economics', 'Medicine', 'Methods', 'Mexican Americans', 'Minority', 'Natural Language Processing', 'Outcome', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Plasma', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Public Health', 'Randomized Clinical Trials', 'Reaction', 'Recruitment Activity', 'Relative (related person)', 'Reporting', 'Research', 'Resources', 'Risk', 'Site', 'Translating', 'Treatment/Psychosocial Effects', 'Variant', 'Work', 'base', 'biobank', 'clinical decision-making', 'clinically actionable', 'cohort', 'community based practice', 'cost', 'density', 'economic implication', 'electronic data', 'genetic variant', 'health care service utilization', 'hypercholesterolemia', 'improved', 'interest', 'multidisciplinary', 'novel', 'novel strategies', 'novel therapeutic intervention', 'pleiotropism', 'point of care', 'psychosocial', 'public health relevance', 'rare variant', 'screening', 'targeted sequencing', 'trait', 'treatment response', 'variant of unknown significance']",NHGRI,MAYO CLINIC ROCHESTER,U01,2015,859901,0.04790996571088306
"The Electronic Medical Records and Genomics (eMERGE) Network, Phase III ﻿    DESCRIPTION (provided by applicant): This application from the Group Health (GH)/University of Washington (UW) eMERGE team proposes specific aims designed to advance integration of genomic data into clinical practice with a focus on clinical discovery and implementation on Mendelian forms of colorectal cancer and/or polyposis (CRC/P) and incidental findings in other actionable genes. Our aims will also allow us to address challenges involved in bringing genomic medicine into standard medical care. Our focus on CRC/P, and quantitative traits and incidental findings (IF) in other actionable genes represents a unique opportunity to move the field forward towards the goal of bringing genomic medicine into effective, standard medical practice in an everyday community practice setting. We have 3 Aims. Aim 1: Genomic medicine discovery and implementation focused on CRC/P, Triglycerides (TG), and neutrophil count (NPC). We proposed sequencing of 1000 CRC and 1000 Asian ancestry participants, to achieve sub- aims of understanding the genetic basis of CRC, TG, and NPC. Aim 2: Integrate genomic information into GH-wide clinical care and the EMR. We will develop intuitive, comprehensive reports to return CRC and other genes deemed actionable by the American College of Medical Genetics and Genomics (ACMG). We will incorporate stakeholder input and then to implement integrated processes and tools into an integrated delivery system with a focus on CRC/P and Long QT syndrome. We will develop and evaluate educational outreach and online resources. Aim 3: Evaluate the effectiveness and economic impact of result return to patients and their families. We will implement a novel tool to increase family communication of CRC genetic results and evaluate the economic impact and cost effectiveness of this tool as well as of returning IFs. Completion of the work in this eMERGE III proposal will guarantee that the Seattle site remains an engaged and effective leader in the eMERGE network in support of NHGRI's mission to ensure that barriers to successful integration of genomic medicine in clinical care are overcome.         PUBLIC HEALTH RELEVANCE: This eMERGE III proposal builds on past discoveries and research designed to translate genomic advances into clinical care involving clinicians, patients and families. This phase focuses on traits associated with preventable health concerns: colon cancer, triglycerides, and immunity. We address optimal methods to share information across families and whether other information found by genomic tests impact the care, health, and medical costs of individuals.                ","The Electronic Medical Records and Genomics (eMERGE) Network, Phase III",8965736,U01HG008657,"['Abbreviations', 'Address', 'Algorithms', 'Amendment', 'American', 'Asians', 'Blood', 'Cardiovascular Diseases', 'Caring', 'Clinical', 'Collaborations', 'Colon Carcinoma', 'Colorectal Cancer', 'Communication', 'Community Practice', 'Complex', 'Computerized Medical Record', 'Coupled', 'Data', 'Development', 'Disease', 'Disease Resistance', 'Economics', 'Education', 'Effectiveness', 'Electronics', 'Ensure', 'Evaluation', 'Family', 'General Population', 'Genes', 'Genetic', 'Genomics', 'Goals', 'Health', 'Health system', 'Healthcare', 'Hereditary Disease', 'Immunity', 'Incidental Findings', 'Individual', 'Integrated Delivery Systems', 'Laboratories', 'Leadership', 'Libraries', 'Link', 'Long QT Syndrome', 'Malignant Neoplasms', 'Medical', 'Medical Genetics', 'Medicine', 'Methods', 'Mission', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Other Genetics', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Penetrance', 'Pharmacogenetics', 'Phase', 'Phenotype', 'Physicians', 'Policy Developments', 'Population', 'Predisposition', 'Preventive screening', 'Primary Health Care', 'Process', 'Provider', 'Randomized Controlled Trials', 'Reporting', 'Research Design', 'Resources', 'Risk', 'Site', 'Social Impacts', 'Solutions', 'Technology', 'Testing', 'Translating', 'Triglycerides', 'Universities', 'Variant', 'Washington', 'Work', 'age related', 'base', 'clinical care', 'clinical practice', 'college', 'cost', 'cost effectiveness', 'design', 'economic cost', 'economic impact', 'economic outcome', 'genetic association', 'genetic variant', 'genome wide association study', 'improved', 'innovation', 'interest', 'medical specialties', 'mortality', 'neutrophil', 'novel', 'outreach', 'polyposis', 'prevent', 'public health relevance', 'rare variant', 'screening', 'tool', 'trait']",NHGRI,KAISER FOUNDATION HEALTH PLAN OF WASHINGTON,U01,2015,859799,0.05932586713238901
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8683213,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2014,221252,0.05409762549592435
"Analytical Approaches to Massive Data Computation with Applications to Genomics DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.",Analytical Approaches to Massive Data Computation with Applications to Genomics,8685211,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Big Data', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics', 'mathematical analysis']",NCI,BROWN UNIVERSITY,R01,2014,69189,0.03352443085074155
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8601147,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'High-Throughput Nucleotide Sequencing', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2014,989800,0.05095728320693667
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8661774,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'MicroRNAs', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablet Computer', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2014,1248956,0.046313509243879565
"Gene Prediction by Markov Models and Complementary Methods DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad. NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8909702,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomic DNA', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'pyrosequencing', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2014,100000,0.06097189047337324
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8725717,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2014,2015775,0.04702421667713641
"KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL     DESCRIPTION (provided by applicant): The primary goal of the proposed Center of Excellence is to build a powerful and scalable Knowledge Engine for Genomics, KnowEnG. KnowEnG will transform the way biomedical researchers analyze their genome-wide data by integrating multiple analytical methods derived from the most advanced data mining and machine learning research to use the full breadth of existing knowledge about the relationships between genes as background, and providing an intuitive and professionally designed user interface. In order to achieve these goals, the project includes the following components: (1) gathering and integrating existing knowledgebases documenting connections between genes and their functions into a single Knowledge Network; (2) developing computational methods for analyzing genome-wide user datasets in the context of this pre-existing knowledge; (3) implementing these methods into scalable software components that can be deployed in a public or private cloud; (4) designing and implementing a Web-based user interface, based on the HUBZero toolkit, that enables the interactive analysis of user-supplied datasets in a graphics-driven and intuitive fashion; (5) thoroughly testing the functionality and usefulness of the KnowEnG environment in three large scale projects in the clinical sciences (pharmacogenomics of breast cancer), behavioral sciences (identification of gene regulatory modules underlying behavioral patterns) and drug discovery (genome-based prediction of the capacity of microorganisms to synthesize novel biologically active compounds). The KnowEng environment will be deployed in a cloud infrastructure and fully available to the community, as will be the software developed by the Center. The proposed Center is a collaboration between the University of Illinois (UIUC), a recognized world leader in computational science and engineering, and the Mayo Clinic, one of the leading clinical care and research organizations in the worid, and will be based at the UIUC Institute for Genomic Biology, which has state-of-the-art facilities and a nationally recognized program of multidisciplinary team-based genomic research.         PUBLIC HEALTH RELEVANCE: Physicians and biologists are now routinely producing very large, genome-wide datasets. These data need to be analyzed in the context of an even larger corpus of publically available data, in a manner that is approachable to non-specialist doctors and scientists. The proposed Center will leverage the latest computational techniques used to mine corporate or Internet data to enable the intuitive analysis and exploration of biomedical Big Data.            ","KnowEng, a Scalable Knowledge Engine for Large-Scale Genomic Data-OVERALL",8774407,U54GM114838,"['Actinobacteria class', 'Algorithms', 'Antibiotics', 'Bacterial Genome', 'Behavioral', 'Behavioral Sciences', 'Big Data', 'Biological', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Brain', 'Businesses', 'Clinic', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Cloud Computing', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computational Science', 'Computational Technique', 'Computer software', 'Computing Methodologies', 'Country', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Educational workshop', 'Engineering', 'Ensure', 'Environment', 'Ethics', 'Fostering', 'Future', 'Gene Expression', 'Generations', 'Genes', 'Genetic Determinism', 'Genome', 'Genomics', 'Goals', 'Illinois', 'Imagery', 'Institutes', 'Internet', 'Knowledge', 'Lead', 'Learning', 'Legal', 'Link', 'Machine Learning', 'Metabolic Pathway', 'Methods', 'Mining', 'Modality', 'Molecular Profiling', 'Online Systems', 'Pattern', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Physicians', 'Privacy', 'Property', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Social Network', 'Stimulus', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Universities', 'Work', 'analytical method', 'base', 'biomedical scientist', 'cancer therapy', 'clinical care', 'data mining', 'design', 'drug discovery', 'gene interaction', 'genome sequencing', 'genome-wide', 'innovation', 'knowledge base', 'malignant breast neoplasm', 'member', 'microorganism', 'multidisciplinary', 'next generation', 'novel', 'programs', 'public health relevance', 'research and development', 'response', 'social', 'software development', 'transcriptomics', 'working group']",NIGMS,UNIVERSITY OF ILLINOIS AT URBANA-CHAMPAIGN,U54,2014,1623265,0.02894662536068666
"Statistical and computational analysis in whole genome sequencing studies.     DESCRIPTION (provided by applicant): This project will investigate several issues arising from the statistical and computational analysis of whole genome sequencing (WGS) based genomics studies. In the area of data management in WGS studies, we address the rapidly increasing cost associated with the transfer and storage of the massive files for the sequence reads and their associated quality scores. We will develop data compression methods to achieve a further compression of several folds beyond current standards, with minimal incurred errors. In the area of secondary analysis, we will develop new statistical learning methods to improve variant quality score recalibration and to filter out unreliable calls. This will improve te reliability of the key information provided by the WGS data, which are the variants calls indicating the locations where the genome differs from the reference and the nature of the differences. We will study methods for case-control studies based on WGS. In particular, we will develop statistical models to enable the integrating of information from multiple types of variants to obtain more powerful tests of association. We will apply the methods developed in this aim to the analysis of WGS data from a study on abdominal aortic aneurysm. Finally, we will address selected new questions associated with population scale WGS projects. Several national programs have recently been initiated to generate WGS data for hundreds of thousands of individuals with longitudinal medical records. The availability of this comprehensive data on a population scale will open up a rich frontier for genome medicine and will pose many new challenges for statistical analysis. We will formulate some of these new challenges and develop the statistical methods needed to meet these challenges.         PUBLIC HEALTH RELEVANCE: The research in this project concerns the design and implementation of statistical and computational methods for the analysis of data from whole genome sequencing studies. Methods will be developed for sequence quality score compression, variant call filtering, and methods for case-control association analysis and mega-cohort analysis based on whole genome sequencing.                ",Statistical and computational analysis in whole genome sequencing studies.,8750827,R01HG007834,"['Abdominal Aortic Aneurysm', 'Address', 'Area', 'Case-Control Studies', 'Cohort Analysis', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Compression', 'Genome', 'Genomics', 'Goals', 'Individual', 'Location', 'Machine Learning', 'Medical Records', 'Medicine', 'Methods', 'Nature', 'Population', 'Reading', 'Research', 'Statistical Methods', 'Statistical Models', 'Testing', 'Variant', 'base', 'case control', 'computerized data processing', 'cost', 'data management', 'design', 'frontier', 'genome sequencing', 'improved', 'meetings', 'population based', 'programs', 'public health relevance']",NHGRI,STANFORD UNIVERSITY,R01,2014,300000,0.061990748459355184
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8737934,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Detection', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Site', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'design', 'genetic variant', 'genome sequencing', 'improved', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2014,2684060,0.04935890152728127
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8635216,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2014,51530,0.04593950634386109
"Heterogeneous and Robust Survival Analysis in Genomic Studies     DESCRIPTION (provided by applicant): The long-term objective of this project is to develop powerful and computationally-efficient statistical methods for statistical modeling of high-dimensional genomic data motivated by important biological problems and experiments. The specific aims of the current project include developing novel survival analysis methods to model the heterogeneity in both patients and biomarkers in genomic studies and developing robust survival analysis methods to analyze high-dimensional genomic data. The proposed methods hinge on a novel integration of methods in high-dimensional data analysis, theory in statistical learning and methods in human genomics. The project will also investigate the robustness, power and efficiencies of these methods and compare them with existing methods. Results from applying the methods to studies of ovarian cancer, lung cancer, brain cancer will help ensure that maximal information is obtained from the high-throughput experiments conducted by our collaborators as well as data that are publicly available. Software will be made available through Bioconductor to ensure that the scientific community benefits from the methods developed.         PUBLIC HEALTH RELEVANCE:     NARRATIVE The last decade of advanced laboratory techniques has had a profound impact on genomic research, however, the development of corresponding statistical methods to analyze the data has not been in the same pace. This project aims to develop, evaluate, and disseminate powerful and computationally-efficient statistical methods to model the heterogeneity in both patients and biomarkers in genomic studies. We believe our proposed methods can help scientific community turn valuable high-throughput measurements into meaningful results.            ",Heterogeneous and Robust Survival Analysis in Genomic Studies,8696520,R01HG007377,"['Address', 'Affect', 'Bioconductor', 'Biological', 'Biological Markers', 'Categories', 'Cause of Death', 'Clinical Treatment', 'Communities', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Detection', 'Development', 'Disease', 'Ensure', 'Failure', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heterogeneity', 'Human', 'Individual', 'Laboratories', 'Lead', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of brain', 'Malignant neoplasm of lung', 'Malignant neoplasm of ovary', 'Measurement', 'Medicine', 'Methods', 'Modeling', 'Outcome', 'Patients', 'Phenotype', 'Population', 'Quality of life', 'Research', 'Statistical Methods', 'Statistical Models', 'Survival Analysis', 'Techniques', 'Time', 'base', 'clinical application', 'hazard', 'improved', 'loss of function', 'novel', 'prevent', 'public health relevance', 'research study', 'response', 'simulation', 'theories', 'treatment strategy']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,255295,0.04886688530656791
"CSHL Computational and Comparative Genomics Course     DESCRIPTION (provided by applicant): The Cold Spring Harbor Laboratory proposes to continue a course entitled ""Computational and Comparative Genomics"", to be held in the fall of 2014 - 2016. The Computational and Comparative Genomics course provides experimental biologists with backgrounds in molecular biology, genetics, and biochemistry with the theoretical background and practical experience necessary to use and evaluate computational approaches to genome annotation and analysis, including protein sequence database searching, multiple sequence alignment, identification of promoters and other genetic regulatory elements, and the integration of sequence information into broader models of biological function. The course also provides computer scientists and mathematicians with an introduction to the algorithms, computational methods, and biological problems that are addressed in biological sequence analysis and computational biology. For post-doctoral fellows, and junior and senior investigators who are interested in changing their research direction towards computational biology, the course provides an introduction to computational biology methods and a survey of future directions. Over a seven day period, the students receive a strong grounding in the both the biological and computer science foundations for genome analysis and practical computer laboratory experience on challenging problems. The course is taught by internationally recognized leaders in the field, who provide hands-on demonstrations of the programs and biological databases they have developed. At the end of the course, students can not only use effectively currently available tools in biological sequence analysis, they can also evaluate critically new computational approaches by considering alternative methods and interpretations, and appreciate the strengths and limitations of computational methods for answering broad biological questions.         PUBLIC HEALTH RELEVANCE: The Computational & Comparative Genomics is a 6 day course designed to meet the continuing need for training in computational biology, statistics, and computer science for molecular biologists and geneticists with backgrounds in experimental biology. In addition, the course presents problems in biological sequence analysis and biological databases to biologists and computer scientists. The course covers research topics and state-of-the-art techniques that, while essential to interpret genome sequence and large-scale functional analysis data from a perspective that balances the theoretical foundations of the approaches and their experimental and analytical limitations.                 ",CSHL Computational and Comparative Genomics Course,8737540,R25HG007819,"['Address', 'Algorithms', 'Amino Acid Sequence', 'Amino Acid Sequence Databases', 'Area', 'Biochemistry', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biological databases', 'Biology', 'Computational Biology', 'Computers', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Analyses', 'Databases', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Equilibrium', 'Faculty', 'Foundations', 'Future', 'Genes', 'Genetic', 'Genomics', 'Home environment', 'Institution', 'Laboratories', 'Laboratory Study', 'Machine Learning', 'Methods', 'Modeling', 'Molecular', 'Molecular Biology', 'Nucleic Acid Regulatory Sequences', 'Other Genetics', 'Peptide Sequence Determination', 'Postdoctoral Fellow', 'Publishing', 'Research', 'Research Personnel', 'Scientist', 'Sequence Alignment', 'Sequence Analysis', 'Statistical Algorithm', 'Students', 'Surveys', 'Techniques', 'Training', 'Universities', 'Update', 'base', 'comparative genomics', 'computer science', 'design', 'experience', 'falls', 'genome analysis', 'genome annotation', 'genome database', 'genome sequencing', 'graduate student', 'instructor', 'interest', 'lecturer', 'meetings', 'programs', 'promoter', 'public health relevance', 'statistics', 'tool']",NHGRI,COLD SPRING HARBOR LABORATORY,R25,2014,52816,0.05545032223131529
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8642168,R01CA180777,"['Bees', 'Big Data', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2014,207764,0.06636736513904233
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8640966,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2014,2699376,0.07150784231277946
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8699810,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic DNA', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2014,641093,0.08763052485644397
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.         PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8518436,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'epigenome', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation sequencing', 'novel', 'open source', 'public health relevance', 'tool', 'transcription factor', 'transcriptome sequencing', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2013,220626,0.05409762549592435
"Analytical Approaches to Massive Data Computation with Applications to Genomics     DESCRIPTION (provided by applicant): We propose to design and test mathematically well founded algorithmic and statistical tectonics for analyzing large scale, heterogeneous and noisy data. We focus on fully analytical evaluation of algorithms' performance and rigorous statistical guarantees on the analysis results. This project will leverage on the PIs' recent work on cancer genomics data analysis and rigorous data mining techniques. Those works were driven by specific applications, while in the current project we aim at developing general principles and techniques that will apply to a broad sets of applications. The proposed research is transformative in its emphasis on rigorous analytical evaluation of algorithms' performance and statistical measures of output uncertainty, in contrast to the primarily heuristic approaches currently used in data ming and machine learning. While we cannot expect full mathematical analysis of all data mining and machine learning techniques, any progress in that direction will have significant contribution to the reliability and scientific impact of this discipline. While ou work is motivated by molecular biology data, we expect the techniques to be useful for other scientific communities with massive multi-variate data analysis challenges. Molecular biology provides an excellent source of data for testing advance data analysis techniques: specifically, DNA/RNA sequence data repositories are growing at a super-exponential rate. The data is typically large and noisy, and it includes both genotype and phenotype features that permit experimental validation of the analysis. One such data repository is The Cancer Genome Atlas (TCGA), which we will use for initial testing of the proposed approaches. RELEVANCE (See instructions): This project will advocate a responsible approach to data analysis, based on well-founded mathematical and Statistical concepts. Such an approach enhances the effectiveness of evidence based medicine and other policy and social applications of big data analysis. The proposed work will be tested on human and cancer genome data, contributing to health IT, one of the National Priority Domain Areas.              n/a",Analytical Approaches to Massive Data Computation with Applications to Genomics,8599823,R01CA180776,"['Advocate', 'Algorithms', 'Area', 'Communities', 'DNA', 'Data', 'Data Analyses', 'Data Sources', 'Databases', 'Discipline', 'Effectiveness', 'Evaluation', 'Evidence Based Medicine', 'Genomics', 'Genotype', 'Health', 'Human Genome', 'Instruction', 'Machine Learning', 'Measures', 'Molecular Biology', 'Output', 'Performance', 'Phenotype', 'RNA Sequences', 'Research', 'Social Policies', 'Techniques', 'Testing', 'The Cancer Genome Atlas', 'Uncertainty', 'Validation', 'Work', 'base', 'cancer genome', 'cancer genomics', 'data mining', 'design', 'heuristics']",NCI,BROWN UNIVERSITY,R01,2013,71329,0.0336620488422792
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.        The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8416349,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Targeting', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'genome analysis', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2013,964551,0.05095728320693667
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.          RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8473164,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2013,1216983,0.046313509243879565
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8722983,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,115680,0.04702421667713641
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8548395,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2013,1871839,0.04702421667713641
"An Integrative Analysis of Structural Variation for the 1000 Genomes Project DESCRIPTION (provided by applicant): Structural variation (SV), involving deletions, duplications, insertions and inversions of DNA segments, accounts for a large proportion of human genetic diversity. Comprehensive identification and analysis of these genetic variants will help us more fully elucidate the biology of their functional effects on human health and demography. Despite recent advances, the tools and data needed to comprehensively identify all types of SVs, genotype each variant, integrate and phase these variants remain lacking. Indeed, the data released from the early phases of the 1000 Genomes Project (1000GP) (1000 Genomes Project Consortium, 2010; 1000 Genomes Project Consortium, 2012) are biased primarily towards the detection of deletions within relatively unique regions of the genome. As a consortium, we propose to pool expertise from various research groups to provide an integrative analysis of SVs by combining rigorous computational algorithmic development with extensive experimental validation. The new algorithms we develop and the high confidence lists of SVs obtained will be rapidly made available as a public resource. n/a",An Integrative Analysis of Structural Variation for the 1000 Genomes Project,8589933,U41HG007497,"['Accounting', 'Algorithms', 'Alleles', 'Benchmarking', 'Biology', 'Chromosomes', 'Complement', 'Complex', 'Consensus', 'DNA', 'DNA Insertion Elements', 'Data', 'Demography', 'Detection', 'Development', 'Future', 'Gene Conversion', 'Genetic Variation', 'Genome', 'Genotype', 'Goals', 'Gold', 'Haplotypes', 'Health', 'Hereditary Disease', 'Human', 'Human Genetics', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Nucleotides', 'Phase', 'Population', 'Process', 'Reading', 'Repetitive Sequence', 'Research', 'Resolution', 'Resources', 'Sampling', 'Site', 'Statistical Models', 'Technology', 'Validation', 'Variant', 'base', 'design', 'genetic variant', 'genome sequencing', 'improved', 'method development', 'novel', 'research study', 'tool']",NHGRI,JACKSON LABORATORY,U41,2013,2766009,0.04935890152728127
"Human-Specific Gain and Loss of Function     DESCRIPTION (provided by applicant): The proposed research will seek to utilize population genetic data to distinguish regions of the human genome experiencing purifying selection from unconstrained genomic regions. Because genomic sequences subject to selective constraint perform functions beneficial to the organism, this work will reveal previously unknown functional regions of the human genome. In particular, since this approach does not rely on comparisons between humans and closely related species, it can uncover regions acquiring or losing selective constraint after humans split from other great apes. Regions acquiring function during this time period would represent an important class of recent human adaptations, and could reveal molecular changes responsible for uniquely human phenotypes. Beyond its evolutionary importance, this work would improve the functional annotation of the human genome, revealing functional regions that could result in harmful effects if disrupted, and that cannot be detected from comparative genomic techniques. In addition to revealing human-specific gains-of- function, the proposed project would allow for detection of losses-of-function occurring since the human- chimpanzee divergence. These events could also underlie important phenotypic changes in recent human evolution, as several known human-specific losses-of-function were adaptive. Even fitness-neutral losses of function are informative, as they may reveal differences in selective pressures allowing certain functions to be lost in humans but requiring them to be maintained in our relatives. Finally, the work proposed here will combine population genetic and phylogenetic data to reveal constrained regions with better accuracy than can be achieved by examining either of these types of data alone. This will result in further improvements to the functional annotation of the human genome, especially with respect to non-protein-coding functional regions that cannot be reliably detected by ab initio techniques.  Performing this research will improve the applicant's knowledge of population genetics and computational methods that can leverage polymorphism to draw inferences about the selective and functional importance of different genomic loci. Instruction from a sponsor and co-sponsor with expertise in both of these areas, as well as interaction with other faculty members and postdocs at the sponsor's institution, will be invaluable for improving the applicant's skills. This experience wil greatly enhance the applicant's chances of achieving his goal of succeeding as an independent scientist running a lab at a research university.         PUBLIC HEALTH RELEVANCE: In addition to its evolutionary significance, the proposed research will reveal previously unknown regions of the human genome that perform beneficial functions. Because disruptions of these regions would have harmful effects, these findings will allow for more complete analyses of the genetic basis of disease in humans.            ",Human-Specific Gain and Loss of Function,8457179,F32GM105231,"['Address', 'Area', 'Beryllium', 'Code', 'Computing Methodologies', 'Data', 'Detection', 'Disease', 'Elements', 'Event', 'Evolution', 'Explosion', 'Faculty', 'Genetic Polymorphism', 'Genome', 'Genomics', 'Goals', 'Human', 'Human Genome', 'Indium', 'Institution', 'Instruction', 'Knowledge', 'Machine Learning', 'Mammals', 'Methods', 'Molecular', 'Mutation', 'Organism', 'Pan Genus', 'Phenotype', 'Phylogenetic Analysis', 'Pongidae', 'Population', 'Population Genetics', 'Postdoctoral Fellow', 'Relative (related person)', 'Research', 'Role', 'Running', 'Scientist', 'Techniques', 'Time', 'Universities', 'Variant', 'Work', 'base', 'comparative genomics', 'driving force', 'experience', 'fitness', 'functional genomics', 'gain of function', 'genetic analysis', 'human population genetics', 'improved', 'loss of function', 'meetings', 'member', 'novel', 'pressure', 'public health relevance', 'skills']",NIGMS,"RUTGERS, THE STATE UNIV OF N.J.",F32,2013,47114,0.04593950634386109
"BIGDATA: DA: Interpreting massive genomic data sets via summarization Genomic data is big and getting ever bigger, but current analysis methods will not scale to the analysis of thousands or millions of genomes. Consequently, a critical technical challenge is to develop new methods that can analyze these enormous data sets. In this proposal, we describe a new computational framework for drawing inferences from massive genomic data sets. Our approach leverages submodular summarization methods that have been developed for analyzing text corpora. We will apply these methods to five big data problems in genomics: 1) identifying functional elements characteristic o f a given human cell type; 2) identifying genomic features associated with a particular subclass of cancer; 3-4) identifying genomic variants representative of ancestrally or phenotypically defined human populations; and 5) finding a set of microbial genes that characterize a given site on the human body. This project will advance discovery and understanding on two fronts. First, we will develop novel methods for summarizing genomic, epigenomic and metagenomic data sets. Indeed, to our knowledge, this grant proposes the first application of summarization methods to genomic data of any kind. The proposed research will significantly advance our ability to apply submodularity to these summarization tasks, particularly with respect to identifying and creating a library of distance functions that have bee validated with respect to the five tasks outlined in the proposal. Second, we will apply our novel methods to problems of profound importance. Indeed, significant progress toward any one of our five tasks would represent an important advance in our scientific understanding of human history, biology or disease. The impact of this project will grow as the big data problem grows, even after the project is complete. The results of this project, both the software that we develop and the summaries that we produce, will be useful for answering a wide array of questions in any field that must cope with big data. Rapid advances in DNA sequencing technology have led to an explosion of genomic data. This data contains valuable knowledge about human biology and human disease, but few existing computational methods are designed to scale to the joint analysis of tens of thousands of human genomes. This proposal adapts and extends recent advances from the field of natural language processing to characterize cancer subtvoesdiscover ofinetic variants associated with disease and characterize human microbial populations.",BIGDATA: DA: Interpreting massive genomic data sets via summarization,8599826,R01CA180777,"['Bees', 'Biology', 'Characteristics', 'Computing Methodologies', 'DNA Sequence', 'Data', 'Data Set', 'Disease', 'Elements', 'Explosion', 'Genes', 'Genome', 'Genomics', 'Grant', 'Human', 'Human Biology', 'Human Genome', 'Human body', 'Joints', 'Knowledge', 'Libraries', 'Malignant Neoplasms', 'Metagenomics', 'Methods', 'Natural Language Processing', 'Population', 'Recording of previous events', 'Research', 'Site', 'Technology', 'Text', 'Variant', 'cell type', 'computer framework', 'coping', 'design', 'epigenomics', 'human disease', 'microbial', 'novel', 'software development']",NCI,UNIVERSITY OF WASHINGTON,R01,2013,214832,0.06636736513904233
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8447583,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'screening', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2013,2703817,0.07150784231277946
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.          Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8537965,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2013,624741,0.08763052485644397
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,0.045710190243620774
"Informatics Tools for High-Throughput Sequences Data Analysis    DESCRIPTION (provided by applicant): The Genome Analysis Toolkit (GATK) is a suite of best-in-class, widely-used, well-supported, open-source tools for processing and analysis of next-generation DNA sequencing (NGS) data. These tools currently  include a multiple sequence realigner, a covariate-correcting base quality score recalibrator, multi-sample  SNP, INDEL, and CNV genotypers, machine learning algorithms for false positive identification, variant  evaluation modules, somatic SNP and indel callers, and hundreds of other tools. Underlying all of these tools is our structured programming framework (GATK-Engine) that uses the functional programming philosophy of MapReduce to make writing feature-rich, efficient and robust analysis tools easy. By centralizing common data management infrastructure, all GATK-based tools benefit from the engine's correctness, CPU and memory efficiency, as well as automatic distributed and shared memory parallelization, essential capabilities given the massive and growing size of NGS datasets. The GATK currently supports all of the major sequencing technologies including lllumina. Life Sciences 454, and ABI SOLID, from hybrid capture of exomes to 1000s of low-pass samples in the 1000 Genomes Project. Our emphasis on technology-agnostic processing tools has helped to popularize the now standard SAM/BAM and VCFs formats for representing NGS data and variation calls, respectively. In this RFA we propose to  continue to develop the GATK-Engine and data processing tools to (1) achieve complete and accurate  variation discovery and genotyping for all major sequencing study designs and NGS technologies (2)  optimize the GATK-Engine and pipelining infrastructure to operate efficiently on distributed data sets at the  scale of tens of thousands of samples (3) extend the GATK data processing tools to support the upcoming  sequencing technologies of Complete Genomics, lon Torrent, and Pacific Biosciences as well as we do  current technologies, (4) expand significantly our educational and support structures to ensure that the longtail  of future NGS users can benefit from the best-practice data processing and analysis tools in the GATK.      PUBLIC HEALTH RELEVANCE: The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.              The proposed project aims to continue to develop the Genome Analysis Toolkit (GATK), a suite of widely used and mission-critical tools for analyzing the next-generation DNA sequencing data. With this grant we will improve these tools, make them more robust, and extend them to new sequencing technologies. This is essential to realize the potential of DNA sequencing to understand human history, diversity, and to discover  new loci associated with human disease, leading to new biologic hypotheses and new drug targets.            ",Informatics Tools for High-Throughput Sequences Data Analysis,8237596,U01HG006569,"['Algorithms', 'Biological Sciences', 'Communities', 'DNA Sequence', 'Data', 'Data Analyses', 'Data Set', 'Documentation', 'Drug Delivery Systems', 'Ensure', 'Evaluation', 'Experimental Designs', 'Floods', 'Future', 'Genome', 'Genomics', 'Genotype', 'Grant', 'Human', 'Hybrids', 'Informatics', 'Machine Learning', 'Medical Genetics', 'Memory', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Philosophy', 'Process', 'Recording of previous events', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'SNP genotyping', 'Sampling', 'Site', 'Structure', 'Techniques', 'Technology', 'Variant', 'Work', 'Writing', 'base', 'cancer genetics', 'computerized data processing', 'data management', 'distributed data', 'distributed memory', 'exome', 'human disease', 'improved', 'next generation', 'novel', 'open source', 'programs', 'shared memory', 'tool']",NHGRI,"BROAD INSTITUTE, INC.",U01,2012,1010000,0.04630462248559627
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8494858,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2012,371054,0.05351426429552623
"Reactome: An Open Knowledgebase of Human Pathways     DESCRIPTION (provided by applicant): We seek renewal of the core operating funding for the Reactome Knowledgebase of Human Biological Pathways and Processes. Reactome is a curated knowledgebase available online as an open access resource that can be freely used and redistributed by all members of the biological research community. It is used by geneticists, genomics researchers, clinical researchers and molecular biologists to interpret the results of high-throughput experimental studies, by bioinformaticians seeking to develop novel algorithms for mining knowledge from genomics studies, and by systems biologists building predictive models of normal and abnormal pathways.  Our curational system draws heavily on the expertise of independent investigators within the community who author precise machine-readable descriptions of human biological pathways under the guidance of a staff of dedicated curators. Each pathway is extensively checked and peer-reviewed prior to publication to ensure its factual accuracy and compliance with the data model. A system of evidence tracking ensures that all assertions are backed up by the primary literature, and that human molecular events inferred from orthologous ones in animal models have an auditable inference chain. Curated pathways described by Reactome currently cover roughly one quarter of the translated portion of the genome. We also offer a network of ""functional interactions"" (FIs) predicted by a conservative machine-learning approach, that covers an additional quarter of the translated genome, for a combined coverage of roughly 50% of the known genome.  Over the next five years, we seek to (1) increase the number of curated proteins and other functional entities to at least 10,500; (2) to supplement normal pathways with variant reactions for 1200 genes representing disease states; (3) increase the size of the Reactome Fl network to 15,000 molecules; and (4) enhance the web site and other resources to meet the needs of a growing and diverse user community.        PUBLIC HEALTH RELEVANCE: RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                  RELEVANCE (See instructions):  Reactome represents one of a very small number of fully open access curated pathway databases. Its contents have contributed both directly and indirectly to large numbers of basic and translational research studies, and it supports a broad, diverse and engaged user community. As such it represents a key and irreplaceable community resource for genomics, genetics, systems biology, and translational researchers.                ",Reactome: An Open Knowledgebase of Human Pathways,8268588,U41HG003751,"['Algorithms', 'Animal Model', 'Back', 'Basic Science', 'Behavior', 'Biological', 'Clinical', 'Communities', 'Computer software', 'Computers', 'Databases', 'Disease', 'Disease Pathway', 'Ensure', 'Event', 'Funding', 'Generations', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Health', 'Human', 'Image', 'Instruction', 'Internet', 'Knowledge', 'Link', 'Lipids', 'Literature', 'Logic', 'Machine Learning', 'Maps', 'Mining', 'Molecular', 'Online Systems', 'Pathway interactions', 'Peer Review', 'Process', 'Protein Isoforms', 'Proteins', 'Publications', 'Reaction', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Software Tools', 'System', 'Systems Biology', 'Tablets', 'Time', 'Translating', 'Translational Research', 'Variant', 'Visual', 'base', 'biological research', 'data exchange', 'data modeling', 'improved', 'knowledge base', 'meetings', 'member', 'novel', 'predictive modeling', 'research study', 'small molecule', 'touchscreen', 'transcription factor', 'usability', 'web services', 'web site']",NHGRI,ONTARIO INSTITUTE FOR CANCER RESEARCH,U41,2012,1300000,0.04289894149309067
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.           Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8320160,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'metagenome', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2012,204974,0.04043680429971729
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8521766,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,75000,0.06097189047337324
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8266525,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'metagenome', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2012,577121,0.06097189047337324
"EDAC: ENCODE Data Analysis Center     DESCRIPTION (provided by applicant): The objective of the Encyclopedia of DNA Elements (ENCODE) Project is to provide a complete inventory of all functional elements in the human genome using high-throughput experiments as well as computational methods. This proposal aims to create the ENCODE Data Analysis Center (EDAC, or the DAC), consisting of a multi-disciplinary group of leading scientists who will respond to directions from the Analysis Working Group (AWG) of ENCODE and thus integrate data generated by all groups in the ENCODE Consortium in an unbiased manner. These analyses will substantially augment the value of the ENCODE data by integrating diverse data types. The DAC members are leaders in their respective fields of bioinformatics, computational machine learning, algorithm development, and statistical theory and application to genomic data (Zhiping Weng, Manolis Kellis, Mark Gerstein, Mark Daly, Roderic Guigo, Shirley Liu, Rafael Irizarry, and William Noble). They have a strong track record of delivering collaborative analysis in the context of the ENCODE and modENCODE Projects, in which this group of researchers was responsible for the much of the analyses and the majority of the figures and tables in the ENCODE and modENCODE papers. The proposed DAC will pursue goals summarized as the following seven aims: Aim 1. To work with the AWG to define and prioritize integrative analyses of ENCODE data; Aim 2.To provide shared computational guidelines and infrastructure for data processing, common analysis tasks, and data exchange; Aim 3. To facilitate and carry out data integration for element-specific analyses; Aim 4.To facilitate and carry out exploratory data analyses across elements; Aim 5.To facilitate and carry out comparative analyses across human, mouse, fly, and worm; Aim 6.To facilitate integration with the genome-wide association studies community and disease datasets; and Aim 7.To facilitate writing Consortium papers and assist evaluating ENCODE data.         RELEVANCE: The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome. This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting this human genome and using it to understand human biology and improve human health.             RELEVANCE (See instructions):  The Encyclopedia of DNA Elements (ENCODE) Project is a coordinated effort to apply high-throughput, cost-efficient approaches to generate a comprehensive catalog of functional elements in the human genome.  This proposal establishes a data analysis center to support, facilitate, and enhance integrative analyses of the ENCODE Consortium, with the ultimate goal of facilitating the scientific and medical communities in interpreting the human genome and using it to understand human biology and improve human health",EDAC: ENCODE Data Analysis Center,8402447,U41HG007000,"['Address', 'Algorithms', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biological Sciences', 'Cataloging', 'Catalogs', 'Communities', 'Complement', 'Computing Methodologies', 'DNA', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Set', 'Development', 'Disease', 'Elements', 'Equipment and supply inventories', 'Freezing', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Indium', 'Instruction', 'Invertebrates', 'Investigation', 'Machine Learning', 'Manuscripts', 'Medical', 'Mus', 'National Human Genome Research Institute', 'Organism', 'Paper', 'Publishing', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Scientist', 'Vertebral column', 'Vertebrates', 'Work', 'Writing', 'comparative', 'computerized data processing', 'cost', 'cost effectiveness', 'data exchange', 'data integration', 'fly', 'foot', 'genome wide association study', 'genome-wide', 'human disease', 'improved', 'insight', 'member', 'novel', 'research study', 'symposium', 'task analysis', 'theories', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U41,2012,2460045,0.04702421667713641
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.         The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8331495,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2012,103535,0.03343666300176368
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8228154,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'ChIP-seq', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,323572,0.013671649157477641
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.         Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8337800,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2012,2751015,0.07150784231277946
"New Physical Methodologies for Genomic Analysis     DESCRIPTION (provided by applicant): Despite substantial efforts in developing sequencing technologies and computational software, spanning over 30 years, the full genome of any but the simplest organisms is still unable to automatically reconstructed. The length of the DNA sequences that can be 'read' by modern sequencing systems is substantially smaller than the length of most genomes (1000s of base-pairs versus millions to billions), making it virtually impossible to use the fragmented information generated by the shotgun sequencing process to reconstruct the long-range information linking together genomic segments belonging to a same chromosome. The main reason why genome assembly is difficult is genomic repeats - segments of DNA that occur in multiple identical or near-identical copies throughout a genome. Any repeats longer than the length of a sequencing read introduce ambiguity in the possible reconstructions of a genome - an exponential (in the number of repeats) number of different genomes can be constructed from the same set of reads, among which only one is the true reconstruction of the genome being assembled. Finding this one correct genome from among the many possible alternatives is impossible without the use of additional information, such as mate-pair information constraining the relative placement of pairs of shotgun reads along the genome. Mate-pair information is routinely generated in sequencing experiments and has been critical to scientists' ability to reconstruct genomes from shotgun data (e.g., mate-pair information was crucial to the success of the first prokaryotic genome project - Haemophilus influenza). Given these outstanding issues, a series of interlocking aims is proposed that center on enhanced optical and electronic detection of specially-decorated, genomic DNA molecules. The aims are designed for enabling new technologies that will provide sufficient physical map information to intimately mix with modern sequencing data for comprehensive assembly of complex genomes. These proposed advancements will be cradled within a new generation of nanofluidic devices engendering novel means for molecular control and detection. Such efforts will be directed by state-of-the art computer simulations that will model novel aspects of the new platforms for allowing rapid loops of design/implementation/testing. The main thrust of these technological developments will be carefully guided and serve a broad-based bioinformatics framework that will be developed for this work while laying the basis for highly integrated approaches to genome assembly and analysis.        PUBLIC HEALTH RELEVANCE: Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.              Development of new machines and software is proposed, which will rapidly analyze a person's genome and reveal new types of information that doctors will be able to use for treating patients. The machines that will be developed are actually very small devices that may one day be sufficiently miniaturized to fit in a person's hand.            ",New Physical Methodologies for Genomic Analysis,8373752,R01HG000225,"['Algorithms', 'Base Pairing', 'Beds', 'Bioinformatics', 'Characteristics', 'Chemistry', 'Chromosomes', 'Complement', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'DNA', 'DNA Sequence', 'DNA Structure', 'Data', 'Data Set', 'Detection', 'Development', 'Devices', 'Electronics', 'Engineering', 'Fluorochrome', 'Generations', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Graph', 'Haemophilus influenzae', 'Hand', 'Image', 'Image Analysis', 'Label', 'Length', 'Link', 'Maps', 'Mechanics', 'Methodology', 'Modeling', 'Molecular', 'Motion', 'Neighborhoods', 'Nucleotides', 'Optics', 'Organism', 'Partner in relationship', 'Patients', 'Persons', 'Polymerase', 'Process', 'Reading', 'Reagent', 'Relative (related person)', 'Scheme', 'Scientist', 'Series', 'Shotgun Sequencing', 'Shotguns', 'Single-Stranded DNA', 'Site', 'Stretching', 'Surface', 'Surgical Flaps', 'System', 'Techniques', 'Technology', 'Testing', 'Translations', 'Validation', 'Vent', 'Vision', 'Work', 'base', 'design', 'ds-DNA', 'engineering design', 'experience', 'genome-wide', 'heuristics', 'miniaturize', 'nanofluidic', 'new technology', 'novel', 'rapid detection', 'reconstruction', 'research study', 'restriction enzyme', 'scaffold', 'success']",NHGRI,UNIVERSITY OF WISCONSIN-MADISON,R01,2012,654177,0.08381404726922746
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8107695,U01HG004695,"['Address', 'Algorithms', 'Area', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Hypersensitivity', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'RNA', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2011,108418,0.05351426429552623
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,8134360,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait', 'treatment strategy']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2011,342569,0.047026166982941854
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,8053866,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2011,577264,0.06097189047337324
"Position Sensitive P-Mer Frequency Clustering with Applications to Classification    DESCRIPTION (provided by applicant):    Position Sensitive P-Mer Frequency Clustering with  Applications to Classification and Differentiation Recent genomic sequencing advances, such as next generation sequencing, and projects like the Human Microbiome Project create extremely large genomic databases. Even though the length of any specific sequence may be much shorter than that of the complete DNA sequence of an organism, looking at enormous libraries of sequences, such as 16S rRNA, presents an equally (if not greater) computational challenge. In traditional genomic analysis, only one sequence may be analyzed at a time. When dealing with metagenomics, thousands (or more) sequences need to be analyzed at the same time. However, to study such problems as environmental biological diversity and human microbiome diversity this is exactly what is needed. Current techniques have several shortcomings which need to be addressed. Techniques involving sequence alignment are typically based on selection of one representative sequence (as is typically done when looking at 16S rRNA data) which introduces selection bias. Genomic databases involving multiple copies of 16S per organism across thousands of organisms, will soon grow too large to practically process just using computationally expensive alignment methods to match sequences, but faster alignment-free methods currently do not provide the needed accuracy and sensitivity. As a complement to existing methods we introduce a novel class of fast high-throughput algorithms based on quasi-alignment using position specific p-mer frequency clustering. Organisms are represented by a directed graph structure that summarizes the ordering between clusters of p-mer frequency histograms at different positions in sequences. This model can be learned using all available 16S copies of an organism and thus eliminates selection bias. Due to the added position information, these algorithms can be used for species (and even strain) classification facilitating the study of strain diversity within species. Our prototype implementation of this new technique shows that it is able to produce compact profiles which can be efficiently stored and used for large scale classification and differentiation down to the strain level. Since the technique incorporates high-throughput data stream clustering, a proven technique in high performance computing, it scales well for very large scale DNA/RNA sequence data as well as massive sets of short sequence snippets collected during metagenomic research. In this project we will develop a suite of tools, profile models, and scoring techniques to model RNA/DNA sequences providing applications of organism classification, and intra/inter-organism similarity/diversity. Our approach provides both the specificity needed to perform strain classification and still avoid the computational overhead of alignment. It is important to note that this is accomplished through dynamic online machine learning techniques without human intervention.      PUBLIC HEALTH RELEVANCE:    Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.                 Recent advances in Metagenomics and the Human Microbiome provide a complex landscape for dealing with a multitude of genomes all at once. One of the many challenges in this field is classification of the genomes present in the sample. Effective metagenomic classification and diversity analysis require complex representations of taxa. The significance of our research is that we develop a suite of tools, based on novel alignment free techniques that will be applied to environmental metagenomics samples as well as human microbiome samples. Providing such methods to rapidly classify organisms using our new approach on a laptop computer instead of several multi-processor servers will facilitate the rapid development of microbiome-based health screening in the near future.            ",Position Sensitive P-Mer Frequency Clustering with Applications to Classification,8192895,R21HG005912,"['Address', 'Algorithms', 'Biodiversity', 'Classification', 'Complement', 'Complex', 'Computational Technique', 'Computers', 'DNA', 'DNA Sequence', 'Data', 'Databases', 'Development', 'Effectiveness', 'Family', 'Frequencies', 'Future', 'Genome', 'Genomics', 'Grant', 'Graph', 'Habitats', 'Health', 'High Performance Computing', 'Human', 'Human Microbiome', 'Intervention', 'Lead', 'Learning', 'Length', 'Libraries', 'Link', 'Machine Learning', 'Metagenomics', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Organism', 'Positioning Attribute', 'Probability', 'Process', 'Property', 'RNA', 'RNA Sequences', 'Research', 'Ribosomal RNA', 'Sampling', 'Screening procedure', 'Selection Bias', 'Sequence Alignment', 'Sequence Analysis', 'Specificity', 'Stream', 'Structure', 'Taxon', 'Techniques', 'Testing', 'Time', 'Update', 'Work', 'base', 'computing resources', 'cost', 'improved', 'laptop', 'microbial', 'microbiome', 'next generation', 'novel', 'novel strategies', 'prototype', 'research study', 'statistics', 'success', 'tool', 'user-friendly', 'web site']",NHGRI,SOUTHERN METHODIST UNIVERSITY,R21,2011,180669,0.043718252587224296
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,8035949,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'amplisome', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2011,321670,0.013671649157477641
"Pattern Discovery for comparative epigenomics    DESCRIPTION (provided by applicant): We propose a training program that will prepare an effective independent investigator in computational genomics. The candidate has a PhD in biology from the University of Cambridge and will extend his skills in both computational and wet-lab methods through a two-year program of organized mentorship and training, and a structured five-year research program.  This program will promote the command of machine learning as applied to functional genomics data. Dr. William Noble will mentor the candidate's scientific development. Dr. Noble is a recognized leader in computational biology and machine learning. He holds a dual appointment as Associate Professor in Genome Sciences and Com- puter Science and Engineering, and has trained numerous postdoctoral fellows and graduate students. Dr. Jeff Bilmes, Associate Professor of Electrical Engineering, will contribute to the mentoring effort, and a committee of experienced genome and computational biologists will advise on science and the candidate's career goals.  Research will focus on the analysis of multiple tracks of data from high-throughput sequencing assays, such as the ChIP-seq data produced by the ENCODE Project. These experiments allow us to obtain a more complete picture of the structure of human chromatin, revealing the behavior of transcription factors, the organization of epigenetic modifications, and the locations of accessible DNA across the entire genome at up to single-base resolution. A current challenge is to discover joint patterns across multiple tracks of these functional genomics results simultaneously. This project will (1) develop computational methods for identifying such patterns, providing new ways of finding both well-understood genomic features and novel functional elements, (2) apply those methods to characterize the similarities and differences among different biological samples, establishing a better understanding of chromatin, the bounds of its variation, and its role in human disease, and (3) validate computational findings with laboratory experiments. The project will use a dynamic Bayesian network (DBN), a type of probabilistic graphical model, to represent the statistical dependencies between observed data, such as sequencing tag density, on an inferred hidden state sequence.  The Department of Genome Sciences of the University Of Washington School Of Medicine provides an ideal setting for training a new independent investigator with an extensive program of formal and informal education for postdoctoral scientists, opportunities for collaboration with researchers with expertise in diverse areas, and modern computational and laboratory resources. This environment maximizes the potential for the candidate to obtain the training and perform the research necessary to establish himself as a skilled investigator with an independent research program.       PUBLIC HEALTH RELEVANCE: The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.              The major outcome of this work will be a trained scientist with the skills to run an independent research pro- gram integrating computational methods and genome biology. Additionally, the research will result in improved methodology and software resources for analyzing functional genomics data, and a better understanding of how chromatin state affects molecular biology and human disease.            ",Pattern Discovery for comparative epigenomics,8164533,K99HG006259,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Appointment', 'Area', 'Base Pairing', 'Behavior', 'Biological', 'Biological Assay', 'Biology', 'Cell physiology', 'Cells', 'Censuses', 'Chromatin', 'Chromatin Structure', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Methylation', 'DNA Sequence', 'DNA-Binding Proteins', 'Data', 'Data Set', 'Data Sources', 'Deoxyribonucleases', 'Dependency', 'Development', 'Digestion', 'Disease', 'Doctor of Philosophy', 'Education', 'Electrical Engineering', 'Elements', 'Engineering', 'Enhancers', 'Environment', 'Epigenetic Process', 'Event', 'Exposure to', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Histocompatibility Testing', 'Human', 'Individual', 'Joints', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Length', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Medicine', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modeling', 'Modification', 'Molecular Biology', 'Molecular and Cellular Biology', 'Outcome', 'Pattern', 'Phase', 'Phenotype', 'Postdoctoral Fellow', 'Probability', 'Qualifying', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Running', 'Sampling', 'Schools', 'Science', 'Scientist', 'Seeds', 'Signal Transduction', 'Signaling Molecule', 'Structure', 'Techniques', 'Time', 'Training', 'Training Programs', 'Transfection', 'Transgenic Mice', 'Universities', 'Variant', 'Washington', 'Work', 'base', 'career', 'cell type', 'chromatin immunoprecipitation', 'clinically relevant', 'comparative', 'computer based statistical methods', 'computer science', 'cytokine', 'density', 'disorder control', 'empowered', 'epigenomics', 'experience', 'follow-up', 'functional genomics', 'genome-wide', 'graduate student', 'histone modification', 'human disease', 'human tissue', 'improved', 'network models', 'new technology', 'novel', 'professor', 'programs', 'promoter', 'research study', 'skills', 'speech processing', 'transcription factor']",NHGRI,UNIVERSITY OF WASHINGTON,K99,2011,102709,0.03643941663342313
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8321717,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,342218,0.010036211174964955
"A Comprehensive catalog of human DNasel hypersensitive sites The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNasel hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNasel hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5). n/a",A Comprehensive catalog of human DNasel hypersensitive sites,8320051,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Insulator Elements', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Molecular', 'Noise', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Predictive Value', 'Preparation', 'Production', 'Public Domains', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'cost effective', 'density', 'design', 'digital', 'experience', 'functional genomics', 'genome-wide', 'high standard', 'high throughput screening', 'histone modification', 'human tissue', 'in vivo', 'insight', 'meetings', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2011,2615448,0.010036211174964955
"Structure-Based Prediction of the Interactome    DESCRIPTION (provided by applicant): Protein-protein interactions (PPIs) play a central role in all biological processes. Akin to the complete sequencing of genomes, complete descriptions of interactomes is a fundamental step towards a deeper understanding of biological processes, and has a vast potential to impact systems biology, genomics, molecular biology and therapeutics. Although high-throughput biochemical approaches for discovering PPIs have proven very successful, the current experimental coverage of the interactome remains inadequate and would benefit from computational tools. The broad, long term goal of this proposal is to harness the information provided by structure-based computational approaches as a potentially high-quality, high-coverage data source for large-scale integrative approaches to interactome construction. Specifically, this project aims to: 1) develop new structure-based prediction methods that can be applied on a genome scale, and 2) integrate these predictions with other functional genomic information to predict PPIs at a genome scale. This project will also generate testable hypotheses for experimental investigations. A key product of the proposed research is the LTHREADER program, a localized threading program that will simultaneously align query sequence-pairs to templates of protein-protein interfaces. By exploiting information contained in the protein complex interfaces, it may significantly improve upon the state-of-the-art in coverage and prediction quality. Some of the core computational aspects are the development of algorithms for threading query sequence-pairs to templates (using linear programming), learning statistical potentials (SVMs), and combining multiple protein interface scores for PPI prediction (boosting). The output from such structure-based approaches will be combined with other functional genomic data in the Struct2Net framework for predicting PPIs (using random forests). A final product of the proposed research will be a comprehensive database of genome-wide PPI predictions derived from purely structure-based as well as integrative approaches. The database will also include extracellular ligand-receptor interactions.       The prediction of PPIs will enable better elucidation of extracellular and intracellular signaling networks, which has direct medical implications in terms of drug target identification. For example, a promising public-health application of this research is the rational design of therapeutics which inhibit or interfere with the binding of extracellular ligands to receptors. All the produced computational algorithms, software, and databases will be made publicly available for further studies.Relevance       Proteins interact with each other to communicate within and between cells, forming networks (the Interactome) that play fundamental roles in all biomedical processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. Understanding these interaction networks on a large scale will empower both rational, targeted drug design and more intelligent disease management. In this project, we develop computational methods for structure-based prediction of protein-protein interactions, and integrate these predictions with available high- throughput genomic data to predict the Interactomes of entire species' genomes.          n/a",Structure-Based Prediction of the Interactome,8054929,R01GM081871,"['Accounting', 'Algorithms', 'Area', 'Avastin', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Categories', 'Cell Communication', 'Cell membrane', 'Cells', 'Chromosome Mapping', 'Classification', 'Co-Immunoprecipitations', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Sources', 'Databases', 'Decision Trees', 'Development', 'Disease', 'Disease Management', 'Drug Delivery Systems', 'Drug Design', 'ERBB2 gene', 'Epidermal Growth Factor Receptor', 'Erbitux', 'Etanercept', 'Evolution', 'Extracellular Protein', 'FGFR1 gene', 'Family', 'Future', 'Generic Drugs', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Homology Modeling', 'Human', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Life', 'Ligand Binding', 'Ligands', 'Linear Programming', 'Link', 'Logistic Regressions', 'Machine Learning', 'Maintenance', 'Measures', 'Medical', 'Membrane Proteins', 'Metabolism', 'Methods', 'Molecular Biology', 'Mutation', 'Oncogenes', 'Ontology', 'Organism', 'Output', 'Performance', 'Play', 'Probability', 'Process', 'Programmed Learning', 'Protein Binding', 'Protein Databases', 'Protein Structure Initiative', 'Proteins', 'Proteome', 'Proteomics', 'Public Health Applications Research', 'Research', 'Roche brand of rituximab', 'Roche brand of trastuzumab', 'Role', 'Scoring Method', 'Signal Transduction', 'Structure', 'Surface', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Translations', 'Validation', 'Yeasts', 'base', 'computerized tools', 'data mining', 'database structure', 'design', 'empowered', 'extracellular', 'fly', 'forest', 'functional genomics', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'interfacial', 'novel', 'novel strategies', 'programs', 'protein complex', 'protein protein interaction', 'protein structure', 'receptor', 'receptor binding', 'research study', 'success', 'therapeutic development', 'yeast two hybrid system']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2011,289921,0.017379151056623875
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8061704,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'information model', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2011,2923298,0.04813082789440468
"Genomic Database for the Yeast Saccharomyces    DESCRIPTION (provided by applicant): The goal of the Saccharomyces Genome Database (SGD) is to continue the development and implementation of a comprehensive resource containing curated information about the genome and its elements of the budding yeast, Saccharomyces cerevisiae. SGD will continue to annotate the genome, assimilate new data, include genomic information from other fungal species, and incorporate formalized and controlled vocabularies to represent biological concepts. We will continue to maintain and broaden relationships with the greater scientific community and make technical improvements through the development of tools and the use of third party tools that will allow us to better serve our users. The database and its associated resources will always remain publicly available without restriction from www.yeastgenome.org.  SGD will continue to provide the S. cerevisiae genome and its gene products culled from the published literature. New user interfaces and analysis resources will be developed for existing information as well as for new types of data, such as results from large scale genomic/proteomic analysis. These improvements will be developed using publicly available tools such as those available from the GMOD project. Query tools will be more enhanced to instantly direct users to the appropriate pages.  SGD has evolved into a substantial service organization, and will maintain its service to the scientific community, reaching out to all yeast researchers as well as scientists outside the fungal community to serve those who have a need for information about budding yeast genes, their products, and their functions. SGD will continue existing services while working to simplify the use and maintenance of our hardware and software environment through the application of new technologies. We will continue to collaborate with the yeast biology community to keep the database accurate and current, and to maintain consensus and order in the naming of genes and other generic elements.      PUBLIC HEALTH RELEVANCE:  Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,            Saccharomyces cerevisiae is a model forth understanding of chromosome maintenance, the cell cycle and cellular biology. S. cerevisiae is used for the development of new genomic and proteomic technologies. S. cerevisiae is the most well studied eukaryofic genome and the experimental literature for this yeast contains these results. The SGD provides a comprehensive resource that facilitates experimentation in other systems,         ",Genomic Database for the Yeast Saccharomyces,8242999,U41HG001315,"['Adopted', 'Affect', 'Architecture', 'Bioinformatics', 'Biological', 'Biology', 'Cell Cycle', 'Cells', 'Cellular biology', 'Chromatin', 'Chromosomes', 'Collaborations', 'Communities', 'Complex', 'Computer Analysis', 'Computer software', 'Consensus', 'Controlled Vocabulary', 'Data', 'Data Display', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Elements', 'Enhancers', 'Environment', 'Generic Drugs', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'Individual', 'Industry', 'Internet', 'Knowledge', 'Laboratories', 'Learning', 'Link', 'Literature', 'Location', 'Maintenance', 'Manuals', 'Maps', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Nomenclature', 'Phenotype', 'Post-Translational Protein Processing', 'Procedures', 'Process', 'Proteins', 'Proteomics', 'Provider', 'Publishing', 'Regulatory Element', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Saccharomycetales', 'Scientist', 'Screening procedure', 'Secure', 'Services', 'Solutions', 'Source', 'System', 'Techniques', 'Technology', 'Universities', 'Untranslated Regions', 'Update', 'Variant', 'Work', 'Yeasts', 'abstracting', 'base', 'data mining', 'design', 'genome database', 'genome sequencing', 'human disease', 'improved', 'model organisms databases', 'mutant', 'new technology', 'promoter', 'tool', 'tool development', 'usability', 'web page']",NHGRI,STANFORD UNIVERSITY,U41,2011,2416667,0.07187658046528386
"Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform    DESCRIPTION (provided by applicant): DNAnexus proposes to develop a complete solution for the identification and stratification of personal genetic variation from ultra-high-throughput sequencing projects. The solution will be implemented as a Web 2.0 service and online browsing tool that will integrate public data sources such as the 1000 genomes project, comparative information, and the ENCODE II project data. Users will be able to browse and stratify the identified variation in the context of these genomic annotations, and according to the likely functional impact. In Phase I of our project, we will develop a basic browser for displaying sequence reads that are mapped to a reference genome with our state-of-the-art read mapper. The browser will support viewing mate paired reads as well as display of the variation between these reads and the reference genome. It will facilitate the algorithmic development that we will perform during Phase II, and it will be the foundation for the more sophisticated variation browser also proposed in Phase II. In Phase II, we will develop algorithms for detecting genomic variation, and a state-of-the-art browser for viewing variation in the context of existing genome annotations, functional genomic and comparative genomic data. Our algorithms for detecting variation will support all major types of genomic variation, including SNPs, microindels, larger insertions and deletions, duplications, copy number variations, inversions, and translocations. Our algorithms will be based on state-of-the-art statistical and machine learning methodology for human genome resequencing. The DNAnexus browser will have two components: a list browser that displays variation as a list filtered and stratified by criteria that a user chooses, and a powerful GUI whose navigation capabilities are inspired by modern online tools such as Google Maps.      PUBLIC HEALTH RELEVANCE: DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.           Project Narrative DNAnexus proposes to develop a complete solution for identifying and analyzing personal genetic variation for individuals whose genomes are sequenced using new sequencing technologies. Users will be able to browse an individual genome in the context of public data sources such as the 1000 genomes project, comparative information to other mammalian species, and functional data from the ENCODE II project.",Human Variation Detection and Visualization on the DNAnexus Web 2.0 platform,7909096,R43HG005794,"['Algorithms', 'Architecture', 'Arts', 'Code', 'Copy Number Polymorphism', 'DNA Resequencing', 'Data', 'Data Display', 'Data Sources', 'Data Storage and Retrieval', 'Detection', 'Development', 'Environment', 'Foundations', 'Genes', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genetics', 'Human Genome', 'Imagery', 'Individual', 'Internet', 'Machine Learning', 'Maps', 'Methodology', 'Partner in relationship', 'Phase', 'Point Mutation', 'Reading', 'Services', 'Solutions', 'Statistical Methods', 'Stratification', 'Technology', 'Variant', 'base', 'comparative', 'comparative genomics', 'cost', 'flexibility', 'functional genomics', 'genome sequencing', 'graphical user interface', 'insertion/deletion mutation', 'public health relevance', 'tool']",NHGRI,"DNANEXUS, INC.",R43,2010,74477,0.07227112774262849
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7913074,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,1248287,0.05351426429552623
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8121894,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,300000,0.05351426429552623
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8144973,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,113520,0.05351426429552623
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,8147585,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2010,151816,0.05351426429552623
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7902231,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative genomics', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2010,346884,0.047026166982941854
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7809669,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2010,573248,0.06097189047337324
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7795846,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2010,326175,0.013671649157477641
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7798186,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'allograft rejection', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2010,289814,0.044712573126176575
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7860712,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2010,297123,0.022016033134299545
"Structure-Based Prediction of the Interactome    DESCRIPTION (provided by applicant): Protein-protein interactions (PPIs) play a central role in all biological processes. Akin to the complete sequencing of genomes, complete descriptions of interactomes is a fundamental step towards a deeper understanding of biological processes, and has a vast potential to impact systems biology, genomics, molecular biology and therapeutics. Although high-throughput biochemical approaches for discovering PPIs have proven very successful, the current experimental coverage of the interactome remains inadequate and would benefit from computational tools. The broad, long term goal of this proposal is to harness the information provided by structure-based computational approaches as a potentially high-quality, high-coverage data source for large-scale integrative approaches to interactome construction. Specifically, this project aims to: 1) develop new structure-based prediction methods that can be applied on a genome scale, and 2) integrate these predictions with other functional genomic information to predict PPIs at a genome scale. This project will also generate testable hypotheses for experimental investigations. A key product of the proposed research is the LTHREADER program, a localized threading program that will simultaneously align query sequence-pairs to templates of protein-protein interfaces. By exploiting information contained in the protein complex interfaces, it may significantly improve upon the state-of-the-art in coverage and prediction quality. Some of the core computational aspects are the development of algorithms for threading query sequence-pairs to templates (using linear programming), learning statistical potentials (SVMs), and combining multiple protein interface scores for PPI prediction (boosting). The output from such structure-based approaches will be combined with other functional genomic data in the Struct2Net framework for predicting PPIs (using random forests). A final product of the proposed research will be a comprehensive database of genome-wide PPI predictions derived from purely structure-based as well as integrative approaches. The database will also include extracellular ligand-receptor interactions.       The prediction of PPIs will enable better elucidation of extracellular and intracellular signaling networks, which has direct medical implications in terms of drug target identification. For example, a promising public-health application of this research is the rational design of therapeutics which inhibit or interfere with the binding of extracellular ligands to receptors. All the produced computational algorithms, software, and databases will be made publicly available for further studies.Relevance       Proteins interact with each other to communicate within and between cells, forming networks (the Interactome) that play fundamental roles in all biomedical processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. Understanding these interaction networks on a large scale will empower both rational, targeted drug design and more intelligent disease management. In this project, we develop computational methods for structure-based prediction of protein-protein interactions, and integrate these predictions with available high- throughput genomic data to predict the Interactomes of entire species' genomes.          n/a",Structure-Based Prediction of the Interactome,7797574,R01GM081871,"['Accounting', 'Algorithms', 'Area', 'Arts', 'Avastin', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Blast Cell', 'Categories', 'Cell Communication', 'Cell membrane', 'Cells', 'Chromosome Mapping', 'Classification', 'Co-Immunoprecipitations', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Sources', 'Databases', 'Decision Trees', 'Development', 'Disease', 'Disease Management', 'Drug Delivery Systems', 'Drug Design', 'ERBB2 gene', 'Epidermal Growth Factor Receptor', 'Erbitux', 'Etanercept', 'Evolution', 'Extracellular Protein', 'FGFR1 gene', 'Family', 'Future', 'Generic Drugs', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Homology Modeling', 'Human', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Life', 'Ligand Binding', 'Ligands', 'Linear Programming', 'Link', 'Logistic Regressions', 'Machine Learning', 'Maintenance', 'Measures', 'Medical', 'Membrane Proteins', 'Metabolism', 'Methods', 'Molecular Biology', 'Mutation', 'Oncogenes', 'Ontology', 'Organism', 'Output', 'Performance', 'Play', 'Probability', 'Process', 'Programmed Learning', 'Protein Binding', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Public Health Applications Research', 'Research', 'Roche brand of rituximab', 'Roche brand of trastuzumab', 'Role', 'Scoring Method', 'Signal Transduction', 'Structure', 'Surface', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Translations', 'Validation', 'Yeasts', 'base', 'computerized tools', 'data mining', 'database structure', 'design', 'empowered', 'extracellular', 'fly', 'forest', 'functional genomics', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'interfacial', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein complex', 'protein protein interaction', 'protein structure', 'receptor', 'receptor binding', 'research study', 'success', 'therapeutic development', 'yeast two hybrid system']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2010,301191,0.017379151056623875
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7780085,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,3513343,0.04813082789440468
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,8138946,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2010,745063,0.04813082789440468
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7622614,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'flexibility', 'foot', 'genome-wide', 'insight', 'meetings', 'member', 'novel', 'quality assurance', 'scale up', 'symposium', 'theories', 'tool', 'working group']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2009,1224323,0.05351426429552623
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7681225,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2009,351164,0.047026166982941854
"Gene Prediction by Markov Models and Complementary Methods    DESCRIPTION (provided by applicant): We propose to extend the ab initio self-training algorithms for eukaryotic gene finding developed in the previous grant period in several important directions. First we will upgrade this algorithm to a multilevel data mining approach to allow construction of a consistent ""genome- transcriptome-proteome"" data structure at the early stages of a genome project. Here, we will compensate for an information deficit in various segments of experimental data (such as EST data) by unsupervised machine learning on existing and abundant data segments (an anonymous genomic sequence) with subsequent computational modeling of missing biological information (protein-coding genes and proteins). An important new feature of the self-training algorithm will be the utilization of protein level information to monitor and increase biological relevance of the models derived by the unsupervised iterative algorithm. Second, we will enhance the self-training algorithm developed earlier on a smaller scale and tested on fungal and other ""compact"" eukaryotic genomes (such as Caenorhabditis elegans and Drosophila melanogaster) to work with most complex eukaryotic genomes. At this higher level of complexity we see species with host genes occupying just a small fraction of genome which can be inhomogeneous in GC composition, populated with transposable elements and pseudogenes (besides animal genomes, genomes of some fungal pathogens as well as human parasites and their vectors fall into this category). Third, for the human microbiome containing bacterial, archaeal, viral and fungal species, situated at yet another end of the genome in homogeneity spectrum, we will develop improved algorithms and tools for ab initio gene identification. This work will be done in close contact with sequencing and annotation groups from leading genome centers both in the US and abroad.           NARRATIVE Rational systems biology, cancer cure, vaccine development, drug design, is impossible without understanding genomic DNA in human cell. Gene prediction is a cornerstone of biological interpretation of DNA sequence. The goal of this proposal is developing automatic and accurate gene prediction algorithms for the most complex genomic sequences important for human health.",Gene Prediction by Markov Models and Complementary Methods,7656528,R01HG000783,"['Address', 'Algorithms', 'Animals', 'Architecture', 'Arts', 'Biological', 'Biological Sciences', 'Biology', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Code', 'Communication', 'Complex', 'Computer Simulation', 'DNA', 'DNA Sequence', 'DNA Transposable Elements', 'Data', 'Development', 'Drosophila melanogaster', 'Drug Design', 'Employee Strikes', 'Escherichia coli', 'Eukaryota', 'Expressed Sequence Tags', 'Feedback', 'Future', 'Gene Expression Profile', 'Gene Proteins', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grant', 'Guanine + Cytosine Composition', 'Haemophilus influenzae', 'Health', 'Human', 'Human Genome', 'Human Microbiome', 'Intercistronic Region', 'Introns', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Parasites', 'Population', 'Prokaryotic Cells', 'Proteins', 'Proteome', 'Pseudogenes', 'RNA Splicing', 'Repetitive Sequence', 'Research', 'Shapes', 'Software Tools', 'Speed', 'Staging', 'Systems Biology', 'Technology', 'Testing', 'Time', 'Training', 'Training Programs', 'Variant', 'Viral', 'Work', 'data mining', 'data structure', 'experience', 'falls', 'improved', 'markov model', 'novel', 'pathogen', 'programs', 'research and development', 'tool', 'vaccine development', 'vector']",NHGRI,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2009,560000,0.06097189047337324
"Algorithmic strategies for detecting structural variation in genomes    DESCRIPTION (provided by applicant): Fine-scale nucleotide changes, along with genetic recombination, are often cited as the major source of human genetic variation [1, 13, 14]. Less is known about larger scale (> 10kb) genomic structural variations. As genomic technologies improve, we are detecting structural variation in ever-increasing numbers, including genomic inversions [24, 48, 71, 65, 31]; insertion/deletion polymorphisms [12, 26, 42]; and, copy number polymorphisms [28, 59, 60]. These large variations can completely disrupt coding and regulatory sites and copy number of genes, and thereby have a huge impact on human phenotypes and disease susceptibility [23, 61]. Deleterious effects have indeed been observed in cancer and other diseases [70, 43]. Our understanding of the scale and impact of these variations can be enhanced by improving computational tools for mining the data from these technologies. Here, I propose the development of algorithms and computational tools to improve detection and resolution (location of breakpoints) of structural variation. Specifically, I will develop algorithms for (a) experimental design of sequencing projects for detecting and resolving structural variations; (b) fine-mapping of breakpoints using end sequence profiling, to detect gene-disruption and gene-fusions; (c) reconstructing tumor genome architectures; (d) detection of targeted genomic variations in a heterogeneous mix of normal versus mutated cells via multiplex PCR; and (e) detection of balanced structural variation in genotype data. The tools will be designed using techniques from statistical machine learning and combinatorial algorithms. Validation will be performed using known structural variations, simulation studies, and extensive experimental collaborations with technology developers and early technology adopters. All of the data, and software will be freely available for academic and non-commercial uses.      PUBLIC HEALTH RELEVANCE: The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and differentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogeneous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term effect on human health.           Project Narrative The proposed computational tools will be used to detect structural variations in human populations as a starting point for understanding their role in normal evolution and disease, specifically cancer. The architecture of tumor genomes will help reveal genes that are disrupted and di!erentially expressed in tumor cells. The targeted detection of genomic lesions in a heterogenous mix of mutated and wildtype cells, will find application as an early diagnostic for cancer. Thus, our computational methods will have an immediate and long term e!ect on human health.",Algorithmic strategies for detecting structural variation in genomes,7635337,R01HG004962,"['Algorithms', 'Architecture', 'Cancer Diagnostics', 'Cataloging', 'Catalogs', 'Cell Fraction', 'Cells', 'Code', 'Collaborations', 'Collection', 'Computer software', 'Computing Methodologies', 'Copy Number Polymorphism', 'DNA Sequence Rearrangement', 'DNA copy number', 'Data', 'Detection', 'Development', 'Diagnostic Neoplasm Staging', 'Disease', 'Disease susceptibility', 'Emerging Technologies', 'Equilibrium', 'Error Sources', 'Event', 'Evolution', 'Experimental Designs', 'Frequencies', 'Gene Dosage', 'Gene Fusion', 'Genes', 'Genetic Polymorphism', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genetics', 'Individual', 'Investigation', 'Length', 'Lesion', 'Location', 'Long-Term Effects', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Microscope', 'Molecular', 'Mutate', 'Mutation', 'Nucleotides', 'Output', 'Phenotype', 'Population', 'Probability', 'Reading', 'Resolution', 'Role', 'Shotguns', 'Site', 'Software Tools', 'Solid', 'Source', 'Spliced Genes', 'Techniques', 'Technology', 'Tumor stage', 'Validation', 'Variant', 'base', 'combinatorial', 'computerized tools', 'cost', 'data mining', 'density', 'design', 'fusion gene', 'improved', 'insertion/deletion mutation', 'neoplastic cell', 'promoter', 'public health relevance', 'simulation', 'statistics', 'structural genomics', 'tool', 'tumor']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2009,330660,0.013671649157477641
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7599555,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'biological systems', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2009,290671,0.044712573126176575
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7595813,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'biological systems', 'comparative', 'computer based statistical methods', 'data integration', 'design', 'flexibility', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface', 'web site']",NIGMS,PRINCETON UNIVERSITY,R01,2009,243004,0.0394466192330621
"Analysis Tool for Heritable and Envirnonmental Network Associations    DESCRIPTION (provided by applicant):       The efforts of the human genome project are beginning to provide important findings for human health. Technological advances in the laboratory, particularly in characterizing human genomic variation, have created new approaches for studying the human genome. However, current statistical and computational strategies are taking only partial advantage of this wealth of information. In the quest for disease susceptibility genes for common, complex disease, we are faced with many challenges. Selecting genetic, clinical, and environmental factors important for the trait of interest is increasingly more difficult as high throughput data generation technologies are developed. We know that genes do not act in isolation, thus numerous other factors are likely important in complex disease phenotypes. However, techniques for robust statistical modeling of important variables to predict clinical outcomes are limited in their capability for interaction effects. Ultimately, we want to know what factors are important to provide superior prevention, diagnosis, and treatment of human disease. Unfortunately, interpretation of statistical models in a meaningful way for biomedical research has been lacking due to the inherent difficulty in making such connections. Thus, a technology that embraces the complexity of human disease and integrates multiple data sources including biological knowledge from the public domain, through a powerful analytical framework is essential for dissecting the architecture of common diseases. ATHENA: the Analysis Tool for Heritable and Environmental Network Associations is a novel framework that incorporates variable selection, modeling, and interpretation to learn more about diseases of public health interest. As the field gains experience in analyzing large scale genomic data, it is crucial that we learn from each other and develop and codify the best strategies.            Many common, complex diseases are likely due to a combination of genetic and environmental risk factors. Out ability to extract all of the meaningful information from very large genomic and phenotypic datasets has been limited by our analytic strategies. The methodology described in this proposal is a powerful new approach to maximize the information learned from large datasets to improve prevention, diagnosis, and treatment of diseases of public health interest.",Analysis Tool for Heritable and Envirnonmental Network Associations,7642231,R01LM010040,"['Architecture', 'Arts', 'Base Pairing', 'Bioinformatics', 'Biological', 'Biological Neural Networks', 'Biology', 'Biomedical Research', 'Candidate Disease Gene', 'Clinical', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Diagnosis', 'Disease', 'Disease susceptibility', 'Environment', 'Environmental Risk Factor', 'Evolution', 'Exhibits', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Models', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health', 'Human', 'Human Genome', 'Human Genome Project', 'Individual', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Methodology', 'Modeling', 'Noise', 'Outcome', 'Prevention', 'Proteomics', 'Public Domains', 'Public Health', 'Research Personnel', 'Resources', 'Sampling', 'Signal Transduction', 'Simulate', 'Single Nucleotide Polymorphism', 'Solutions', 'Statistical Models', 'Susceptibility Gene', 'Techniques', 'Technology', 'Time', 'Variant', 'Vision', 'base', 'computerized tools', 'disease phenotype', 'disorder risk', 'experience', 'flexibility', 'follow-up', 'gene environment interaction', 'gene interaction', 'genetic analysis', 'genome wide association study', 'human disease', 'improved', 'interest', 'novel', 'novel strategies', 'simulation', 'success', 'tool', 'tool development', 'trait']",NLM,VANDERBILT UNIVERSITY,R01,2009,922959,0.022016033134299545
"Structure-Based Prediction of the Interactome    DESCRIPTION (provided by applicant): Protein-protein interactions (PPIs) play a central role in all biological processes. Akin to the complete sequencing of genomes, complete descriptions of interactomes is a fundamental step towards a deeper understanding of biological processes, and has a vast potential to impact systems biology, genomics, molecular biology and therapeutics. Although high-throughput biochemical approaches for discovering PPIs have proven very successful, the current experimental coverage of the interactome remains inadequate and would benefit from computational tools. The broad, long term goal of this proposal is to harness the information provided by structure-based computational approaches as a potentially high-quality, high-coverage data source for large-scale integrative approaches to interactome construction. Specifically, this project aims to: 1) develop new structure-based prediction methods that can be applied on a genome scale, and 2) integrate these predictions with other functional genomic information to predict PPIs at a genome scale. This project will also generate testable hypotheses for experimental investigations. A key product of the proposed research is the LTHREADER program, a localized threading program that will simultaneously align query sequence-pairs to templates of protein-protein interfaces. By exploiting information contained in the protein complex interfaces, it may significantly improve upon the state-of-the-art in coverage and prediction quality. Some of the core computational aspects are the development of algorithms for threading query sequence-pairs to templates (using linear programming), learning statistical potentials (SVMs), and combining multiple protein interface scores for PPI prediction (boosting). The output from such structure-based approaches will be combined with other functional genomic data in the Struct2Net framework for predicting PPIs (using random forests). A final product of the proposed research will be a comprehensive database of genome-wide PPI predictions derived from purely structure-based as well as integrative approaches. The database will also include extracellular ligand-receptor interactions.       The prediction of PPIs will enable better elucidation of extracellular and intracellular signaling networks, which has direct medical implications in terms of drug target identification. For example, a promising public-health application of this research is the rational design of therapeutics which inhibit or interfere with the binding of extracellular ligands to receptors. All the produced computational algorithms, software, and databases will be made publicly available for further studies.Relevance       Proteins interact with each other to communicate within and between cells, forming networks (the Interactome) that play fundamental roles in all biomedical processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. Understanding these interaction networks on a large scale will empower both rational, targeted drug design and more intelligent disease management. In this project, we develop computational methods for structure-based prediction of protein-protein interactions, and integrate these predictions with available high- throughput genomic data to predict the Interactomes of entire species' genomes.          n/a",Structure-Based Prediction of the Interactome,7585723,R01GM081871,"['Accounting', 'Algorithms', 'Area', 'Arts', 'Avastin', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Blast Cell', 'Categories', 'Cell Communication', 'Cell membrane', 'Cells', 'Chromosome Mapping', 'Classification', 'Co-Immunoprecipitations', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Sources', 'Databases', 'Decision Trees', 'Development', 'Disease', 'Disease Management', 'Drug Delivery Systems', 'Drug Design', 'ERBB2 gene', 'Epidermal Growth Factor Receptor', 'Erbitux', 'Etanercept', 'Evolution', 'Extracellular Protein', 'FGFR1 gene', 'Family', 'Future', 'Generic Drugs', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Homology Modeling', 'Human', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Life', 'Ligand Binding', 'Ligands', 'Linear Programming', 'Link', 'Logistic Regressions', 'Machine Learning', 'Maintenance', 'Measures', 'Medical', 'Membrane Proteins', 'Metabolism', 'Methods', 'Molecular Biology', 'Mutation', 'Oncogenes', 'Ontology', 'Organism', 'Output', 'Performance', 'Play', 'Probability', 'Process', 'Programmed Learning', 'Protein Binding', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Public Health Applications Research', 'Research', 'Roche brand of rituximab', 'Roche brand of trastuzumab', 'Role', 'Scoring Method', 'Signal Transduction', 'Structure', 'Surface', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Translations', 'Validation', 'Yeasts', 'base', 'computerized tools', 'data mining', 'database structure', 'design', 'empowered', 'extracellular', 'fly', 'forest', 'functional genomics', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'interfacial', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein complex', 'protein protein interaction', 'protein structure', 'receptor', 'receptor binding', 'research study', 'success', 'therapeutic development', 'yeast two hybrid system']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,293189,0.017379151056623875
"Structure-Based Prediction of the Interactome    DESCRIPTION (provided by applicant): Protein-protein interactions (PPIs) play a central role in all biological processes. Akin to the complete sequencing of genomes, complete descriptions of interactomes is a fundamental step towards a deeper understanding of biological processes, and has a vast potential to impact systems biology, genomics, molecular biology and therapeutics. Although high-throughput biochemical approaches for discovering PPIs have proven very successful, the current experimental coverage of the interactome remains inadequate and would benefit from computational tools. The broad, long term goal of this proposal is to harness the information provided by structure-based computational approaches as a potentially high-quality, high-coverage data source for large-scale integrative approaches to interactome construction. Specifically, this project aims to: 1) develop new structure-based prediction methods that can be applied on a genome scale, and 2) integrate these predictions with other functional genomic information to predict PPIs at a genome scale. This project will also generate testable hypotheses for experimental investigations. A key product of the proposed research is the LTHREADER program, a localized threading program that will simultaneously align query sequence-pairs to templates of protein-protein interfaces. By exploiting information contained in the protein complex interfaces, it may significantly improve upon the state-of-the-art in coverage and prediction quality. Some of the core computational aspects are the development of algorithms for threading query sequence-pairs to templates (using linear programming), learning statistical potentials (SVMs), and combining multiple protein interface scores for PPI prediction (boosting). The output from such structure-based approaches will be combined with other functional genomic data in the Struct2Net framework for predicting PPIs (using random forests). A final product of the proposed research will be a comprehensive database of genome-wide PPI predictions derived from purely structure-based as well as integrative approaches. The database will also include extracellular ligand-receptor interactions.       The prediction of PPIs will enable better elucidation of extracellular and intracellular signaling networks, which has direct medical implications in terms of drug target identification. For example, a promising public-health application of this research is the rational design of therapeutics which inhibit or interfere with the binding of extracellular ligands to receptors. All the produced computational algorithms, software, and databases will be made publicly available for further studies.Relevance       Proteins interact with each other to communicate within and between cells, forming networks (the Interactome) that play fundamental roles in all biomedical processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. Understanding these interaction networks on a large scale will empower both rational, targeted drug design and more intelligent disease management. In this project, we develop computational methods for structure-based prediction of protein-protein interactions, and integrate these predictions with available high- throughput genomic data to predict the Interactomes of entire species' genomes.          n/a",Structure-Based Prediction of the Interactome,7895360,R01GM081871,"['Accounting', 'Algorithms', 'Area', 'Arts', 'Avastin', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Blast Cell', 'Categories', 'Cell Communication', 'Cell membrane', 'Cells', 'Chromosome Mapping', 'Classification', 'Co-Immunoprecipitations', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Sources', 'Databases', 'Decision Trees', 'Development', 'Disease', 'Disease Management', 'Drug Delivery Systems', 'Drug Design', 'ERBB2 gene', 'Epidermal Growth Factor Receptor', 'Erbitux', 'Etanercept', 'Evolution', 'Extracellular Protein', 'FGFR1 gene', 'Family', 'Future', 'Generic Drugs', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Homology Modeling', 'Human', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Life', 'Ligand Binding', 'Ligands', 'Linear Programming', 'Link', 'Logistic Regressions', 'Machine Learning', 'Maintenance', 'Measures', 'Medical', 'Membrane Proteins', 'Metabolism', 'Methods', 'Molecular Biology', 'Mutation', 'Oncogenes', 'Ontology', 'Organism', 'Output', 'Performance', 'Play', 'Probability', 'Process', 'Programmed Learning', 'Protein Binding', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Public Health Applications Research', 'Research', 'Roche brand of rituximab', 'Roche brand of trastuzumab', 'Role', 'Scoring Method', 'Signal Transduction', 'Structure', 'Surface', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Translations', 'Validation', 'Yeasts', 'base', 'computerized tools', 'data mining', 'database structure', 'design', 'empowered', 'extracellular', 'fly', 'forest', 'functional genomics', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'interfacial', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein complex', 'protein protein interaction', 'protein structure', 'receptor', 'receptor binding', 'research study', 'success', 'therapeutic development', 'yeast two hybrid system']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2009,289931,0.017379151056623875
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7941562,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,1038804,0.04813082789440468
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7581087,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Data', 'Databases', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'data format', 'empowered', 'functional genomics', 'genetic element', 'human disease', 'interest', 'model organisms databases', 'repository', 'tool']",NHGRI,JACKSON LABORATORY,P41,2009,3437506,0.04813082789440468
"EDAC: ENCODE Data Analysis Center    DESCRIPTION (provided by applicant):   The ENCODE Data Analysis Center (EDAC) proposal aims to provide a flexible analysis resource for the ENCODE project. The ENCODE project is a large multi center project which aims to define all the functional elements in the human genome. This will be achieved using many different experimental techniques coupled with numerous computational techniques. A critical part in delivering this set of functional elements is the integration of data from multiple sources. The ED AC proposal aims to provide this integration. As proscribed by the RFA for this proposal, the precise prioritization for the EDAC's work will be set by an external group, the Analysis Working Group (AWG). Based on previous experience, these analysis methods will require a variety of techniques. We expect to have to apply sophisticated statistical models to the integration of the data, in particular mitigating the problems of the extensive heterogeneity and correlation of variables on the human genome. We have statistical experts who can use the large size of the human genome, coupled with a limited number of sensible assumptions to produce statistical techniques which are robust to this considerable heterogeneity. We also expect to apply machine learning techniques to build integration methods combining datasets. These included Bayesian based inference methods and the robust computer science technique of Support Vector Machines. Each of these methods have performed well in the ENCODE pilot project and we expect them to be even more useful in the full ENCODE project. We will also provide quality assurance and summary metrics of genome-wide multiple alignments. This area has a number of complex statistical, algorithmic and engineering issues, which we will solve using state of the art techniques. Overall we aim to provide deep integration of the ENCODE data, under the direction of the AWG and in tight collaboration with the other members of the ENCODE consortium.           n/a",EDAC: ENCODE Data Analysis Center,7499147,U01HG004695,"['Address', 'Algorithms', 'Area', 'Arts', 'Be++ element', 'Behavior', 'Beryllium', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Collaborations', 'Complex', 'Computational Technique', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Depth', 'Development', 'Educational workshop', 'Engineering', 'Equipment and supply inventories', 'Freezing', 'Gene Expression Regulation', 'Genome', 'Genomics', 'Goals', 'Heterogeneity', 'Human Genome', 'Indium', 'Link', 'Machine Learning', 'Manuscripts', 'Maps', 'Methods', 'Metric', 'Nature', 'Numbers', 'Phase', 'Pilot Projects', 'Publications', 'Records', 'Reporting', 'Research Personnel', 'Resources', 'Scientist', 'Source', 'Statistical Models', 'Structure', 'Techniques', 'Telephone', 'Transcript', 'Vertebral column', 'Work', 'base', 'computer science', 'data integration', 'experience', 'experimental analysis', 'foot', 'insight', 'member', 'novel', 'quality assurance', 'scale up', 'size', 'symposium', 'theories', 'tool']",NHGRI,EUROPEAN MOLECULAR BIOLOGY LABORATORY,U01,2008,1200000,0.05351426429552623
"What Made Us Human?    DESCRIPTION (provided by applicant): Comparative genomics promises to shed light on those genetic changes that gave rise to the modern human species. Mounting evidence suggests that the vast majority of functional differences between the human and chimpanzee genomes are in regions that do not code for proteins. Focusing on these non-coding regions, we will investigate lineage-specific evolution in the human genome. Our approach includes developing likelihood ratio tests for identifying changes in either the rate or the pattern of nucleotide substitution in a single lineage. These novel methods will be implemented in open source software that can be used to scan an entire genome. We will apply this evolutionary analysis to multiple sequence alignments of human and other vertebrates, including several closely related species (macaque, chimpanzee, Neanderthal), allowing us to identify recent changes in the human genome. In order to concentrate on functionally relevant changes, evolutionary testing will be limited to sets of candidate regions with specific known or predicted functions (e.g. regulatory regions, RNA genes). Predicted functional regions will be identified using machine learning classification techniques. These classifiers will employ measures of sequence conservation as well as the rapidly expanding collection of experimental and bioinformatic annotations of the human genome, including results of the ENCODE Project and other functional genomic studies. After identifying those regions that were most significantly altered in the human lineage, we will use this functional information to develop testable hypotheses about the effects of the observed changes. Experimental investigations of these genomic regions will lead to new understanding of the evolution of human biology and health.       PROJECT NARRATIVE: This project will vastly expand knowledge of biologically relevant features of the human genome that are unique to our species. Identification and characterization of the genetic changes leading to modern humans is of fundamental interest. These investigations also promise to contribute to our understanding of the causal mechanisms behind human diseases, leading to directed treatment and prevention strategies.          n/a",What Made Us Human?,7522602,R01GM082901,"['Affect', 'Amino Acids', 'Bioinformatics', 'Categories', 'Class', 'Classification', 'Code', 'Collaborations', 'Collection', 'Computer software', 'DNA', 'Data', 'Databases', 'Evolution', 'Functional RNA', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Investigation', 'Knowledge', 'Lead', 'Light', 'Macaca', 'Machine Learning', 'Mammals', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Mutation', 'Nucleotides', 'Pan Genus', 'Pan troglodytes', 'Pattern', 'Prevention strategy', 'Process', 'Proteins', 'Public Domains', 'Rate', 'Relative (related person)', 'Ribonucleic Acid Regulatory Sequences', 'Scanning', 'Sequence Alignment', 'Site', 'Techniques', 'Testing', 'Vertebrates', 'base', 'comparative', 'experience', 'functional genomics', 'genome wide association study', 'human disease', 'insight', 'interest', 'novel', 'open source', 'simulation', 'trait']",NIGMS,J. DAVID GLADSTONE INSTITUTES,R01,2008,369424,0.047026166982941854
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7407451,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2008,291451,0.044712573126176575
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7404447,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2008,243004,0.0394466192330621
"Structure-Based Prediction of the Interactome    DESCRIPTION (provided by applicant): Protein-protein interactions (PPIs) play a central role in all biological processes. Akin to the complete sequencing of genomes, complete descriptions of interactomes is a fundamental step towards a deeper understanding of biological processes, and has a vast potential to impact systems biology, genomics, molecular biology and therapeutics. Although high-throughput biochemical approaches for discovering PPIs have proven very successful, the current experimental coverage of the interactome remains inadequate and would benefit from computational tools. The broad, long term goal of this proposal is to harness the information provided by structure-based computational approaches as a potentially high-quality, high-coverage data source for large-scale integrative approaches to interactome construction. Specifically, this project aims to: 1) develop new structure-based prediction methods that can be applied on a genome scale, and 2) integrate these predictions with other functional genomic information to predict PPIs at a genome scale. This project will also generate testable hypotheses for experimental investigations. A key product of the proposed research is the LTHREADER program, a localized threading program that will simultaneously align query sequence-pairs to templates of protein-protein interfaces. By exploiting information contained in the protein complex interfaces, it may significantly improve upon the state-of-the-art in coverage and prediction quality. Some of the core computational aspects are the development of algorithms for threading query sequence-pairs to templates (using linear programming), learning statistical potentials (SVMs), and combining multiple protein interface scores for PPI prediction (boosting). The output from such structure-based approaches will be combined with other functional genomic data in the Struct2Net framework for predicting PPIs (using random forests). A final product of the proposed research will be a comprehensive database of genome-wide PPI predictions derived from purely structure-based as well as integrative approaches. The database will also include extracellular ligand-receptor interactions.       The prediction of PPIs will enable better elucidation of extracellular and intracellular signaling networks, which has direct medical implications in terms of drug target identification. For example, a promising public-health application of this research is the rational design of therapeutics which inhibit or interfere with the binding of extracellular ligands to receptors. All the produced computational algorithms, software, and databases will be made publicly available for further studies.Relevance       Proteins interact with each other to communicate within and between cells, forming networks (the Interactome) that play fundamental roles in all biomedical processes including the maintenance of cellular integrity, metabolism, transcription/translation, and cell-cell communication. Understanding these interaction networks on a large scale will empower both rational, targeted drug design and more intelligent disease management. In this project, we develop computational methods for structure-based prediction of protein-protein interactions, and integrate these predictions with available high- throughput genomic data to predict the Interactomes of entire species' genomes.          n/a",Structure-Based Prediction of the Interactome,7464375,R01GM081871,"['Accounting', 'Algorithms', 'Area', 'Arts', 'Avastin', 'Binding', 'Biochemical', 'Biological', 'Biological Process', 'Blast Cell', 'Categories', 'Cell Communication', 'Cell membrane', 'Cells', 'Chromosome Mapping', 'Classification', 'Co-Immunoprecipitations', 'Collaborations', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Consultations', 'Data', 'Data Sources', 'Databases', 'Decision Trees', 'Depth', 'Development', 'Disease', 'Disease Management', 'Drug Delivery Systems', 'Drug Design', 'ERBB2 gene', 'Epidermal Growth Factor Receptor', 'Erbitux', 'Etanercept', 'Evolution', 'Extracellular Protein', 'FGFR1 gene', 'Facility Construction Funding Category', 'Family', 'Future', 'Generic Drugs', 'Genes', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Homology Modeling', 'Human', 'Human Genome', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Life', 'Ligand Binding', 'Ligands', 'Linear Programming', 'Link', 'Localized', 'Logistic Regressions', 'Machine Learning', 'Maintenance', 'Measures', 'Medical', 'Membrane Proteins', 'Metabolism', 'Methods', 'Molecular Biology', 'Mutation', 'Numbers', 'Oncogenes', 'Ontology', 'Organism', 'Output', 'Performance', 'Play', 'Probability', 'Process', 'Programmed Learning', 'Protein Binding', 'Protein Databases', 'Proteins', 'Proteome', 'Proteomics', 'Public Health Applications Research', 'Research', 'Roche brand of rituximab', 'Roche brand of trastuzumab', 'Role', 'Score', 'Scoring Method', 'Signal Transduction', 'Structure', 'Surface', 'Systems Biology', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Training', 'Translations', 'Validation', 'Yeasts', 'base', 'computerized tools', 'data mining', 'design', 'extracellular', 'fly', 'forest', 'functional genomics', 'genome database', 'genome sequencing', 'human disease', 'improved', 'interfacial', 'novel', 'novel strategies', 'numb protein', 'programs', 'protein protein interaction', 'protein structure', 'receptor', 'receptor binding', 'research study', 'success', 'yeast two hybrid system']",NIGMS,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2008,280891,0.017379151056623875
"WormBase: a core data resource for C elegans and other nematodes    DESCRIPTION (provided by applicant):  Caenorhabditis elegans is a major model system for basic biological and biomedical research and the first animal for which there is a complete description of its genome, anatomy and development, and some information about each of its ~22,000 genes. Five years of funding is requested to maintain and expand WormBase, a Model Organism Database (MOD), with complete coverage of core genomic, genetic, anatomical and functional information about this and other nematodes. Such a database is necessary to allow the entire biomedical research community to make full use of nematode genomic sequences. The two top priorities will be intensive data curation and user interface improvement. WormBase will include up-to-date annotation of the genomic data, the current genetic and physical maps and many experimental data such as genome-scale datasets connected to the function and interactions of cells and genes, as well as development, physiology and behavior. Direct access to the sources of biological material, such as the strain collection of the Caenorhabditis Genetics Center and direct links to data sets maintained by others will be provided. Data will be recovered from the existing resources, from direct contribution of the individual laboratories, and from the literature. While WormBase will act as a central forum through which every laboratory will be able to contribute constructively to the global effort to fully comprehend this metazoan organism, WormBase professional curators will ensure detailed attribution of data sources and check consistency and integrity. To facilitate communication, WormBase will use technology, terminology and style concordant with other databases wherever possible. WormBase will maintain ontologies for nematode anatomy and phenotypes. WormBase will be Web-based and easy to use. Multiple relational databases will be used for data management; the object-based Acedb database system will be used for integration, and this integrated database plus ""slave"" relational databases will be used to drive the website. Coordination of the project and the main curation site will be at Caltech under the supervision of a C. elegans biologist. Curation and annotation of genomic sequence will take place at the centers - the Sanger Institute and Washington University - that generated the entire genome sequence. Oxford University will maintain genetic nomenclature.  Nematodes (roundworms) are major parasites of humans, livestock and crops, and extension of WormBase to broader coverage of nematode genomics will facilitate research into the diagnosis and treatment of nematode-based disease. Studies of C. elegans have informed us of basic principles of normal development and the molecular basis of aging, cancer, nicotine addiction, as well as a variety of fundamental biological processes such as cell migration, cell differentiation and cell death.              n/a",WormBase: a core data resource for C elegans and other nematodes,7502984,P41HG002223,"['Ablation', 'Age', 'Agriculture', 'Alleles', 'Anatomy', 'Animals', 'Antibodies', 'Architecture', 'Base Sequence', 'Behavior', 'Binding Sites', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biomedical Research', 'Caenorhabditis', 'Caenorhabditis elegans', 'Cell Communication', 'Cell Death', 'Cell Differentiation process', 'Cell physiology', 'Cells', 'Chromosome Mapping', 'Code', 'Collection', 'Communication', 'Communities', 'Comparative Anatomy', 'Compatible', 'DNA Sequence', 'Data', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Elements', 'Ensure', 'Expressed Sequence Tags', 'Funding', 'Gene Expression Regulation', 'Gene Proteins', 'Gene Structure', 'Genes', 'Genetic', 'Genetic Processes', 'Genetic Recombination', 'Genetic Variation', 'Genome', 'Genomics', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Institutes', 'Internet', 'Knock-out', 'Knowledge', 'Laboratories', 'Link', 'Literature', 'Livestock', 'Longevity', 'Malignant Neoplasms', 'Maps', 'Medical', 'Metabolic', 'Methods', 'Molecular', 'Molecular Genetics', 'Mutation', 'Names', 'Natural Language Processing', 'Nature', 'Nematoda', 'Nicotine Dependence', 'Nomenclature', 'Online Systems', 'Ontology', 'Organism', 'Paper', 'Parasites', 'Parasitic nematode', 'Pathway interactions', 'Phenotype', 'Physical Chromosome Mapping', 'Physiology', 'Pliability', 'Process', 'Proteins', 'Proteomics', 'RNA Interference', 'Reagent', 'Regulation', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Secure', 'Site', 'Slave', 'Source', 'Subcellular Anatomy', 'Supervision', 'System', 'Techniques', 'Technology', 'Terminology', 'Tertiary Protein Structure', 'Transcript', 'Transgenes', 'Transgenic Organisms', 'Universities', 'Variant', 'Washington', 'Yeasts', 'base', 'cell motility', 'chromatin immunoprecipitation', 'comparative', 'comparative genomic hybridization', 'data integration', 'data management', 'data modeling', 'design', 'experience', 'functional genomics', 'gene function', 'genetic analysis', 'genome sequencing', 'improved', 'interoperability', 'member', 'migration', 'model organisms databases', 'programs', 'research study', 'small molecule', 'tool', 'transcription factor', 'usability', 'web interface', 'yeast two hybrid system']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,P41,2008,2750000,0.03738615221495066
"Methods for genomic data with graphical structures    DESCRIPTION (provided by applicant): The broad, long-term objective of this project concerns the development of novel statistical methods and computational tools for statistical and probabilistic modeling of genomic data motivated by important biological questions and experiments. The specific aim of the current project is to develop new statistical models and methods for analysis of genomic data with graphical structures, focusing on methods for analyzing genetic pathways and networks, including the development of nonparametric pathway-smooth tests for two-sample and analysis of variance problems for identifying pathways with perturbed activity between two or multiple experimental conditions, the development of group Lasso and group threshold gradient descent regularized estimation procedures for the pathway-smoothed generalized linear models, Cox proportional hazards models and the accelerated failure time models in order to identify pathways that are related to various clinical phenotypes. These methods hinge on novel integration of spectral graph theory, non-parametric methods for analysis of multivariate data and regularized estimation methods fro statistical learning. The new methods can be applied to different types of genomic data and will ideally facilitate the identification of genes and biological pathways underlying various complex human diseases and complex biological processes. The project will also investigate the robustness, power and efficiencies o these methods and compare them with existing methods. In addition, this project will develop practical a feasible computer programs in order to implement the proposed methods, to evaluate the performance o these methods through application to real data on microarray gene expression studies of human hear failure, cardiac allograft rejection and neuroblastoma. The work proposed here will contribute both statistical methodology to modeling genomic data with graphical structures, to studying complex phenotypes and biological systems and methods for high-dimensional data analysis, and offer insight into each of the clinical areas represented by the various data sets to evaluate these new methods. All programs developed under this grant and detailed documentation will be made available free-of-charge to interested researchers via the World Wide Web.          n/a",Methods for genomic data with graphical structures,7247404,R01CA127334,"['Address', 'Analysis of Variance', 'Area', 'Biological', 'Biological Process', 'Charge', 'Clinical', 'Collaborations', 'Complex', 'Computer software', 'Condition', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Disease regression', 'Documentation', 'Event', 'Failure', 'Gene Expression', 'Genes', 'Genomics', 'Grant', 'Graph', 'Hearing', 'Heart failure', 'Human', 'Internet', 'Lasso', 'Linear Models', 'Machine Learning', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Multivariate Analysis', 'Neuroblastoma', 'Pathway interactions', 'Pennsylvania', 'Performance', 'Phenotype', 'Procedures', 'Proteomics', 'Regulatory Pathway', 'Research Personnel', 'Sampling', 'Signal Pathway', 'Statistical Methods', 'Statistical Models', 'Structure', 'System', 'Testing', 'Time', 'Universities', 'Work', 'clinical phenotype', 'computer program', 'computerized tools', 'genetic analysis', 'heart allograft', 'high throughput technology', 'human disease', 'insight', 'interest', 'novel', 'programs', 'research study', 'response', 'software development', 'theories', 'vector']",NCI,UNIVERSITY OF PENNSYLVANIA,R01,2007,292160,0.044712573126176575
"A Comprehensive catalog of human DNasel hypersensitive sites    DESCRIPTION (provided by applicant):   The overall aim of this proposal is to establish a comprehensive, high-quality catalogue of human DNaseI hypersensitive sites (DHSs) spanning all major tissue lineages. We plan to map DNaseI hypersensitive sites at physiological resolution across the genome with high sensitivity and specificity. The major focus of our production effort will be on data quality, a strategy that served the Human Genome Project well. Accordingly, samples will be rigorously screened in a pipeline fashion, with only a select set advancing to whole-genome data collection (Specific Aim 1). To ensure the broadest possible coverage of both unique and non-unique genomic territories, a synergistic combination of three technologies (DNase-array, digital mapping of DNAasel cleave site sequences, and Quantitative Chromatin Profiling) will be applied (Specific Aim 2). This combination will enable mapping of >95% of the DHSs in the genome of each cell type. Independent validation provides the ultimate quality standard. We therefore plan to validate the DHS catalogue in a statistically rigorous fashion using hypersensitivity Southerns, a well-established, gold standard assay (Specific Aim 3). Since DNAasel hypersensitive sites are generic markers of a broad spectrum of human cis-regulatory sequences, the utility of the catalogue will be greatly enhanced by the classification of DHSs into major functional categories including promoters, distal elements (enhancers, LCRs), and insulators (Specific Aim 4). Validation of DHS functional classes will be accomplished using well-tested cell and transgenic assays of biological function (Specific Aim 5).           n/a",A Comprehensive catalog of human DNasel hypersensitive sites,7410206,U54HG004592,"['Algorithms', 'Biological', 'Biological Assay', 'Biological Process', 'Biological Testing', 'Biology', 'Boundary Elements', 'Cataloging', 'Catalogs', 'Categories', 'Cell Nucleus', 'Cells', 'Chromatin', 'Class', 'Classification', 'Cleaved cell', 'Communities', 'Custom', 'Data', 'Data Collection', 'Data Quality', 'Deoxyribonuclease I', 'Deoxyribonucleases', 'Detection', 'Digestion', 'Distal', 'Distal Enhancer Elements', 'Elements', 'Employee Strikes', 'Enhancers', 'Ensure', 'Environment', 'Exhibits', 'Generations', 'Generic Drugs', 'Genes', 'Genome', 'Genomics', 'Goals', 'Gold', 'Histones', 'Human', 'Human Genome', 'Human Genome Project', 'Hypersensitivity', 'Individual', 'Informatics', 'Laboratories', 'Locales', 'Locus Control Region', 'Machine Learning', 'Maps', 'Methods', 'Metric', 'Modification', 'Molecular', 'Noise', 'Numbers', 'Physiological', 'Pilot Projects', 'Plague', 'Positioning Attribute', 'Preparation', 'Production', 'Public Domains', 'Range', 'Rate', 'Regulation', 'Research Infrastructure', 'Resolution', 'Sample Size', 'Sampling', 'Score', 'Sensitivity and Specificity', 'Signal Transduction', 'Site', 'Staging', 'Standards of Weights and Measures', 'Surveys', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Transgenic Mice', 'Transgenic Organisms', 'Validation', 'base', 'cell type', 'cost', 'density', 'design', 'digital', 'experience', 'functional genomics', 'high throughput screening', 'human tissue', 'in vivo', 'insight', 'promoter']",NHGRI,UNIVERSITY OF WASHINGTON,U54,2007,3114596,0.010623563689215515
"Integration and Visualization of Diverse Biological Data DESCRIPTION (provided by applicant): Currently a gap exists between the explosion of high-throughput data generation in molecular biology and the relatively slower growth of reliable functional information extracted from the data. This gap is largely due to the lack of specificity necessary for accurate gene function prediction in the currently available large-scale experimental technologies for rapid protein function assessment. Bioinformatics methods that integrate diverse data sources in their analysis achieve higher accuracy and thus alleviate this lack of specificity, but there's a paucity of generalizable, efficient, and accurate methods for data integration. In addition, no efficient methods exist to effectively display diverse genomic data, even though visualization has been very valuable for analysis of data from large scale technologies such as gene expression microarrays. The long-term goal of this proposal is to develop an accurate and generalizable bioinformatics framework for integrated analysis and visualization of heterogeneous biological data.      We propose to address the data integration problem with a Bayesian network approach and effective visualization methods. We have shown the efficacy of this method in a proof-of-principle system that increased the accuracy of gene function prediction for Saccharomyces cerevisiae compared to individual data sources. Building on our previous work, we present a two-part plan to improve and expand our system and to develop novel visualization methods for genomic data based on the scalable display technology. First, we will investigate the computational and theoretical issues behind accurate integration, analysis and effective visualization of heterogeneous high-throughput data. Then, leveraging our existing system and algorithmic improvements developed in the first part of the project, we will design and implement a full-scale data integration and function prediction system for Saccharomyces cerevisiae that will be incorporated with the Saccharomyces Genome Database (SGD), a model organism database for yeast.      The proposed system would provide highly accurate automatic function prediction that can accelerate genomic functional annotation through targeted experimental testing. Furthermore, our system will perform general integration and will offer researchers a unified view of the diverse high-throughput data through effective integration and visualization tools, thereby facilitating hypothesis generation and data analysis. Our scalable visualization methods will enable teams of researchers to examine biological data interactively and thus support the highly collaborative nature of genomic research. In addition to contributing to S. cerevisiae genomics, the technology for efficient and accurate heterogeneous data integration and visualization developed as a result of this proposal will form a basis for systems that address the same set of issues for other organisms, including the human. n/a",Integration and Visualization of Diverse Biological Data,7214148,R01GM071966,"['Address', 'Algorithms', 'Binding', 'Bioinformatics', 'Biological', 'Biological Models', 'Biological Process', 'Biology', 'Collaborations', 'Communities', 'Compatible', 'Computer software', 'Consultations', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Depth', 'Development', 'Effectiveness', 'Evaluation', 'Expert Systems', 'Explosion', 'Gene Expression', 'Generations', 'Genes', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Growth', 'Human', 'Human Genome', 'Imagery', 'Individual', 'Information Systems', 'Institutes', 'Investigation', 'Knock-out', 'Knowledge', 'Laboratories', 'Learning', 'Literature', 'Machine Learning', 'Magic', 'Methods', 'Molecular Biology', 'Monitor', 'Nature', 'Online Systems', 'Organism', 'Phenotype', 'Pliability', 'Probability', 'Process', 'Protein-Protein Interaction Map', 'Proteomics', 'Regulation', 'Research', 'Research Personnel', 'Resolution', 'Saccharomyces', 'Saccharomyces cerevisiae', 'Scientist', 'Side', 'Source', 'Specificity', 'Staining method', 'Stains', 'Structure', 'System', 'Systems Biology', 'Technology', 'Test Result', 'Testing', 'Two-Hybrid System Techniques', 'Universities', 'Visualization software', 'Work', 'Yeasts', 'base', 'comparative', 'computer based statistical methods', 'concept', 'data integration', 'design', 'functional genomics', 'gene function', 'genome database', 'high throughput analysis', 'improved', 'model organisms databases', 'novel', 'parallel computing', 'programs', 'protein function', 'prototype', 'research study', 'software development', 'tandem mass spectrometry', 'tool', 'ultra high resolution', 'web interface']",NIGMS,PRINCETON UNIVERSITY,R01,2007,240927,0.0394466192330621
"Genetical Genomics Analysis Software    DESCRIPTION (provided by applicant): Response to drug treatment is thought dependent upon genotype for many modern therapies. Knowledge of how each genotype responds to a particular therapy is bene?cial only in that one can identify portions of the population which cannot reap the benefits of said treatment. A better course of action is to identify not only which genotype responds, or not, to a particular therapy, but to identify which region of the genome is responding, or not, and how. We believe that this information will lead to new drug targets and better therapies that benefit a larger portion of the population. The goal of this proposal is to provide a suite of software tools for genetic and genomic scientists performing gene mapping experiments with genomic data as the response variable. These tools will ideally provide functionality for 1) detecting polymorphic regions of the genome that con- fer transcript expression differences, 2) identify polymorphic regions of the genome that impart expression differences in genes located elsewhere in the genome, and 3) detecting interactions between loci that may correspond to epistatic effects on transcription. Some software already exists to perform each of these tasks as distinct independent solutions. This proposal intends to produce an integrated solution, S+EQTL (S-PLUS for expression quantitative trait loci mapping), that utilizes the power of S-PLUS and both incorporates and extends the functionality of an exist- ing genetics suite. By providing scientists with an integrated set of tools for genomics experiments with a genetic component, more productive time can be spent interpreting the results rather than transforming data into different formats to be processed by multiple software analysis packages. This software should also address one of the most dif?cult aspects of genetical genomics exper- iments, the so called curse of dimensionality. As the genomics community continues gathering knowledge of transcripts in various organisms, the arrays that interrogate transcript abundance only grow larger in the number of transcript species included. In the absence of tools designed for this purpose, the research scientist is left with the option of either focusing on a narrow set of previously known genes or performing a grid-wise search on all genes in the array. The former is not interesting as these genes are likely well studied and may provide little novel insight. The latter is computationally demanding and may not be possible on the new, larger arrays. A recent publication presents a novel solution that may be enhanced to gain both power and scale using Bayesian methodology. Knowledge of how each genotype responds to a particular drug therapy is beneficial only in that one can identify portions of the population which cannot reap the benefits of said treatment. A better course of action is to identify not only which genotype responds, or not, to a particular therapy, but to identify which region of the genome is responding, or not, and how. We believe that the development of analytic tools for gene mapping experiments to identify this information will lead to new drug targets and better therapies that benefit a larger portion of the population.          n/a",Genetical Genomics Analysis Software,7216142,R43GM079852,"['Address', 'Air', 'Algorithms', 'Animal Genetics', 'Anus', 'Arizona', 'Bioconductor', 'Bioinformatics', 'Biological Markers', 'Biometry', 'Biotechnology', 'Bovine Spongiform Encephalopathy', 'Cations', 'Cattle', 'Chromosome Mapping', 'Code', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Con-fer', 'Data', 'Data Analyses', 'Data Set', 'Department of Defense', 'Depth', 'Development', 'Diagnostic', 'Disease', 'Disease regression', 'Doctor of Philosophy', 'Drug Delivery Systems', 'Educational process of instructing', 'Educational workshop', 'Employment', 'Ensure', 'Exons', 'Family suidae', 'Fatty acid glycerol esters', 'Foundations', 'Gene Combinations', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Research', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Government', 'Government Agencies', 'Graph', 'Imagery', 'Individual', 'Industry', 'Institution', 'International', 'Investments', 'Iowa', 'Knowledge', 'Lead', 'Left', 'Libraries', 'Literature', 'Liver', 'Location', 'Machine Learning', 'Manuals', 'Maps', 'Methodology', 'Methods', 'Microarray Analysis', 'Modeling', 'Molecular Genetics', 'Mus', 'Nebraska', 'North Carolina', 'Numbers', 'Obese Mice', 'Obesity', 'Organism', 'Output', 'Pathway interactions', 'Performance', 'Pharmaceutical Preparations', 'Pharmaceutical Services', 'Pharmacotherapy', 'Phase', 'Phenotype', 'Population', 'Principal Investigator', 'Process', 'Publications', 'Purpose', 'Quantitative Genetics', 'Quantitative Trait Loci', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Single Nucleotide Polymorphism', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'Small Business Technology Transfer Research', 'Software Tools', 'Solutions', 'Standards of Weights and Measures', 'Sus scrofa', 'Techniques', 'Telecommunications', 'Testing', 'Therapeutic', 'Thermogenesis', 'Thinking', 'Time', 'Time Series Analysis', 'Tissues', 'Training', 'Transcript', 'Treatment Protocols', 'United States National Aeronautics and Space Administration', 'United States National Institutes of Health', 'Universities', 'Washington', 'Work', 'animal breeding', 'base', 'design', 'experience', 'expression vector', 'genetic pedigree', 'hazard', 'improved', 'insight', 'interest', 'lecturer', 'novel', 'professor', 'programs', 'prototype', 'research and development', 'research study', 'response', 'skills', 'software development', 'success', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R43,2007,101707,0.015617927890928845
"Gene Ontology Consortium    DESCRIPTION (provided by applicant): Our objective is to provide the scientific community with a consistent, robust information environment for describing, sharing, integrating and comparing the functional roles of genes, proteins and functional RNAs within and across all organisms. The Gene Ontology (GO) Consortium is an international collaboration of model organism database and genome annotation groups who have joined together to establish standards for describing genomes and gene products and to provide tools and support for the consistent application of these standards for functional annotations that facilitate and enable biological research. The GO provides specific classifications including well-defined, biologically descriptive terms that are organized into specialization and part-of hierarchies for the domains of genome feature, molecular function, biological process and cellular component. The GO classifications are independent of any particular technology, an uncoupling of terminology from technology that encourages application of these semantic standards by organism annotation groups that utilize a wide range of technical environments. The GO has been widely adopted and used for representation of complex biological information for model organism genomes, and is increasingly used for the functional annotation of emerging genomes. With the increased use of the GO, the Consortium must actively work to ensure both the accuracy of the ontologies as well as consistency and quality of annotations so that these resources may be reliably used to draw inferences and make biological predictions. We will do so by focusing on four key aims: 1) We will maintain logically rigorous and biologically precise ontologies; 2) We will ensure comprehensive annotation of reference genomes, including human, using the GO; 3) We will support GO annotation efforts for emerging genomes and for those specialized sets of genes and proteins of particular community interest; and 4) We will provide annotations and tools to the research community thus supporting experimental biologists, genome informaticists, and computational biologists who are using GO annotations in their research particularly in the areas of functional genomics and comparative biology. The relevance of this work for public health is that comprehensive integration and standardization of biomedical and genomics information is an essential component of advancing the understanding of the molecular systems underlying human health and disease outcomes.             n/a",Gene Ontology Consortium,7185305,P41HG002273,"['Adopted', 'Adoption', 'Animal Model', 'Area', 'Biological', 'Biological Process', 'Biology, Other', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Comparative Biology', 'Complex', 'Computer information processing', 'Data', 'Databases', 'Depth', 'Disease', 'Disease Outcome', 'EST Library', 'Ensure', 'Environment', 'Experimental Models', 'Functional RNA', 'Gene Proteins', 'Genes', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Information Retrieval', 'International', 'Knowledge', 'Literature', 'Methodology', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Organism', 'Proteins', 'Proteomics', 'Public Health', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Semantics', 'Standardization', 'Standards of Weights and Measures', 'Structure', 'System', 'Technology', 'Terminology', 'Translating', 'Work', 'base', 'biological research', 'comparative', 'functional genomics', 'genetic element', 'genome database', 'human disease', 'interest', 'model organisms databases', 'repository', 'size', 'tool']",NHGRI,JACKSON LABORATORY,P41,2007,3146180,0.04813082789440468
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,7120160,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2006,414036,0.037236140205056396
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6923756,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2005,395905,0.032357787809098364
"BioHDF - Open Binary File Standards for Bioinformatics DESCRIPTION (provided by applicant):  Geospiza Inc. and the National Center for Supercomputing Applications (NCSA) are creating a standards based software framework around NCSA's Heirarchical Data Format (HDF5). The envisioned framework will integrate algorithms important in DNA and protein sequence analysis to create scalable high throughput software systems which will be accessed using new graphical user interfaces (GUIs) to provide researchers with new views of their data to finish sequencing projects in large-scale genome sequencing, microbial genome sequencing, viral epidemiology, polymorphism detection, phylogenetic analysis, multi-locus sequence typing, confirmatory sequencing, and EST analysis.    In our vision, algorithms will be either integrated into the system to directly read and write from HDF5 project files, or they will communicate with project files via filter programs that produce standardized XML formatted data. Through this model, a scalable solution will support different applications of DNA sequencing, fulfilling the many needs and requirements expressed by the medical research community now and into the future. As the first step in this process we will, define requirements for editing and versioning data in DNA sequencing, research and propose data models for the computational phases of DNA sequencing and annotating DNA sequence data using existing standards, create a prototype application for DNA sequencing based SNP discovery, and engage the bioinformatics community for BioHDF adoption.       In the past ten years the cost of sequencing DNA has dropped over 1000 fold and the amount of raw sequence data, entering our national repositories is doubling every 12 months. DNA sequencing is fundamental to biological research activities such as genomics, systems biology, and clinical medicine. Proposals are being sought to decrease sequencing costs by two orders of magnitude through technology refinements with an ultimate vision of developing technology to sequence human genome equivalents for $1000 each. The amount of data that will be produced through these endeavors is unimaginable. However, the $1,000 genome will not advance medical research unless we integrate all phases of the DNA sequencing process and treat the creation, management, finishing, analysis, and sharing of the data as common goals. n/a",BioHDF - Open Binary File Standards for Bioinformatics,6992995,R41HG003792,"['DNA', 'artificial intelligence', 'bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'functional /structural genomics', 'genetic mapping', 'genetic polymorphism', 'mathematics', 'molecular biology information system', 'nucleic acid sequence', 'single nucleotide polymorphism', 'virus genetics']",NHGRI,"GEOSPIZA, INC.",R41,2005,142775,0.030953935230037453
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6952028,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2005,412000,0.037236140205056396
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6912979,R44HG002244,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data management', 'genotype', 'informatics', 'mathematical model', 'nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2004,191986,0.04619860236859574
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6777028,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,341671,0.032357787809098364
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6936159,R01GM061372,"['Internet', 'artificial intelligence', 'automated data processing', 'biological signal transduction', 'biomedical automation', 'computer system design /evaluation', 'functional /structural genomics', 'high throughput technology', 'intermolecular interaction', 'method development', 'molecular biology information system', 'statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2004,52940,0.032357787809098364
"Detecting relations among heterogeneous genomic datasets    DESCRIPTION (provided by applicant): During the past decade, the new focus on genomics has highlighted a particular challenge: to integrate the different views of the genome that are provided by various types of experimental data. The long-term objective of this work is to provide a coherent computational framework for integrating and drawing inferences from a collection of genome-wide measurements. Hence, the proposed research plan develops algorithms and computational tools for learning from heterogeneous data sets. We focus on the analysis of the yeast genome because so many genome-wide data sets are currently available; however, the tools we develop will be applicable to any genome. We approach this task using two recent trends from the field of machine learning: kernel algorithms that represent data via specialized similarity functions, and transductive algorithms that exploit the availability of unlabeled test data during the training phase of the algorithm. We apply focus on two tasks: (1) classifying groups of genes that are of interest to our collaborators, including components of the spindle pole body, cell cycle regulated genes, and genes involved in meiosis and sporulation, splicing, alcohol metabolism, etc., and (2) prediction of protein-protein interactions. These two specific aims are not only important scientific tasks, but also represent typical challenges that future genomic studies will face. Accomplishing these aims requires the integration of many heterogeneous sources of data, the prediction of multiple properties of genes and proteins, the explicit introduction of domain knowledge, the automatic introduction of knowledge from side information, scalability to large data sizes, and tolerance of large levels of noise.         n/a",Detecting relations among heterogeneous genomic datasets,6737944,R33HG003070,"['cell cycle', 'computer system design /evaluation', 'data collection', 'genome', 'informatics', 'meiosis', 'metabolism', 'protein protein interaction']",NHGRI,UNIVERSITY OF WASHINGTON,R33,2004,400000,0.037236140205056396
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6622259,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2003,560392,0.04619860236859574
"Development of Bioinformatic Tools for Virtual Cloning    DESCRIPTION (provided by applicant): The ability to delineate (at least in theory) all the proteins encoded in the human genome and all of those encoded by the genomes of major human parasites has given us an unprecedented opportunity to address the causes and treatment of every major human disease.  However, the vast increase in biological knowledge that has resulted from the last decade of genomic DNA sequencing has led us to a a crisis in bioinformatics.  This crisis is two-fold: analysis of data and planning of experiments.  Most of the scientific resources being expended in bioinformatics are being spent on data analysis tools. While these are essential, we should not neglect the opportunity to accelerate the progress of actual experimental biology.  All modern experimental molecular biology (and, increasingly, structural biology) depends upon the availability of plasmid clones to address specific scientific questions.  Although software facilitating DNA manipulations exists, few programs advise users of optimal strategies and none automate the process of clone generation. Genomics initiatives identify proteins at the genome level and demand the generation of hundreds of expression clones for recombinant protein production in exogenous hosts such as E. coli.  Establishment of libraries of expression clones requires automation and optimization as well as effective means of data storage, archiving, annotation and query.  To address these needs, as well as to facilitate routine DNA manipulations in virtually any molecular biology laboratory, we propose (1) to test and build a task centered virtual cloning expert system that serves as a knowledge base for DNA manipulations, and (2) to test and build an information automaton for the construction of expression clone libraries in support of structural genomics initiatives and other high throughput experiments.         n/a",Development of Bioinformatic Tools for Virtual Cloning,6583437,R43GM067279,"['artificial intelligence', ' biomedical automation', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' expression cloning', ' gene expression', ' genetic library', ' genetic manipulation', ' informatics', ' molecular biology information system', ' transfection /expression vector']",NIGMS,"VIRMATICS, LLC",R43,2003,100000,0.01883857774083531
"Computer Systems for Functional Analysis of Genomic Data DESCRIPTION (provided by applicant):    We propose computational approaches aiding automated compilation of molecular networks from research literature, cleansing of the resulting database, and assessing reliability of facts stored in the database.         n/a",Computer Systems for Functional Analysis of Genomic Data,6685421,R01GM061372,"['Internet', ' artificial intelligence', ' automated data processing', ' biological signal transduction', ' biomedical automation', ' computer system design /evaluation', ' functional /structural genomics', ' high throughput technology', ' intermolecular interaction', ' method development', ' molecular biology information system', ' statistics /biometry']",NIGMS,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2003,323936,0.032357787809098364
"Second Generation DNA Sequence Management Tools   DESCRIPTION (provided by applicant): The human genome project spurred the            development of high throughput technologies, especially in the area of DNA           sequencing. Not only has this effort produced a draft of the human genome, it's      catalyzed development of an entire industry based on DNA sequencing and              genomics. Since these technologies produce enormous amounts of data they depend      on bioinformatics programs for data management. Phrap, Cross_Match,                  RepeatMasker and Consed are four programs that played an integral role in the        human genome project and became accepted as standard. However, as the                technology for sequencing has evolved, so too, have the applications. These new      applications include sequencing additional genomes, EST cluster analysis, and        genotyping and they have highlighted the need to update standard bioinformatics      programs to meet the current needs of a broader community. In this project we        will re-engineer Phrap, Cross_Match and Repeat Masker to improve performance by      optimizing these algorithms and developing a hierarchical data file to store         and manipulate assembled sequence data. Phrap and Cross_Match will also be           modified to use XML-formatted data allowing users to apply constraints to            sequence assembly. Lastly, we will develop a new program to review, edit, and        manipulate sequences, thus giving users unprecedented control over their data.      PROPOSED COMMERCIAL APPLICATION:                                                                                     Phrap is widely used in industry and academia for applications involving DNA sequences.  There are over 100 commercial sites that would benefit from new versions of Phrap that support incremental assemblies and utilize computer resources better.  An API for Phrap will encourage application development creating additional commercialization possibilities for algorithm and application developers. n/a",Second Generation DNA Sequence Management Tools,6444292,R44HG002244,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' data management', ' genotype', ' informatics', ' mathematical model', ' nucleic acid sequence']",NHGRI,"GEOSPIZA, INC.",R44,2002,531259,0.04619860236859574
"Functional Genomics Software   DESCRIPTION (Applicant's abstract): A substantial commercial potential exists        for software tools that allow a biomedical research scientist to use genomic         data to form experimentally testable hypotheses. These will be used to exploit       genomic sequence data to understand the aetiology of disease, to improve             diagnostic tools, and to develop more effective therapies. The Master Catalog,       a commercial product developed jointly by EraGen Biosciences and the Benner          laboratory at the University of Florida, provides a convenient framework for         implementing heuristics that do this. The Master Catalog is a naturally              organized database that contains evolutionary trees, multiple sequence               alignments, and reconstructed evolutionary intermediates for all of the              proteins in the GenBank database. The Benner laboratory has developed and            anecdotally tested heuristics that date events in the molecular history,             provide evidence for and against functional recruitment within a protein             family, detect distant homologs, associate individual residues important for         functional changes with a crystal structure, find metabolic and regulatory           pathways, and correlate events in the molecular record with the history of life      on Earth. This Phase I proposal seeks to validate a set of these heuristics          more broadly to determine their suitability for database-wide application. In        Phase II, we will implement these within the Master Catalog, and launch a            commercial bioinformatics product to support functional analysis of genomic          databases.                                                                           PROPOSED COMMERCIAL APPLICATION:  In its present version, the Master Catalog is a successful commercial product within a  niche: ""best in class"" of bioinformatics databases.  Adding a validated set of heuristics  for extracting functional information from genome databases will make it the software  of choice for most functional genomics work, and be a central tool in the pharmaceutical/  biotechnology industries.  Academic versions and student versions will find markets in most  universities.                                                                                      n/a",Functional Genomics Software,6337786,R41HG002331,"['artificial intelligence', ' biochemical evolution', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' functional /structural genomics', ' informatics', ' molecular biology information system', ' nucleic acid sequence']",NHGRI,"ERAGEN BIOSCIENCES, INC.",R41,2001,96855,0.02055274087774893
"Deep learning for population genetics Project Summary The revolution in genome sequencing technologies over the past 15 years has created an explosion of population genomic data but has left in its wake a gap in our ability to make sense of data at this scale. In particular, whereas population genetics as a field has been traditionally data-limited, the massive volume of current sequencing means that previously unanswerable questions may now be within reach. To capitalize on this flood of information we need new methods and modes of analysis.  In the past 5 years the world of machine learning has been revolutionized by the rise of deep neural networks. These so-called deep learning methods offer incredible flexibility as well as astounding improvements in performance for a wide array of machine learning tasks, including computer vision, speech recognition, and natural language processing. This proposal aims to harness the great potential of deep learning for population genetic inference.  In recent years our group has made great strides in using supervised machine learning for population genomic analysis (reviewed in Schrider and Kern 2018). However, this work has focused primarily on using more traditional machine learning methods such as random forests. As we argue in this proposal, DNA sequence data are particularly well suited for modern deep learning techniques, and we demonstrate that the application of these methods can rapidly lead to state-of-the-art performance in very difficult population genetic tasks such as estimating rates of recombination. The power of these methods for handling genetic data stems in part from their ability to automatically learn to extract as much useful information as possible from an alignment of DNA sequences in order to solve the task at hand, rather than relying on one or more predefined summary statistics which are generally problem-specific and may omit information present in the raw data.  In this proposal we lay out a systematic approach for both empowering the field with these tools and understanding their shortcomings. In particular, we propose to design deep neural networks for solving population genetic problems, and incorporate successful networks into user-friendly software tools that will be shared with the community. We will also investigate a variety of methods for estimating the uncertainty of predictions produced by deep learning methods; this area is understudied in machine learning but of great importance to biological researchers who require an accurate measure of the degree of uncertainty surrounding an estimate. Finally, we will explore the impact of training data misspecification—wherein the data used to train a machine learning method differ systematically from the data to which it will be applied in practice. We will devise techniques to mitigate the impact of such misspecification in order to ensure that our tools will be robust to the complications inherent in analyzing real genomic data sets. Together, these advances have the potential to transform the methodological landscape of population genetic inference. Project Narrative Deep learning has revolutionized such disparate fields as computer vision, natural language processing, and speech recognition. In this proposal we aim to harness the great potential of deep learning for population genetic inference. We will design, implement, and apply novel deep learning methods and provide open source software for others to both use and build upon, thereby producing valuable tools for the genetics researchers at large.",Deep learning for population genetics,9976348,R01HG010774,"['Algorithms', 'Area', 'Biological', 'Biology', 'Classification', 'Code', 'Communities', 'Computer Vision Systems', 'Computer software', 'DNA Sequence', 'Data', 'Development', 'Ensure', 'Floods', 'Genetic', 'Genetic Recombination', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Image', 'Lead', 'Learning', 'Left', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Natural Language Processing', 'Natural Selections', 'Nature', 'Performance', 'Population', 'Population Explosions', 'Population Genetics', 'Process', 'Program Development', 'Publishing', 'Research Personnel', 'Sequence Alignment', 'Software Tools', 'Techniques', 'Technology', 'Training', 'Trees', 'Uncertainty', 'Ursidae Family', 'Work', 'base', 'computational chemistry', 'convolutional neural network', 'deep learning', 'deep neural network', 'design', 'empowered', 'flexibility', 'genetic information', 'genome sequencing', 'genomic data', 'infancy', 'innovation', 'learning classifier', 'learning strategy', 'machine learning algorithm', 'machine learning method', 'network architecture', 'neural network', 'neural network architecture', 'next generation', 'novel', 'open source', 'random forest', 'recurrent neural network', 'research and development', 'speech recognition', 'statistics', 'stem', 'success', 'supervised learning', 'tool', 'tool development', 'user friendly software']",NHGRI,UNIVERSITY OF OREGON,R01,2020,529154,0.011572845815203325
"Center for Critical Assessment of Genome Interpretation Genomic data hold the promise of revolutionizing our understanding and treatment of human disease. Multiple barriers stand between the acquisition of the data and realizing these and other benefits. Rapid accumulation of genomic data far exceeds our capacity to reliably interpret genomic variation. New developments in artificial intelligence and machine learning, combined with increased computing power and domain knowledge, provide hope for the deployment of enhanced computational tools in both basic research and clinical practice. Use of these methods critically depends upon reliable characterization of their performance.  The Center for Critical Assessment of Genome Interpretation (C-CAGI) will address these needs, through objective evaluation of the state of the art in relating human genetic variation and health. CAGI has had five editions since 2010 with 50 challenges posed to the community taken on by hundreds of predictors, leading to scores of publications about prediction methods and their assessment. We propose for C-CAGI to continue to advance the field of variant interpretation through the following Specific Aims: 1. Develop community experiments to evaluate the quality of computational methods for interpreting genomic variation data. C-CAGI will conduct community experiments in which participants make bona fide blinded predictions of disease related phenotypes on the basis of genomic data. We will engage a diverse predictor community to spur innovation. The CAGI Ethics Forum will vet studies to ensure that privacy and sharing maintain the highest standards and will educate the community. 2. Assess the quality of current computational methods for interpreting genomic variation data; highlight innovations and progress at interactive conferences. Predictions will be evaluated by independent assessors, who will be supported by new assessment approaches from C-CAGI. Results will be presented at CAGI experiment conferences with deep technical engagement, which will be interleaved with reflective CAGIâ meetings that create an environment for a comprehensive evaluation of the field, facilitating identification of major bottlenecks and problems faced by the current genome interpretation approaches. 3. Broadly disseminate the results and conclusions from the CAGI experiments and analysis. C-CAGI will outreach to the broader scientific and clinical community through its publications, and the creation of a calibrated reference integrated into the most common workflows for ready adoption. CAGI will also be represented at international meetings with presentations and workshops. 4. Operate effectively and responsively. C-CAGI will operate efficiently as it closely interacts with hundreds of participants. CAGI will build upon a robust information infrastructure that securely facilitates data dissemination, prediction submission, and assessment. Genomic variation is responsible for numerous rare diseases, for propensity for many common traits and diseases, for drug response, and is a key characteristic of cancer evolution. At present, our ability to characterize genetic differences far exceeds our capacity to interpret them either for basic research understanding or for clinical application. The Center for Critical Assessment of Genome Interpretation, operating on robust ethical foundations, will provide an evaluation of the current state of the art and help promote progress in understanding the impact of genomic variation.",Center for Critical Assessment of Genome Interpretation,9937546,U24HG007346,"['Address', 'Adoption', 'Affect', 'Amino Acid Sequence', 'Artificial Intelligence', 'Basic Science', 'Blinded', 'Characteristics', 'Clinical', 'Communities', 'Computing Methodologies', 'Copy Number Polymorphism', 'Data', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Ensure', 'Environment', 'Ethics', 'Evaluation', 'Evolution', 'Foundations', 'Genetic', 'Genetic Variation', 'Genome', 'Health', 'Human Genetics', 'Infrastructure', 'International', 'Knowledge', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Molecular', 'Nucleotides', 'Participant', 'Performance', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Privacy', 'Provider', 'Publications', 'RNA Splicing', 'Rare Diseases', 'Secure', 'Structure', 'Trust', 'Variant', 'Work', 'base', 'clinical application', 'clinical practice', 'computerized tools', 'data acquisition', 'data dissemination', 'exome', 'experimental study', 'genetic information', 'genetic variant', 'genomic data', 'genomic variation', 'high standard', 'human disease', 'innovation', 'meetings', 'multiple omics', 'operation', 'outreach', 'response', 'symposium', 'trait', 'whole genome']",NHGRI,UNIVERSITY OF CALIFORNIA BERKELEY,U24,2020,314933,0.057912069761788344
"Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders The purpose of this training and research application is to study the functional impact of mobile element insertions (MEIs) in neurological disorders (NDs) using new developments in deep learning techniques. MEIs are transposable DNA fragments that are able to insert throughout the human genome. There are at least 124 independent MEIs associated with human diseases. Approximately 20% of these diseases represent a spectrum of NDs, yet the overall contribute of MEIs to the etiology of NDs has not been systematically estimated. To address this, we will (1) characterize functional MEIs in GTEx cohorts in healthy individuals; (2) build a comprehensive functional map of MEIs to determine tissue-specific and brain-specific impact; and (3) impute transcriptional changes on various NDs where whole-genome sequencing (WGS) data will be generated. The proposed application will also develop an extensive research program for Dr. Dadi Gao, a computational biologist and statistical geneticist who has trained in functional genomic studies of alternative splicing in neurodegenerative disorders and therapeutic targeting of a splicing defect that causes a severe neurodevelopmental disorder. He has developed novel methods to investigate regulation of the transcriptome and to facilitate analyses in drug development. He now seeks to expand his expertise by applying statistical and deep learning models on large cohorts of sequencing data from controls and cases with NDs from post-mortem tissues, then impute functional consequences of MEIs from WGS in large-scale disease cohorts. The training plan consists of two years of mentored research to learn new skills in genome analysis, MEI characterization, and advanced deep learning techniques, followed by three years of shaping an independent laboratory. The research plan is developed to comprehensively explore functional variation in the genome by decomposing transcriptomic changes against MEIs. Dr. Michael Talkowski at Massachusetts General Hospital, Harvard, and the Broad Institute will serve as the primary mentor, while Dr. Manolis Kellis at MIT and the MIT Computational Biology Group, and the Broad Institute will serve as a co-mentor and close collaborator. These mentors are recognized experts in genomic structural variants, functional genomics, the genetics of neurological disorders, and computational modeling to establish functional elements in the human genome. In addition, a team of independent investigators from basic and translational research will provide Dr. Gao with comprehensive feedback to keep both his science and career development on track. The highly collaborative environment in CGM, MGH, Harvard Medical School, the Broad Institute and the University of Michigan Medical School will prepare Dr. Gao for his transition to an independent investigator. This outstanding mentorship team and training program will facilitate the career development of Dr. Gao as he seeks to redefine the functional maps of MEIs in the human genome and to impute their impact in large-scale neurological disorders. Mobile element insertions (MEIs) represent a largely undefined component of the genetic architecture of neurological disorders, as a number of MEIs have been associated with alternative splicing in these disorders but large-scale genome-wide functional characterization has not been systematically performed across tissues. This program study will functionally characterize the impact of MEIs on alternative splicing from whole-genome sequencing and transcriptome sequencing in large cohorts using new developments in deep learning models. These results will enhance our understanding of the etiological role and pathogenic mechanisms associated with MEIs in neuronal development and human neurological disorders.",Deep learning approaches to decipher the impact of mobile element insertion on alternative splicing in neurological disorders,10041366,K99NS118109,"['Address', 'Algorithms', 'Alternative Splicing', 'Alzheimer&apos', 's Disease', 'Autopsy', 'Basic Science', 'Biology', 'Blood', 'Brain', 'Brain Diseases', 'CRISPR/Cas technology', 'Cells', 'Chromosome Pairing', 'Cohort Studies', 'Computational Biology', 'Computer Analysis', 'Computer Models', 'DNA', 'DNA Insertion Elements', 'Data', 'Data Set', 'Defect', 'Detection', 'Development', 'Disease', 'Disease model', 'Dorsal', 'Dystonia', 'Elements', 'Etiology', 'Event', 'Evolution', 'Excision Repair', 'Familial Dysautonomia', 'Feedback', 'Fellowship', 'Filipino', 'General Hospitals', 'Generations', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Haplotypes', 'Human', 'Human Genome', 'Individual', 'Institutes', 'International', 'Introns', 'Laboratories', 'Lateral', 'Lead', 'Learning', 'Linear Regressions', 'Link', 'Machine Learning', 'Maps', 'Massachusetts', 'Measures', 'Mentors', 'Mentorship', 'Methods', 'Michigan', 'Mind', 'Minisatellite Repeats', 'Modeling', 'Molecular', 'Mosaicism', 'Neurodegenerative Disorders', 'Neurodevelopmental Disorder', 'Neuromuscular Diseases', 'Neurons', 'Outcome', 'Parkinsonian Disorders', 'Pathogenicity', 'Pattern', 'Peripheral', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Prefrontal Cortex', 'Process', 'Property', 'RNA Splicing', 'Regulation', 'Research', 'Research Personnel', 'Retroelements', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Shapes', 'Short Interspersed Nucleotide Elements', 'Source', 'Specificity', 'Structure', 'TAF1 gene', 'Techniques', 'Therapeutic Trials', 'Tissue-Specific Splicing', 'Tissues', 'Training', 'Training Programs', 'Transcription Alteration', 'Translational Research', 'Universities', 'Untranslated RNA', 'Variant', 'Work', 'brain tissue', 'career development', 'cohort', 'collaborative environment', 'convolutional neural network', 'deep learning', 'drug development', 'functional genomics', 'functional outcomes', 'gene function', 'genetic architecture', 'genome analysis', 'genome editing', 'genome sequencing', 'genome-wide', 'human disease', 'in silico', 'insight', 'medical schools', 'mind control', 'nervous system disorder', 'neuron development', 'novel', 'programs', 'response', 'skills', 'statistical learning', 'structural genomics', 'therapeutic target', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NINDS,MASSACHUSETTS GENERAL HOSPITAL,K99,2020,91226,0.02056644480266365
"A novel human T-cell platform to define biological effects of genome editing PROJECT SUMMARY Genome editing technologies have extraordinary potential as new genomic medicines that address underlying genetic causes of human disease; however, it remains challenging to predict their long-term safety, because we do not know the consequences of potential side effects of genome editing such as off-target mutations or immunogenicity. Our long-term goal is to understand and predict such unintended biological effects to advance the development of safe and effective therapies. T-cells are an ideal cellular model because: 1) they are highly relevant as the most widely used cells for development of therapeutic genome editing strategies (such as cell-based treatments for HIV and cancer) and 2) mature T-cells encode a diverse T-cell receptor repertoire that can be exploited as built-in cellular barcodes for quantifying clonal expansion or depletion in response to specific treatments. We, therefore, propose the following specific aims: 1) to predict which unintended editing sites have biological effects on human T-cells by integrating large-scale genome-wide activity and epigenomic profiles with state-of-the-art deep learning models and 2) to develop a human primary T-cell platform to detect functional effects of genome editing by measuring clonal representation, off-target mutation frequencies, immunogenicity, or gene expression. If successful, our experimental and predictive framework will profoundly increase confidence in the safety of the next generation of promising genome editing therapies. PROJECT NARRATIVE Genome editing technologies have extraordinary potential as the basis of new genomic medicines that address the underlying genetic causes of human disease; however, it is challenging to predict their long-term safety, because we do not know the consequences of potential unintended side effects of genome editing such as off-target mutations or immunogenicity. To define the biological effects of genome editing strategies, we will develop a human primary T-cell platform to sensitively detect functional effects coupled with an empirically-trained artificial intelligence models to predict them. Together, our platform will significantly improve confidence in safety assessments of promising genome editing therapeutics.",A novel human T-cell platform to define biological effects of genome editing,10016298,U01AI157189,"['Address', 'Advanced Development', 'Adverse effects', 'Affect', 'Artificial Intelligence', 'Bar Codes', 'Benign', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Cell model', 'Cell physiology', 'Cells', 'Chromatin', 'Clonal Expansion', 'Complex', 'Coupled', 'DNA Methylation', 'Detection', 'Engineering', 'Epitopes', 'Frequencies', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Transcription', 'Genetic Variation', 'Genomic medicine', 'Genomics', 'Goals', 'HIV', 'Human', 'Human Genetics', 'Human Genome', 'Immunologic Deficiency Syndromes', 'In Vitro', 'Inherited', 'Malignant Neoplasms', 'Maps', 'Mature T-Lymphocyte', 'Measures', 'Methods', 'Modeling', 'Mutation', 'Oncogenic', 'Organizational Change', 'Outcome', 'Peptide Library', 'Peripheral Blood Mononuclear Cell', 'Phenotype', 'Population', 'Proto-Oncogenes', 'Regulatory Element', 'Retroviral Vector', 'Ribonucleoproteins', 'Safety', 'Site', 'Site-Directed Mutagenesis', 'Standardization', 'Streptococcus pyogenes', 'T cell response', 'T-Cell Proliferation', 'T-Cell Receptor', 'T-Lymphocyte', 'T-cell receptor repertoire', 'Technology', 'Testing', 'Therapeutic', 'Training', 'Variant', 'adaptive immune response', 'adverse outcome', 'base', 'comparative genomics', 'cytokine', 'deep learning', 'effective therapy', 'epigenomics', 'functional genomics', 'gene therapy', 'genome editing', 'genome-wide', 'genotoxicity', 'histone modification', 'human disease', 'immunogenic', 'immunogenicity', 'improved', 'in vivo', 'machine learning method', 'next generation', 'novel', 'novel therapeutics', 'off-target mutation', 'off-target site', 'response', 'safety assessment', 'safety testing', 'side effect', 'therapeutic development', 'therapeutic gene', 'therapeutic genome editing', 'transcriptome sequencing']",NIAID,ST. JUDE CHILDREN'S RESEARCH HOSPITAL,U01,2020,610710,0.02069136529176344
"Scalable detection and interpretation of structural variation in human genomes PROJECT SUMMARY Structural variation (SV), is a diverse class of genome variation that includes copy number variants (CNVs) such as deletions and duplications, as well as balanced rearrangements, such as inversions and reciprocal translocations. A typical human genome harbors >4,000 SVs larger than 300bp and their large size increases the potential to delete or duplicate genes, disrupt chromatin structure, and alter expression. Despite their prevalence and potential for phenotypic consequence, SVs remain notoriously difficult to detect and genotype with high accuracy. Much of this difficulty is driven by the fact DNA sequence alignment “signals” indicating SVs are far more complex than for single-nucleotide and insertion deletion variants. Unlike SNP alignments that vary only in allele state, alignments supporting SVs vary in state (supports an alternate structure or not) alignment location, and type. Consequently, the accuracy of SV discovery is much lower than that of SNPs and INDELs. Furthermore, SV pipelines scale poorly and are difficult to run. These challenges are a barrier for single genome analysis and studies of families must invest substantial effort into eliminating a sea of false positives. These problems become exponentially more acute for large-scale sequencing efforts such as TOPmed, the Centers for Common Disease Genetics, and the All of Us program. Software efficiency is key to scalability for such projects. However, of equal importance is comprehensive, accurate discovery.  Building upon more than a decade of software development experience and analyzing SV in diverse disease contexts, we have invested significant effort into understanding the causes of the insufficient accuracy for SV discovery. These efforts, together with our research and development experience in this area, give us unique insight into improving the accuracy and scalability of SV discovery. Our goal is to narrow the accuracy gap between SNP/INDEL variation and structural variation discovery. These developments will empower studies of human genomes in diverse contexts and will therefore have broad impact. Our goals are to: 1. Develop a deep learning model to correct systematic variation in sequence depth. This new machine  learning model will correct systematic biases in DNA sequence depth and dramatically improve the  discovery of deletions and duplications. 2. Improve the speed, scalability, and accuracy of SV detection and genotyping. Using new algorithms,  we will bring the accuracy of SV detection much closer to that of SNP and INDEL discovery and allow  accurate SV discovery to be deployed at scale. 3. Create a map of genomic constraint for SV from population-scale genome analysis. We will deploy  our new methods to detect and genotype structural variation among tens of thousands of human genomes.  The resulting SV map will empower the creation of a model of genomic constraint for SV and enable new  software to predict deleterious SVs, especially in the noncoding genome. PROJECT NARRATIVE Single-nucleotide DNA changes paint an incomplete picture of a human’s genome. A more complete picture must include a genome's structural variation (SV), an important class of genome variation that includes copy number variants (CNVs) such as deletions and duplications. However, existing methods have poor accuracy. As the genetics community transitions to large-scale genome sequencing studies, there is an acute need for improved SV discovery methods. This proposal introduces a series of algorithmic and software innovations that will empower SV discovery, genotyping, and interpretation in large-scale human disease studies.",Scalable detection and interpretation of structural variation in human genomes,9973582,R01HG010757,"['Acute', 'Affect', 'Algorithmic Software', 'Algorithms', 'All of Us Research Program', 'Alleles', 'Area', 'Automobile Driving', 'Biological Assay', 'Chromatin Structure', 'Chromosome Structures', 'Clip', 'Cloud Computing', 'Code', 'Communities', 'Complex', 'Computer software', 'Copy Number Polymorphism', 'DNA', 'DNA Sequence', 'Data', 'Data Reporting', 'Detection', 'Development', 'Disease', 'Environment', 'Error Sources', 'Exhibits', 'Family Study', 'Funding', 'Future', 'Gene Duplication', 'Gene Expression', 'Gene Fusion', 'Gene Structure', 'Genetic', 'Genetic Diseases', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Human', 'Human Genome', 'Individual', 'Laboratories', 'Large-Scale Sequencing', 'Location', 'Machine Learning', 'Maps', 'Methods', 'Modeling', 'Noise', 'Nucleotides', 'Paint', 'Pathogenicity', 'Performance', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Process', 'Reciprocal Translocation', 'Research', 'Running', 'Sampling', 'Sea', 'Sensitivity and Specificity', 'Sequence Alignment', 'Series', 'Signal Transduction', 'Software Tools', 'Source', 'Speed', 'Structure', 'Systematic Bias', 'Techniques', 'Technology', 'Training', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'algorithm development', 'base', 'convolutional neural network', 'deep learning', 'developmental disease', 'dosage', 'exome', 'experience', 'genome analysis', 'genome sequencing', 'genome-wide', 'human disease', 'improved', 'innovation', 'insertion/deletion mutation', 'insight', 'large datasets', 'method development', 'nanopore', 'novel', 'prevent', 'research and development', 'software development', 'success', 'tool', 'variant detection', 'whole genome']",NHGRI,UNIVERSITY OF UTAH,R01,2020,692048,0.0577676229069239
"Inferring selection from human population genomic data Project Summary/Abstract Identifying genomic regions responsible for recent adaptation is a major challenge in population genetics. Particularly in humans, the task of confidently detecting the action of recent adaptive natural selection (or positive selection) has proved troublesome. Indeed there is considerable controversy over whether recent positive selection has a substantial impact on human genetic variation. The work proposed here will address this problem by creating a more complete map of positive selection across many human populations, identifying selection on de novo mutations as well as selection on previously standing variation.  Specifically, the proposed research seeks to construct a scan for positives election that is more robust and accurate than any currently existing methods (Aim 1). This tool will utilize supervised machine learning techniques allowing it combine information from a number of existing tests for natural selection, and will be tested extensively on a large suite of population genetic simulations presenting a wide range of potentially confounding scenarios. This tool will then be released to the public. Next, it will be applied to 26 human populations in which a large sample of genomes have been sequenced by the 1000 Genomes Project (Aim 2), revealing similarities and differences in the tempo, mode, and targets of adaptive evolution across human populations. Finally, because selection on both beneficial and deleterious mutations skews genetic variation, our method will be used to identify regions of the genome least affected by natural selection, which will in turn be used to produce more accurate inferences of human demographic histories (Aim 3).  The mentored phase of this work will be performed within the Department of Genetics at Rutgers University. This is an intellectually stimulating environment with numerous journal clubs, an excellent seminar series, and several other research groups using computational techniques. The project will be performed under the stewardship of Dr. Andrew Kern, from whom the candidate will also receive training in machine learning and population genetics. Dr. Schrider will also receive training in population genetics and guidance from Dr. Jody Hey (Co-mentor) at nearby Temple University. This training will help Dr. Schrider acquire skills that will aid not only in the completion of the proposed work but also his transition to principle investigator of an internationally recognized independent research program studying the evolutionary forces driving patterns of human genetic variation. Project Narrative Detecting genes underpinning recent human adaptation remains a major challenge, and such genes are often associated with human disease. The work proposed here seeks to use supervised machine learning techniques to detect genomic regions responsible for recent adaptation across 26 different human populations. This work will also clarify human population size and migration histories, information that has implications for the prevalence of disease-causing mutations and efforts to identify them.",Inferring selection from human population genomic data,9868315,R00HG008696,"['Address', 'Affect', 'Africa South of the Sahara', 'Computational Technique', 'Data', 'Environment', 'Evolution', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genome', 'Genomic Segment', 'Genomics', 'Goals', 'Homo sapiens', 'Human', 'Human Genetics', 'Human Genome', 'International', 'Journals', 'Link', 'Machine Learning', 'Maps', 'Mentors', 'Methods', 'Mutation', 'Natural Selections', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Population Genetics', 'Population Sizes', 'Prevalence', 'Recording of previous events', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Series', 'Site', 'Techniques', 'Testing', 'Training', 'Universities', 'Variant', 'Work', 'base', 'de novo mutation', 'disease-causing mutation', 'driving force', 'fitness', 'genomic data', 'human disease', 'human population genetics', 'machine learning method', 'population migration', 'pressure', 'programs', 'sample fixation', 'simulation', 'skills', 'statistics', 'supervised learning', 'tool']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,R00,2020,237600,0.00864324943971247
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,0.056249219120396
"Conceptualizing Actionability in Clinical Genomic Screening Project Summary/Abstract. Clinical genomic sequencing (CGS) produces large amounts of data, much of which is hard to characterize or may have a negligible influence on health. The concept of actionability is commonly used to help separate information that may be useful from information that is likely irrelevant for patients. Actionability directs attention to whether genomic information warrants action and reflects its initial development as a strategy to augment diagnosis and treatment in sick patients. As CGS expands towards healthy populations in primary care settings, actionability is still widely embraced despite little consensus regarding its definition and use. Because this ambiguity could become an obstacle to the successful implementation of clinical genomic sequencing in healthy populations, greater clarity about this concept is necessary. The proposed research will fulfill this need by characterizing the emergence and varied meanings of actionability in clinical genomics, focusing on clinical genomics' transition into primary care settings. By identifying underlying values and assumptions related to actionability, this research will push beyond definitional disputes and provide a deeper framework for assessing how genetic information is valued. The specific aims are: 1. Identify and characterize, through in-depth interviews, how genomics experts and primary care providers conceptualize what makes genomic information actionable for healthy populations. 2. Identify and characterize, through a natural language processing (NLP) analysis of published literature, how the concept of actionability emerged, spread, and is used throughout clinical genomics. 3. Convene a workshop with genomics experts, primary care providers, and ELSI scholars to produce a white paper on actionability and the ethical, effective integration of CGS into primary care, guided by the results from Aims 1 and 2. This K99/R00 Pathway to Independence Award includes a highly-structured, mentored training program that will support the candidate's goal to become an independent, mixed-methods ELSI investigator focused on assessing the value of genomic information. To achieve this career goal, the candidate will: 1. Receive training in genetic and genomic science to facilitate collaboration with genomics care teams and make scientifically accurate policy recommendations 2. Build new methodological skills in biomedical informatics and natural language processing to conduct generalizable research 3. Publish and engage with scientific and medical audiences to have a more direct impact on future guidelines and policies. 4. Develop a collaborative and interdisciplinary research network. This training will include coursework, guided readings, network building, and sustained mentorship by a highly-qualified team of faculty with expertise in ELSI research, bioethics, clinical genomics, biomedical informatics, and the history and sociology of medicine. This training will prepare the candidate to transition to an independent ELSI investigator focused on ethical issues related to the actionability of genomic health information – an ELSI research priority in Genetic and Genomic Healthcare. Project Narrative. This K99/R00 Pathway to Independence Award will prepare the candidate to become an independent, mixed-methods ELSI researcher pursing a research program on ethical issues related to the actionability of genomic information. The study examines the values and assumptions underlying conceptualizations of the actionability of genomic information for healthy populations. Results of the study will contribute to the ethical and effective implementation of genomic sequencing into care for healthy populations.",Conceptualizing Actionability in Clinical Genomic Screening,10054993,K99HG010905,"['American', 'Award', 'Bioethics', 'Caring', 'Clinical', 'Collaborations', 'Consensus', 'Data', 'Development', 'Diagnosis', 'Disease', 'Disputes', 'Educational workshop', 'Ethical Issues', 'Ethics', 'Faculty', 'Future', 'Genetic', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Healthcare', 'Individual', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Laboratories', 'Level of Evidence', 'Literature', 'Medical', 'Medical Genetics', 'Medical Sociology', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Natural Language Processing', 'Outcome', 'Paper', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Penetrance', 'Policies', 'Population', 'Positioning Attribute', 'Primary Health Care', 'Provider', 'Publishing', 'Qualitative Methods', 'Reading', 'Recommendation', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Priority', 'Risk', 'Science', 'Severities', 'Structure', 'Surveys', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Variant', 'biomedical informatics', 'care providers', 'career', 'clinical implementation', 'directed attention', 'genetic information', 'genome sciences', 'health management', 'innovation', 'lifestyle intervention', 'medical schools', 'prevent', 'primary care setting', 'programs', 'screening', 'skills']",NHGRI,UNIVERSITY OF PENNSYLVANIA,K99,2020,103353,0.07569283344450095
"Identification of Transposable Element Insertions in the Kids First Data Project Summary Insertion of transposable elements (TEs, sometimes referred to as “jumping genes”) into the human genome can be pathogenic. Our aim in this project is to use sophisticated computational approaches to characterize TE insertions in the whole-genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program and identify any insertional mutations that may disrupt gene function. The large scale of the Kids First program provides an unprecedented opportunity to investigate the role of TE insertions in childhood cancers and structural birth defects, as well as to create a resource of reference TE maps that will be important for all other TE studies. We will first modify our existing algorithm called xTEA for the trio design of the Kids First studies and increase the accuracy and efficiency of the algorithm. Then, we will apply it to the thousands of trios that have been profiled in the Kids First program, using a pipeline optimized for the cloud environment. The resulting set of TE insertions (especially L1, Alu, SVA, and HERV insertions) will be curated with all relevant features and be made into a database for the community. We will also apply machine learning methods to improve the calls once a sufficient amount of training data have been obtained. To investigate the potential pathogenicity of the mutation, we will first focus on insertions within genes, but we will also explore those in regulatory elements inferred from epigenetic profiling data. PROJECT NARRATIVE Transposable elements, or “jumping genes”, are genetic elements that can alter the DNA of an individual. We aim to utilize a computational method to identify such elements in the genome sequencing data generated in the Gabriella Miller Kids First Pediatric Research Program. Our analysis will identify transposable elements that may be causal for a disease phenotype.",Identification of Transposable Element Insertions in the Kids First Data,9957262,R03CA249364,"['Algorithms', 'Communities', 'Computer software', 'Computing Methodologies', 'DNA', 'DNA Insertion Elements', 'DNA Transposable Elements', 'Data', 'Data Set', 'Databases', 'Disease', 'Elements', 'Endogenous Retroviruses', 'Environment', 'Gene Frequency', 'Genes', 'Genetic Diseases', 'Genome', 'Genomics', 'Genotype', 'Human Genome', 'Individual', 'Inherited', 'Insertion Mutation', 'Jumping Genes', 'Length', 'Location', 'Machine Learning', 'Malignant Childhood Neoplasm', 'Malignant Neoplasms', 'Maps', 'Mendelian disorder', 'Methods', 'Modeling', 'Mutation', 'Neurons', 'Output', 'Parents', 'Paste substance', 'Pathogenicity', 'Pediatric Research', 'Play', 'Population', 'Regulatory Element', 'Reporting', 'Resources', 'Retrotransposon', 'Role', 'Sampling', 'Sensitivity and Specificity', 'Single Nucleotide Polymorphism', 'Site', 'Source', 'Speed', 'Structural Congenital Anomalies', 'Training', 'base', 'cloud based', 'cohort', 'design', 'disease phenotype', 'epigenetic profiling', 'gene function', 'genetic element', 'genome sequencing', 'improved', 'machine learning method', 'proband', 'programs', 'transcriptome sequencing', 'whole genome']",NCI,HARVARD MEDICAL SCHOOL,R03,2020,169041,-0.017232293916301523
"Advancing evolutionary genetic inference in humans and other taxa Project Summary/Abstract Background: A major challenge in evolutionary genomics is to characterize the forces shaping present-day patterns of genetic variation. For instance, the extent and manner in which natural selection affects genetic diversity remains highly controversial. Researchers have largely addressed this problem by developing statistical tests or summaries of genome sequence variation that provide insights into the evolutionary forces at play. However, because such approaches typically rely on a single univariate summary of the data, valuable discriminatory information present in the original dataset is lost. A more fruitful strategy would thus be to use multidimensional summaries of genomic data (e.g. a large vector of summary statistics) or even the totality of the input data (e.g. a matrix-representation of a sequence alignment) to make more accurate inferences. An even more powerful approach is to utilize data sets in which the same population is sampled at multiple time points, allowing one to observe evolutionary dynamics in action. Although such genomic time-series data are becoming more prevalent, the development of appropriate computational methodologies has lagged behind the proliferation of such data. Proposal: The Schrider Lab seeks to develop and apply powerful machine learning methods for evolutionary inference. Our work over the next five years will yield powerful software tools leveraging novel representations of genomic datasets, including time-series data. These efforts will dramatically improve researchers' ability to make accurate evolutionary inferences from both population genomic and phylogenetic data. Indeed, preliminary results demonstrate that our methods vastly outperform current approaches in evolutionary genetics. More importantly, we will use these tools to answer pressing evolutionary questions. In particular, our use of time-series data will reveal loci responsible for recent adaptation with much greater confidence than currently possible. Our efforts will help to resolve the controversy over the role of adaptation in shaping patterns of diversity across the human genome. This research has important implications for public health as well, as genes underlying recent adaptations are enriched for disease-associations. Moreover, we are constructing a time-series dataset in the mosquito vector species Aedes aegypti and Aedes albopictus. We will interrogate these data for evidence of recent and ongoing adaptation—this work will reveal loci responsible for the evolution of resistance to insecticides and other control efforts. Encouraging preliminary data also suggest that our work in phylogenetics will substantially improve inferential power in this important research area. More broadly, the success of the novel approaches described in this proposal has the potential to transform the methodological landscape of evolutionary genomic data analysis. Project Narrative The work proposed here seeks to develop and apply powerful machine-learning based software tools for evolutionary genetic inference in humans, mosquito vectors, and other species. Such efforts have important health implications, as they can identify genes involved in adaptation, which in humans are often associated with disease and in mosquitos are often associated with resistance to insecticides and other control efforts.",Advancing evolutionary genetic inference in humans and other taxa,10028474,R35GM138286,"['Address', 'Aedes', 'Affect', 'Area', 'Computing Methodologies', 'Culicidae', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Health', 'Human', 'Human Genome', 'Insecticides', 'Machine Learning', 'Methodology', 'Methods', 'Natural Selections', 'Pattern', 'Phylogenetic Analysis', 'Play', 'Population', 'Public Health', 'Research', 'Research Personnel', 'Resistance', 'Role', 'Sampling', 'Sequence Alignment', 'Series', 'Shapes', 'Software Tools', 'Testing', 'Time', 'Variant', 'Work', 'base', 'genomic data', 'improved', 'insight', 'machine learning method', 'novel', 'novel strategies', 'statistics', 'success', 'time use', 'tool', 'vector', 'vector mosquito']",NIGMS,UNIV OF NORTH CAROLINA CHAPEL HILL,R35,2020,382894,0.04313270687791916
"Scalable tools to effectively translate genomic discoveries into the clinic PROJECT SUMMARY We are in the midst of a genomic revolution; more than 250,000 human genomes have been sequenced, generating over a petabase of genomic data. While these new data hold great promise to impact health, there is a disconnect between genomic discovery and clinical care. Providers frequently misinterpret genomic information, patients often don't understand their own test results, and genomic information about disease risk is infrequently shared between patients and family members. Importantly, ineffective communication and data misinterpretation has devastating consequences- including unnecessary organ removal, missed disease prevention opportunities, and premature death. We are addressing these genomic care gaps by developing and testing tools that optimize the integration of whole-exome and whole-genome sequencing (WES, WGS) for general clinical practice. My vision for improving genomic medicine is based on my work within multidisciplinary consortia and addresses the National Human Genome Research Institute's priority research area of improving the effectiveness of healthcare. In the proposed work we will test the effectiveness of a multilevel genomic e-Health intervention in cancer (Aim 1). Our intervention 1) educates physicians and patients about genomics, 2) enables direct-to-patient return-of- results, 3) provides physicians with patient-specific results and resources for interpretation, and 4) facilitates sharing of genomic results within families. We hypothesize that intervention use will result in higher rates of uptake of high-quality, genetically guided care. We will test our hypothesis in a randomized controlled trial among academic and community physicians who use WES for their patients. Next, we will use an iterative process, with stakeholder engagement, to adapt and pilot test our tool for Spanish and Mandarin speaking patients and for patients who have diabetes (Aim 2). Finally, we will create and assess new, moderated, social networks as a platform for genomic information sharing (Aim 3). Our hypothesis is that providers, patients and family members will engage with the genomic information sharing social networks and find them to be highly useful. Our general approach includes 1) creating the secure social networks, 2) integrating the networks into our e-Health intervention, and 3) using complementary methods, such as interviews and natural language processing, to assess stakeholders' network-related attitudes and network information quality. If successful, we will be well positioned to widely disseminate our e-Health tools. In sum, this work stands to transform how people obtain, process and share genomic information in the context of clinical care. Our tools reconceive genetic communication to allow for multi-directional flow of information, connects multiple stakeholders with one another, and integrates high-quality dynamic web-based resources to improve genomic care. In creating and deploying tools that both respond to and leverage the complexities of our information environment, we intend to transform genomic research and clinical practice. PROJECT NARRATIVE/ RELEVANCE OF PROJECT TO RESEARCH AND PUBLIC HEALTH Widespread utilization of genomic sequencing in medicine creates an urgent need to educate providers and patients. Currently, providers frequently misinterpret genomic information and patients often don't understand their own test results. In order to address this critical need, we propose to design and test multiple e-Health communication tools that will help providers and patients to better understand genomic data, lead to higher quality patient care, and facilitate genomic information sharing within families.",Scalable tools to effectively translate genomic discoveries into the clinic,10003373,R35HG010721,"['Address', 'Area', 'Attitude', 'Caring', 'Cessation of life', 'Clinic', 'Communication', 'Communication Tools', 'Community Physician', 'Data', 'Diabetes Mellitus', 'Effectiveness', 'Environment', 'Excision', 'Family', 'Family member', 'Genetic', 'Genome', 'Genomic medicine', 'Genomics', 'Health', 'Healthcare', 'Human Genome', 'Information Networks', 'Intervention', 'Interview', 'Lead', 'Malignant Neoplasms', 'Medicine', 'Methods', 'National Human Genome Research Institute', 'Natural Language Processing', 'Organ', 'Patient Care', 'Patients', 'Physicians', 'Positioning Attribute', 'Process', 'Provider', 'Public Health', 'Randomized Controlled Trials', 'Research', 'Research Priority', 'Resources', 'Secure', 'Social Network', 'Sum', 'Test Result', 'Testing', 'Translating', 'Vision', 'Work', 'base', 'clinical care', 'clinical practice', 'design', 'disorder prevention', 'disorder risk', 'eHealth', 'effectiveness testing', 'exome', 'genome sequencing', 'genomic data', 'genomic platform', 'improved', 'multidisciplinary', 'online resource', 'premature', 'tool', 'uptake', 'whole genome']",NHGRI,BECKMAN RESEARCH INSTITUTE/CITY OF HOPE,R35,2020,540906,0.10166019455093214
"Clinical Research Education in Genome Science (CREiGS) Project Summary/Abstract  The sensitivity and availability of omic technologies have enabled the genomic, transcriptomic and proteomic characterization of disease phenotypes, at the tissue and even the single cell level. This has allowed development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. Patients of providers who have participated in these educational initiatives also benefit as it allows for more rapid integration of genomic study findings into the clinical care setting. Thus, in response to PAR-19-185, we propose to develop and implement the Clinical Research Education in Genome Science (CREiGS) program that will not only focus on the analysis of genomic data, but also on gene-expression data, the integration of these two data types, as well as introductory theory and application of statistical and machine learning methods. Specifically we propose to accomplish the following specific aims: 1. Develop and successfully implement the online and in-person phases of CREiGS to increase the methodologic ingenuity by which researchers tackle important genomics-related clinical problems. 2. Establish a Diversity Recruitment External Advisory Board to ensure that the most effective strategies are employed to recruit URM doctoral students, postdoctoral fellows, and faculty from academic institutions nationwide into CREiGS. 3. Enhance the dissemination phase of CREiGS by packaging and uploading the asynchronous lectures and the online critical thinking/problem solving assessments with solutions for publicly available, online teaching resources. 4. Implement effective methods to evaluate the efficacy of CREiGS by examining:1) the participants' grasp of the CREiGS core competencies, 2) the clarity and quality of the curriculum, 3) program logistics and operation, and 4) the participants' short-term and long-term success attributed to participation in CREiGS. In summary, we posit that CREiGS will provide participants with a solid foundation in genomics science to answer complex, clinical questions. We believe that CREiGS supports the mission of the NHGRI by providing researchers with rigorous training to “accelerate medical breakthroughs that improve human health.” Project Narrative The sensitivity and availability of omic technologies have allowed for the development of treatments that target specific disease subtypes, most notably in cancer treatment, and thus opened up opportunities for the development of precision/personalized medicine strategies for optimizing treatments for individual patients. Thus, new genomic science educational initiatives need to be continually updated to educate the clinical and translational workforce on how to effectively interpret and apply the findings from genomics studies. The overall goal of the Clinical Research Education in Genome Science program is to increase the methodologic ingenuity of students, postdoctoral fellows, and faculty from academic institutions nationwide through a solid foundation in genomics science to answer complex, clinical research questions and improve patient care.",Clinical Research Education in Genome Science (CREiGS),9934567,R25HG011021,"['Area', 'Biomedical Research', 'Cells', 'Clinical', 'Clinical Data', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Critical Thinking', 'Data', 'Data Analyses', 'Development', 'Educational Curriculum', 'Educational process of instructing', 'Ensure', 'Exercise', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Hour', 'Human', 'Hybrids', 'Institution', 'Knowledge', 'Logistics', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Human Genome Research Institute', 'Outcome', 'Participant', 'Patient Care', 'Patients', 'Persons', 'Phase', 'Phenotype', 'Play', 'Postdoctoral Fellow', 'Problem Solving', 'Proteomics', 'Provider', 'Recruitment Activity', 'Reproducibility', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Role', 'Single Nucleotide Polymorphism', 'Solid', 'Statistical Methods', 'Students', 'Technology', 'Tissues', 'Training', 'Translational Research', 'Treatment outcome', 'Underrepresented Minority', 'Underserved Population', 'Update', 'cancer therapy', 'clinical care', 'clinical efficacy', 'computerized tools', 'data integration', 'data management', 'disease phenotype', 'disorder subtype', 'doctoral student', 'education research', 'genetic analysis', 'genome sciences', 'genomic data', 'grasp', 'health disparity', 'improved', 'individual patient', 'innovation', 'lectures', 'machine learning method', 'operation', 'personalized medicine', 'precision medicine', 'programs', 'recruit', 'response', 'statistical and machine learning', 'success', 'theories', 'therapy development', 'tool', 'transcriptomics', 'treatment optimization', 'virtual']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R25,2020,161662,0.018499319485537298
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,10124880,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,50000,0.023158671956817763
"Center for Undiagnosed Diseases at Stanford Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",Center for Undiagnosed Diseases at Stanford,9980967,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,1100000,0.023158671956817763
"What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives Abstract The Undiagnosed Diseases Network (UDN) has increased access for patients with undiagnosed diseases to the nation’s leading clinicians and scientists. Phase II of the Network will facilitate the transition of UDN efforts toward sustainability, through the expansion of clinical sites, refinement of methods, and integration with regular clinical practice. Here, we propose a program of study that will (1) facilitate timely, accurate diagnosis of patients with undiagnosed diseases; (2) improve diagnostic rates through novel approaches to data analysis and integration; and (3) explore underlying mechanisms of disease to accelerate therapeutic drug discovery. In Aim 1, we propose to evaluate patients referred to the UDN through a protocol that includes pre-visit chart review and genetic counseling followed by an individualized visit during which standardized phenotypic and environmental data are collected. Biosamples facilitate genomic, multi-omic, and cellular evaluation of disease. Expansion of fibroblasts and, in selected cases, generation of induced Pluripotent Stem Cell (iPSC) lines facilitates scientific investigation of the underlying diseases. We will expand our program of patient outreach, particularly to under-served populations. We will extend our UDN-based genomic medicine educational program both in scope and by broadening its eligibility. In Aim 2, we propose to develop and implement novel methods in areas of high potential to increase diagnostic yield. This includes algorithms for the detection of small genomic insertions and deletions as well as large scale structural variation. We will develop alignment algorithms using graph reference genomes and promote the use of long-read sequencing technologies. We will apply machine learning to the systematic integration of RNA sequencing, metabolomic, and phenotypic data with the electronic medical record and the entire medical literature to improve diagnostic yield. In Aim 3, we propose to facilitate diagnosis through enhanced cellular and model organisms phenotyping. We will implement immunomic and metagenomic approaches such as T cell, B cell and unknown organism sequencing for undiagnosed cases. We will utilize methods for moderate- and high-throughput phenotyping of iPS-derived cells and promote novel drug discovery via high throughput drug screening both with FDA- approved drugs and large scale small molecule libraries. Beyond Phase II, Stanford Medicine has made a strong commitment to the continuation of the Center for Undiagnosed Diseases at Stanford through a multi- million dollar institutional commitment. In summary, we aim to build on the success of Phase I of the UDN by streamlining processes, maximizing collaboration and outreach, optimizing computational algorithms, extending scientific investigation towards therapeutic discovery, and promoting engagement of hospital leaders, clinicians, scientists, policy-makers, and philanthropists to ensure this national resource is sustained long beyond the duration of this award. Narrative We will refine the operations of the Center for Undiagnosed Diseases at Stanford in coordination with other Phase II sites of the Undiagnosed Diseases Network to diagnose the undiagnosed and facilitate a transition to sustainability. Our Center will bring Stanford’s long history in technology development, genomic data analysis, stem cell biology, and translational science to the team-based diagnosis and care of patients with undiagnosed disease. We will refine existing procedures to further optimize the diagnostic process and integrate care of the undiagnosed into clinical practice while preserving the scientific mission of the Undiagnosed Diseases Network.",What comes next? Engaging stakeholders in governance of participant data and relationships during the sunset of large genomic medicine research initiatives,10162151,U01HG010218,"['Algorithms', 'Animal Model', 'Area', 'Award', 'B-Lymphocytes', 'Biological Assay', 'Caring', 'Cell Line', 'Cell model', 'Cells', 'Child Health', 'Collaborations', 'Committee Membership', 'Computational algorithm', 'Computerized Medical Record', 'Consent', 'Country', 'Data', 'Data Analyses', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Education', 'Eligibility Determination', 'Ensure', 'Evaluation', 'FDA approved', 'Family', 'Fibroblasts', 'Gene Silencing', 'Generations', 'Genetic Counseling', 'Genomic medicine', 'Genomics', 'Goals', 'Graph', 'Healthcare', 'Hospitals', 'Human', 'International', 'Investigation', 'Investments', 'Leadership', 'Libraries', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Metagenomics', 'Methods', 'Mission', 'Modeling', 'Multiomic Data', 'Network-based', 'Ontology', 'Organism', 'Organoids', 'Participant', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Physicians', 'Play', 'Policy Maker', 'Principal Investigator', 'Procedures', 'Process', 'Protocols documentation', 'Publications', 'Reagent', 'Recording of previous events', 'Research', 'Resources', 'Robotics', 'Role', 'Scientist', 'Site', 'Standardization', 'Structure', 'System', 'T-Lymphocyte', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Training', 'Translational Research', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Variant', 'Visit', 'accurate diagnosis', 'base', 'clinical practice', 'clinical research site', 'cohort', 'data integration', 'deep learning', 'drug discovery', 'experience', 'follow-up', 'genome-wide', 'genomic data', 'high-throughput drug screening', 'improved', 'induced pluripotent stem cell', 'innovation', 'insertion/deletion mutation', 'meetings', 'metabolomics', 'multiple omics', 'next generation', 'novel', 'novel strategies', 'novel therapeutics', 'operation', 'outreach', 'patient outreach', 'phenotypic data', 'preservation', 'programs', 'reference genome', 'relating to nervous system', 'research clinical testing', 'sample collection', 'screening', 'small molecule libraries', 'socioeconomics', 'stem cell biology', 'success', 'support network', 'technology development', 'tool', 'transcriptome sequencing', 'variant detection', 'virtual screening']",NHGRI,STANFORD UNIVERSITY,U01,2020,100000,0.02828026824008313
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,0.03374052294878724
"Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease Project Summary Over the past decade, it has become clear that mixture between diverged populations (admixture) has been a recurrent feature in human evolution. It has also become evident that a detailed understanding of admixture is essential for effective disease gene mapping as well as evolutionary inference. Nevertheless, adequate analytical tools to dissect admixture and its impact on phenotype are lacking. As a result, disease gene mapping or evolutionary studies have either excluded admixed populations or relied on simplified models at the risk of inaccurate inferences. This proposal proposes to develop computational methods to infer the genomic structure and history of admixed populations across a range of evolutionary time scales and to leverage this structure to obtain a comprehensive understanding of the genetic architecture and evolution of complex phenotypes. The proposed methods will integrate powerful sources of information from ancient DNA with genomes from present-day human populations. These methods will enable populations with a history of admixture to be studied just as effectively as homogeneous populations. The first step in obtaining a thorough understanding of admixture is a principled and scalable statistical framework to infer fine-scale genomic structure (local ancestry) and evolutionary relationships. This proposal leverages recent advances in statistical machine learning to develop effective tools for the increasingly common and challenging problem of local ancestry inference where reference genomes for ancestral populations are unavailable (de-novo local ancestry). Further, the proposal intends to develop models to infer complex evolutionary histories as well as realistic mating patterns in admixed populations. These inferences will form the starting point to systematically understand how admixture has shaped phenotypes. For example, it is becoming clear that admixture between modern humans and archaic humans (Neanderthals and Denisovans) could have had a major impact on human phenotypes. This question will be explored by applying novel statistical methods to large genetic datasets with phenotypic measurements to assess the adaptive as well as phenotypic impact of Neanderthal alleles. Finally, large collections of genomes from extinct populations that are now becoming available due to advances in ancient DNA technologies can lead to vastly more powerful methods for evolutionary inference that overcome the limitation of methods that rely only on extant genomes. Statistical models that use ancient genome time-series to efficiently infer admixture histories, local ancestry and selection will be developed. Project Narrative Although mixture events between human populations (admixture) are now known to have been common throughout human history and are likely to have had a major impact on human phenotypes, we lack adequate methods to study these processes. Our work will lead to a suite of powerful tools to understand the history of admixture, the impact of admixture on fine-scale genomic structure and function. Our work not only lead to new insights into the genetic basis and evolution of complex phenotypes but will ensure that major population groups, many of whom descend from admixture events or from ancestral groups distinct from those of Europeans, can benefit from the advances in genomics.",Statistical Models for Dissecting Human Population Admixture and its Role in Evolution and Disease,9990809,R35GM125055,"['Admixture', 'Alleles', 'Chromosome Mapping', 'Collection', 'Complex', 'Computing Methodologies', 'DNA', 'Data Set', 'Disease', 'Ensure', 'European', 'Event', 'Evolution', 'Genetic', 'Genome', 'Genomics', 'Human', 'Lead', 'Measurement', 'Methods', 'Modeling', 'Modernization', 'Partner in relationship', 'Pattern', 'Phenotype', 'Population', 'Population Group', 'Process', 'Recording of previous events', 'Recurrence', 'Risk', 'Role', 'Series', 'Source', 'Statistical Methods', 'Statistical Models', 'Structure', 'Technology', 'Time', 'Work', 'analytical tool', 'genetic architecture', 'genetic evolution', 'insight', 'novel', 'reference genome', 'statistical and machine learning', 'structural genomics', 'tool']",NIGMS,UNIVERSITY OF CALIFORNIA LOS ANGELES,R35,2020,332952,0.04930739218467366
"Advanced computational methods in analyzing high-throughput sequencing data Sequencing technologies have become an essential tool to the study of human evolution, to the understanding of the genetic bases of diseases and to the clinical detection and treatment of genetic disorders. Computational algorithms are indispensible to the analysis of large-scale sequencing data and have received broad attention. However, developed several years ago, many mainstream software packages for sequence alignment, assembly and variant calling have gradually lagged behind the rapid development of sequencing technologies. They are unable to process the latest long reads or assembled contigs, and will be outpaced by upcoming technologies in terms of throughput. The development of advanced algorithms is critical to the applications of sequencing technologies in the near future. This project will address this pressing need with four proposals: (1) developing a fast and accurate aligner that accelerates short-read alignment and can map megabase-long assemblies against large sequence collections of over 100 gigabases in size; (2) developing an integrated caller for small sequence variations that is faster to run, more sensitive to moderately longer insertions and more accessible to biologists without extended expertise in bioinformatics; (3) developing a generic variant filtering tool that uses a novel deep learning model to achieve human-level accuracy on identifying false positive calls; (4) developing a new de novo assembler that works with the latest nanopore reads of ~100 kilobases in length and may achieve good contiguity at low coverage. Upon completion, the proposed studies will dramatically reduce the computational cost of data processing in most research labs and commercial entities, and will enable the applications of long reads in genome assembly, in the study of structural variations and in cancer researches. Computational algorithms are essential to the analysis of high-throughput sequencing data produced for the detection, prevention and treatment of cancers and genetic disorders. The proposed studies aim to address new challenges arising from the latest sequencing data and to develop faster and more accurate solutions to existing applications. The success of this proposal is likely to unlock the full power of recent sequencing technologies in disease studies and will dramatically reduce the cost of data analyses.",Advanced computational methods in analyzing high-throughput sequencing data,9870944,R01HG010040,"['Address', 'Advanced Development', 'Algorithms', 'Attention', 'Bioinformatics', 'Biological', 'Characteristics', 'Chromosomes', 'Clinical', 'Clinical Data', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Dependence', 'Detection', 'Development', 'Dimensions', 'Disease', 'Evolution', 'Future', 'Generations', 'Genetic', 'Genetic Diseases', 'Genome', 'High-Throughput Nucleotide Sequencing', 'Hour', 'Human', 'Large-Scale Sequencing', 'Length', 'Mainstreaming', 'Maps', 'Medical Genetics', 'Modeling', 'Modernization', 'Performance', 'Population Genetics', 'Prevention', 'Process', 'Production', 'Research', 'Research Personnel', 'Running', 'Seeds', 'Sequence Alignment', 'Sequence Analysis', 'Site', 'Speed', 'Stress', 'Structure', 'Technology', 'Text', 'Time', 'Variant', 'Work', 'anticancer research', 'base', 'bioinformatics tool', 'cancer therapy', 'computerized data processing', 'contig', 'convolutional neural network', 'cost', 'deep learning', 'deep sequencing', 'design', 'experimental study', 'genome analysis', 'high throughput analysis', 'improved', 'indexing', 'light weight', 'mammalian genome', 'nanopore', 'novel', 'open source', 'preservation', 'programs', 'success', 'tool', 'user-friendly', 'whole genome']",NHGRI,DANA-FARBER CANCER INST,R01,2020,397125,0.025751572558715952
"Direct Determination of Multiple Specific Forms of DNA Chemical Modifications in Human Genome PROJECT SUMMARY/ABSTRACT The information content of DNA is not limited to the primary sequence (A, C, G, T), but is also conveyed by chemical modifications of individual bases. For example, DNA methylation, specifically 5-methylcytosine (5mC), has been widely studied for its important regulatory roles in human development and diseases. In addition, the discovery of active demethylation of 5mC, mediated by TET enzymes, into 5-hydroxymethylcytosine (5hmC), 5-formylcytosine (5fC) and 5-carboxylcytosine (5caC) revealed great insights into the dynamic nature of the human methylome and its close relevance to multiple human diseases. Beyond these chemical modifications to cytosine, recent studies by us and others discovered that N6-methyladenine (6mA), another form of methylation previously thought exclusively existing in bacteria and protozoa, also exists in eukaryotic genomes including the human genome. In addition to these epigenetic marks, different forms of DNA damages represent another category of DNA chemical modifications that are of important biological relevance. Although a few methods for mapping individual chemical modifications have been developed and some are widely used, it is usually hard for broad researchers to master every protocol to map each form of modification. While third- generation sequencing technologies support the direct detection of DNA modifications, they face fundamental challenges distinguishing among different forms of modifications. The objective of this project is to develop a novel technology for the direct mapping of multiple forms of DNA methylation and DNA damage events simultaneously. The core idea is that each form of nucleic acid modification has a unique signature in terms of their physical interaction with DNA polymerase, or nanopores in third-generation sequencing; and these signatures can be modeled by deep learning methods. We will develop this technology using multiple innovative strategies to address a few fundamental challenges, and then comprehensively evaluate the technology to facilitate broad applications. PUBLIC HEALTH RELEVANCE Chemical DNA modifications are crucial components of human genome that controls many important biological processes in human development and human diseases. In this project, we will develop a novel technology for direct mapping of multiple and specific forms of DNA modifications, which will enable us and a large number of researchers to more effectively study the functions of DNA modifications in human genome. !",Direct Determination of Multiple Specific Forms of DNA Chemical Modifications in Human Genome,10267380,R56HG011095,"['Address', 'Antibodies', 'Bacteria', 'Biological', 'Biological Process', 'Categories', 'Characteristics', 'Chemicals', 'Chemistry', 'Classification', 'Complex', 'Cytosine', 'DNA', 'DNA Damage', 'DNA Methylation', 'DNA Modification Process', 'DNA-Directed DNA Polymerase', 'Data', 'Data Set', 'Detection', 'Development', 'Ensure', 'Enzymes', 'Epigenetic Process', 'Event', 'Face', 'Future', 'Genome', 'Goals', 'Heterogeneity', 'Human', 'Human Characteristics', 'Human Development', 'Human Genome', 'Immunoprecipitation', 'Individual', 'Maps', 'Mediating', 'Methodology', 'Methods', 'Methylation', 'Modeling', 'Modification', 'Neurons', 'Nucleic Acids', 'Oligonucleotides', 'Ploidies', 'Positioning Attribute', 'Protocols documentation', 'Protozoa', 'Research Personnel', 'Resolution', 'Role', 'Technology', 'Third Generation Sequencing', 'Thymine', 'Time', 'Training', 'Variant', 'base', 'bisulfite sequencing', 'cancer risk', 'cell type', 'cost', 'deep learning', 'demethylation', 'human disease', 'innovation', 'insight', 'learning strategy', 'methylome', 'nanopore', 'network models', 'new technology', 'novel', 'predictive modeling', 'public health relevance', 'sequencing platform', 'single molecule real time sequencing']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R56,2020,233456,-0.00406122277588358
"Addressing Open Challenges of Computational Genome Annotation We propose to capitalize on success of ongoing collaboration between the bioinformatics teams at the University of Greifswald (Germany) and at the Georgia Institute of Technology (USA) and address open challenges in computational genome annotation. In the course of this development, we plan to implement new algorithmic ideas and satisfy the needs of unbiased integration of different types of OMICS data.  We plan to address one of the long-standing problems at interface of bioinformatics and machine learning – automatic generative and discriminative parameterization of gene finding algorithms. Current methods of combining OMICS evidence frequently result in under predicting or over predicting tools. Having good understanding of the difficulties and the properties of different types of OMICS evidence we propose an optimized approach to the full unsupervised, generative and discriminative training.  We will introduce novel means to optimize integration of multiple OMICS evidence into gene prediction. These ideas will develop further the protein family-based gene finding implemented in AUGUSTUS-PPX. We propose to create representations of protein families for gene finding that for the first time include cross-species gene structure information.  We will develop a new approach that will unify two advanced research areas - transcript reconstruction from RNA-Seq and statistical gene finding that integrates RNA-Seq and homology information. We will describe a new, comprehensive model and EM-like algorithmic technique (the “wholistic” approach) to identify the sets of transcripts and their expression levels that best fit the available OMICS evidence.  We will also develop an automatic gene-finding algorithm for a full content of metagenomes including eukaryotic and viral metagenomic sequences. This task is conventionally considered too challenging. We propose a solution exploiting and advancing algorithmic ideas and approaches that we mastered in the course of creating gene finders for prokaryotic metagenomes as well as eukaryotic genomes.  All new tools will be available to the community under open source licenses. The goal of this project is to advance the science of genome interpretation by developing much needed computational methods and tools for high precision annotation of eukaryotic genomes and metagenomes. This advance will make an impact in research on model and non-model organisms including important human pathogens, parasites and viruses. New high throughput technologies generate volumes of sequence data on complex genomes as well as metagenomes. Still these big data volumes have to be transformed into scientific knowledge. Our new bioinformatics tools, matching the latest sequencing technology in speed and performance, will make a significant impact in genomic research aiming at ultimate understanding of human health and disease.",Addressing Open Challenges of Computational Genome Annotation,9975182,R01GM128145,"['Address', 'Algorithms', 'Alternative Splicing', 'Area', 'Bacteriophages', 'Benchmarking', 'Big Data', 'Bioinformatics', 'Chronic', 'Code', 'Collaborations', 'Collection', 'Communities', 'Complement', 'Complex', 'Computing Methodologies', 'Data', 'Deterioration', 'Development', 'Development Plans', 'Disease', 'Gene Family', 'Gene Structure', 'Genes', 'Genome', 'Genomics', 'Germany', 'Goals', 'Health', 'Human', 'Insecta', 'Institutes', 'Introns', 'Knowledge', 'Length', 'Licensing', 'Machine Learning', 'Maintenance', 'Metagenomics', 'Methods', 'Modeling', 'Modernization', 'Nested Genes', 'Noise', 'Overlapping Genes', 'Parasites', 'Performance', 'Population', 'Positioning Attribute', 'Property', 'Protein Family', 'Protein Isoforms', 'Proteins', 'Proteomics', 'RNA Splicing', 'Research', 'Running', 'Speed', 'Spliced Genes', 'Statistical Models', 'Supervision', 'Techniques', 'Technology', 'Time', 'Training', 'Transcript', 'Universities', 'Viral', 'Virus', 'annotation  system', 'base', 'bioinformatics tool', 'computerized tools', 'cost', 'course development', 'design', 'evidence base', 'expectation', 'gene complementation', 'genome annotation', 'genome sciences', 'high throughput technology', 'human pathogen', 'improved', 'instrument', 'member', 'metagenome', 'multiple omics', 'nanopore', 'new technology', 'novel', 'novel strategies', 'open source', 'operation', 'predictive tools', 'protein profiling', 'reconstruction', 'success', 'tool', 'transcriptome', 'transcriptome sequencing', 'transcriptomics', 'whole genome']",NIGMS,GEORGIA INSTITUTE OF TECHNOLOGY,R01,2020,342390,0.025109976801364994
"Uncovering the Human Secretome PROJECT SUMMARY / ABSTRACT Peptide hormones regulate embryonic development and most physiological processes by acting as endocrine or paracrine signals. They are also a rich source of relatively safe medicines to treat both common and rare diseases. Yet finding peptide-coding genes below ~300 base pairs is inherently difficult because they lie within the noise of the genome. Recent multidisciplinary, proteophylogenomic studies in lower species, such as yeast and flies, have uncovered hundreds of new small protein-coding genes called “smORFs”. In humans, recent work on the mitochondrial genome has also uncovered dozens of small peptide hormone genes called MDPs. Based on these and other studies, it is estimated that about 5% of proteins in the human nuclear genome have not yet been discovered, particularly those that encode small peptides below 100 amino acids. It is a well documented but rarely challenged practice to discard large quantities of sequencing and proteomic data because they do not match the annotated human genome. My overarching goal is to discover the human “secretome” and make practical use of it to improve the human condition. Over the past few years, we have developed a unique pipeline of technologies that combines breakthroughs in math, computer hardware and software, proteomics, mass spectrometry, and HTS screening, each of which has been optimized and integrated. Our GeneFinder software modules, based on machine-learning, can process data 100 times faster than traditional methods and rapidly validate small human genes using public and in-house generated databases of genetic and proteomic data. Using the prototype version of the platform that finds conservation between humans, chimp, and macaque, we have discovered thousands of putative peptide-coding genes and validated hundreds of them. We aim to (1) further improve the algorithm to increase its speed and accuracy, (2) improve the genome annotation for thousands of small novel genes, (3) determine their expression profiles in normal and diseased tissues, (4) explore their genetic association with disease loci, and (5) screen the first secretomic library to find hormones with novel biological and therapeutically relevant activities. The data, the software package, and libraries will be made available to the research community. In doing so, we will shed light on the dark matter of the human genome, the parts with the greatest therapeutic potential, thereby helping to steer and accelerate the pace of research and drug development for generations to come. PROJECT NARRATIVE There has been a rapid expansion in the use of peptide hormones as drugs over the last decade, yet new research indicates that more than 90% of all hormones in the body (encoded by an additional 5% of the human genome) remain to be discovered. As a result, terabytes of data are discarded each week and innumerable opportunities for biological discovery are missed because, according to our findings, the majority of genes below ~300 base pairs are missing from the annotated human genome. We propose an integrated, multi- disciplinary approach to find, validate and characterize an estimated 4000-5000 new peptide-coding genes using a pioneering technology platform that combines breakthroughs in math, custom-built computer hardware and software, and wet-lab approaches, providing a far more complete roadmap for biology and medicine in the 21st century.",Uncovering the Human Secretome,9928344,DP1AG058605,"['Algorithms', 'Amino Acids', 'Base Pairing', 'Biological', 'Biological Response Modifier Therapy', 'Biology', 'Code', 'Communities', 'Computer Hardware', 'Computer software', 'Custom', 'Data', 'Disease', 'Embryonic Development', 'Endocrine', 'Expression Profiling', 'Generations', 'Genes', 'Genetic Databases', 'Genome', 'Goals', 'Hormones', 'Human', 'Human Genome', 'Libraries', 'Light', 'Macaca', 'Machine Learning', 'Mass Spectrum Analysis', 'Mathematics', 'Medicine', 'Methods', 'Noise', 'Nuclear', 'Pan Genus', 'Paracrine Communication', 'Peptides', 'Pharmaceutical Preparations', 'Physiological Processes', 'Process', 'Proteins', 'Proteomics', 'Rare Diseases', 'Research', 'Source', 'Speed', 'Technology', 'Therapeutic', 'Time', 'Tissues', 'Work', 'Yeasts', 'base', 'dark matter', 'drug development', 'fly', 'genetic association', 'genome annotation', 'improved', 'interdisciplinary approach', 'mitochondrial genome', 'multidisciplinary', 'novel', 'peptide hormone', 'prototype', 'research and development', 'screening', 'terabyte']",NIA,HARVARD MEDICAL SCHOOL,DP1,2020,1186500,0.03349182712941097
"Advanced algorithms to infer and analyze 3D genome structures Project Summary For the past decade, the population-cell Hi-C technique has significantly improved our ability to discover genome-wide DNA proximities. However, because population Hi-C is based on a pool of cells, it will not help us reveal each single cell's 3D genome structure or understand cell-to-cell variability in terms of 3D genome structure and gene regulation. It is also difficult to achieve a high resolution, such as 1 Kbp, with population Hi- C; therefore, when finding and analyzing the spatial interactions for the promoter or enhancer regions typically associated with biologically-important regulatory elements, population Hi-C data's resolution is too low to be useful. Moreover, while we know that the CTCF-cohesin complex plays a key role in the formation of genome 3D structures, the question is whether long non-coding RNAs (lncRNAs) are involved in the process since lncRNAs have been found to recruit proteins needed for chromatin remodeling, and our preliminary research has found that lncRNA LINC00346 directly interacts with CTCF. Finally, while members of the bioinformatics community, including the PI, have developed many algorithms to reconstruct 3D genome structures based on population Hi-C data, important questions still must be answered regarding how 3D genome structures are involved in gene regulation and whether there are relationships between 3D genome structures and genetic and epigenetic features. The PI proposes to conduct leading research to overcome these challenges and address these questions. During the next five years, the PI will develop algorithms to reconstruct the 3D whole- genome structures for single cells and analyze cell-to-cell variabilities in terms of 3D genome structure and gene regulation. The PI will develop a deep learning algorithm to enhance the resolution of population Hi-C data to that of Capture Hi-C data (1 Kbp) so that we can make good use of the large amount of Hi-C data accumulated in the past decade. An online database will be built to allow the community to access both population and single-cell 3D genome structures in an integrated way. The PI will work with a cancer biologist to discover any lncRNAs that function as a scaffold to fine-tune the CTCF-cohesin protein complex, as well as two neuron scientists to develop a more complete understanding of gene regulation while considering 3D genome and other genetic and epigenetic features. Given the PI's track record and productivity, having three computational goals and two collaborative goals is not only feasible but computationally and biologically rewarding. In five years, once the proposed studies are accomplished, the PI should have established a uniquely independent place in the field of 3D genome, maintaining leading positions in inferring single-cell 3D genome structures, enhancing Hi-C data resolution, and building 3D genome databases, while establishing similar positions in reconstructing high-resolution 3D genome structures, finding lncRNAs' roles in the formation of genome structures, and understanding how 3D genome structures are involved in gene regulation. Project Narrative The three-dimensional (3D) structure of genome is critically important for gene regulation; and the abnormal 3D genome structures are often associated with diseases such as cancer. This proposal aims to reconstruct and analyze the 3D genome structures of thousands of individual cells, study how 3D genome structures participate in gene regulation, determine whether long noncoding RNAs (lncRNAs) are involved in the formation of genome structures, and build an online knowledge base integrating 3D genome structures with other biological information.",Advanced algorithms to infer and analyze 3D genome structures,10027542,R35GM137974,"['3-Dimensional', 'Address', 'Algorithms', 'Bioinformatics', 'Biological', 'Cells', 'Communities', 'Complex', 'DNA', 'Data', 'Databases', 'Disease', 'Enhancers', 'Epigenetic Process', 'Gene Expression Regulation', 'Genetic', 'Genome', 'Goals', 'Individual', 'Malignant Neoplasms', 'Neurons', 'Other Genetics', 'Play', 'Population', 'Positioning Attribute', 'Process', 'Productivity', 'Proteins', 'Regulatory Element', 'Research', 'Resolution', 'Rewards', 'Role', 'Scientist', 'Structure', 'Techniques', 'Untranslated RNA', 'Work', 'base', 'chromatin remodeling', 'cohesin', 'deep learning algorithm', 'genome database', 'genome-wide', 'improved', 'knowledge base', 'member', 'promoter', 'protein complex', 'recruit', 'scaffold', 'three dimensional structure', 'whole genome']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R35,2020,354694,0.010462631978915518
"Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation To be fully understood, the human genome must be considered in the context of evolution. The activities that have dominated human genomics for three decades — such as genome sequencing and annotation, interrogation with high-throughput biochemical assays, and the identification of associations between genetic variants and diseases — have been enormously informative, but these descriptive studies must eventually be understood within the theoretical framework of evolutionary genetics. We must continue to press forward from the what? to the why? and how? of human genetics.  The goal of my laboratory is to interpret high-throughput genomic data from an evolutionary perspective. Drawing from ideas and techniques in molecular evolution, population genetics, statistics, and computer science, we aim both to understand the evolutionary forces that have shaped human genomes, and to use evolution to shed light on the phenotypic importance of particular sequences. Our recent activities have focused in three major areas: (1)  reconstruction  of  features  of  human  evolution  based  on  genome  sequences;  (2)  prediction  of  the  fitness consequences  of  human  mutations;  and  (3)  the  study  of  transcriptional  regulation  and  its  evolution  in primates.   We have reported major findings in each of these areas, including the existence of gene flow from early modern humans to Eastern Neandertals, a map of fitness consequences for mutations across the human genome, and an analysis showing that the architecture of transcription initiation is highly similar at enhancers and promoters in the human genome.  Here we propose to extend our research substantially in each of these areas, working together with a broad range  of  experimental  and  theoretical  collaborators.    Our  new  goals  include  the  development  of  improved methods for reconstructing human demography, with a focus on ancient gene flow; extensions of our ancestral recombination graph (ARG) sampling methods to accommodate much larger samples sizes, with applications in association mapping and the detection of natural selection; two complementary machine-learning approaches for  improving  the  prediction  of  fitness  consequences  from  sequence  data;  an  experimental  collaboration  to leverage CRISPR-Cas9 screens in characterizing noncoding mutations; a multi-pronged study of the sequence determinants of RNA stability and their implications for the evolution of transcription units; and development of a new probabilistic model for turnover of regulatory elements.  Together, these projects will address a wide variety of fundamental questions about the function and evolution of sequences in the human genome. Vast quantities of genomic data are now available to describe patterns of genetic variation within  human populations and across species, and various measures of biochemical activity along the human  genome. These data need to be interpreted in light of the fundamental forces of mutation,  recombination, natural selection, and genetic drift that have shaped genetic variation. This  proposal describes a series of projects that make use of new computational, statistical, and  theoretical methods to address fundamental questions in human evolutionary genetics, including how  humans arose   from our archaic hominin and ape cousins, how human populations diverged from one  another, how new mutations influence human health and fitness, and how regulatory sequences  contribute to unique aspects of human biology.","Evolutionary Human Genomics: Demography, Natural Selection, and Transcriptional Regulation",9876220,R35GM127070,"['Address', 'Architecture', 'Area', 'Biochemical', 'Biological Assay', 'CRISPR screen', 'Collaborations', 'Data', 'Demography', 'Detection', 'Development', 'Enhancers', 'Evolution', 'Genes', 'Genetic', 'Genetic Diseases', 'Genetic Drift', 'Genetic Recombination', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Goals', 'Graph', 'Health', 'Human', 'Human Biology', 'Human Genetics', 'Human Genome', 'Laboratories', 'Light', 'Machine Learning', 'Maps', 'Measures', 'Methods', 'Modernization', 'Molecular Evolution', 'Mutation', 'Natural Selections', 'Pattern', 'Phenotype', 'Pongidae', 'Population', 'Population Genetics', 'Primates', 'RNA Stability', 'Regulatory Element', 'Reporting', 'Research', 'Sample Size', 'Sampling', 'Series', 'Statistical Models', 'Techniques', 'Transcription Initiation', 'Transcriptional Regulation', 'Untranslated RNA', 'base', 'computer science', 'fitness', 'genetic variant', 'genome analysis', 'genome annotation', 'genome sequencing', 'genomic data', 'human genomics', 'improved', 'promoter', 'reconstruction', 'statistics']",NIGMS,COLD SPRING HARBOR LABORATORY,R35,2020,479215,0.010230319840489638
"EDAC: ENCODE Data Analysis Center PROJECT SUMMARY The goal of the Encyclopedia of DNA Elements (ENCODE) project is to catalog all functional elements in the human genome through the integration and analysis of high-throughput data. We propose to continue the ENCODE Data Analysis Center (EDAC, DAC) which will provide support and leadership in analyzing and integrating data from the ENCODE project as well as work closely with other ENCODE groups including the Data Coordination Center. Our proposed DAC team (Zhiping Weng, Mark Gerstein, Manolis Kellis, Roderic Guigo, Rafael Irizarry, X. Shirley Liu, Anshul Kundaje, and William Noble) has expertise across a wide range of fields including transcriptional regulation, epigenetics, evolution, genomics and proteomics, regulatory RNA, biophysics, and computational biology, where they are the leaders in machine learning, statistical genetics, networks, and gene annotation. These investigators also have a history of successfully working collaboratively in large consortia, particularly with other ENCODE groups. Their publication records demonstrate their synergistic approach to producing high-impact science and useful resources that benefit the broader biomedical communities. The proposed DAC will pursue the following four aims: Aim 1. Analyze and integrate data and metadata from a broad range of functional genomics projects; Aim 2. Serve as an informatics resource by supporting the activities of the ENCODE Analysis Working Group; Aim 3. Create high-quality Encyclopedias of DNA elements in the human and mouse genomes; Aim 4. Assess quality and utility of the ENCODE data and provide feedback to NHGRI and the Consortium. RELEVANCE The goal of the Encyclopedia of DNA Elements (ENCODE) project is a highly collaborative effort aiming to develop a comprehensive list of functional elements in the human genome. This proposal creates a data analysis center to provide support and computational prowess for this effort in collaboration with other ENCODE groups. This comprehensive list will be of use to the wider research community and will aid in understanding human biology particularly in the context of disease, ultimately leading to improvements in human health.",EDAC: ENCODE Data Analysis Center,9858390,U24HG009446,"['ATAC-seq', 'Alleles', 'Binding', 'Biochemical', 'Biological', 'Biological Assay', 'Biophysics', 'Catalogs', 'ChIP-seq', 'Chromatin', 'Collaborations', 'Communities', 'Computational Biology', 'Computing Methodologies', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Collection', 'Data Coordinating Center', 'Data Element', 'Data Set', 'Deoxyribonucleases', 'Development', 'Disease', 'Elements', 'Encyclopedia of DNA Elements', 'Encyclopedias', 'Enhancers', 'Epigenetic Process', 'Event', 'Evolution', 'Feedback', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomic Segment', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Intuition', 'Leadership', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Metadata', 'Methods', 'Mus', 'National Human Genome Research Institute', 'Nucleotides', 'Pathway Analysis', 'Process', 'Proteomics', 'Publications', 'RNA', 'RNA-Binding Proteins', 'Recording of previous events', 'Records', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Resource Informatics', 'Resources', 'Science', 'Signal Transduction', 'Subgroup', 'Techniques', 'The Cancer Genome Atlas', 'Transcriptional Regulation', 'Variant', 'Work', 'Writing', 'analysis pipeline', 'base', 'bisulfite sequencing', 'cell type', 'comparative', 'computerized data processing', 'data exchange', 'data infrastructure', 'data integration', 'data standards', 'experience', 'experimental study', 'functional genomics', 'genetic variant', 'genome wide association study', 'high throughput analysis', 'histone modification', 'insight', 'large scale data', 'member', 'mouse genome', 'multiple data types', 'novel', 'symposium', 'transcription factor', 'transcriptome sequencing', 'whole genome', 'working group']",NHGRI,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U24,2020,2000000,0.025853178847588157
"Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases ﻿    DESCRIPTION (provided by applicant): The NHGRI Genome Sequencing Program (GSP) will identify genomic variants relevant to health and disease by genome sequencing over 225,000 participants across a multitude of diseases. The GSP will also serve as a pilot for the Precision Medicine Initiative that aims to enroll and sequence more than a million people representative of U.S. ethnic diversity. Here, we propose a GSP analysis center focused on Multi- and Trans- ethnic Mapping of Mendelian and Complex Diseases. There is a growing recognition of the substantial scientific advantages, as well as public health importance, of conducting biomedical research across ethnically diverse cohorts. We propose to develop scalable methods that incorporate ancestry to optimize medical genomic study design and improve power for uncovering the role of common and rare variants in disease. Achieving this goal requires expertise across diverse domains of knowledge including: medical and population genomics, algorithm development for complex disease mapping, and expertise in management of large-scale databases. Here, we have assembled a world-class team of medical and population geneticists, computer scientists, statisticians and clinicians, with leading expertise in the development of novel and scalable strategies for characterizing sequence variants and their role in disease. Importantly, our group has been at the forefront of development of resources, study designs and methods to enable genomic research in U.S. minority populations. Our project has three main objectives. First, we will develop an Automated Scalable Ancestry Pipeline (ASAP) for common disease mapping in diverse populations. ASAP will improve the computational efficiency of existing state-of-the-art methods for ancestry inference and develop important extensions to linear mixed models (LMMs) and other mapping strategies leveraging local and global ancestry. We will also develop methods to refine phenotypes and identify common controls for disease studies and define endpoints. Secondly, we will develop tools and resources for trans- and multi-population rare variant discovery that incorporate patterns of local and sub-continental ancestry. We will also develop machine-learning tools for variant annotation that leverage ancestral information, patterns of sequence evolution, and protein structure in a unified framework. Furthermore, we will incorporate population-specific patterns of cellular phenotypes to improve functional prediction algorithms for non-coding and coding variants. Lastly, we will disseminate our results through web-based resource that empower the biomedical research community. We will augment existing resources including ClinGen by annotating and characterizing pathogenic variants across diverse populations. We will develop a secure web-server that allows sharing of summary statistics and analysis pipelines to enable discovery, fine-mapping and functional prediction of genetic variants. Our team has ample experience with NIH-funded consortia and is dedicated to meeting the overall GSP project goals through collaborative work with NHGRI leadership and other funded investigators. PUBLIC HEALTH RELEVANCE The goal of our project is to accelerate the discovery of DNA variation relevant to health and disease by analyzing data from over 225,000 ethnically and racially diverse patients that will undergo genome sequencing. Of particular importance is ensuring we have powerful statistical methods for analyzing data from underserved groups including U.S. minority populations. Achieving this goal requires expertise across many domains of knowledge including: medical and population genomics, algorithm development for disease mapping, and expertise in large-scale databases.",Center for Multi- and Trans-ethnic Mapping of Mendelian and Complex Diseases,10126204,U01HG009080,"['Architecture', 'Biomedical Research', 'Chronic Disease', 'Clinical', 'Code', 'Communities', 'Complex', 'Computers', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Disease', 'Enrollment', 'Ensure', 'Evolution', 'Funding', 'Genetic', 'Genetic Transcription', 'Genetic Variation', 'Genome', 'Genomics', 'Genotype-Tissue Expression Project', 'Goals', 'Health', 'Human Genome', 'Investigation', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Methodology', 'Methods', 'Methylation', 'Minority', 'Modeling', 'Molecular Conformation', 'National Human Genome Research Institute', 'Participant', 'Pathogenicity', 'Patients', 'Pattern', 'Phenotype', 'Population', 'Population Heterogeneity', 'Precision Medicine Initiative', 'Privatization', 'Protocols documentation', 'Public Health', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Resource Development', 'Resources', 'Role', 'Scientist', 'Secure', 'Shoulder', 'Statistical Methods', 'Target Populations', 'Testing', 'Trans-Omics for Precision Medicine', 'United States', 'United States National Institutes of Health', 'Untranslated RNA', 'Variant', 'Work', 'algorithm development', 'analysis pipeline', 'cohort', 'disorder control', 'ethnic diversity', 'experience', 'genetic variant', 'genome sequencing', 'genomic tools', 'improved', 'innovation', 'large-scale database', 'meetings', 'novel', 'online resource', 'patient privacy', 'phenome', 'prediction algorithm', 'predictive modeling', 'preservation', 'programs', 'protective allele', 'protein structure', 'public health relevance', 'racial diversity', 'rare variant', 'risk variant', 'simulation', 'statistics', 'tool', 'tool development', 'web server', 'web-based tool']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U01,2020,962530,0.0445636893265572
"Integrating multidimensional genomic data to discover clinically-relevant predictive models The goal of this NIH Pathway to Independence award is to provide Dr. Brittany Lasseigne with an extensive training program to prepare her to be an effective independent investigator who uses computational genomics to study complex human diseases. We propose a formal one-year training and mentoring program in genomics, computer science, statistics, and career development to build on her 8+ years of hands-on training, followed by a three-year structured and independent research program. Research will focus on the integration of multidimensional genomic data sets in the context of complex human diseases. A critical barrier in genomic research is the complexity of data integration: the ability to leverage overlapping and unique information captured by different genomic assays would improve our understanding of data integration and generate clinically relevant genomic signatures. To meet this need, we propose to integrate a combination of genomic data we generated with public data to (1) infer genomic instability signatures from different data types, (2) improve clinically relevant phenotype prediction by building multi-omics machine learning classifiers and reducing phenotype heterogeneity, and (3) create a cloud-enabled R package and associated Shiny application to accelerate future research. The proposed work will advance our understanding of data integration, allow inference of genomic instabilities across data sets, and generate high performance classifiers for assessing clinically relevant phenotypes in both cancer and psychiatric disease using frameworks that will be broadly applicable across other complex diseases. It will also facilitate prioritization of experiments in future studies by informing on the orthogonality of genomic assays, thereby allowing more efficient study designs to capture as much information as possible within a given sample size or scope of experimentation. Collectively, this additional training will allow Dr. Lasseigne to develop new multidimensional data integration approaches and translational questions applicable across complex diseases when independent. Dr. Richard Myers (HudsonAlpha) and Dr. Gregory Cooper (HudsonAlpha), leaders in applying genetics and genomics to complex human diseases, and an Advisory Committee of additional experts including Dr. Barbara Wold (Caltech), Dr. Eddy Yang (UAB), and Dr. Timothy Reddy (Duke), will provide mentoring throughout this award. The mentored phase will take place at the HudsonAlpha Institute for Biotechnology, an ideal environment for this training with extensive translational science collaborations, expert faculty and staff, and state-of-the art computational and laboratory resources devoted to genomics. This combination will maximize Dr. Lasseigne's training program, facilitating her transition to an independent, tenure-track investigator at a university with a strong commitment to data-driven approaches to complex human disease research, i.e. strong genomics research programs with clinical collaborators, ideally at, or affiliated with, an academic medical center. Project Narrative The major outcome of this project will be a scientist with the necessary research, mentoring, teaching, and career development training to run an independent research program in computational genomics. The research proposed will apply novel strategies to further develop integrative machine learning analyses of multidimensional genomic data, discover clinically relevant predictive models, and create computational tools to accelerate future research.",Integrating multidimensional genomic data to discover clinically-relevant predictive models,9936223,R00HG009678,"['Academic Medical Centers', 'Advisory Committees', 'Award', 'Bioconductor', 'Biological', 'Biological Assay', 'Biotechnology', 'Budgets', 'Cancer Etiology', 'Cell Proliferation', 'Cells', 'Characteristics', 'Chemotherapy-Oncologic Procedure', 'Chromosomal Instability', 'Chromosomes', 'Classification', 'Clinical', 'Clinical Data', 'Collaborations', 'Complex', 'Copy Number Polymorphism', 'Coupling', 'CpG Island Methylator Phenotype', 'DNA', 'DNA Methylation', 'Data', 'Data Analyses', 'Data Set', 'Disease', 'Educational process of instructing', 'Environment', 'Faculty', 'Future', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genetic', 'Genomic Instability', 'Genomics', 'Goals', 'Heterogeneity', 'Individual', 'Institutes', 'Instruction', 'Laboratories', 'Lasso', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mental disorders', 'Mentors', 'Methodology', 'Methylation', 'MicroRNAs', 'Microsatellite Instability', 'Modeling', 'Molecular Profiling', 'Neurons', 'Outcome', 'Pathway interactions', 'Performance', 'Phase', 'Phenotype', 'Procedures', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Signal Transduction', 'Structure', 'Systems Biology', 'Techniques', 'Testing', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Validation', 'Variant', 'Visualization', 'Work', 'Yang', 'biomarker performance', 'career development', 'clinically relevant', 'computer science', 'computerized tools', 'data framework', 'data integration', 'data reduction', 'data standards', 'experimental study', 'genomic data', 'genomic signature', 'human disease', 'improved', 'insight', 'learning classifier', 'multidimensional data', 'multiple omics', 'novel strategies', 'predictive modeling', 'programs', 'promoter', 'protein metabolite', 'response', 'single cell sequencing', 'statistics', 'tenure track', 'tool']",NHGRI,UNIVERSITY OF ALABAMA AT BIRMINGHAM,R00,2020,249000,0.08208810549771806
"Discovering Novel Structural Genomic Rearrangements Using Deep Neural Networks Abstract Accurately detecting structural variation in the genome is a challenging task. Many approaches have been developed over the last few decades, yet it is estimated that tens of thousands of variants are still being missed in a given sample. Many of these variants are missed due to the limitations of using short-read sequencing to identify large variants. Although many of these missed variants are located within complex regions of the genome, it has been shown that some still have clinical relevance making their discovery important. New platforms have been developed for sequencing the genome using long-reads and show promise for overcoming many of these limitations creating the ability to identify the full spectrum of simple and complex structural variants. Because this technology is relatively young, new computational approaches to support the analysis of long-read sequencing data can aid in the discovery of these variants which are still being missed. In addition to detecting novel variation in samples with long-read sequencing data, computational approaches can be developed to leverage these novel variant calls to reanalyze the hundreds of thousands of short-read datasets currently available. In this proposal, we plan to develop new computational approaches to identify novel structural variation in the genome. In Aim 1, we will apply a recurrence approach to analyze long read sequencing datasets utilizing deep neural networks. In Aim 2, we will develop a tool to derive profiles of structural variants predicted in long- reads which can be used to identify and genotype structural variants calls in short read data-sets. Together, these approaches will allow researchers to accurately characterize structural variation in both long and short- read datasets. Narrative Structural variation has been implicated in numerous human diseases but there are still tens of thousands of variants being overlooked in the genome. The proposed research aims to detect novel variation by developing new computational tools to analyze data generated by state-of-the-art sequencing methods. These tools will aid in the discovery of variants associated with human health.",Discovering Novel Structural Genomic Rearrangements Using Deep Neural Networks,9911983,F31HG010569,"['Affect', 'Algorithms', 'Benchmarking', 'Biological Sciences', 'Categories', 'Complex', 'Computing Methodologies', 'DNA', 'DNA Resequencing', 'DNA Sequence Rearrangement', 'Data', 'Data Set', 'Detection', 'Diagnostic', 'Disease', 'Future', 'Genome', 'Genotype', 'Haplotypes', 'Health', 'Human', 'Human Genome', 'Image', 'Image Analysis', 'Label', 'Methods', 'Molecular', 'Molecular Computations', 'Pattern', 'Process', 'Recurrence', 'Repetitive Sequence', 'Research', 'Research Personnel', 'Sampling', 'Structure', 'Techniques', 'Technology', 'Training', 'Validation', 'Variant', 'base', 'clinically relevant', 'comparative', 'computerized tools', 'cost', 'deep learning', 'deep neural network', 'design', 'genome sequencing', 'human disease', 'insertion/deletion mutation', 'new technology', 'novel', 'reference genome', 'structural genomics', 'tool', 'variant detection']",NHGRI,UNIVERSITY OF MICHIGAN AT ANN ARBOR,F31,2020,37657,0.016576208793395993
"Genomic risk in clinic care to promote health equity in New York City patients PROJECT SUMMARY Building on our track record in genomic research, clinical trials, and genomic medicine in diverse, underserved patients from NYC, we propose to develop new frameworks to bring genomic risk into clinical care to promote health equity. Polygenic risk scores (PRS) are entering an exciting phase where they are poised to improve health outcomes for myriad complex diseases through enhanced risk stratification and clinical decision making. However, major challenges exist for clinical PRS implementation today. The vast majority of PRS have far greater predictive value in individuals of European ancestry than other ancestries, and issues of access to leading-edge genomic technology, research, and testing disproportionately impact underserved populations. To address this, Mount Sinai experts in statistical genetics and population genetics, with decade-long experience in building methods tailored to diverse and admixed populations, will work together to rigorously develop multi-ethnic PRS. We will integrate multi-ethnic PRS with standard clinical risk and family history information to generate genomic risk assessments for 15 common diseases. Drawing on Mount Sinai's century of experience serving one of the most diverse patient populations in the world, we will recruit 2,500 adult and pediatric patients from diverse and underserved populations into a clinical trial. We will estimate participants' individualized risk for each condition, and investigate the impact of genomic risk communication to patients and their physicians, including patient understanding and uptake of recommended risk-reducing interventions. We will explore attitudes, barriers, and communication preferences related to genomic risk assessment in diverse populations. Knowledge gained will be used to guide the development of a new multilingual patient-facing digital platform supporting patient education and communication of genomic risk. We will track patient engagement with their results through the platform, and assess the impact of individualized genomic risk assessments on patient-reported psychosocial outcomes and experiences. As of today, the path to effectively integrate genomic risk into clinical care in busy health systems, particularly for diverse patients, is unclear. Hence, we are partnering with clinicians, scientists, industry experts, and community stakeholders to explore a range of strategies to assess, communicate, and reduce disease risk, in order to maximize the efficiency of genomic medicine delivery, and promote health equity. PROJECT NARRATIVE Major challenges exist for including genomic information broadly into clinical risk assessment in diverse and multi-ethnic populations, and issues of bias in research databases, community engagement, and access to leading-edge digital technology disproportionately impact underserved populations. We will recruit 2,500 ethnically diverse and medically underserved NYC patients into a clinical trial to estimate individualized disease risk and investigate the impact of genomic risk communication to patients and their physicians, including trust and understanding of the information, uptake of recommended interventions, and psychosocial outcomes. We will work closely with our NHGRI, eMERGE, community, industry, and health system partners, and use lessons learned to inform future research and clinical strategies, maximize efficiency for genomic medicine delivery, and promote health equity.",Genomic risk in clinic care to promote health equity in New York City patients,9988767,U01HG011176,"['Address', 'Adult', 'Artificial Intelligence', 'Attitude', 'Behavioral', 'Benchmarking', 'Caring', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Trials', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Data', 'Databases', 'Development', 'Disease', 'Ensure', 'Ethnic group', 'European', 'Family', 'Genetic', 'Genomic approach', 'Genomic medicine', 'Genomics', 'Health', 'Health Promotion', 'Health system', 'Healthcare Systems', 'Individual', 'Industry', 'Institutes', 'Intervention', 'Investments', 'Knowledge', 'Learning', 'Life', 'Link', 'Methods', 'Mission', 'Multilingualism', 'National Human Genome Research Institute', 'New York City', 'Outcome', 'Participant', 'Patient Education', 'Patient Self-Report', 'Patients', 'Perception', 'Phase', 'Physicians', 'Population', 'Population Genetics', 'Population Heterogeneity', 'Predictive Value', 'Primary Health Care', 'Qualitative Research', 'Race', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Risk Assessment', 'Risk stratification', 'Role', 'Science', 'Scientist', 'Speed', 'Technology', 'Testing', 'Trust', 'Underserved Population', 'Urban Health', 'Work', 'biobank', 'clinical care', 'clinical decision-making', 'clinical research site', 'clinical risk', 'cohort', 'data resource', 'design', 'digital', 'disorder risk', 'ethnic diversity', 'experience', 'flexibility', 'health application', 'health disparity', 'health equity', 'health literacy', 'high risk', 'improved', 'industry partner', 'innovation', 'interdisciplinary collaboration', 'mathematical ability', 'medically underserved', 'member', 'patient engagement', 'patient population', 'pediatric patients', 'polygenic risk score', 'preference', 'psychologic', 'psychosocial', 'recruit', 'social', 'tool', 'uptake']",NHGRI,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,U01,2020,1401278,0.04867002287127233
"Computational Methods for Next-Generation Comparative Genomics PROJECT SUMMARY Recent advances in regulatory genomics, especially 3D genome organization in cell nucleus, suggest that existing methods for cross-species comparisons are limited in their ability to fully understand the evolution of non-coding genome function. In particular, it is known that genomes are compartmentalized to distinct compartments in the nucleus such as nuclear lamina and nuclear speckles. Such nuclear compartmentalization is an essential feature of higher-order genome organization and is linked to various important genome functions such as DNA replication timing and transcription. Unfortunately, to date no study exists that directly compares nuclear compartmentalization between human and other mammals. In addition, there are no computational models available that consider the continuous nature of multiple features of nuclear compartmentalization and function, which is critical to integrate genome-wide functional genomic data and datasets that measure cytological distance to multiple compartments across species. In this project, we will develop novel algorithms and generate new datasets to directly address two key questions: (1) How to identify the evolutionary patterns of nuclear compartmentalization? (2) What types of sequence evolution may drive spatial localization changes across species? The proposed project represents the first endeavor in comparative genomics for nuclear compartmentalization. Our Specific Aims are: (1) Developing new probabilistic models for identifying evolutionary patterns of nuclear compartmentalization. (2) Identifying genome-wide evolutionary patterns of nuclear compartmentalization in primate species based on TSA-seq and Repli-seq. (3) Developing new algorithms to connect sequence features to nuclear compartmentalization through cross-species comparisons. Successful completion of these aims will result in novel computational tools and new datasets that will be highly valuable for the comparative genomics community. Integrating the new computational tools and unique datasets will provide invaluable insights into the relationship between sequence evolution and changes in nuclear genome organization in mammalian species. Therefore, the proposed research is expected to advance comparative genomics to a new frontier and provide new perspectives for studying human genome function PROJECT NARRATIVE The proposed research is relevant to public health because the outcome of the project is expected to enhance the analyses of nuclear genome organizations across primate species to better understand genome function and human biology. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Computational Methods for Next-Generation Comparative Genomics,9959498,R01HG007352,"['3-Dimensional', 'Address', 'Algorithms', 'CRISPR/Cas technology', 'Cell Nucleus', 'Cells', 'Communities', 'Complement', 'Computer Models', 'Computing Methodologies', 'Crete', 'Cytology', 'DNA Insertion Elements', 'DNA Replication Timing', 'Data Set', 'Development', 'Disease', 'Evolution', 'Genetic Transcription', 'Genome', 'Genomics', 'Health', 'Human', 'Human Biology', 'Human Genome', 'Knowledge', 'Lamin Type B', 'Link', 'Machine Learning', 'Mammals', 'Maps', 'Measures', 'Mediating', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Molecular Profiling', 'Nature', 'Nuclear', 'Nuclear Lamina', 'Outcome', 'Pattern', 'Phenotype', 'Primates', 'Psyche structure', 'Public Health', 'Research', 'Signal Transduction', 'Statistical Models', 'Techniques', 'Time', 'Translating', 'United States National Institutes of Health', 'Untranslated RNA', 'Visualization', 'base', 'comparative genomics', 'computerized tools', 'frontier', 'functional genomics', 'genetic variant', 'genome-wide', 'genomic data', 'genomic locus', 'improved', 'insight', 'mental function', 'next generation', 'novel', 'predictive modeling']",NHGRI,CARNEGIE-MELLON UNIVERSITY,R01,2020,360268,0.06757966858378074
"Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement Project Summary/Abstract The mandate of the PsychENCODE Data Analysis Core (DAC) includes the development of novel integrative methodologies to construct a coherent interpretational framework for the data emerging from the consortium. The complexity of building such a framework lies in the diversity of experimental assays and their associated confounding factors, as well as in the inherent uncertainty regarding how the various target biological components function together. As a result, any analytical and computational methods would need to capture this high dimensionality of structure in the data. While classical, parallel computation advances at an incredible pace and continues to serve the needs of the research community, our experience with the ever- increasing complexity of neuropsychiatric datasets has motivated us to also look at other promising technological avenues. Accordingly, motivated by recent developments in the field of quantum computing (QC), we herein explore the use of QC algorithms as applied to two problems of relevance to the PsychENCODE DAC: (1) the prediction of brain-specific enhancers based on variants and functional genomic assays (Aim S1; related to Aim 1 of the parent grant); and (2) the calculation of the contributions of cell types to tissue-level gene expression and to the occurrence of psychiatric disorders like schizophrenia, autism spectrum disorder and bipolar disorder (Aim S2; related to Aim 1 of the parent grant). The nascency of QC hardware technologies and the complexity of simulating quantum algorithms on classical computing resources means that our exploration will be confined to smaller, judiciously chosen datasets.Nevertheless, the work in this supplement will serve to evaluate future prospects for the use of QC algorithms and hardware in genomic analyses. We also consider two different paradigms of QC, the quantum annealer and the quantum gate model, and weigh their efficiency relative to classical computing. Finally, we will incorporate the QC and classical predictions into PsychENCODE consortium's database and online portal for visualizing the relationships between different genetic and genomic elements, and evaluate corroborating evidence for the predictions (Aim S3; related to Aim 2 of the parent grant). Project Narrative The PsychENCODE consortium has conducted extensive functional genomic analyses of samples from individuals diagnosed with psychiatric disorders aim to discover the complex biological architecture that lead from genetic and epigenetic markers of disease to the observed phenotypes. To reveal this underlying structure, the consortium relies on the use of sophisticated computational methods, including machine learning techniques, implemented on cutting-edge massively parallel computing resources by the consrtium’s Data Analysis Core (DAC). However, the scale and complexity of the tasks place significant burdens on these resources, and suggest the need for exploring alternative computing hardware technologies. This supplement to the DAC parent grant evaluates the promise of the emerging field of quantum computing to speed up large-scale computations and more efficiently explore the model landscape, using a comparative analysis of classical and quantum computing algorithms applied to problems relevant to the PsychENCODE DAC: the annotation of brain-specific enhancers and the quantification of cell-type contributions to bulk tissue gene expression.",Discovery and validation of neuronal enhancers as development of psychiatric disorders supplement,10047746,U01MH116492,"['Algorithms', 'Architecture', 'Biological', 'Biological Assay', 'Bipolar Disorder', 'Brain', 'Cells', 'Communities', 'Complex', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Marker', 'Electronic Medical Records and Genomics Network', 'Elements', 'Enhancers', 'Future', 'Gene Expression', 'Genetic', 'Genetic Markers', 'Genomics', 'Goals', 'Individual', 'Lead', 'Least-Squares Analysis', 'Machine Learning', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Neurons', 'Output', 'Performance', 'Phenotype', 'Publishing', 'Research', 'Resources', 'Running', 'Sampling', 'Schizophrenia', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Toy', 'Training', 'Uncertainty', 'Validation', 'Variant', 'Visualization', 'Work', 'analytical method', 'autism spectrum disorder', 'base', 'cell type', 'comparative', 'computing resources', 'data framework', 'design', 'epigenetic marker', 'epigenomics', 'experience', 'functional genomics', 'high dimensionality', 'neuropsychiatry', 'novel', 'parallel computer', 'parent grant', 'prototype', 'quantum', 'quantum computing', 'simulation', 'transcriptome sequencing', 'web portal']",NIMH,UNIV OF MASSACHUSETTS MED SCH WORCESTER,U01,2020,195697,0.011180021789901243
"Computational tools for regulome mapping using single-cell genomic data Project Summary Understanding how genes' activities are controlled is crucial for elucidating the basic operating rules of biology and molecular mechanisms of diseases. Recent innovations in single-cell genomic technologies have opened the door to analyzing a variety of functional genomic features in individual cells. These technologies enable scientists to systematically discover unknown cell subpopulations in complex tissue and disease samples, and allow them to reconstruct a sample's gene regulatory landscape at an unprecedented cellular resolution. Despite these promising developments, many challenges still exist and must be overcome before one can fully decode gene regulation at the single-cell resolution. In particular, current technologies lack the ability to accurately measure the activity of each individual cis-regulatory element (CRE) in a single cell. They also cannot measure all functional genomic data types in the same cell. Moreover, the prevalent technical biases and noises in single-cell genomic data make computational analysis non-trivial. With rapid growth of data, lack of computational tools for data analysis has become a rate-limiting factor for effective applications of single-cell genomic technologies.  The objective of this proposal is to develop computational and statistical methods and software tools for mapping and analyzing gene regulatory landscape using single-cell genomic data. Our Aim 1 addresses the challenge of accurately measuring CRE activities in single cells using single-cell regulome data. Regulome, deﬁned as the activities of all cis-regulatory elements in a genome, contains crucial information for understanding gene regulation. The state-of-the-art technologies for mapping regulome in a single cell produce sparse data that cannot accurately measure activities of individual CREs. We will develop a new computational framework to allow more accurate analysis of individual CREs' activities in single cells using sparse data. Our Aim 2 addresses the challenge of collecting multiple functional genomic data types in the same cell. We will develop a method that uses single-cell RNA sequencing (scRNA-seq), the most widely used single-cell functional genomic technology, to predict cells' regulatory landscape. Since most scRNA-seq datasets do not have accompanying single-cell data for other -omics data types, our method will also signiﬁcantly expand the utility and increase the value of scRNA- seq experiments. Our Aim 3 addresses the challenge of integrating different data types generated by different single-cell genomic technologies from different cells. We will develop a method to align single-cell RNA-seq and single-cell regulome data to generate an integrated map of transcriptome and regulome.  Upon completion of this proposal, we will deliver our methods through open-source software tools. These tools will be widely useful for analyzing and integrating single-cell regulome and transcriptome data. By addressing several major challenges in single-cell genomics, our new methods and tools will help unleash the full potential of single-cell genomic technologies for studying gene regulation. As such, they can have a major impact on advancing our understanding of both basic biology and human diseases. Project Narrative Understanding how genes' activities are controlled at single-cell resolution is crucial for studying human diseases. This proposal will develop a coordinated set of computational and statistical methods and software tools for mapping and analyzing gene regulatory programs using single-cell genomic data. These methods and tools will allow scientists to more accurately and comprehensively reconstruct gene regulatory landscape of individual cells in complex tissue and disease samples, and they can have a major impact on advancing our understanding of both basic biology and human diseases.",Computational tools for regulome mapping using single-cell genomic data,10001077,R01HG010889,"['Address', 'Atlases', 'Behavior', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Cells', 'Cellular Assay', 'Chromatin', 'Complex', 'Computer Analysis', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Disease', 'Emerging Technologies', 'Foundations', 'Gene Expression Regulation', 'Genes', 'Genome', 'Genomics', 'Human', 'Immune system', 'Individual', 'Knowledge', 'Malignant Neoplasms', 'Maps', 'Measures', 'Methods', 'Modality', 'Molecular', 'Multiomic Data', 'Noise', 'Organ', 'Phase', 'Population', 'Regulator Genes', 'Regulatory Element', 'Research Personnel', 'Resolution', 'Sampling', 'Scientist', 'Software Tools', 'Statistical Methods', 'Stem Cell Development', 'System', 'Technology', 'Therapeutic', 'Tissues', 'Training', 'Transposase', 'base', 'computer framework', 'computerized tools', 'epigenome', 'experimental study', 'functional genomics', 'genomic data', 'histone modification', 'human disease', 'innovation', 'multiple data types', 'multiple omics', 'novel strategies', 'open source', 'predictive modeling', 'programs', 'rapid growth', 'single cell analysis', 'single cell technology', 'single-cell RNA sequencing', 'supervised learning', 'tool', 'transcriptome', 'transcriptome sequencing', 'user-friendly']",NHGRI,JOHNS HOPKINS UNIVERSITY,R01,2020,409375,-0.022871361330505907
"Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation Project Summary/Abstract: With the surge of large genomics data, there is an immense increase in the breadth and depth of different omics datasets and an increasing importance in the topic of privacy of individuals in genomic data science. Detailed genetic and environmental characterization of diseases and conditions relies on the large-scale mining of functional genomics data; hence, there is great desire to share data as broadly as possible. However, there is a scarcity of privacy studies focused on such data. A key first step in reducing private information leakage is to measure the amount of information leakage in functional genomics data, particularly in different data file types. To this end, we propose to to derive information-theoretic measures for private information leakage in different data types from functional genomics data. We will also develop various file formats to reduce this leakage during sharing. We will approach the privacy analysis under three aims. First, we will develop statistical metrics that can be used to quantify the sensitive information leakage from raw reads. We will systematically analyze how linking attacks can be instantiated using various genotyping methods such as single nucleotide variant and structural variant calling from raw reads, signal profiles, Hi-C interaction matrices, and gene expression matrices. Second, we will study different algorithms to implement privacy-preserving transformations to the functional genomics data in various forms. Particularly, we will create privacy-preserving file formats for raw sequence alignment maps, signal track files, three-dimensional interaction matrices, and gene expression quantification matrices that contain information from multiple individuals. This will allow us to study the sources of sensitive information leakages other than raw reads, for example signal profiles, splicing and isoform transcription, and abnormal three-dimensional genomic interactions. Third, we will investigate the reads that can be mapped to the microbiome in the raw human functional genomics datasets. We will use inferred microbial information to characterize private information about individuals, and then combine the microbial information with the information from human mapped reads to increase the re-identification accuracy in the linking attacks described in the second aim. We will use the tools to quantify the sensitive information and privacy-preserving file formats in the available datasets from large sequencing projects, such as the ENCODE, The Cancer Genome Atlas, 1,000 Genomes, gEUVADIS, and Genotype-Tissue Expression projects. Project Narrative: Sharing large-scale functional genomics data is critical for scientific discovery, but comes with important privacy concerns related to the possible misuse of such data. This proposal will quantify and manage the rieslkasted to releasing functional genomics datasets, based on integrating inferred genotypes from the raw sequence files, signal tracks, and microbiome mapped sequences. Finally, we will develop file formats, statistical methodologies, and related software for anonymization of functional genomics data that enable open sharing.",Enhancing open data sharing for functional genomics experiments: Measures to quantify genomic information leakage and file formats for privacy preservation,9970939,R01HG010749,"['3-Dimensional', 'Address', 'Algorithms', 'Assessment tool', 'Biology', 'ChIP-seq', 'Code', 'Computer software', 'Consent', 'DNA sequencing', 'Data', 'Data Files', 'Data Science', 'Data Set', 'Databases', 'Diet', 'Disease', 'Environment', 'Equilibrium', 'Extravasation', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Transcription', 'Genome', 'Genomics', 'Genotype', 'Genotype-Tissue Expression Project', 'Glean', 'Human', 'Individual', 'Institutes', 'Laws', 'Learning', 'Letters', 'Life Style', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical Research', 'Methodology', 'Methods', 'Mining', 'Motivation', 'Participant', 'Patients', 'Phenotype', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Protein Isoforms', 'Protocols documentation', 'Provider', 'Pythons', 'Quantitative Trait Loci', 'RNA Splicing', 'Research Personnel', 'Risk', 'Risk Assessment', 'Sampling', 'Sequence Alignment', 'Signal Transduction', 'Single Nucleotide Polymorphism', 'Smoker', 'Source', 'Structure', 'Techniques', 'The Cancer Genome Atlas', 'Tissues', 'Variant', 'base', 'clinically relevant', 'computerized data processing', 'data mining', 'data sharing', 'experimental study', 'file format', 'functional genomics', 'genome sequencing', 'genomic data', 'human tissue', 'interest', 'large datasets', 'microbial', 'microbiome', 'open data', 'privacy preservation', 'social', 'tool', 'transcriptome sequencing']",NHGRI,YALE UNIVERSITY,R01,2020,523409,0.05100466123824034
"Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases ﻿    DESCRIPTION (provided by applicant) Over the past 10 years human genetics studies made unprecedented numbers of discoveries relating genome variation to common diseases. While it is unquestionably true that we have learned substantially from the discoveries made to date, we must also acknowledge that with respect to the identification and characterization of the genome variation affecting risk of common human diseases - those accounting for the overwhelming majority of public health care expenditures - our discoveries have not generated nearly the knowledge of disease etiology that we would have expected given the sheer number of these discoveries. The combination of these observations coupled with the dramatic reduction in the cost of sequencing is driving the case for moving to whole genome sequencing studies for common disease. We have focused our application on three critical challenges for methods development and analysis of whole genome sequence data. While there has been substantial progress in methods development for analysis of exome sequencing, with gene-based tests that are relatively robust to the direction of effects of rare coding variants as well as the proportionof the gene's rare variants contributing to phenotype, it is clear that methods integrating the analysis of common and rare variation as well as functional genomics data will be essential for appropriate analysis of whole genome sequence data. Thus, we propose in Specific Aim 1 to develop novel methods, software and analysis pipelines for integrated analysis of common and rare variants for the analysis of whole genome sequence on 50,000- 100,000 individuals for any given common disease, and in Specific Aim 2 to develop and apply novel approaches for prioritizing results from these large-scale sequencing studies using BioVU, the 200,000 member biobank at Vanderbilt that is associated with 30 years of high quality electronic health records, and in Specific Aim 3 to develop queriable results databases and a comprehensive web portal to serve results of our studies on the sequence data for both the internal sequencing community and for the broader scientific community. PUBLIC HEALTH RELEVANCE Large-scale whole genome sequencing studies are being carried out to understand the genetic architecture of human complex diseases. It remains challenging to decipher the genetic mechanisms of disease etiology. In this application we aim to develop a suite of statistical and computational methods to identify genes and variants associated with complex disease and use BioVU, a DNA BioBank linked to electronic health records, to validate and investigate genetic architecture of multiple complex diseases.","Analysis, Validation and Resource Creation for Genome Sequencing of Complex Diseases",10116927,U01HG009086,"['Accounting', 'Affect', 'Automobile Driving', 'Code', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Coupled', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Disease', 'Electronic Health Record', 'Etiology', 'Face', 'Generations', 'Genes', 'Genetic', 'Genetic Predisposition to Disease', 'Genetic study', 'Genome', 'Government', 'Health Expenditures', 'Human', 'Human Genetics', 'Individual', 'Knowledge', 'Large-Scale Sequencing', 'Link', 'Machine Learning', 'Methods', 'National Human Genome Research Institute', 'Phenotype', 'Policies', 'Public Health', 'Regulator Genes', 'Reporting', 'Research', 'Resources', 'Risk', 'Science', 'Statistical Methods', 'Statistical Models', 'Technology', 'Time', 'Tissues', 'Untranslated RNA', 'Validation', 'Variant', 'analysis pipeline', 'base', 'biobank', 'cost', 'design', 'disease phenotype', 'exome', 'exome sequencing', 'expectation', 'experience', 'experimental study', 'functional genomics', 'genetic architecture', 'genetic testing', 'genetic variant', 'genome sciences', 'genome sequencing', 'genome wide association study', 'genomic data', 'human disease', 'human genomics', 'improved', 'insight', 'member', 'method development', 'novel', 'novel strategies', 'personalized medicine', 'pleiotropism', 'public health relevance', 'rare variant', 'success', 'web portal', 'whole genome']",NHGRI,VANDERBILT UNIVERSITY,U01,2020,864183,0.043457165064557726
"TOOLS FOR NORMALIZING AND INTERPRETING THE CLINICAL ACTIONABILITY OF GENOMIC VARIANTS PROJECT SUMMARY/ABSTRACT The availability of high-throughput, low-cost sequencing has transformed the landscape of biomedical research by dramatically expanding our capacity to interrogate the sequence of the human genome. Consequently, there has been an explosion of biomedical literature describing the role of specific genomic variants and their impact on human diseases. These advances are bringing sequencing into the clinic to shape clinical practice from the patient’s genomic content, a paradigm colloquially referred to as genomic or precision medicine. There remain many obstacles to fully realizing our potential in the era of precision medicine. Among them is a recognized need for robust, well-engineered systems that provide knowledge about genomic variants and their role in disease. Ideally, such systems would provide a comprehensive summary of all knowledge that is relevant to the patient’s unique genomic content. An early bottleneck to realizing precision medicine was that, despite the substantial literature and several established knowledgebases that define interactions between drugs and genes, querying across them was extremely challenging. In response to this need, the Drug-Gene Interaction database (DGIdb, dgidb.org) was developed. Through a combination of automated processing and manual curation, drug-gene interaction information was collected, structured, and connected (normalized) from these diverse sources of data and entered into a database with a user-friendly search interface and an application programming interface (API). However, linking drug and drug-gene interaction concepts across resources remains an extremely challenging task, and aggregated drug-gene interactions are also challenging to represent in a way that highlights the utility of the collected knowledge for precision medicine efforts. This proposal seeks to improve our ability to normalize and interpret drug-gene interactions corresponding to patient genomic variants. We will achieve this goal through two specific aims. First, the DGIdb normalization routines will be improved through incorporation of new content and features. Among these, the DGIdb will support collections of drugs, including combination therapies and drug classes. Also, the DGIdb will have new community submission and curation features, allowing users to incorporate new knowledge into the database. Second, the Variant Interpretation Aggregator database (VIAdb) will be created to normalize knowledge across several disparate sources focused on the clinical interpretations of genomic variants. The VIAdb will operate as a stand-alone web tool and API and will behave as a source of relevant interpretations to DGIdb. Finally, we will develop techniques for automated identification of drug-gene interactions and variant interpretation consensus to assist community curation efforts. If successful, this research will improve breadth and consistency of variant interpretations and drug-gene interactions for precision medicine efforts. PROJECT NARRATIVE This research will improve our ability to interpret genomic variations in human patients in support of precision medicine efforts. Specifically, it will provide web-based tools for identifying potential therapies that specifically target the patient’s individual genes or variants, and an assessment of the clinical actionability of those drugs.",TOOLS FOR NORMALIZING AND INTERPRETING THE CLINICAL ACTIONABILITY OF GENOMIC VARIANTS,9938666,K99HG010157,"['Biological Markers', 'Biomedical Research', 'Client', 'Clinic', 'Clinical', 'Clinical Trials', 'Clinical assessments', 'Cohort Studies', 'Collection', 'Combined Modality Therapy', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Disease', 'Engineering', 'Explosion', 'Genes', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Internet', 'Knowledge', 'Level of Evidence', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Procedures', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Role', 'Services', 'Shapes', 'Side', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'The Vanderbilt-Ingram Cancer Center at the Vanderbilt University', 'Variant', 'Visualization', 'Work', 'application programming interface', 'base', 'cancer type', 'clinical practice', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'design', 'drug discovery', 'drug resource', 'gene interaction', 'genetic variant', 'genomic variation', 'human disease', 'improved', 'individual patient', 'knowledge base', 'learning strategy', 'molecular marker', 'novel', 'novel therapeutics', 'open source', 'precision medicine', 'profiles in patients', 'programs', 'response', 'therapeutic biomarker', 'tool', 'tumor', 'user-friendly', 'web app', 'web interface', 'web-based tool']",NHGRI,WASHINGTON UNIVERSITY,K99,2020,18900,0.04035525324432359
"Tools for Normalizing and Interpreting the Clinical Actionability of Genomic Variants PROJECT SUMMARY/ABSTRACT The availability of high-throughput, low-cost sequencing has transformed the landscape of biomedical research by dramatically expanding our capacity to interrogate the sequence of the human genome. Consequently, there has been an explosion of biomedical literature describing the role of specific genomic variants and their impact on human diseases. These advances are bringing sequencing into the clinic to shape clinical practice from the patient’s genomic content, a paradigm colloquially referred to as genomic or precision medicine. There remain many obstacles to fully realizing our potential in the era of precision medicine. Among them is a recognized need for robust, well-engineered systems that provide knowledge about genomic variants and their role in disease. Ideally, such systems would provide a comprehensive summary of all knowledge that is relevant to the patient’s unique genomic content. An early bottleneck to realizing precision medicine was that, despite the substantial literature and several established knowledgebases that define interactions between drugs and genes, querying across them was extremely challenging. In response to this need, the Drug-Gene Interaction database (DGIdb, dgidb.org) was developed. Through a combination of automated processing and manual curation, drug-gene interaction information was collected, structured, and connected (normalized) from these diverse sources of data and entered into a database with a user-friendly search interface and an application programming interface (API). However, linking drug and drug-gene interaction concepts across resources remains an extremely challenging task, and aggregated drug-gene interactions are also challenging to represent in a way that highlights the utility of the collected knowledge for precision medicine efforts. This proposal seeks to improve our ability to normalize and interpret drug-gene interactions corresponding to patient genomic variants. We will achieve this goal through two specific aims. First, the DGIdb normalization routines will be improved through incorporation of new content and features. Among these, the DGIdb will support collections of drugs, including combination therapies and drug classes. Also, the DGIdb will have new community submission and curation features, allowing users to incorporate new knowledge into the database. Second, the Variant Interpretation Aggregator database (VIAdb) will be created to normalize knowledge across several disparate sources focused on the clinical interpretations of genomic variants. The VIAdb will operate as a stand-alone web tool and API and will behave as a source of relevant interpretations to DGIdb. Finally, we will develop techniques for automated identification of drug-gene interactions and variant interpretation consensus to assist community curation efforts. If successful, this research will improve breadth and consistency of variant interpretations and drug-gene interactions for precision medicine efforts. PROJECT NARRATIVE This research will improve our ability to interpret genomic variations in human patients in support of precision medicine efforts. Specifically, it will provide web-based tools for identifying potential therapies that specifically target the patient’s individual genes or variants, and an assessment of the clinical actionability of those drugs.",Tools for Normalizing and Interpreting the Clinical Actionability of Genomic Variants,10212005,K99HG010157,"['Biological Markers', 'Biomedical Research', 'Client', 'Clinic', 'Clinical', 'Clinical Trials', 'Clinical assessments', 'Cohort Studies', 'Collection', 'Combined Modality Therapy', 'Communities', 'Consensus', 'Data', 'Databases', 'Development', 'Disease', 'Engineering', 'Explosion', 'Genes', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Human', 'Human Genome', 'Internet', 'Knowledge', 'Level of Evidence', 'Link', 'Literature', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Methods', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Procedures', 'Recording of previous events', 'Reporting', 'Research', 'Research Personnel', 'Resource Development', 'Resources', 'Role', 'Services', 'Shapes', 'Side', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'The Vanderbilt-Ingram Cancer Center at the Vanderbilt University', 'Variant', 'Visualization', 'Work', 'application programming interface', 'base', 'cancer type', 'clinical practice', 'clinically actionable', 'cohort', 'cost', 'data modeling', 'design', 'drug discovery', 'drug resource', 'gene interaction', 'genetic variant', 'genomic variation', 'human disease', 'improved', 'individual patient', 'knowledge base', 'learning strategy', 'molecular marker', 'novel', 'novel therapeutics', 'open source', 'precision medicine', 'profiles in patients', 'programs', 'response', 'therapeutic biomarker', 'tool', 'tumor', 'user-friendly', 'web app', 'web interface', 'web-based tool']",NHGRI,RESEARCH INST NATIONWIDE CHILDREN'S HOSP,K99,2020,92118,0.04035525324432359
"Organizational and Cultural Dynamics in Genomics Companies: Industry Engagement in Navigating Social and Ethical Issues This K99/R00 Award is designed to generate scholarship and interventions to guide genomics companies towards more just practices. It does so through a five-year training and research project, which investigates perspectives from members of the genomics industry, and leverages them to inform normative analyses and identify feasible paths towards concrete change. The project addresses issues of price, access and industrial control, with a focus on the ethics of profit and social responsibility. Through the proposed training program, the project prepares the candidate as an independent scholar whose research program links empirical and normative research with practical interventions to positively influence ethics and justice in private sector genomics. The first phase of research (conducted during the K phase, alongside training activities) is a comparative study examining social and cultural factors involved in perceptions of ethical issues among members of industry. The comparative study focuses on three biotech hubs in the US (the Bay Area, San Diego, and Boston), as well as an additional site in South Africa (Cape Town). The South African site helps place investigation of the US industry in light of a shifting global industry, and especially South Africa’s policy focus on genomics for population health (as well as its role in the H3Africa initiative). The study also considers industry subsector and individual company culture, in examining influences on industry approaches to social and ethical issues. During its R phase, the project then involves a systematic normative analysis of profit and social obligation in the genomics industry, based on key theories from bioethics and business ethics. The project will then feed this scholarly analysis back to industry stakeholders; using a Delphi study, it allows members of industry to use results of the normative analysis in strategizing interventions with the greatest likelihood of influencing the genomics industry in ethically desirable ways. This project addresses the NHGRI ELSI research priorities of genomic equity and social justice, as well as genomics and public health. Its career development plan focuses on training in normative analysis and Delphi methods, as well as content training in genomics and entrepreneurship. The training will be conducted at Johns Hopkins University, where the school’s preeminent bioethics center houses world experts in qualitative and quantitative empirical bioethics, alongside the University’s leading research and training in the biomedical sciences and their application in industry. The integrated research and training plan will prepare the candidate as an independent ELSI scholar with a rigorous research program focused on empirical and normative analysis of the business of biomedicine and genomics, and engagement with industry stakeholders. This Pathway to Independence Award (K99/R00) investigates and leverages perspectives from members of the health-related private sector genomics industry, to develop guidance for improving approaches to social and ethical issues in the industry. It does so through in-depth qualitative analysis (interviews, cases studies, comparative analysis), scholarly normative analysis (drawing on theories from bioethics and business ethics), and a Delphi process of iterative questionnaires with industry stakeholders, aimed at strategizing concrete change regarding social obligations of the industry. The research and training program is designed to contribute to a more just distribution of the benefits of genomic technologies.",Organizational and Cultural Dynamics in Genomics Companies: Industry Engagement in Navigating Social and Ethical Issues,10017284,K99HG010499,"['Address', 'Agreement', 'American', 'Area', 'Attention', 'Award', 'Back', 'Bioethics', 'Bioethics Consultants', 'Biotechnology', 'Boston', 'Businesses', 'California', 'Case Study', 'Comparative Study', 'Complement', 'Corporate Ethics', 'DNA', 'Decision Making', 'Delphi Study', 'Development', 'Development Plans', 'Diagnostic', 'Empirical Research', 'Employee', 'Entrepreneurship', 'Ethical Issues', 'Ethics', 'Face', 'Genetic', 'Genomic approach', 'Genomics', 'Goals', 'Government', 'Health', 'Health Technology', 'India', 'Individual', 'Industrialization', 'Industry', 'Inequality', 'Intervention', 'Interview', 'Investigation', 'Justice', 'Licensing', 'Light', 'Link', 'Machine Learning', 'Methods', 'Modeling', 'National Human Genome Research Institute', 'Pathway interactions', 'Patients', 'Perception', 'Phase', 'Play', 'Policies', 'Price', 'Privacy', 'Private Enterprises', 'Private Sector', 'Privatization', 'Process', 'Public Health', 'Questionnaires', 'Research', 'Research Priority', 'Research Project Grants', 'Research Training', 'Role', 'Scholarship', 'Schools', 'Science', 'Shapes', 'Silicon', 'Site', 'Social Justice', 'Social Obligations', 'Social Responsibility', 'South Africa', 'South African', 'Strategic Planning', 'Structure', 'Technology', 'Therapeutic', 'Training', 'Training Activity', 'Training Programs', 'Universities', 'Work', 'base', 'career development', 'comparative', 'curative treatments', 'design', 'drug development', 'experience', 'gene therapy', 'genome editing', 'improved', 'individual response', 'innovation', 'insight', 'member', 'nebula', 'next generation sequencing', 'novel diagnostics', 'novel therapeutics', 'operation', 'population health', 'programs', 'research and development', 'social', 'theories', 'training project']",NHGRI,JOHNS HOPKINS UNIVERSITY,K99,2020,54450,0.031121367090132288
"Center for Sub-Cellular Genomics A cell is a highly complex system with distributed molecular physiologies in structured sub- cellular compartments whose interplay with the nuclear genome determine the functional characteristics of the cell. A classic example of distributed genomic processes is found in neurons. Learning and memory requires modulation of individual synapses through RNA localization, localized translation, and localized metabolites such as those from dendritic mitochondria. Dendrites of neurons integrate distributed synaptic signals into both electrical and nuclear transcriptional response. Dysfunction of these distributed genomic functions in neurons can result in a broad spectrum of neuropsychiatric diseases such as bipolar and depressive disorders, autism, among others. Understanding complex genomic interactions within a single cell requires new technologies: we need nano-scale ability to make genome-wide measurements at highly localized compartments and to effect highly localized functional genomic manipulations, especially in live tissues. To address this need, we propose to establish a Center for Sub-Cellular Genomics using neurons as model systems. The center will develop new optical and nanotechnology approaches to isolate sub-cellular scale components for genomic, metabolomics, and lipidomic analyses. The center will also develop new mass spectrometry methods, molecular biology methods, and informatics models to create a platform technology for sub-cellular genomics. Many human diseases, especially neuropsychiatric diseases, can be traced to dysfunction of organelles and other sub-cellular components. This project will create novel technologies to study function and dysfunction of sub-cellular processes and apply them to study of neurons.",Center for Sub-Cellular Genomics,9973075,RM1HG010023,"['Address', 'Alzheimer&apos', 's Disease', 'Attention', 'Automobile Driving', 'Awareness', 'Biological Assay', 'Biological Models', 'Bipolar Disorder', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Cellular biology', 'Characteristics', 'Chromatin', 'Communities', 'Complex', 'Data', 'Dendrites', 'Depressive disorder', 'Devices', 'Disease', 'Education and Outreach', 'Educational workshop', 'Functional disorder', 'Genetic Transcription', 'Genome', 'Genomics', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Informatics', 'Institution', 'Investigation', 'Learning', 'Liquid substance', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Memory', 'Mental Depression', 'Methods', 'Mitochondria', 'Modeling', 'Molecular', 'Molecular Biology', 'Motivation', 'Nanotechnology', 'Neurodegenerative Disorders', 'Neurons', 'Nuclear', 'Optical Methods', 'Optics', 'Organelles', 'Output', 'Pathway interactions', 'Phenotype', 'Philadelphia', 'Physiology', 'Process', 'Proteins', 'RNA', 'Regulation', 'Research Project Grants', 'Resolution', 'Risk', 'Rodent', 'Role', 'Schizophrenia', 'Signal Transduction', 'Structure', 'Synapses', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Traumatic Brain Injury', 'Variant', 'Work', 'autism spectrum disorder', 'base', 'deep learning', 'epigenomics', 'exosome', 'experience', 'flexibility', 'functional genomics', 'genome-wide', 'human disease', 'informatics tool', 'macromolecule', 'metabolomics', 'multimodality', 'nanoscale', 'neuropsychiatric disorder', 'new technology', 'novel', 'outreach', 'response', 'subcellular targeting', 'technology development', 'technology validation', 'transcriptome', 'welfare', 'whole genome']",NHGRI,UNIVERSITY OF PENNSYLVANIA,RM1,2020,500000,0.03585550551534488
"Privacy-preserving genomic medicine at scale 1 Project Summary  2  3 High-throughput sequencing, biomedical imaging, and electronic health record technologies are 4 generating health-related datasets of unprecedented scale. Integrative analysis of these  5 resources promises to reveal new biology and drive personal and precision medicine. Yet, the  6 sensitive nature of these data often requires that they be kept in isolated silos, limiting their 7 usefulness to science. The goal of this project is to develop innovative privacy-preserving  8 algorithms to enable data sharing and drive genomic medicine. Crucially, we will draw upon our  9 past success in secure genome analysis and algorithmic expertise in computational biology to 10 address the imminent need to perform complex integrative analyses securely and at scale. 11 Current privacy-preserving tools are prohibitively too costly to perform the complex 12 calculations required in genomic analysis. We previously leveraged the highly structured nature 13 of biological data and novel optimization strategies to implement efficient pipelines for secure 14 genome-wide association studies (GWAS) and drug interaction predictions which scaled to 15 millions of samples. In this project, we will further exploit the unique properties of biomedical data 16 to: (i) develop secure integrative analysis methods for genomic medicine; (ii) develop an easy-to- 17 use programming environment with advanced automated optimizations to facilitate the adoption 18 of privacy-preserving analyses; and (iii) promote the use of our privacy techniques to gain novel 19 biological insights through large-scale collaborative genetic studies of multi-ethnic cohorts. 20 With co-I’s Amarasinghe (MIT) and Cho (Broad Institute), we aim to apply these tools to 21 realize the first multi-institution, multi-national secure genetic studies with our partners at the 22 Swiss Personalized Health Network, UK Biobank, Finnish FinnGen, All of Us, NIH NCBI, Broad 23 and Barcelona Supercomputing Center (Letters of Support). We will also use our privacy- 24 preserving approaches to study genomic origins of polygenic traits for disease as well as 25 neuroimaging and other clinical phenotypes. We will continue to actively integrate our methods 26 into community standards (MPEG-G, GA4GH). 27 Successful completion of these aims will result in computational methods and open-source, 28 easy-to-use, production-grade implementations that open the door to secure integration and 29 analysis of massive sets of sensitive genomic and clinical data. With input from our collaborations, 30 we will build these tools and apply them to better understand the molecular causes of human 31 health and its translation to the clinic. Project Narrative Combining genomic and health-related data from millions of patients will empower the development of clinically relevant measures of human health and disease risks. However, this task requires securely sharing sensitive data at an immense scale beyond what existing cryptographic platforms can achieve. Here we develop novel computational methods to enable biomedical data integration, analysis, and interpretation in a privacy-preserving and highly scalable manner.",Privacy-preserving genomic medicine at scale,9998648,R01HG010959,"['Address', 'Adoption', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biological', 'Biology', 'Clinic', 'Clinical Data', 'Collaborations', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Consumption', 'Data', 'Data Analyses', 'Data Pooling', 'Data Security', 'Data Set', 'Disease', 'Drug Interactions', 'Electronic Health Record', 'Engineering', 'Environment', 'Genetic', 'Genetic study', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'High-Throughput Nucleotide Sequencing', 'Human', 'Individual', 'Institutes', 'Institution', 'Knowledge', 'Letters', 'Machine Learning', 'Mainstreaming', 'Measures', 'Medical Imaging', 'Medical Records', 'Medicine', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Patients', 'Performance', 'Pharmacology', 'Polygenic Traits', 'Privacy', 'Process', 'Production', 'Property', 'Research Personnel', 'Resources', 'Risk', 'Sampling', 'Science', 'Secure', 'Security', 'Software Engineering', 'Software Tools', 'Standardization', 'Stream', 'Structure', 'Supercomputing', 'Techniques', 'Technology', 'Time', 'Translations', 'United States National Institutes of Health', 'Work', 'analysis pipeline', 'base', 'biobank', 'bioimaging', 'clinical development', 'clinical phenotype', 'clinically relevant', 'cohort', 'computer framework', 'cost', 'cryptography', 'data analysis pipeline', 'data integration', 'data sharing', 'data warehouse', 'disorder risk', 'epidemiology study', 'experimental study', 'genome analysis', 'genome wide association study', 'genomic data', 'health data', 'innovation', 'insight', 'monomethoxypolyethylene glycol', 'neuroimaging', 'novel', 'open source', 'polygenic risk score', 'precision medicine', 'preservation', 'privacy preservation', 'statistics', 'success', 'task analysis', 'theories', 'tool']",NHGRI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R01,2020,636185,0.06996934426117209
"Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration PROJECT SUMMARY The cell nucleus is a heterogeneous organelle that consists of nuclear bodies such as nuclear lamina, speckles, nucleoli and PML bodies. These structures continuously tether and tug chromatin at the small and large scales to synergistically orchestrate dynamic functions in distinct spatio-temporal compartments. A major obstacle to the production of navigable 4D reference maps and relating structure to function in the nucleus remains understanding how these different scales of organization influence each other. In particular, we have a poor understanding of the large-scale genome organization. Growing evidence suggests that such nuclear compartmentalization is causally connected with vital genome functions in human health and disease. However, the principles of this nuclear compartmentalization, its dynamics during changes in cell conditions, and its functional relevance are poorly understood. One lesson from Phase 1 4DN was the huge gap in throughput between imaging methods, that directly measure large-scale multi-landmark relationships, and genomic methods, that aim for whole genome high-resolution maps but are indirect measurements and provide limited information about large-scale compartments. For this 4DN UM1 Center application, we propose to meet these needs through the following Aims: (1) Generate multi-modal imaging and genomic datasets to reveal the structure, dynamics, and function of nuclear compartmentalization; (2) Develop and apply computational tools for data-driven genome structure modeling and integrative analysis of nuclear compartmentalization; (3) Develop an integrative analysis and visualization platform with navigable 4D reference maps of nuclear organization. The combined datasets and results of our proposed approaches will advance our understanding of nuclear compartmentalization, the interwoven connections among different nuclear components, and their functional significance. Our new integrative analysis tools and data-driven predictive models will produce more complete nuclear organization reference maps that integrate large-scale chromosome structure data from live and super-resolution microscopy with multi-modal genomic data including smaller scale chromatin interaction maps and predict functional relationships and dynamic responses. Our navigable reference maps will be publicly accessible through an analysis platform that provides interactive visualization of multiple data types, thus enabling investigators with diverse expertise to simultaneously explore their own data and related datasets/tools and promoting collaborations that will open new horizons into the role of the 4D nucleome in human health and disease. PROJECT NARRATIVE The proposed research is relevant to public health because it will enhance our understanding of nuclear genome organization and functions that are increasingly being linked to health and disease. Because we develop tools to disseminate this information and enable others to work with our data and their own data, we will also bring nuclear architecture to bear on a broad range of ongoing health related research. Thus, the proposed research is relevant to NIH’s mission that seeks to obtain fundamental knowledge that will help to improve human health.",Multiscale Analyses of 4D Nucleome Structure and Function by Comprehensive Multimodal Data Integration,10156141,UM1HG011593,"['Address', 'Architecture', 'Atlases', 'Binding', 'Biochemical', 'Cell Nucleus', 'Cell physiology', 'Cells', 'Chromatin', 'Chromatin Loop', 'Chromatin Structure', 'Chromosome Structures', 'Chromosomes', 'Collaborations', 'Communities', 'Complement', 'Computing Methodologies', 'Cytology', 'DNA Replication Timing', 'Data', 'Data Set', 'Development', 'Disease', 'Formulation', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Interphase Chromosome', 'Intuition', 'Knowledge', 'Link', 'Maps', 'Measurement', 'Measures', 'Methods', 'Microscopy', 'Mission', 'Modality', 'Modeling', 'Molecular', 'Multimodal Imaging', 'Nuclear', 'Nuclear Lamina', 'Nuclear Structure', 'Organelles', 'Outcome', 'Output', 'Phase', 'Population', 'Production', 'Public Health', 'Research', 'Research Personnel', 'Resolution', 'Role', 'Structural Models', 'Structure', 'Technology', 'Three-Dimensional Imaging', 'United States National Institutes of Health', 'Ursidae Family', 'Validation', 'Variant', 'Visualization', 'Work', 'base', 'cell cycle genetics', 'cell type', 'computer framework', 'computerized tools', 'data exploration', 'data integration', 'data tools', 'experimental study', 'genome-wide', 'genomic data', 'histone modification', 'imaging modality', 'improved', 'insight', 'machine learning algorithm', 'mental function', 'multimodal data', 'multimodality', 'multiple data types', 'multiscale data', 'predictive modeling', 'response', 'spatiotemporal', 'structured data', 'tool', 'transcription factor', 'transcriptome sequencing', 'user-friendly', 'whole genome']",NHGRI,CARNEGIE-MELLON UNIVERSITY,UM1,2020,2075409,0.026813275989768605

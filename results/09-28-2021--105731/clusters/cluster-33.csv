text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"A hybrid artificial intelligence framework for glaucoma monitoring Glaucoma is a complex neurodegenerative disease that results in degeneration of retinal ganglion cells and their axons. With older people making up the fastest growing part of the US population, glaucoma will become even more prevalent in the US in the coming decades. Due to the complex interaction of multiple factors in glaucoma, better structural and functional predictors are needed for its progression. The main impediments are massive health record data and sophisticated computational models. Our overall goal is to leverage the power of big data and rapidly evolving machine learning approaches. The NEI's “Big Data to Knowledge (BD2K)” initiative and the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) registry are all efforts to exploit the power of data and to better understand diseases and to provide improved prevention and treatment.  In this multi-PI proposal, we offer to assemble over 1 million optical coherence tomography (OCT) and visual fields (VFs) from the glaucoma research network (GRN). We propose to develop a hybrid artificial intelligence (AI) algorithm that synthesizes Gaussian mixture model expectation maximization (GEM) and archetypal machine learning approach to identify glaucoma progression and its monitoring using VFs and retinal nerve fiber layer (RNFL) thickness measurements. We will make these tools openly available to the vision and ophthalmology research communities.  Our proposed studies could offer substantial improvements in the prognosis of glaucoma as well as potentially providing OCT and joint VF/OCT surrogate endpoints to be used in glaucoma clinical trials. Leveraging big data in eye care is challenging. This study uses big functional and structural glaucoma data and develops hybrid machine learning models to identify glaucoma progression and its monitoring. Results could offer substantial improvements in prognosis of glaucoma and may provide surrogate endpoints for use in glaucoma clinical trials.",A hybrid artificial intelligence framework for glaucoma monitoring,9723609,R21EY030142,"['Academy', 'Address', 'Algorithms', 'American', 'Artificial Intelligence', 'Axon', 'Bayesian Modeling', 'Big Data', 'Big Data to Knowledge', 'Blindness', 'Caring', 'Clinical', 'Clinical Trials', 'Communities', 'Complex', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Detection', 'Devices', 'Disease', 'Disease Progression', 'Evolution', 'Eye', 'Gaussian model', 'Glaucoma', 'Goals', 'Hybrids', 'Institutes', 'Intelligence', 'Joints', 'Judgment', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Monitor', 'Neurodegenerative Disorders', 'Ophthalmology', 'Optic Nerve', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevention', 'Public Health', 'Registries', 'Research', 'Retinal', 'Severity of illness', 'Source', 'Structural Models', 'Structure', 'Surrogate Endpoint', 'Testing', 'Thick', 'Thinness', 'Time', 'Training', 'Vision', 'Visual Fields', 'analytical method', 'clinical Diagnosis', 'data space', 'design', 'diagnostic accuracy', 'early onset', 'evidence base', 'expectation', 'field study', 'health record', 'high dimensionality', 'improved', 'machine learning algorithm', 'multidimensional data', 'novel', 'open source', 'optical imaging', 'outcome forecast', 'programs', 'retinal ganglion cell degeneration', 'retinal nerve fiber layer', 'tool']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R21,2019,278421,0.05777723383336114
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,9916219,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2019,195245,-0.0122933876772248
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,9756406,K99EY029329,"['Affect', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Imagery', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'machine learning algorithm', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'regression algorithm', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality']",NEI,UNIVERSITY OF WASHINGTON,K99,2019,122039,0.014962473180550493
"Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1 Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at certain stages of the disease severity spectrum, specifically in the early stage and in advanced disease. These difficulties are due to a variety of causes that change over the course of the disease, including large between-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we build on our long-standing contribution to ocular imaging and propose novel and sensitive means to detect glaucoma and its progression that are optimized to the various stages of disease severity. We will use information gathered from visual fields (functional information) and a leading ocular imaging technology – optical coherence tomography (OCT; structural information) to map the capability of detecting changes across the entire disease severity spectrum to identify optimal parameters for each stage of the disease. Both commonly used parameters provided by the technologies and newly developed parameters with good diagnostic potential will be analyzed. We will use state-of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. We will also utilize a new imaging technology, the visible light OCT, to generate retinal images with outstanding resolution to extract information about the oxygen saturation of the tissue. This will provide in-vivo, real time, and noninvasive insight into tissue functionality. Taken together, this program will advance the use of structural and functional information with a substantial impact on the clinical management of subjects with glaucoma Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies that will substantially improve detection of glaucoma and its progression monitoring in order to prevent blindness.",Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1,9819321,R01EY013178,"['3-Dimensional', 'Blindness', 'Characteristics', 'Clinical', 'Clinical Management', 'Clinical Research', 'Complex', 'Data', 'Detection', 'Development', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Evaluation', 'Eye', 'Floor', 'Future', 'Glaucoma', 'Health', 'Human', 'Image', 'Imaging technology', 'Inner Plexiform Layer', 'Knowledge', 'Laboratories', 'Lead', 'Light', 'Machine Learning', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Oxygen Consumption', 'Oxygen saturation measurement', 'Pathology', 'Research Proposals', 'Resolution', 'Retinal', 'Retinal Diseases', 'Scanning', 'Severities', 'Severity of illness', 'Signal Transduction', 'Source', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Technology', 'Thick', 'Time', 'Tissue Extracts', 'Tissues', 'Translating', 'Visible Radiation', 'Vision', 'Visual Fields', 'Width', 'advanced disease', 'analytical method', 'base', 'clinical practice', 'cohort', 'computerized', 'deep learning', 'density', 'ganglion cell', 'improved', 'in vivo', 'innovation', 'innovative technologies', 'insight', 'instrument', 'invention', 'knowledge base', 'learning strategy', 'longitudinal dataset', 'macula', 'mathematical methods', 'new technology', 'novel', 'novel strategies', 'ocular imaging', 'preservation', 'prevent', 'programs', 'research study', 'retinal imaging', 'retinal nerve fiber layer', 'tissue oxygenation', 'tool']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2019,715489,0.007166499458797702
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9697824,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Consumption', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neural network', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2019,564228,0.05451976952800462
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user's location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user's location by recognizing standard informational signs present in the environment, tracking the user's trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9934891,R01EY029033,"['Adoption', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Environment', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Medical center', 'Process', 'Research', 'Schools', 'System', 'Tactile', 'Time', 'Travel', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'interest', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,105337,0.05020248786479943
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9663319,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416374,0.050803426385949035
"Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure Project Summary Glaucoma is the second leading cause of blindness globally, and is characterized by optic nerve damage that leads to the death of retinal ganglion cells with accompanying visual field (VF) loss. The optic nerve head (ONH) is the site of injury to the optic nerve fibers and plays a central role in glaucoma pathogenesis and diagnosis. Traditionally, glaucoma is diagnosed based on fundus inspection of the ONH, which provides information about the surface contour of the ONH. However, the optic nerve damage occurs in the deeper layers. With the development of optical coherence tomography (OCT) techniques for three-dimensional (3D) retinal imaging, parameters derived from the 3D ONH related structure (e.g., Bruch's membrane opening minimum rim width, peripapillary retinal nerve fiber layer thickness, disc tilt etc.) have been studied to better understand glaucoma pathogenesis, and are used to supplement clinical diagnosis. In addition, studies of the ONH biomechanics have also shown that the strain level at the ONH at any given intraocular pressure level depends on the 3D geometry of the ONH related structure. A high strain level is hypothesized to contribute to retinal ganglion cell injury. Previous research has suggested that the 3D ONH related structure is correlated to glaucoma pathogenesis and critically important to glaucoma diagnosis. However, to date, a systematic study using clinical data to determine the impact of the 3D ONH related structure on glaucoma has not been conducted.  We propose to study the relationship between the 3D ONH related structure and glaucoma with a diverse set of combined techniques including image processing, computational mechanics and machine learning. The specific aims of this project are to: (1) Derive features from the 3D ONH related structure and study their implications on VF loss patterns (K99 Phase). (2) Investigate the impact of the strain field patterns at the ONH on glaucoma (K99 Phase). (3) Study the effect of the 3D ONH related features on OCT diagnostic parameters (R00 Phase). (4) Model central vision loss from the 3D ONH related structural features (R00 Phase). Collectively, these studies will provide new insights and perspectives into the structure-function relationships in glaucoma and establish ocular anatomy specific norms of retinal nerve fiber layer profiles, which will advance our current understanding of glaucoma pathogenesis and improve glaucoma diagnosis. Our research is of high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.  Through the proposed research and training plans, the applicant will build a solid knowledge base in ophthalmology and further improve his expertise in mathematical modeling and data science. This project will provide critical training opportunities to further enhance the applicant's capabilities to become an independent computational vision scientist in ophthalmology. Project Narrative The proposed research will study the relationship between the three-dimensional (3D) optic nerve head (ONH) related structure and glaucoma. Our study will advance the current understanding of glaucoma pathogenesis and improve glaucoma diagnosis by gaining new insights into the structure-function relationships in glaucoma and establishing ocular anatomy specific norms of retinal nerve fiber layer profiles. Our research has high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.",Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure,9666293,K99EY028631,"['3-Dimensional', 'Age', 'Anatomy', 'Biomechanics', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Cessation of life', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Cross-Sectional Studies', 'Data', 'Data Science', 'Development', 'Diagnosis', 'Diagnostic', 'Dimensions', 'Ear', 'Elements', 'Eye', 'Fundus', 'Gaussian model', 'Geometry', 'Glaucoma', 'Hour', 'Image', 'Individual', 'Injury', 'Lead', 'Linear Models', 'Linear Regressions', 'Location', 'Machine Learning', 'Measurement', 'Mechanics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Multivariate Analysis', 'Nerve Fibers', 'Observational Study', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Pathogenesis', 'Patients', 'Pattern', 'Phase', 'Physiologic Intraocular Pressure', 'Play', 'Population Study', 'Process', 'Quality of life', 'Research', 'Research Training', 'Resolution', 'Retinal Ganglion Cells', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Severities', 'Site', 'Solid', 'Source', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Techniques', 'Testing', 'Thick', 'Translating', 'Variant', 'Vision', 'Visual Fields', 'Width', 'base', 'cell injury', 'clinical Diagnosis', 'clinical practice', 'clinically relevant', 'deep neural network', 'demographics', 'fundus imaging', 'image processing', 'improved', 'independent component analysis', 'insight', 'knowledge base', 'learning strategy', 'mathematical model', 'nonlinear regression', 'optic cup', 'retina blood vessel structure', 'retinal imaging', 'retinal nerve fiber layer', 'study population', 'training opportunity', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,K99,2019,146837,0.052850798256662446
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9988039,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,49496,0.00972678657597071
"Natural image processing in the visual cortex Project Summary Signals from the natural environment are processed by neuronal populations in the cortex. Understanding the relationship between those signals and cortical activity is central to understanding normal cortical function and how it is impaired in psychiatric and neurodevelopmental disorders. Substantial progress has been made in elucidating cortical processing of simple, parametric stimuli, and computational technology is improving descriptions of neural responses to naturalistic stimuli. However, how cortical populations encode the complex, natural inputs received during every day perceptual experience is largely unknown. This project aims to elucidate how natural visual inputs are represented by neuronal populations in primary visual cortex (V1). Progress to date has been limited primarily by two factors. First, during natural vision, the inputs to V1 neurons are always embedded in a spatial and temporal context, but how V1 integrates this contextual information in natural visual inputs is poorly understood. Second, prior work focused almost exclusively on single-neuron firing rate, but to understand cortical representations one must consider the structure of population activity— the substantial trial-to-trial variability that is shared among neurons and evolves dynamically—as this structure influences population information and perception. The central hypothesis of this project is that cortical response structure is modulated by visual context to approximate an optimal representation of natural visual inputs. To test the hypothesis, this project combines machine learning to quantify the statistical properties of natural visual inputs, with a theory of how cortical populations should encode those images to achieve an optimal representation, to arrive at concrete, falsifiable predictions for V1 response structure. The predictions will be tested with measurements of population activity in V1 of awake monkeys viewing natural images and movies. Specific Aim 1 will determine whether modulation of V1 response structure by spatial context in static images is consistent with optimal encoding of those images, and will compare the predictive power of the proposed model to alternative models. Specific Aim 2 addresses V1 encoding of dynamic natural inputs, and will test whether modulation of V1 activity by temporal context is tuned to the temporal structure of natural sensory signals, as required for optimality. As both spatial and temporal are present simultaneously during natural vision, Specific Aim 3 will determine visual input statistics in free-viewing animals, and test space-time interactions in V1 activity evoked by those inputs. This project will provide the first test of a unified functional theory of contextual modulation in V1 encoding of natural visual inputs, and shed light on key aspects of natural vision that have been neglected to date. Project Narrative This project aims to determine how neurons in the visual cortex represent the inputs encountered during perceptual experience in the natural environment, through correct integration of visual information across space and time. In individuals with neurodevelopmental and psychiatric disorders, integration is often miscalibrated leading to perceptual impairments. Our study will advance knowledge of the relationship between natural sensory inputs and cortical activity, which is central to understanding normal cortical function and how it is impaired in patient populations.",Natural image processing in the visual cortex,9801995,R01EY030578,"['Address', 'Animal Testing', 'Area', 'Complex', 'Dependence', 'Development', 'Environment', 'Experimental Designs', 'Goals', 'Image', 'Impairment', 'Individual', 'Knowledge', 'Light', 'Location', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Monkeys', 'Motion', 'Neurodevelopmental Disorder', 'Neurons', 'Perception', 'Population', 'Process', 'Property', 'Publications', 'Recording of previous events', 'Sampling', 'Sensory', 'Signal Transduction', 'Stimulus', 'Structure', 'Technology', 'Testing', 'Time', 'V1 neuron', 'Vision', 'Visual', 'Visual Cortex', 'Work', 'area striata', 'awake', 'base', 'computer framework', 'experience', 'experimental study', 'image processing', 'improved', 'model development', 'movie', 'neglect', 'patient population', 'relating to nervous system', 'response', 'sensory input', 'spatiotemporal', 'statistics', 'theories', 'vision science', 'visual information']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2019,417500,0.0024316805501231166
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9741121,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'side effect', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,281932,0.006251159384113965
"Device to control circadian-effective light in Alzheimer's disease environments Project Summary This proposed project will develop and field-test a device that accurately monitors and controls the circadian stimulus (CS) for Alzheimer disease (AD) and Alzheimer-disease-related dementia (ADRD) patients in nursing homes. Human biology has evolved to have two distinct optical systems: the visual system, by which we see and process images, and the circadian system, which regulates our biological clock and associated biological systems. These two systems have significantly different spectral and temporal responses to optical input. Specifically, circadian stimulation peaks at 460 nm and responds after several minutes of optical activation, while the visual system peaks at 555 nm and responds nearly instantaneously to inputs. All lighting systems are designed and installed in buildings with consideration only given to the photopic (visual) system and all light meters used to characterize lighting buildings are calibrated to measure photopic light, not CS. While a broad and growing body of research has documented the impacts of the circadian system on human health, including regulating sleep and improving cognition in AD/ADRD patients, research on the CS experienced by AD/ADRD patients is extremely limited. Researchers at the Lighting Research Center at Rensselaer Polytechnic Institute developed the Daysimeter, a calibrated light meter that measures circadian light and circadian stimulus. In Phase I of this project, researchers modified an existing workstation-based lighting control system they previously developed for the visual system to include Daysimeter technology, allowing this control system to record CS measurements. The accuracy of these CS measurements was confirmed in the laboratory and field-testing of 20 of devices is currently ongoing in AD/ADRD nursing homes. In this Phase II application, researchers propose adding control features to this device so that lighting can be controlled to optimize CS dosages in AD/ADRD patient environments. Machine learning-based lighting control algorithms will be driven by continuous light level and spectrum measurements as well as periodic (e.g., daily) patient health data. Data from these devices would be wirelessly transmitted to researchers via an Internet gateway and associated cloud-based data management systems. These data would be of immediate value for gaining a better understanding of AD/ADRD patients' CS exposure and could ultimately result in new lighting systems and/or building codes that consider both our visual and circadian systems. Following the development phase, 30 CS-enabled lighting control systems will be field tested over a 22-week test period. Researchers aim to commercialize this CS-enabled lighting control system shortly after the completion of this field test and the Phase II project specifically targeting AD/ADRD nursing home applications. Project Narrative A growing body of research has demonstrated how light impacts human circadian systems and how these impacts can affect sleep, alertness, cognition and agitation in people with Alzheimer's disease (AD) and Alzheimer's-disease-related dementia (ADRD). Still, significant knowledge gaps exist in determining how much circadian stimulation is typically provided to AD/ADRD patients and there are no commercial products designed to control lighting in AD/ADRD environments in ways that promote circadian-related health. This project aims to fill in these gaps by developing and testing a device specifically designed to measure and control the circadian stimulation experienced by AD/ADRD patients in nursing homes.",Device to control circadian-effective light in Alzheimer's disease environments,9907480,R44AG060857,"['Affect', 'Agitation', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Back', 'Behavior', 'Biological Clocks', 'Building Codes', 'Characteristics', 'Clinical Trials', 'Cognition', 'Data', 'Database Management Systems', 'Development', 'Device or Instrument Development', 'Devices', 'Dose', 'Effectiveness', 'Elderly', 'Environment', 'Feeds', 'Health', 'Hour', 'Human', 'Human Biology', 'Image', 'Institutes', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Light', 'Lighting', 'Machine Learning', 'Measurement', 'Measures', 'Monitor', 'Moods', 'Nursing Homes', 'Optics', 'Patients', 'Pattern', 'Performance', 'Periodicity', 'Phase', 'Phototherapy', 'Planet Earth', 'Population', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Rotation', 'Running', 'Sleep', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Visual', 'Visual system structure', 'Wakefulness', 'Wireless Technology', 'Work', 'active control', 'alertness', 'appropriate dose', 'awake', 'base', 'biological systems', 'circadian', 'circadian pacemaker', 'cloud based', 'commercialization', 'design', 'dosage', 'experience', 'falls', 'feeding', 'field study', 'health data', 'improved', 'interest', 'meter', 'next generation', 'novel', 'prototype', 'residence', 'response', 'success', 'therapy design']",NIA,"ERIK PAGE AND ASSOCIATES, INC.",R44,2019,1240470,-0.015580797755010917
"Structural and functional tests of ganglion cell damage in glaucoma This project will use a combination of structural and functional measurements to test the hypothesis that early- stage damage in human glaucoma occurs first in the inner plexiform layer (IPL) of the retina – especially its OFF sub-lamina – as suggested by murine glaucoma models. In the first Aim, we will use a novel visible-light optical coherence tomograph (VIS OCT) to study structural changes in the retina of glaucoma patients. The newly developed VIS OCT has sufficient image contrast and resolution to segment the IPL boundaries and to define sub-lamination in volumetric OCT data, something not currently possible with existing near-infrared OCT instruments. We will make comparative measurements within the IPL and between the IPL, the ganglion cell layer (GCL) and the retinal nerve fiber layer (RNFL). Because data from mouse models of glaucoma suggests that early damage occurs preferentially within the OFF sub-lamina of the IPL, we will make separate VIS OCT measurements biased for the OFF- and ON-sublaminae of the IPL and use machine learning approaches to determine whether a similar damage process can be demonstrated in human. To test whether OFF-pathway function is preferentially lost in glaucoma, we will use a novel Steady-State Visual Evoked Potential (SSVEP) paradigm that employs sawtooth increments and decrements to bias the measurement to ON vs OFF pathways, respectively, a paradigm our data suggests discriminates glaucoma from control patients. The second Aim will optimize this SSVEP measurement for testing localized areas of the visual field. The third Aim will make comparative measurements of visual-field, VIS OCT and SSVEP loss patterns in a large sample of glaucoma patients and in age- and sex-matched controls. Thickness and interface reflectivity amplitude maps derived from VIS OCT imaging of the RNFL, GCL and IPL including sublaminae will be correlated topographically with visual field defects to assess the relative sensitivity of our structural biomarkers at and near visual field locations with demonstrable losses on conventional (Humphrey) perimetry. Similarly, SSVEP responses from different locations in the visual field will be correlated topographically with visual field loss patterns and to VIS OCT losses, with special emphasis on correlating structural damage in OFF vs ON sub-laminae of the IPL with the functional correlates derived from regional decremental and incremental SSVEPs. Separately and in combination, our structural and functional measurements are designed to provide strong tests of the biological hypothesis that the OFF pathway is preferentially damaged in human glaucoma, and to reveal new biomarkers for the disease. Improving visual outcomes in glaucoma will require a better understanding of the earliest sites and processes of damage and methods to measure them quickly and accurately in patients. This project will address both needs through a combination of novel Optical Coherence Tomography and electrophysiological measurements. The new imaging and electrophysiological tests that will be developed here, either separately or together, could eventually replace conventional visual field testing which is time-consuming and unreliable.",Structural and functional tests of ganglion cell damage in glaucoma,9765006,R01EY030361,"['Address', 'Affect', 'Age', 'Animal Model', 'Area', 'Atrophic', 'Biological Assay', 'Biological Markers', 'Biological Testing', 'Clinical', 'Complex', 'Consumption', 'Data', 'Disease', 'Early Diagnosis', 'Early treatment', 'Economic Burden', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Frequencies', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Inner Plexiform Layer', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modification', 'Mus', 'Noise', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Perimetry', 'Process', 'Property', 'Resolution', 'Retina', 'Retinal', 'Retinal Ganglion Cells', 'Rodent Model', 'Sampling', 'Scotoma', 'Severities', 'Signal Transduction', 'Site', 'Specificity', 'Speed', 'Structural defect', 'Structure', 'Synapses', 'Techniques', 'Testing', 'Thick', 'Time', 'Visible Radiation', 'Vision', 'Visual', 'Visual Fields', 'Visual evoked cortical potential', 'base', 'cell injury', 'comparative', 'contrast imaging', 'design', 'extrastriate visual cortex', 'field study', 'ganglion cell', 'improved', 'instrument', 'mouse model', 'novel', 'optic nerve disorder', 'response', 'retinal imaging', 'retinal nerve fiber layer', 'sex']",NEI,STANFORD UNIVERSITY,R01,2019,551932,0.0592899317691764
"SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions  The project will investigate prosthetic support for people with visual impairment (PVI) that integrates computer vision-based prosthetics with video-mediated human-in-the-loop prosthetics. Computer vision- based (CV) prosthetics construe the fundamental technical challenge for visual prosthetics as one of parsing and identifying objects across scales, distances, and orientations. Visual prosthetic applications have been central drivers in the development of computer vision technology through the past 50 years. Video-mediated remote sighted assistance (RSA) prosthetics are more recent, enabled by different technologies, and construe the orienting technical challenge for visual prosthetics as one of effective helping interactions. RSA services are commercially available now, and have evoked much excitement in the PVI community. The two approaches, CV and RSA, will be successively integrated through a series of increasingly refined Wizard of Oz simulations, and investigate possible synergies between the two approaches. We will employ a human-centered design approach, identifying a set of key assistive interaction scenarios that represent authentic needs and concerns of PVIs, by leveraging our 6-year relationship working directly with our local chapter of the National Federation of the Blind. RELEVANCE (See Instructions): 23.7 million American adults have vision loss; 1.3 million people in US are legally blind. This project addresses a transformational opportunity to enhance human performance and experience, to diversify workplace participation, and to enhance economic and social well-being. n/a",SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions ,9928587,R01LM013330,"['Address', 'Adult', 'American', 'Articulation', 'Back', 'Blindness', 'Communities', 'Computer Vision Systems', 'Computers', 'Data Set', 'Development', 'Economics', 'Emotional', 'Female', 'Goals', 'Human', 'Information Sciences', 'Instruction', 'Mediating', 'Modeling', 'Ocular Prosthesis', 'Performance', 'Prosthesis', 'Route', 'Self-Help Devices', 'Series', 'Services', 'Social Well-Being', 'Technology', 'Time', 'Underrepresented Students', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'design', 'experience', 'graduate student', 'human-in-the-loop', 'learning materials', 'legally blind', 'outreach', 'prototype', 'simulation', 'synergism', 'undergraduate student']",NLM,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2019,225147,0.06449300074445148
"Vision in Natural Tasks Summary/Abstract  In the context of natural behavior, humans make continuous sequences of sensory-motor decisions to satisfy current behavioral goals, and vision must provide the information needed to achieve those goals. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks in natural locomotion, or the requisite information, and the proposal attempts to specify these.  in the context of natural gait, the patterns of optic flow are unexpectedly complex, raising questions about its role. The patterns of motion on the retina during locomotion depend critically on both eye and body motion, and these in turn depend on behavioral goals. Our first Aim is therefore to comprehensively describe the statistics of retinal motion patterns in a variety of terrains and task contexts. We will measure binocular eye and body movements while walking in outdoor terrains of varying roughness, crossing a busy intersection, and making coffee. These contexts will induce different gaze patterns. We will provide a comprehensive description of the motion stimulus in natural locomotion and help separate out self-motion signals from externally generated motion. These data will allow a more precise specification of the response patterns in cortical motion sensitive areas. Because of the complexity of natural motion patterns, we will re-examine the influence of optic flow on walking direction in a virtual reality environment and test alternative explanations for the role of flow.  A central task in walking is foot placement, and we will focus on identifying the image properties that make a good foothold. Stereo, structure from motion, and spatial image structure are all likely contenders. We directly investigate the role of stereo in foothold selection by examining gait patterns in stereo-deficient subjects in terrains with varying degrees of roughness. Using a different strategy, we will attempt to predict gaze locations and footholds in rough terrain using convolution neural nets (CNN’s) to identify potential search templates for footholds in rough terrain. We will describe fixation patterns from crosswalk and sidewalk navigation and attempt to make inferences about their purpose, and use Modular Inverse Reinforcement Learning (MIRL) to predict direction decisions and decompose the behavior into sub-tasks.  The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists The work will be strengthened by the investigation of stereo- deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available. Project Narrative  The central goal of this work is to understand vision in its natural context. This is very important information in order to devise suitable vision aids and rehabilitation strategies for individuals with visual impairments, and it is becoming increasingly accessible because of developments in technology for monitoring eye and body movements. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks and requisite information in natural locomotion, and the proposal attempts to specify these. The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists. The work will be strengthened by the investigation of stereo-deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available.",Vision in Natural Tasks,9830977,R01EY005729,"['Affect', 'Area', 'Behavior', 'Behavioral', 'Binocular Vision', 'Cells', 'Characteristics', 'Coffee', 'Collection', 'Complex', 'Cues', 'Data', 'Data Set', 'Development', 'Distant', 'Environment', 'Eye', 'Eye Movements', 'Gait', 'Goals', 'Grant', 'Head', 'Human', 'Image', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Link', 'Location', 'Locomotion', 'Machine Learning', 'Measures', 'Monitor', 'Motion', 'Motor', 'Movement', 'Pattern', 'Psychological reinforcement', 'Retina', 'Retinal', 'Rewards', 'Robot', 'Robotics', 'Role', 'Sampling', 'Seminal', 'Sensory', 'Signal Transduction', 'Speed', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Visit', 'Visual', 'Visual Fields', 'Visual impairment', 'Walkers', 'Walking', 'Work', 'base', 'convolutional neural network', 'cost', 'experimental study', 'foot', 'gaze', 'imaging properties', 'innovation', 'kinematics', 'novel', 'optic flow', 'rehabilitation strategy', 'response', 'sample fixation', 'statistics', 'virtual reality', 'vision aid', 'vision development', 'vision rehabilitation', 'visual information']",NEI,"UNIVERSITY OF TEXAS, AUSTIN",R01,2019,369009,-0.06225153009517022
"Accelerating Community-Driven Medical Innovation with VTK Abstract Thousands of medical researchers around the world use VTK —the Visualization Toolkit— an open-source, freely available software development toolkit providing advanced 3D interactive visualization, image processing and data analysis algorithms. They either use VTK directly in their in-house research applications or indirectly via one of the multitude of medical image analysis and bioinformatics applications that is built using VTK: Osirix, 3D Slicer, BioImageXD, MedINRIA, SCIRun, ParaView, and others. Furthermore, VTK also provides 3D visualizations for clinical applications such as BrainLAB’s VectorVision surgical guidance system and Zimmer’s prosthesis design and evaluation platform. VTK has been downloaded many hundreds of thousands of times since its initial release in 1993. Considering its broad distribution and prevalent use, it can be argued that VTK has had a greater impact on medical research, and patient care, than any other open-source visualization package.  This proposal is in response to the multitude of requests we have been receiving from the VTK medical community. The aims are as follows:  1. Aim 1: Adaptive visualization framework: Produce an integrated framework that supports  visualization applications that balance server-side and client-side processing depending on data size,  analysis requirements, and the user platform (e.g., phone, tablet, or GPU-enabled desktop).  2. Aim 2: Integrated, interactive applications: Extend VTK to support a diversity of programming  paradigms ranging from C++ to JavaScript to Python and associated tools such as Jupyter Notebooks,  integrating with emerging technologies such as deep learning technologies.  3. Aim 3: Advanced rendering, including AR/VR: Target shader-based rendering systems and AR/VR  libraries that achieve high frame rates with minimal latency for ubiquitous applications that combine  low-cost, portable devices such as phones, ultrasound transducers, and other biometric sensors for  visually monitoring, guiding, and delivering advanced healthcare.  4. Aim 4: Infrastructure, Outreach, and Validation: Engage the VTK community and the proposed  External Advisory Board during the creation and assessment of the proposed work and corresponding  modern, digital documentation in the form of videos and interactive web-based content. Project Narrative The Visualization Toolkit (VTK) is an open source, freely available software library for the interactive display and processing of medical images. It is being used in most major medical imaging research applications, e.g., 3D Slicer and Osirix, and in several commercial medical applications, e.g., BrainLAB’s VectorVision surgical guidance system. VTK development began in 1993 and since then an extensive community of users and developers has grown around it. However, the rapid advancement of cloud computing, GPU hardware, deep learning algorithms, and VR/AR systems require corresponding advances in VTK so that the research and products that depend on VTK continue to deliver leading edge healthcare technologies. With the proposed updates, not only will existing applications continue to provide advanced healthcare, but new, innovative medical applications will also be inspired.",Accelerating Community-Driven Medical Innovation with VTK,9740493,R01EB014955,"['3-Dimensional', 'Adopted', 'Algorithmic Analysis', 'Algorithms', 'Augmented Reality', 'Bioinformatics', 'Biomechanics', 'Biomedical Technology', 'Biometry', 'Client', 'Cloud Computing', 'Cloud Service', 'Code', 'Communities', 'Computational Geometry', 'Computer software', 'Data', 'Data Analyses', 'Development', 'Devices', 'Documentation', 'Emerging Technologies', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Explosion', 'Foundations', 'Funding', 'Grant', 'Health Technology', 'Healthcare', 'Hybrids', 'Image Analysis', 'Imagery', 'Industry', 'Infrastructure', 'Internet', 'Language', 'Letters', 'Libraries', 'Licensing', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modernization', 'Monitor', 'Online Systems', 'Operative Surgical Procedures', 'Patient Care', 'Prevalence', 'Process', 'Prosthesis Design', 'Publications', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Side', 'Surveys', 'System', 'Tablets', 'Techniques', 'Technology', 'Telephone', 'TensorFlow', 'Testing', 'Time', 'Training', 'Ultrasonic Transducer', 'Update', 'Validation', 'Visual', 'Work', 'base', 'clinical application', 'cloud based', 'computerized data processing', 'cost', 'deep learning', 'deep learning algorithm', 'design', 'digital', 'health care delivery', 'image processing', 'innovation', 'interest', 'learning strategy', 'meetings', 'new technology', 'open source', 'outreach', 'point of care', 'portability', 'processing speed', 'real world application', 'response', 'sensor', 'software development', 'statistics', 'success', 'supercomputer', 'synergism', 'tool', 'trend', 'virtual reality', 'web services']",NIBIB,"KITWARE, INC.",R01,2019,508446,0.0019318346445335102
"Personalizing Glaucoma Diagnosis by Disease Specific Patterns and Individual Eye Anatomy Project Summary/Abstract Glaucoma is a disease of the optic nerve which is accompanied by visual ﬁeld (VF) loss. While accurate VF loss diagnosis and the detection of its progression over time is of high relevance to clinical practitioners as it indicates the initiation of or change in ocular therapy, there is no consensus on objective measures for this purpose, and VF measurements are known to be often unreliable. The main objective of this project is to develop clinically applicable measures to improve the diagnosis of glaucomatous VF loss and of its progression by two approaches: First, the identiﬁcation of representative loss patterns and their progression, achieved by large-scale, customized bioinformatical procedures applied to data from glaucoma patients from nine clinical centers and second, the inclusion of eye and patient speciﬁc personalized parameters. In total, 480,486 VFs, are available for this project. One major aim is to develop novel diagnostic indices based on computationally identiﬁed evolution patterns of VF loss, particularly (1) an index that denotes the probability of glaucomatous vision loss and (2) an index that assigns probabilities to a VF that follow-up measurements will be in a certain defect class. The indices will be statistically evaluated on separate VF samples and compared to existing approaches. Routinely available patient speciﬁc parameters which are candidates to impact glaucomatous vision loss are patient ethnicity, type of glaucoma, spherical equivalent (SE) of refractive error and the location of the blind spot relative to ﬁxation. The effect of these parameters on the vision loss patterns will be systematically studied. The impact of their inclusion in the novel diagnostic indices and their potential improvement on glaucoma diagnosis will be quantiﬁed on a separate data set. A further aim is the calculation of a spatial map speciﬁc to a measured VF that represents the preferred VF locations of future defects as well as their reliability as an aid to event-based progression diagnosis. A second major objective is the investigation of the relationship of VF loss and individual parameters related to retinal structure, based on retinal nerve ﬁber layer thickness (RNFLT) measurements around the optic disc. The inter-relationship of representative patterns of RNFLT and its decrease over time with trajectories of major retinal arteries, SE, and blind spot location is systematically studied, and the impact on patterns of VF loss is quantitatively analyzed with the goal to improve the interpretation of existing VF loss and to predict future glaucomatous vision loss. Main contributions of the project with relevance to clinical practice are publicly available open-source software implementations of new diagnostic indices and maps, enhanced by individual functional and structural parameters, and a detailed and personalized model for the relationship between retinal structure and glaucomatous vision loss. Project Narrative Glaucoma is an ocular disease accompanied by vision loss which may progress over time up to total blindness, but the assessment of glaucomatous vision loss is noisy, and it is often hard for clinical practitioners to decide whether changes over time reﬂect true changes of functional vision or are the result of normal measurement variations or artifacts. This project contributes directly and immediately to public health by exploring the impact of individual anatomical parameters on the spatial patterns of glaucomatous vision loss in order to improve the diagnosis of vision loss and of its progression. Main objective of the project is the development of new quantitative diagnostic indices, implemented as publicly available software.",Personalizing Glaucoma Diagnosis by Disease Specific Patterns and Individual Eye Anatomy,9802123,R01EY030575,"['Anatomy', 'Atrophic', 'Axon', 'Bioinformatics', 'Blindness', 'Clinical', 'Cluster Analysis', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Defect', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Ethnic Origin', 'Event', 'Evolution', 'Eye', 'Future', 'Glaucoma', 'Goals', 'Hemorrhage', 'Impairment', 'Individual', 'Investigation', 'Length', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Modeling', 'Morphologic artifacts', 'Nerve Fibers', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Probability', 'Procedures', 'Public Health', 'Refractive Errors', 'Retinal', 'Retinal Defect', 'Retinal Ganglion Cells', 'Retinal blind spot', 'Sampling', 'Structure', 'Structure-Activity Relationship', 'System', 'Thick', 'Time', 'Variant', 'Vision', 'Visual Fields', 'Work', 'base', 'central retinal artery', 'clinical application', 'clinical practice', 'disease diagnosis', 'follow-up', 'fovea centralis', 'improved', 'indexing', 'multidisciplinary', 'neglect', 'novel diagnostics', 'open source', 'optic nerve disorder', 'outcome forecast', 'retinal nerve fiber layer', 'sample fixation', 'sex']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,R01,2019,534037,0.04287552804385671
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9750520,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2019,507856,0.01698143430422622
"Human Face Representation in Deep Convolutional Neural Networks The human visual system can recognize a familiar face across wide variations of viewpoint, illumination, expression, and appearance. This remarkable computational feat is accomplished by large-­scale networks of neurons. We will test a face space theory of the representations that emerge at the top layer of deep learning convolutional neural networks (DCNNs) as a model of human visual representations of faces. Computer-based face recognition has improved in recent years due to DCNNs and the easy availability of labeled training data (faces and identities) from the web. Inspired by the primate visual system, DCNNs are feed­forward artificial neural networks that can map images of faces into representations that support recognition over widely variable images. Although the calculations executed by the simulated neurons are simple, enormous numbers of computations are used to convert an image into a representation. The end result of this processing is a highly compact representation of a face that retains image detail in an invariant, identity­-specific face code. This code is fundamentally different than any representation of faces considered in vision science. This theory we test combines key components of previous face space models (similarity, learning history) with new features (imaging conditions, personal face history) in a unitary space that represents both identity and facial appearance across variable images. We will test whether this model can account for human recognition of familiar faces, which is highly robust to image variability (pose, illumination, expression). The model will also be applied to understanding long standing difficulties humans (and machines) have with faces of other races. We aim to bridge critical gaps in our knowledge of how DCNNs work, linking psychological, neural, and computational perspectives. A fundamentally new theory of face representation will alter the questions we ask about face representations in all three fields. A new focus on understanding how we (or neural networks) “perceive” a single familiar identity in widely variable images will give rise to a search for representations that gracefully merge the properties of faces with the real-­world image conditions in which they are experienced. This project presents a unique opportunity to study, manipulate, and learn from these representations, and to apply the findings to broader questions about high-­level vision from neural and perceptual perspectives. Human recognition of familiar faces is highly robust to image variability (pose, illumination, expression)—a skill that is likely due to the quality and quantity of experience we have with the faces of people we know well. Deep convolutional neural networks are modeled after the primate visual system and have made impressive gains recently on the problem of robust face recognition. Understanding the visual nature of the face “feature” codes that emerge in these networks can give insight into long-standing questions about how the human visual system can, but does not always, represent a face in a way that generalizes across images that vary widely.",Human Face Representation in Deep Convolutional Neural Networks,9641233,R01EY029692,"['Affect', 'Appearance', 'Categories', 'Code', 'Computers', 'Data', 'Data Set', 'Face', 'Face Processing', 'Familiarity', 'Human', 'Image', 'Individual', 'Internet', 'Knowledge', 'Label', 'Learning', 'Lighting', 'Link', 'Maps', 'Methods', 'Modeling', 'Nature', 'Neural Network Simulation', 'Neurons', 'Performance', 'Persons', 'Primates', 'Property', 'Published Comment', 'Race', 'Recording of previous events', 'Space Models', 'Testing', 'Training', 'Variant', 'Vision', 'Visual', 'Visual system structure', 'Work', 'artificial neural network', 'base', 'convolutional neural network', 'deep learning', 'experience', 'feedforward neural network', 'feeding', 'human model', 'improved', 'insight', 'neural network', 'psychologic', 'relating to nervous system', 'representation theory', 'skills', 'theories', 'vision science']",NEI,UNIVERSITY OF TEXAS DALLAS,R01,2019,387918,-0.006876735032169978
"Towards a Compositional Generative Model of Human Vision Understanding object recognition has long been a central problem in vision science, because of its applied utility and computational difficulty. Progress has been slow, because of an inability to process complex natural images, where the largest challenges arise. Recently, advances in Deep Convolutional Neural Networks (DCNNs) spurred unprecedented success in natural image recognition. The general goal of this proposal is to leverage this success to test computational theories of human object recognition in natural images. However, DCNNs still markedly underperform humans when challenged with high levels of ambiguity, occlusion, and articulation. We hypothesize that humans' superior performance arises from the use of knowledge about how images and objects are structured. Preliminary evidence for this claim comes from the success of hybrid models, that combine DCNNS for identifying features and parts in images, with explicit knowledge of object and image structure. These computations occur within a hierarchy, which includes both top-down and bottom- up processing. The specific goal of the work proposed here is to strongly test whether these computational strategies, structured, hierarchical representations and bidirectional processing, are used to recognize objects in natural images. Human bodies are composed of hierarchically organized configurable parts, making them an ideal test domain. We examine the complete recognition process, from parts, to pairs of parts, to whole bodies, each in its own aim. Each aim also tests important sub-hypotheses about when and how the computational strategies are used. Aim 1 examines recognition of individual body parts, testing whether it is dependent on parsing images into more basic features and relationships, for example edges and materials. Aim 2 examines pairs of parts, testing the importance of knowledge of body connectedness relationships. Aim 3 examines perception of entire bodies, testing whether knowledge of global body structure guides bidirectional processing. In each aim, we first develop nested computer vision models that either do or do not make use of structural knowledge, to test whether it aids recognition. We then test whether human performance can be accounted for by the availability of that structural knowledge. We next measure neural activity with functional MRI to identify where and how it is used in cortex. Finally, we integrate these results to produce even stronger tests, using the nested models to predict human performance and confusion matrices as well as fMRI activity levels and confusion matrices. Altogether, this work will strongly test key theoretical accounts of object recognition in the most important domain, perception of natural images. The work, based on extensive preliminary data, measures and models the entire body recognition system. The models developed and tested here should surpass the state-of-the-art, and be useful for many real-world recognition tasks. The proposal will also lay the groundwork for future studies of recognition impaired by disease. This research uses computational, behavioral, and brain imaging methods to investigate how the visual system represents and processes information about human bodies. The studies will reveal how and when people can accurately recognize objects in natural images, how the brain supports this function, and how loss of information, similar to that that accompanies visual disease, may affect the ability to interpret everyday scenes.",Towards a Compositional Generative Model of Human Vision,9818274,R01EY029700,"['Affect', 'Area', 'Articulation', 'Behavioral', 'Body Image', 'Body part', 'Brain', 'Brain imaging', 'Complex', 'Computer Vision Systems', 'Confusion', 'Cues', 'Data', 'Development', 'Disease', 'Elbow', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Human body', 'Hybrids', 'Image', 'Impairment', 'Individual', 'Knowledge', 'Link', 'Measures', 'Modeling', 'Perception', 'Performance', 'Predictive Value', 'Process', 'Psychophysics', 'Published Comment', 'Research', 'Structure', 'System', 'Testing', 'Training', 'Vision', 'Visual', 'Visual system structure', 'Work', 'Wrist', 'base', 'convolutional neural network', 'crowdsourcing', 'human model', 'imaging modality', 'improved', 'object recognition', 'relating to nervous system', 'spatial relationship', 'success', 'theories', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2019,352996,-0.018619043212244106
"Access to parietal action representations after stroke lesions in visual cortex PROJECT SUMMARY  The ability to recognize and use objects according to their function (e.g., fork, hammer, pencil) requires integration of visual, semantic and action knowledge across occipital, temporal and parietal areas. Left parietal regions support critical aspects of object-directed action, such as grasping and object manipulation. This research activity uses a combination of fMRI and behavioral measures in patients with ischemic strokes to early and extrastriate visual areas to test the following hypotheses: Aim 1: There is a visual pathway to the parietal grasp region (aIPS) that bypasses processing in primary visual cortex. Aim 2: Left ventral extrastriate cortex is necessary to access manipulation information for visually presented objects. Aim 3A: Ballistic grasping actions to objects in the hemianopic field are influenced by volumetric properties (size, orientation) of targets. Aim 3B: Left ventral extrastriate lesions impair object function (e.g., `scissors used to cut') and disrupt access to manipulation knowledge from visual input. The research leverages strengths of fMRI (whole brain correlational measure) and neuropsychology (causal inference) to test new hypotheses about vision and action.  `Tools' (i.e., small manipulable objects) are an excellent domain in which to address broader questions about the integration of sensory, motor and cognitive processing. This is because tool recognition and tool use require the integration of distinct sensory, motor and cognitive representations, and the neural substrates of tool processing are well described. The research program emphasizes fresh perspectives on longstanding ideas about the dorsal and ventral visual pathways, by a) undertaking the first systematic investigation of the types of information about objects that are extracted by visual pathways that bypass primary visual cortex, and by b) studying how some parietal areas depend on inputs from the ventral stream in order to access the correct action for a given object. The research activity innovates by testing hypotheses about how lesions at different stages in the cortical visual hierarchy affect downstream processing in parietal cortex, combining neural and behavioral measures to study brain damaged patients (generating causal evidence), and by combining univariate and multivariate measures to `read out' the information content of brain regions (parietal cortex) that are anatomically remote from a lesion. The research advances understanding of how lesions in one brain region disrupt computations in other parts of the brain that depend on the damaged region for their inputs, a phenomenon (`dynamic diaschisis') that applies to brain injury generally. Advancing understanding of these basic issues using causal data has broad implications for understanding how the brain selects the correct action for the correct object, and more generally for theories of conceptual organization and causal reasoning. Understanding how the brain accesses actions from visual input has implications for related fields, such as robotics, neuroprosthetics, and evidenced based approaches for rehabilitating function after brain injury. Project Narrative Functional MRI and behavioral testing are used to test hypotheses about how strokes affecting occipital and temporal cortex disrupt access to action representations in parietal cortex. This research will advance understanding of how the brain processes visual information in support of everyday actions.",Access to parietal action representations after stroke lesions in visual cortex,9659002,R01EY028535,"['Address', 'Affect', 'Alexia', 'Anatomy', 'Area', 'Ballistics', 'Behavioral Assay', 'Brain', 'Brain Injuries', 'Brain region', 'Bypass', 'Cognitive', 'Data', 'Dorsal', 'Eating', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Imaging Device', 'Impairment', 'Investigation', 'Ischemic Stroke', 'Knowledge', 'Left', 'Lesion', 'Literature', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Neural Pathways', 'Neuropsychology', 'Occipital lobe', 'Parahippocampal Gyrus', 'Parietal', 'Parietal Lobe', 'Participant', 'Pathway interactions', 'Patients', 'Population', 'Process', 'Property', 'Prosopagnosia', 'Reading', 'Research', 'Research Activity', 'Robotics', 'Role', 'Semantics', 'Sensory', 'Stimulus', 'Stream', 'Stroke', 'Structure of supramarginal gyrus', 'Temporal Lobe', 'Testing', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual system structure', 'Work', 'area striata', 'base', 'behavior measurement', 'behavior test', 'blind', 'cognitive development', 'evidence base', 'extrastriate', 'extrastriate visual cortex', 'fovea centralis', 'grasp', 'information processing', 'innovation', 'lens', 'multisensory', 'neuroprosthesis', 'post stroke', 'programs', 'relating to nervous system', 'sensory integration', 'theories', 'tool', 'visual information', 'visual motor', 'visual object processing', 'visual process', 'visual processing']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2019,439559,0.0017978131651840562
"Studying crowding as a window into object recognition and development and health of visual cortex ABSTRACT  Our long-term goal is to understand how the human brain recognizes objects. This 3-year project will characterize the computational kernel (computation that is applied independently to many parts of the image data) that is isolated by crowding experiments. We present the discovery that recognition of simple objects is performed by recognition units implementing the same computation at every eccentricity. These units are dense in the fovea and thus hard to isolate there, but they are sparse in the periphery, and easily isolated. Our fMRI & psychophysics pilot data show that each of these units, at every eccentricity, has a circular receptive field with a radius of 2.6±1.5 mm (mean±SD) in human cortical area hV4. Because of cortical magnification, that 2.6 mm corresponds to a tiny 0.05 deg in the fovea, but grows linearly with eccentricity, to a comfortable 3 deg at 10 deg eccentricity. We test this idea by pursuing its implications physiologically (Aim 1), clinically (Aim 2), and psychophysically and computationally (Aim 3).  Aim 1. Better noninvasive measures for the health and development of visual cortex are needed. Conservation of crowding distance (in mm) in a particular cortical area (hV4) would validate crowding distance as a quick, noninvasive measure of that area's condition. Aim 2. Huge public interventions seek to help dyslexic children read faster and identify amblyopic children sooner. It would be valuable to know whether crowding contributes to reading problems and provides a basis for effective screening for dyslexia and amblyopia, as it can be measured before children learn to read. Aim 3. Documenting conservation of efficiency gives evidence that the same universal computation recognizes objects at every eccentricity. We are testing the first computational model of object recognition that accounts for many human characteristics of simple-object recognition. The new work extends to effect of receptive field size and learning. Project Narrative (relevance to public health) This proposal is a collaboration between a psychophysicist, expert on human object recognition, a computer scientist, expert on machine learning for object recognition by computers, and a brain imager, expert on brain mapping, to discover to what extent computer models of object recognition and the brain can account for key properties of human performance. Our first aim tracks the development of crowding in normal and amblyopic children, in collaboration with experts in optometry, reading, and development. Advances in this area could shed light on the problems of people with impaired object recognition, including amblyopia and dyslexia, with a potential for development of early pre-literate screening tests for amblyopia and risk of dyslexia.",Studying crowding as a window into object recognition and development and health of visual cortex,9637403,R01EY027964,"['Address', 'Adult', 'Affect', 'Age', 'Amblyopia', 'Area', 'Atlases', 'Biological', 'Brain', 'Brain Mapping', 'Bypass', 'Child', 'Clinical', 'Collaborations', 'Complex', 'Computer Simulation', 'Computers', 'Crowding', 'Data', 'Development', 'Disease', 'Dyslexia', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Human', 'Human Characteristics', 'Image', 'Immunity', 'Impairment', 'Intervention', 'Italy', 'Joints', 'Learning', 'Letters', 'Light', 'Location', 'Machine Learning', 'Magic', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Neurosciences', 'Noise', 'Nose', 'Occipital lobe', 'Optometry', 'Participant', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Population Heterogeneity', 'Postdoctoral Fellow', 'Property', 'Psychophysics', 'Public Health', 'Radial', 'Reading', 'Rest', 'Risk', 'Rome', 'Scientist', 'Speed', 'Stereotyping', 'Stimulus Deprivation-Induced Amblyopia', 'Surface', 'Testing', 'Vision', 'Visual Cortex', 'Visual Fields', 'Work', 'assault', 'cerebral atrophy', 'clinical application', 'convolutional neural network', 'crowdsourcing', 'experimental study', 'extrastriate visual cortex', 'fovea centralis', 'imager', 'literate', 'object recognition', 'physiologic model', 'receptive field', 'relating to nervous system', 'research clinical testing', 'sample fixation', 'screening', 'vision development']",NEI,NEW YORK UNIVERSITY,R01,2019,388812,0.02044151489920811
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9653180,R01EY025332,"['3-Dimensional', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2019,416574,0.07793366125332993
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,9727981,R01EY027023,"['3-Dimensional', 'Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'predictive modeling', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2019,399500,0.013035873771238502
"Annual meeting of the Vision Sciences Society: Travel grants for junior investigators PROJECT SUMMARY/ABSTRACT The Vision Sciences Society is a nonprofit membership organization of nearly 2000 scientists interested in the functional aspects of vision. VSS was founded in 2001 with the purpose of bringing together scientists from a broad range of disciplines including visual psychophysics, visual neuroscience, computational vision and visual cognition. The scientific content of the meetings reflects the breadth of topics and interconnected ideas and approaches in modern vision science, from visual coding to perception, recognition and the visual control of action, as well as recent developments in cognitive psychology, computer vision and neuroimaging. Since its founding, VSS has provided a forum and framework for communicating advances in vision science, and VSS has become a flagship conference for the field. The interdisciplinary nature of VSS is reflected in the deliberately diverse membership of the Board of Directors and Abstract Review Committee, and by its formal relationship with the more clinically-oriented Association for Research in Vision and Ophthalmology. Many of the faculty from institutions in the United States who attend VSS are principal investigators of National Eye Institute grants; hence, the research objectives of the programs of the National Institutes of Health and of the National Eye Institute are well-represented in the program planning and individual presentations. Over 60% of participants are predoctoral and postdoctoral trainees. Of these 55% are US citizens. VSS provides multiple career development opportunities: (1) the platform and poster presentations provide a forum for trainees to showcase their work and receive feedback, (2) career-development workshops cover topics such as “Getting that Faculty Job”, “Reviewing and Responding to Reviews”, “The Public Face of your Science”, “Careers in Industry and Government”, “Faculty Careers at Primarily Undergraduate Institutions”, and include panel discussions with journal editors, NIH and NSF grant officers, and academic and industry representatives, (3) a “Meet the Professors” event in which trainees meet in small groups with members of the VSS Board and other professors for free-wheeling, open-ended discussions, and (4), a partnership with ARVO through which trainees from one society can carry out research or attend the meeting of the other. Informally, VSS provides opportunities for networking with peers and senior colleagues in a comfortable and engaging setting. The large contingent of early-stage investigators at VSS is a sign of the strong health of the field and the opportunity VSS provides for advancing the field. Our goal is to facilitate access and participation for this next generation of vision scientists. The purpose of this grant is to provide 35 travel awards (5 for ARVO affiliates) for early-career investigators to attend the 2019 meeting, with the focus on attracting and supporting a diverse pool of pre- doctoral students, postdoctoral trainees, and pre-tenure faculty who demonstrate potential for future success as vision researchers and whose research findings will be presented at the meeting. Funds are also requested to support childcare services for awardees and workshops and social events for all early-career participants. PROJECT NARRATIVE The Vision Science Society (VSS) organizes an annual meeting for presenting cutting edge research on the mechanisms and principles of normal and abnormal visual perception. The aging American population will experience an increasing rate of visual loss due to diseases and disorders of the eye and central visual pathway. Understanding the perceptual sequelae of and developing effective therapies for visual disorders requires knowledge in the diverse research domains sponsored by the annual meeting of VSS.",Annual meeting of the Vision Sciences Society: Travel grants for junior investigators,9763082,R13EY030356,"['3-Dimensional', 'Address', 'Aging', 'American', 'Area', 'Attention', 'Award', 'Binocular Vision', 'Blindness', 'Child Care', 'Clinical', 'Coffee', 'Cognitive Science', 'Color', 'Complement', 'Computer Simulation', 'Computer Vision Systems', 'Development', 'Discipline', 'Disease', 'Educational workshop', 'Event', 'Exhibits', 'Eye Movements', 'Eye diseases', 'Face', 'Faculty', 'Feedback', 'Female', 'Florida', 'Funding', 'Future', 'Goals', 'Government', 'Grant', 'Health', 'Hour', 'Human', 'Individual', 'Industry', 'Institution', 'Island', 'Journals', 'Knowledge', 'Light', 'Medal', 'Mediation', 'Mentors', 'Mentorship', 'Modernization', 'Motion Perception', 'Music', 'National Eye Institute', 'Nature', 'Occupations', 'Ophthalmology', 'Participant', 'Perception', 'Perceptual learning', 'Population', 'Principal Investigator', 'Psychophysics', 'Publishing', 'Research', 'Research Personnel', 'Resort', 'Review Committee', 'Science', 'Scientist', 'Services', 'Societies', 'Students', 'Time', 'Training', 'Travel', 'United States', 'United States National Institutes of Health', 'Vendor', 'Vision', 'Vision Disorders', 'Visual Illusions', 'Visual Pathways', 'Visual Perception', 'Visual Psychophysics', 'Work', 'care systems', 'career', 'career development', 'design', 'doctoral student', 'effective therapy', 'experience', 'face perception', 'fovea centralis', 'graduate student', 'interest', 'mathematical model', 'meetings', 'member', 'multisensory', 'neuroimaging', 'next generation', 'object recognition', 'peer', 'perceptual organization', 'posters', 'pre-doctoral', 'professor', 'programs', 'science and society', 'social', 'spatial vision', 'success', 'symposium', 'undergraduate student', 'vision science', 'visual coding', 'visual cognition', 'visual control', 'visual memory', 'visual neuroscience', 'visual processing', 'visual search']",NEI,UNIVERSITY OF NEVADA RENO,R13,2019,47210,0.021019612211648712
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",9919259,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Work', 'base', 'cell type', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2019,600000,-0.009387851834965735
"Genetic Modulators of Glaucoma Glaucoma is the leading cause of irreversible blindness in the world. While elevated intraocular pressure (IOP) is a major risk factor, damage and death of retinal ganglion cells (RGCs) underlies visual field loss. However, a thorough understanding of this disease is a major challenge because its genetic basis is heterogeneous and it represents a family of age-related disorders resulting from intersecting gene-regulated pathophysiologic networks. We propose to continue to use the BXD (C57BL/6 x DBA/2J) family of recombinant inbred (RI) lines of mice as a genetic reference panel (GRP) and to combine our work with human genome wide association studies (GWAS), to uncover and clarify the genetic heterogeneity that underlies optic nerve (ON) damage. We have had recent success using this combined approach in the regulation of intraocular pressure (IOP). We are very well positioned to take the next step and apply this approach to define cellular targets of RGC damage and death. We propose to uncover phenotypic diversities of glaucoma-related ON damage and uncover common underlying mechanisms that are shared with IOP modulation. Our long-term research goal is to identify disease mechanisms and develop neuroprotective therapies to preserve retinal health in patients at risk for glaucoma. Our overall objective is to identify novel gene products and related mechanisms that lead to glaucomatous endophenotypes using multi-dimensional genetic analyses, cross-species comparisons (mouse, rat and human) and validation using novel murine glaucoma models. Our central hypothesis is that molecular processes leading to glaucoma associated-endophenotypes, such as elevated IOP and ON damage, are shared across species, and that species comparisons can uncover common underlying mechanisms, and efficient testing of targeted glaucoma therapeutics. In the current investigation, we perform a systematic analysis of ON damage, and an additional species—rat. We will mine the extensive databases of IOP and ON damage that we are generating for more than 70 BXD strains across five age cohorts with the goal of defining new models of glaucoma. An overall strength of this proposal is the combination of cutting-edge systems genetics methods, species comparisons of glaucoma phenotypes, and a strong interdisciplinary team that includes investigators with extensive experience in systems genetics, glaucoma, GWAS in human and rats, and advanced computational methods. To test our hypothesis, we will perform the following thress studies: 1) Identify the candidate gene on chromosome 12 that modulates ON damage; 2) Determine if modulation of IOP and/or ON damage is shared across rodent species; and 3) Identify novel spontaneous glaucoma models through a comprehensive analysis of our enlarged BXD GRP of 100 or more BXD strains. The outcomes of these studies will define novel genes and molecular networks that underlie glaucoma-associated phenotypes and also provide unique glaucoma models for future analysis. These results are expected to fundamentally advance the field of glaucoma disease mechanisms and enable targeted therapeutic development. Glaucoma is the leading cause of irreversible blindness in the world and a thorough understanding of this disease is a major challenge because its genetic basis is heterogeneous, and it likely represents a family of disorders resulting from intersecting gene-regulated pathophysiologic pathways. Our goals are to: identify candidate gene(s) that modulate optic nerve damage; determine if regulation of intraocular pressure and/or optic nerve damage are shared across species; and identify novel spontaneous glaucoma models. These outcomes will fundamentally advance the field of glaucoma disease mechanisms and enable targeted therapeutic development.",Genetic Modulators of Glaucoma,9648959,R01EY021200,"['Age', 'Axon', 'Blindness', 'Candidate Disease Gene', 'Cell Death', 'Cellular Assay', 'Cessation of life', 'Chromosomes, Human, Pair 12', 'Clinical', 'Computing Methodologies', 'Data', 'Databases', 'Disease', 'Family', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Heterogeneity', 'Genomics', 'Glaucoma', 'Goals', 'Health', 'Human', 'Human Genome', 'Inbred Strains Rats', 'Inbreeding', 'Investigation', 'Laboratories', 'Lead', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Ocular Hypertension', 'Optic Nerve', 'Outcome', 'Outcome Study', 'Pathway interactions', 'Patients', 'Phenotype', 'Physiologic Intraocular Pressure', 'Population', 'Positioning Attribute', 'Process', 'Publications', 'Quantitative Trait Loci', 'Rattus', 'Recombinants', 'Regulation', 'Research', 'Research Personnel', 'Retinal', 'Retinal Ganglion Cells', 'Risk', 'Risk Factors', 'Rodent', 'System', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual Fields', 'Work', 'age related', 'cell injury', 'cellular targeting', 'clinical subtypes', 'cohort', 'deep neural network', 'density', 'endophenotype', 'experience', 'gene product', 'genetic analysis', 'genome wide association study', 'human data', 'human model', 'lead candidate', 'novel', 'novel therapeutics', 'preservation', 'success', 'targeted treatment', 'therapeutic development', 'treatment strategy']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2019,391271,0.015056607471508601
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,9818350,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2019,641060,0.04829393478010805
"GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN PROJECT SUMMARY & ABSTRACT  Human locomotion through natural environments requires the coordination of all levels of the sensorimotor hierarchy, from the cortical areas involved in processing of visual information and high level planning to the subcortical and spinal structures involved in the regulation of the gait and posture. However, despite the complex neural bases of human locomotion, the output is highly regular and well organized around the basic physical dynamics and biomechanics that define the stability and energetic costs of moving a bipedal body through space. There is a rich and growing body of literature describing detailed knowledge each of the individual components of human locomotion, including neural mechanisms, muscular neuromechanics, and biomechanics. However, very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion through a complex and dynamic world. This lack of integrative research not only restricts the breadth of impact of research from these individual disciplines, but also limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to to fill this critical gap in our knowledge about human locomotion, it is necessary to develop an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world. In service of this general goal, this proposal outlines research projects aimed at specific unanswered questions about locomotion over different terrains. This proposal comprises three specific research and training aims on the visual control of locomotion over rough terrain. Aim 1 focuses on the behavioral task itself, Aim 2 investigates the sensory stimulus experienced during real-world locomotion, and Aim 3 examines the motor integration of visually specified goals into the ongoing gait cycle. Aim 1 investigates effects of changing environmental uncertainty and task demands on gaze allocation strategies during locomotion over real-world rough terrain. Aim 2 analyzes and models the visual stimulus experienced during locomotion over real-world rough terrain. Aim 3 determines how visually specified target footholds and targets are integrated into the ongoing preferred steady-state gait. Together these aims will significantly advance our understanding of how humans use vision to control their movement through the natural world, which greatly increase our ability to develop clinical diagnosis and treatment for loss of locomotor function. PROJECT NARRATIVE  Very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion. This lack of integrative research limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to fill this critical gap in our knowledge about human locomotion, this proposal develops an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world.",GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN,9999721,R00EY028229,"['3-Dimensional', 'Aging', 'Algorithms', 'Area', 'Attention', 'Behavior', 'Behavioral', 'Biomechanics', 'Clinical Treatment', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Development', 'Discipline', 'Environment', 'Eye', 'Gait', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Link', 'Literature', 'Locomotion', 'Measures', 'Mechanics', 'Mentors', 'Modeling', 'Motion', 'Motor', 'Movement', 'Muscle', 'Musculoskeletal', 'Nature', 'Neuromechanics', 'Output', 'Parkinson Disease', 'Pattern', 'Phase', 'Photic Stimulation', 'Positioning Attribute', 'Postdoctoral Fellow', 'Posture', 'Protocols documentation', 'Regulation', 'Research', 'Research Activity', 'Research Project Grants', 'Research Training', 'Services', 'Signal Transduction', 'Specific qualifier value', 'Spinal', 'Stroke', 'Structure', 'System', 'Training', 'Training Programs', 'Uncertainty', 'Vision', 'Visual', 'Visual Fields', 'Walking', 'Wireless Technology', 'area MST', 'area MT', 'base', 'clinical Diagnosis', 'cost', 'design', 'environmental change', 'experience', 'experimental study', 'foot', 'gaze', 'insight', 'instrument', 'kinematics', 'multidisciplinary', 'neuromechanism', 'neuromuscular', 'novel', 'optic flow', 'programs', 'relating to nervous system', 'response', 'sensory stimulus', 'skills', 'statistics', 'treadmill', 'treatment planning', 'visual control', 'visual information', 'visual processing', 'visual stimulus', 'visual-motor integration']",NEI,NORTHEASTERN UNIVERSITY,R00,2019,249000,-0.001188020647229395
"The nGoggle: A portable brain-based device for assessment of visual function deficits PROJECT SUMMARY Assessment of loss of visual function outside the foveal area is an essential component of the management of numerous conditions, including glaucoma, retinal and neurological disorders. Despite the significant progress achieved with the development of standard automated perimetry (SAP) many decades ago, assessment of visual field loss with SAP still has significant drawbacks. SAP testing is limited by subjectivity of patient responses and high test-retest variability, frequently requiring many tests for effective detection of change over time. Moreover, as these tests are generally conducted in clinic-based settings, limited patient availability and health care resources often result in an insufficient number of tests acquired over time, with delayed diagnosis and detection of disease progression. The requirement for highly trained technicians, cost, complexity, and lack of portability of SAP also preclude its use for screening of visual field loss in underserved populations. To address shortcomings of current methods to assess visual function, we have developed the nGoggle, a wearable device that uses a head-mounted display (HMD) integrated with wireless electroencephalography (EEG), capable of objectively assessing visual field deficits using multifocal steady-state visual-evoked potentials (mfSSVEP). As part of the funded NEI SBIR Phase I, we developed the nGoggle prototype using a modified smartphone-based HMD display and non-disposable electrodes. In our Phase I studies, we conducted benchmarking tests on signal quality of EEG acquisition, developed methods for EEG data extraction and analysis, and conducted a pilot study demonstrating the ability of the device to detect visual field loss in glaucoma, a progressive neuropathy that results in characteristic damage to the optic nerve and resulting visual field defects. We also identified limitations of current existing displays and electrodes, as well as potential avenues for enhancing test reliability and improving user interface. Based on the encouraging results from Phase I and a clear delineation of the steps needed to bring the device into its final commercial product form, we now propose a series of Phase II studies. We hypothesize that optimization of nGoggle's accuracy and repeatability in detecting visual function loss can be achieved through the development of a customized head-mounted display with front-view eye/pupil tracking cameras and disposable no-prep electrodes, as well as enhancement of the visual stimulation protocol and data analytics. The specific aims of this proposal are: 1) To develop a customized head-mounted display and enhanced no-prep electrodes for improving nGoggle's ability to acquire users' mfSSVEP with high signal-to- noise ratios (SNR) in response to visual stimulation; 2) To optimize and validate mfSSVEP stimuli design and data analytics to enhance the accuracy and repeatability of assessing visual function loss with the nGoggle. 3) Complete pivotal clinical studies to support FDA approval. PROJECT NARRATIVE NGoggle Inc. has developed the nGoggle, a wearable device that uses a head-mounted display integrated with wireless electroencephalography, capable of objectively assessing visual field deficits using multifocal steady- state visual-evoked potentials. NGoggle Inc is now proposing to optimize nGoggle's accuracy and repeatability in detecting visual function loss with the use of a customized display, adherent no-prep electrodes, optimized visual stimuli and data analytics. It will also complete pivotal clinical studies to support FDA approval.",The nGoggle: A portable brain-based device for assessment of visual function deficits,9772484,R42EY027651,"['Address', 'Area', 'Base of the Brain', 'Benchmarking', 'Blindness', 'Brain', 'Cellular Phone', 'Characteristics', 'Client satisfaction', 'Clinic', 'Clinical Research', 'Custom', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Disease Progression', 'Elastomers', 'Electrodes', 'Electroencephalography', 'Electrooculogram', 'Exhibits', 'Eye', 'Funding', 'Glaucoma', 'Head', 'Healthcare', 'Methods', 'Neuropathy', 'Noise', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Perimetry', 'Phase', 'Photic Stimulation', 'Pilot Projects', 'Protocols documentation', 'Pupil', 'Resources', 'Retinal Diseases', 'Scotoma', 'Series', 'Signal Transduction', 'Skin', 'Small Business Innovation Research Grant', 'Source', 'Stimulus', 'Testing', 'Time', 'Training', 'Underserved Population', 'Vision', 'Visual Fields', 'Visual evoked cortical potential', 'Wireless Technology', 'base', 'cost', 'design', 'field study', 'improved', 'loss of function', 'machine learning algorithm', 'nervous system disorder', 'patient response', 'phase 1 study', 'phase 2 study', 'portability', 'prototype', 'real world application', 'relating to nervous system', 'response', 'sample fixation', 'screening', 'virtual reality', 'visual stimulus', 'wearable device']",NEI,"NGOGGLE, INC.",R42,2019,648557,0.06460612915645945
"Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma PROJECT SUMMARY This project aims to apply novel machine learning techniques to recently developed optical imaging measurement to improve the accurate prediction and detection of glaucomatous progression. Complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and advanced pattern recognition/machine learning-based analysis techniques can find and use that hidden information. We will use mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1,800 patient and healthy eyes, available as the result of long-term NIH funding. We also will investigate deep learning and novel statistical techniques for this purpose. The required longitudinal measurements from several newly developed optical imaging techniques were not available to our previously funded NEI- supported work. The proposed work potentially can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care by informing clinical decision-making based on mathematically based, externally validated methods. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. PROJECT NARRATIVE The proposed project will improve machine learning techniques for predicting and detecting glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments and will make use of a very large amount of data, obtained using previously awarded NIH funds, to do so. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neuro- degeneration within the visual pathways at structural and functional levels. The development of a clinically useful novel, empirical system for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and on the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma,9517942,R21EY027945,"['Address', 'Algorithms', 'Anatomy', 'Award', 'Caring', 'Classification', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Defect', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Environment', 'Eye', 'Frequencies', 'Funding', 'Future', 'Gaussian model', 'Generations', 'Glaucoma', 'Goals', 'Health Personnel', 'Image', 'Imaging Device', 'Imaging Techniques', 'Instruction', 'Laboratories', 'Lasers', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Perimetry', 'Physiologic Intraocular Pressure', 'Reporting', 'Research', 'Scanning', 'Science', 'Series', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Variant', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'care providers', 'clinical care', 'clinical decision-making', 'cost', 'deep learning', 'expectation', 'glaucoma test', 'high dimensionality', 'improved', 'independent component analysis', 'instrument', 'learning strategy', 'markov model', 'mathematical model', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'recruit', 'retinal nerve fiber layer', 'tool']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2018,193750,0.01434596650519646
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9507909,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Grain', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'cognitive development', 'computerized', 'cost', 'deep learning', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'sensor technology', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2018,203125,-9.574923133733544e-05
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9564111,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biological Neural Networks', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2018,564228,0.05451976952800462
"Representation of information across the human visual cortex ﻿    DESCRIPTION (provided by applicant): The human visual system is organized as a parallel, hierarchical network, and successive stages of visual processing appear to represent increasingly complicated aspects of shape-related and semantic information. However, the way that shape-related and semantic information is represented across much of the visual hierarchy is still poorly understood. The primary goal of this proposal is to understand how information about object shape and semantic category is represented explicitly across mid- and high-level visual areas. To address this important issue we propose to undertake a series of human functional MRI (fMRI) studies, using both synthetic and natural movies. Data will be analyzed by means of a powerful voxel-wise modeling (VM) approach that has been developed in my laboratory over the past several years. In Aim 1 we propose to measure human brain activity evoked by synthetic naturalistic movies, and to use VM to evaluate and compare several competing theories of shape representation across the entire visual cortex. In Aim 2 we propose to use VM to evaluate and compare competing theories of semantic representation. In Aim 3 we propose to use machine learning and and VM to discover new aspects of shape and semantic representation. These experiments will provide fundamental new insights about the representation of visual information across visual cortex. PUBLIC HEALTH RELEVANCE: Disorders of central vision can severely affect quality of life and the design of treatments and devices for improving visual function will depend critically on understanding the organization of visual cortex. We propose to use functional MRI and sophisticated computational data analysis and modeling procedures to evaluate and compare multiple theories of visual function. The results will reveal how visual information is represented across the several dozen distinct functional areas that constitute human visual cortex.",Representation of information across the human visual cortex,9542335,R01EY019684,"['Address', 'Affect', 'Area', 'Award', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Devices', 'Dimensions', 'Disease', 'Elements', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Human', 'Image', 'Individual', 'Laboratories', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Motion', 'Perception', 'Procedures', 'Quality of life', 'Semantics', 'Series', 'Shapes', 'Surface', 'System', 'Testing', 'Training', 'V2 neuron', 'V4 neuron', 'Vision', 'Vision research', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'area striata', 'artificial neural network', 'base', 'data modeling', 'design', 'exhaustion', 'experimental study', 'extrastriate visual cortex', 'high dimensionality', 'improved', 'innovation', 'insight', 'learning strategy', 'movie', 'novel', 'object recognition', 'object shape', 'public health relevance', 'receptive field', 'theories', 'therapy design', 'visual information', 'visual neuroscience', 'visual processing']",NEI,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2018,376543,0.008828729380989775
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9499823,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Research Infrastructure', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radiofrequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416374,0.050803426385949035
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,9581681,K99EY029329,"['Affect', 'Algorithms', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Imagery', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality']",NEI,UNIVERSITY OF WASHINGTON,K99,2018,122763,0.014962473180550493
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE Mapping effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing of different types of individual cells, understanding the func- tion and relationships between those cell types, and modeling their individual and collective function. In order to exploit human and machine intelligence, different visual interfaces will be implemented that use the CCF in support of data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, mathematical biology, and biomedical data standards to develop a highly accurate and extensible multidimen- sional spatial basemap of the human body and associated data overlays that can be interactively explored online as an atlas of tissue maps. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across the human body along multiple functional contexts (e.g., systems physiology, vascular, or endocrine systems), and connect and integrate further computational, analytical, visualization, and biometric resources as driven by the context or “position” on the map. The CCF and the interactive data visualizations will be multi-level and multi-scale sup- porting the exploration and communication of tissue and publication data--from single cell to whole body. In the first year, the proposed Mapping Component will run user needs analyses, compile an initial CCF using pre-existing classifications and ontologies; implement two interactive data visualizations; and evaluate the usa- bility and effectiveness of the CCF and associated visualizations in formal user studies. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an ex- tensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spa- tial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high- resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector, and Meta Datasets",9687220,OT2OD026671,"['Address', 'Anatomy', 'Artificial Intelligence', 'Atlases', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Classification', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Set', 'Ecosystem', 'Educational workshop', 'Effectiveness', 'Endocrine system', 'Future', 'Genetic', 'Goals', 'Human', 'Human body', 'Image', 'Imagery', 'Individual', 'Investigation', 'Knowledge', 'Machine Learning', 'Maps', 'Mathematical Biology', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Ontology', 'Organ', 'Participant', 'Physiological', 'Physiology', 'Positioning Attribute', 'Production', 'Publications', 'Research Infrastructure', 'Resolution', 'Resources', 'Running', 'Services', 'System', 'Tissues', 'Update', 'Vascular System', 'Visual', 'Visualization software', 'Work', 'base', 'cell type', 'computing resources', 'data integration', 'data mining', 'data visualization', 'design', 'hackathon', 'human imaging', 'interoperability', 'member', 'systematic review', 'usability', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2018,330000,0.00972678657597071
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9711194,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2018,99914,-0.005427903516466572
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9572992,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,297237,0.006251159384113965
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9465330,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,503162,0.01971900594895264
"A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device ABSTRACT This project will create the first objective measurement tool, the VisBox, for the vision subtype of concussion (VSC). This will enable physicians to identify VSC without an eye-care professional, for referral to a vision specialist for personalized vision therapy recommendations. The persistence of concussion symptoms beyond several weeks is often a life-altering situation for affected individuals, and children are particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties. A lack of accessible, objective vision diagnostics are critical barriers to identification of VSC and referral for treatment. The VisBox will be a software product that is used with the OcuTracker, Oculogica’s proprietary eye-tracking hardware platform. The VisBox will input eye movement measurements from the OcuTracker, calculate metrics that correspond to aspects of cranial nerve function affected during a concussion, and use those metrics to calculate a score to predict VSC using an algorithm developed with guided machine learning in the course of this study. The VisBox will be used by non- vision specialists to objectively measure three vision disorders related to concussion: convergence insufficiency (CI), accommodative insufficiency (AI), and saccadic dysfunction (SD) in under 4 minutes, during the clinical visit where the concussion is diagnosed. The long-term goal is to develop an objective assessment of vision characteristics, that will enable physicians that are non-specialists in vision to 1) screen for concussion-related vision disorders; 2) identify VSC; 3) make decisions about the necessity of a referral for a comprehensive vision examination; 4) monitor the effectiveness of vision treatment. Phase I Hypothesis. VisBox can produce an output score that correlates with the presence or absence of TBI- related vision disorder, i.e., VSC, by leveraging the OcuTracker visual stimulus and eye tracking system. Specific Aim I. Generate OcuTracker eye tracking data and the diagnosis of TBI-related vision disorder in 250 pediatric concussion patients. Specific Aim II. Develop and validate VisBox algorithm for assessing CI, AI, and SD using OcuTracker data. Plans for Phase II. The VisBox score will be used to predict responsiveness to vision therapy in a prospective randomized clinical study. Phase II will be a multi-armed study comparing vision therapy with placebo therapy in concussion patients and assessing whether the VisBox software can predict which patients are responsive to vision therapy. Commercial Opportunity. VisBox customers are non-eye care specialists including neurologists, pediatricians, emergency room physicians, sports medicine physicians, and concussion specialists. The total addressable market is $400M, assuming 4M annual scans at $100/scan needed for concussions in the US. PUBLIC HEALTH RELEVANCE STATEMENT At least 4 million concussions occur in the US each year, and up to 30% of these injuries persist beyond 4 weeks in a condition known as persistent post-concussion symptoms, which is often a life-altering situation for affected individuals, with children particularly vulnerable as they are at risk for co-morbidities such as chronic pain, fatigue, depression, anxiety, and learning difficulties – with often serious consequences. The lack of an accessible, objective vision diagnostics presents a critical barrier to identification of the vision disorder concussion sub-type (VSC) and referral for treatment. The proposed technology will be the first objective tool that can be used by non-vision-specialists to identify concussion-related vision symptoms that is accessible to a broad range of facilities and will enable non-specialist physicians the ability to refer patients to concussion specialists to improve outcomes, decrease the time it takes patients to return to work or play, and reduce healthcare costs associated with this debilitating condition.",A software tool for objective identification of concussion-related vision disorders using a novel eye-tracking device,9698505,R41NS103698,"['Address', 'Affect', 'Algorithms', 'Anxiety', 'Area Under Curve', 'Brain Concussion', 'Caring', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Clinical Research', 'Clinical assessments', 'Comorbidity', 'Computer software', 'Convergence Insufficiency', 'Cranial Nerves', 'Data', 'Data Analyses', 'Decision Making', 'Devices', 'Diagnosis', 'Diagnostic', 'Economics', 'Effectiveness', 'Emergency Department Physician', 'Evaluation', 'Eye', 'Eye Movements', 'Family', 'Fatigue', 'Fees', 'Functional disorder', 'Goals', 'Health Care Costs', 'Individual', 'Injury', 'Intervention', 'Learning', 'Life', 'Machine Learning', 'Measurement', 'Measures', 'Mental Depression', 'Monitor', 'Neurologist', 'Neurosurgeon', 'Optometrist', 'Output', 'Patients', 'Performance', 'Phase', 'Physicians', 'Placebos', 'Play', 'Post-Concussion Syndrome', 'Prevalence', 'Primary Care Physician', 'Randomized', 'Recommendation', 'Research Personnel', 'Resolution', 'Rest', 'Risk', 'Sampling', 'Scanning', 'Small Business Technology Transfer Research', 'Software Tools', 'Specialist', 'Sports Medicine', 'Symptoms', 'System', 'TBI Patients', 'Technology', 'Therapeutic Intervention', 'Time', 'Traumatic Brain Injury recovery', 'Treatment Efficacy', 'Vision', 'Vision Disorders', 'Visit', 'Work', 'associated symptom', 'chronic pain', 'commercial application', 'concussive symptom', 'disabling symptom', 'disorder subtype', 'economic impact', 'handheld equipment', 'improved outcome', 'lens', 'novel', 'patient response', 'pediatrician', 'population based', 'prospective', 'public health relevance', 'recruit', 'skills', 'software development', 'success', 'therapy outcome', 'tool', 'tv watching', 'visual stimulus']",NINDS,"OCULOGICA, INC.",R41,2018,50000,0.01971900594895264
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9644103,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Personal Satisfaction', 'Persons', 'Phase', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'manufacturability', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2018,519349,0.017099294478208085
"Studying crowding as a window into object recognition and development and health of visual cortex ABSTRACT  Our long-term goal is to understand how the human brain recognizes objects. This 3-year project will characterize the computational kernel (computation that is applied independently to many parts of the image data) that is isolated by crowding experiments. We present the discovery that recognition of simple objects is performed by recognition units implementing the same computation at every eccentricity. These units are dense in the fovea and thus hard to isolate there, but they are sparse in the periphery, and easily isolated. Our fMRI & psychophysics pilot data show that each of these units, at every eccentricity, has a circular receptive field with a radius of 2.6±1.5 mm (mean±SD) in human cortical area hV4. Because of cortical magnification, that 2.6 mm corresponds to a tiny 0.05 deg in the fovea, but grows linearly with eccentricity, to a comfortable 3 deg at 10 deg eccentricity. We test this idea by pursuing its implications physiologically (Aim 1), clinically (Aim 2), and psychophysically and computationally (Aim 3).  Aim 1. Better noninvasive measures for the health and development of visual cortex are needed. Conservation of crowding distance (in mm) in a particular cortical area (hV4) would validate crowding distance as a quick, noninvasive measure of that area's condition. Aim 2. Huge public interventions seek to help dyslexic children read faster and identify amblyopic children sooner. It would be valuable to know whether crowding contributes to reading problems and provides a basis for effective screening for dyslexia and amblyopia, as it can be measured before children learn to read. Aim 3. Documenting conservation of efficiency gives evidence that the same universal computation recognizes objects at every eccentricity. We are testing the first computational model of object recognition that accounts for many human characteristics of simple-object recognition. The new work extends to effect of receptive field size and learning. Project Narrative (relevance to public health) This proposal is a collaboration between a psychophysicist, expert on human object recognition, a computer scientist, expert on machine learning for object recognition by computers, and a brain imager, expert on brain mapping, to discover to what extent computer models of object recognition and the brain can account for key properties of human performance. Our first aim tracks the development of crowding in normal and amblyopic children, in collaboration with experts in optometry, reading, and development. Advances in this area could shed light on the problems of people with impaired object recognition, including amblyopia and dyslexia, with a potential for development of early pre-literate screening tests for amblyopia and risk of dyslexia.",Studying crowding as a window into object recognition and development and health of visual cortex,9455331,R01EY027964,"['Address', 'Adult', 'Affect', 'Age', 'Amblyopia', 'Area', 'Atlases', 'Biological', 'Brain', 'Brain Mapping', 'Bypass', 'Child', 'Clinical', 'Collaborations', 'Complex', 'Computer Simulation', 'Computers', 'Crowding', 'Data', 'Development', 'Disease', 'Dyslexia', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Human', 'Human Characteristics', 'Image', 'Immunity', 'Impairment', 'Intervention', 'Italy', 'Joints', 'Learning', 'Letters', 'Light', 'Location', 'Machine Learning', 'Magic', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Neurosciences', 'Noise', 'Nose', 'Occipital lobe', 'Optometry', 'Participant', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Population Heterogeneity', 'Postdoctoral Fellow', 'Property', 'Psychophysics', 'Public Health', 'Radial', 'Reading', 'Rest', 'Risk', 'Rome', 'Scientist', 'Speed', 'Stereotyping', 'Stimulus Deprivation-Induced Amblyopia', 'Surface', 'Testing', 'Vision', 'Visual Cortex', 'Visual Fields', 'Work', 'assault', 'cerebral atrophy', 'clinical application', 'crowdsourcing', 'experimental study', 'extrastriate visual cortex', 'fovea centralis', 'imager', 'literate', 'object recognition', 'physiologic model', 'receptive field', 'relating to nervous system', 'research clinical testing', 'sample fixation', 'screening', 'vision development']",NEI,NEW YORK UNIVERSITY,R01,2018,388993,0.02044151489920811
"The nGoggle: A portable brain-based device for assessment of visual function deficits PROJECT SUMMARY Assessment of loss of visual function outside the foveal area is an essential component of the management of numerous conditions, including glaucoma, retinal and neurological disorders. Despite the significant progress achieved with the development of standard automated perimetry (SAP) many decades ago, assessment of visual field loss with SAP still has significant drawbacks. SAP testing is limited by subjectivity of patient responses and high test-retest variability, frequently requiring many tests for effective detection of change over time. Moreover, as these tests are generally conducted in clinic-based settings, limited patient availability and health care resources often result in an insufficient number of tests acquired over time, with delayed diagnosis and detection of disease progression. The requirement for highly trained technicians, cost, complexity, and lack of portability of SAP also preclude its use for screening of visual field loss in underserved populations. To address shortcomings of current methods to assess visual function, we have developed the nGoggle, a wearable device that uses a head-mounted display (HMD) integrated with wireless electroencephalography (EEG), capable of objectively assessing visual field deficits using multifocal steady-state visual-evoked potentials (mfSSVEP). As part of the funded NEI SBIR Phase I, we developed the nGoggle prototype using a modified smartphone-based HMD display and non-disposable electrodes. In our Phase I studies, we conducted benchmarking tests on signal quality of EEG acquisition, developed methods for EEG data extraction and analysis, and conducted a pilot study demonstrating the ability of the device to detect visual field loss in glaucoma, a progressive neuropathy that results in characteristic damage to the optic nerve and resulting visual field defects. We also identified limitations of current existing displays and electrodes, as well as potential avenues for enhancing test reliability and improving user interface. Based on the encouraging results from Phase I and a clear delineation of the steps needed to bring the device into its final commercial product form, we now propose a series of Phase II studies. We hypothesize that optimization of nGoggle's accuracy and repeatability in detecting visual function loss can be achieved through the development of a customized head-mounted display with front-view eye/pupil tracking cameras and disposable no-prep electrodes, as well as enhancement of the visual stimulation protocol and data analytics. The specific aims of this proposal are: 1) To develop a customized head-mounted display and enhanced no-prep electrodes for improving nGoggle's ability to acquire users' mfSSVEP with high signal-to- noise ratios (SNR) in response to visual stimulation; 2) To optimize and validate mfSSVEP stimuli design and data analytics to enhance the accuracy and repeatability of assessing visual function loss with the nGoggle. 3) Complete pivotal clinical studies to support FDA approval. PROJECT NARRATIVE NGoggle Inc. has developed the nGoggle, a wearable device that uses a head-mounted display integrated with wireless electroencephalography, capable of objectively assessing visual field deficits using multifocal steady- state visual-evoked potentials. NGoggle Inc is now proposing to optimize nGoggle's accuracy and repeatability in detecting visual function loss with the use of a customized display, adherent no-prep electrodes, optimized visual stimuli and data analytics. It will also complete pivotal clinical studies to support FDA approval.",The nGoggle: A portable brain-based device for assessment of visual function deficits,9559052,R42EY027651,"['Address', 'Algorithms', 'Area', 'Base of the Brain', 'Benchmarking', 'Blindness', 'Brain', 'Cellular Phone', 'Characteristics', 'Client satisfaction', 'Clinic', 'Clinical Research', 'Custom', 'Data', 'Data Analytics', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Disease Progression', 'Elastomers', 'Electrodes', 'Electroencephalography', 'Electrooculogram', 'Exhibits', 'Eye', 'Funding', 'Glaucoma', 'Head', 'Healthcare', 'Machine Learning', 'Methods', 'Neuropathy', 'Noise', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Perimetry', 'Phase', 'Photic Stimulation', 'Pilot Projects', 'Protocols documentation', 'Pupil', 'Resources', 'Retinal Diseases', 'Scotoma', 'Series', 'Signal Transduction', 'Skin', 'Small Business Innovation Research Grant', 'Source', 'Stimulus', 'Testing', 'Time', 'Training', 'Underserved Population', 'Vision', 'Visual Fields', 'Visual evoked cortical potential', 'Wireless Technology', 'base', 'cost', 'design', 'field study', 'improved', 'loss of function', 'nervous system disorder', 'patient response', 'phase 1 study', 'phase 2 study', 'portability', 'prototype', 'real world application', 'relating to nervous system', 'response', 'sample fixation', 'screening', 'virtual reality', 'visual stimulus', 'wearable device']",NEI,"NGOGGLE, INC.",R42,2018,849367,0.06460612915645945
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9438535,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2018,416574,0.07793366125332993
"Post-natal development of high-level visual representation in primates View-invariant object recognition is a complex cognitive task that is critical to everyday functioning. A key neural correlate of high-level object recognition is inferior temporal (IT) cortex, a brain area present in both humans and non-human primates. Recent advances in visual systems neuroscience have begun to uncover how images are encoded in the adult IT object representation, however the learning rules by which high level visual areas (especially IT) develop remain mysterious, with both the magnitude and qualitative nature of developmental changes remaining almost completely unknown — in part because, over the last thirty years, there have been practically no studies of spiking neural responses in the higher ventral cortical areas of developing primates. There is thus a significant gap in our understanding of how visual development proceeds.  This exploratory proposal aims to characterize how representation in higher primate visual cortex changes during development. We first aim (Aim 1) to implant chronic electrode arrays to record hundreds of IT neuronal sites in response to thousands of image stimuli in awake behaving juvenile macaques. These data will comprise a snapshot of the developing primate visual representation, and will be particularly powerful because we have already extensively measured adult monkey IT using the same stimuli and methods. By comparing juvenile and adult neuronal responses at both single site and population levels, we will obtain a unprecedentedly large-scale and detailed picture of the neural correlates of high-level visual development (Aim 2).  Aims 1 and 2 are exploratory, but potentially transformative – they will result in publicly available neuronal IT development benchmarks against which any proposed model of high level visual development can be rigorously tested, and will spur the development of those models in our lab and others. In that context, we will also seek (Aim 3) to improve known semi- and un-supervised learning rules from the computer vision and computational neuroscience literature, and to compare them to both recent high-performing (but biologically implausible) supervised models as well to the rich developmental measurements obtained in Aims 1 and 2.  Establishing experimental and surgical procedures for juvenile array recordings will create the future opportunity to observe changes in high level neural visual representations while experience is manipulated in early development, and will enable experiments in other sensory, motor, or decision making domains. If successful, the proposed work will yield a deeper understanding of the principles underlying visual cortex development, understanding which will in turn be helpful for treating neurodevelopmental disorders that implicate cortical circuits, including amblyopia and autism. Project narrative  Visual object recognition is fundamental to our everyday functioning. While the brain is remarkably good at accomplishing these challenging tasks, we do not yet know how it learns this ability during development. The goal of these experiments is to develop new experimental and computational tools to discover the neural learning principles that underlie that visual ability.",Post-natal development of high-level visual representation in primates,9455686,R21EY025863,"['Adolescent', 'Adult', 'Amblyopia', 'Animals', 'Area', 'Autistic Disorder', 'Behavioral', 'Benchmarking', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Chronic', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Conflict (Psychology)', 'Data', 'Data Set', 'Decision Making', 'Development', 'Electrodes', 'Exhibits', 'Eye', 'Face', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Head', 'Human', 'Image', 'Implant', 'Inferior', 'Learning', 'Literature', 'Macaca', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monkeys', 'Motivation', 'Motor', 'Nature', 'Neural Network Simulation', 'Neurodevelopmental Disorder', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Performance', 'Population', 'Primates', 'Procedures', 'Process', 'Research', 'Rewards', 'Sensory', 'Site', 'Stimulus', 'Stream', 'Supervision', 'System', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Variant', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'awake', 'base', 'cognitive task', 'computational neuroscience', 'computerized tools', 'experience', 'experimental study', 'extrastriate visual cortex', 'improved', 'juvenile animal', 'learning network', 'mature animal', 'multi-electrode arrays', 'network models', 'neural correlate', 'nonhuman primate', 'novel', 'object recognition', 'postnatal', 'predictive modeling', 'receptive field', 'relating to nervous system', 'response', 'statistics', 'vision development']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2018,232050,0.013561403434114648
"Insightful Surgical Vision Technology (ISVision) for intelligent real-time display of anatomic, physiologic, and pathologic information in surgery Abstract Intelligent intraoperative display of anatomic and physiologic information coupled to real time situational awareness to critical tissues will benefit all surgery. The current surgical vision is limited to naked human eyes in open surgery or more recently using laparoscopic digital imaging for minimally invasive surgery and robot assisted surgery. In laparoscopic vision, although the visualization of surface anatomy and tissue physiology in the surgical field-of-view has been enhanced by recent advances in optical imaging technology, the imaging paradigm still remains passive and still poses a significant challenge due to the inherent narrow peripheral vision and limited depth perception. We propose the next generation laparoscopic vision system that provides intelligent display of unrecognized tissue structures to human eyes. Our proposed use of a new paradigm and surgical vision technology (called ISVisionTM), equipped with modular plenoptic 3-D Color/HD camera and snapshot NIR hyperspectral imager, will permit a clear visualization of the critical target tissue-of-interest, surrounding anatomy into the operative field, and situational awareness for both the tool and tissue to the surgeon. Our Phase I effort comprises of engineering the clinically viable ISVision platform and testing the performance of its intraoperative use. Specifically, the ISVision system will clearly and precisely identify different tissue characteristics, accurately displaying physiologic information including tissue perfusion and blood flow. Following the Phase 1, we anticipate undertaking a Phase II project, during which we will develop a clinical grade ISVision system and investigate its performance and utility in the operating room. While the initial focus of the Phase I and II effort is on the complex surgery such as liver resection due to crucial nature of critical sub-surface structures located underneath liver hilum and parenchyma, the ISVision system will potentially play an enabling role in all surgeries and establish it as a vital component of the operating room of the future. Narrative Real-time display of the accurate anatomic and tissue physiology during surgery is necessary and crucial in improving surgical outcomes. This proposal aims to develop the next generation surgical vision system that provides intelligent display of unrecognized, unseen tissue structures to human eyes. The current state of the art technology still remains passive, focuses on anatomic display only. A new surgical vision technology, called ISVisionTM, will provide clear display of important anatomic structures, identify not only their distinct shape and position, and offer insightful intelligent function during surgical procedures. The academic partner in this proposal has developed a novel research prototype and has shown that the new imaging tool can enhance surgical vision beyond human visibility for preclinical testing. The small business partner backed by a Venture company from the Bay area has a strong background for successful commercialization. Together, we aim to convert the academic research prototype into a clinically viable market product that will significantly improve the safety, function, and outcome of surgery, not limited by surgeons’ visual perception, training or experience. The ISVision technology will make the surgery safer and more effective.","Insightful Surgical Vision Technology (ISVision) for intelligent real-time display of anatomic, physiologic, and pathologic information in surgery",9781563,R41EB026402,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anastomosis - action', 'Anatomy', 'Area', 'Awareness', 'Back', 'Blood flow', 'Businesses', 'Characteristics', 'Child', 'Clinical', 'Clinical Engineering', 'Cognitive', 'Color', 'Complex', 'Complication', 'Conventional Surgery', 'Coupled', 'Depth Perception', 'Development', 'Engineering', 'Environment', 'Excision', 'Eye', 'Family suidae', 'Fostering', 'Future', 'Goals', 'Health system', 'Hemorrhage', 'Hepatic artery', 'Hospitals', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Imaging technology', 'Incubators', 'Individual', 'Injury', 'Intestines', 'Investments', 'Laparoscopes', 'Laparoscopy', 'Liver', 'Machine Learning', 'Measures', 'Modeling', 'Nature', 'Operating Rooms', 'Operative Surgical Procedures', 'Optics', 'Outcome', 'Pathologic', 'Pathway interactions', 'Pediatric Research', 'Performance', 'Perfusion', 'Peripheral', 'Phase', 'Physiological', 'Physiology', 'Play', 'Positioning Attribute', 'Pre-Clinical Model', 'Preclinical Testing', 'Process', 'Research', 'Robot', 'Robotics', 'Role', 'Safety', 'Seeds', 'Sensitivity and Specificity', 'Shapes', 'Site', 'Small Business Technology Transfer Research', 'Structure', 'Surface', 'Surgeon', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Validation', 'Vision', 'Visual Perception', 'Work', 'base', 'bile duct', 'clinical translation', 'commercialization', 'data acquisition', 'design', 'digital', 'digital imaging', 'experience', 'fluorescence imaging', 'graphical user interface', 'image guided', 'image processing', 'imager', 'imaging system', 'improved', 'interest', 'meetings', 'millisecond', 'minimally invasive', 'next generation', 'novel', 'optical imaging', 'performance tests', 'pre-clinical', 'prototype', 'research and development', 'robot assistance', 'sensor', 'soft tissue', 'surgery outcome', 'tool']",NIBIB,"ACTIV SURGICAL, INC",R41,2018,42000,0.014310013708458956
"Insightful Surgical Vision Technology (ISVision) for intelligent real-time display of anatomic, physiologic, and pathologic information in surgery Abstract Intelligent intraoperative display of anatomic and physiologic information coupled to real time situational awareness to critical tissues will benefit all surgery. The current surgical vision is limited to naked human eyes in open surgery or more recently using laparoscopic digital imaging for minimally invasive surgery and robot assisted surgery. In laparoscopic vision, although the visualization of surface anatomy and tissue physiology in the surgical field-of-view has been enhanced by recent advances in optical imaging technology, the imaging paradigm still remains passive and still poses a significant challenge due to the inherent narrow peripheral vision and limited depth perception. We propose the next generation laparoscopic vision system that provides intelligent display of unrecognized tissue structures to human eyes. Our proposed use of a new paradigm and surgical vision technology (called ISVisionTM), equipped with modular plenoptic 3-D Color/HD camera and snapshot NIR hyperspectral imager, will permit a clear visualization of the critical target tissue-of-interest, surrounding anatomy into the operative field, and situational awareness for both the tool and tissue to the surgeon. Our Phase I effort comprises of engineering the clinically viable ISVision platform and testing the performance of its intraoperative use. Specifically, the ISVision system will clearly and precisely identify different tissue characteristics, accurately displaying physiologic information including tissue perfusion and blood flow. Following the Phase 1, we anticipate undertaking a Phase II project, during which we will develop a clinical grade ISVision system and investigate its performance and utility in the operating room. While the initial focus of the Phase I and II effort is on the complex surgery such as liver resection due to crucial nature of critical sub-surface structures located underneath liver hilum and parenchyma, the ISVision system will potentially play an enabling role in all surgeries and establish it as a vital component of the operating room of the future. Narrative Real-time display of the accurate anatomic and tissue physiology during surgery is necessary and crucial in improving surgical outcomes. This proposal aims to develop the next generation surgical vision system that provides intelligent display of unrecognized, unseen tissue structures to human eyes. The current state of the art technology still remains passive, focuses on anatomic display only. A new surgical vision technology, called ISVisionTM, will provide clear display of important anatomic structures, identify not only their distinct shape and position, and offer insightful intelligent function during surgical procedures. The academic partner in this proposal has developed a novel research prototype and has shown that the new imaging tool can enhance surgical vision beyond human visibility for preclinical testing. The small business partner backed by a Venture company from the Bay area has a strong background for successful commercialization. Together, we aim to convert the academic research prototype into a clinically viable market product that will significantly improve the safety, function, and outcome of surgery, not limited by surgeons’ visual perception, training or experience. The ISVision technology will make the surgery safer and more effective.","Insightful Surgical Vision Technology (ISVision) for intelligent real-time display of anatomic, physiologic, and pathologic information in surgery",9622159,R41EB026402,"['3-Dimensional', 'Adoption', 'Algorithms', 'Anastomosis - action', 'Anatomy', 'Area', 'Awareness', 'Back', 'Blood flow', 'Businesses', 'Characteristics', 'Child', 'Clinical', 'Clinical Engineering', 'Cognitive', 'Color', 'Complex', 'Complication', 'Conventional Surgery', 'Coupled', 'Depth Perception', 'Development', 'Engineering', 'Environment', 'Excision', 'Eye', 'Family suidae', 'Fostering', 'Future', 'Goals', 'Health system', 'Hemorrhage', 'Hepatic artery', 'Hospitals', 'Human', 'Image', 'Imagery', 'Imaging Device', 'Imaging technology', 'Incubators', 'Individual', 'Injury', 'Intestines', 'Investments', 'Laparoscopes', 'Laparoscopy', 'Liver', 'Machine Learning', 'Measures', 'Modeling', 'Nature', 'Operating Rooms', 'Operative Surgical Procedures', 'Optics', 'Outcome', 'Pathologic', 'Pathway interactions', 'Pediatric Research', 'Performance', 'Perfusion', 'Peripheral', 'Phase', 'Physiological', 'Physiology', 'Play', 'Positioning Attribute', 'Pre-Clinical Model', 'Preclinical Testing', 'Process', 'Research', 'Robot', 'Robotics', 'Role', 'Safety', 'Seeds', 'Sensitivity and Specificity', 'Shapes', 'Site', 'Small Business Technology Transfer Research', 'Structure', 'Surface', 'Surgeon', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Training', 'Validation', 'Vision', 'Visual Perception', 'Work', 'base', 'bile duct', 'clinical translation', 'commercialization', 'data acquisition', 'design', 'digital', 'digital imaging', 'experience', 'fluorescence imaging', 'graphical user interface', 'image guided', 'image processing', 'imager', 'imaging system', 'improved', 'interest', 'meetings', 'millisecond', 'minimally invasive', 'next generation', 'novel', 'optical imaging', 'performance tests', 'pre-clinical', 'prototype', 'research and development', 'robot assistance', 'sensor', 'soft tissue', 'surgery outcome', 'tool']",NIBIB,"ACTIV SURGICAL, INC",R41,2018,200769,0.014310013708458956
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,9567839,R01EY027023,"['Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'predictive modeling', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2018,413375,0.013035873771238502
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9474120,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2018,364257,0.05220409794558505
"GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN PROJECT SUMMARY & ABSTRACT  Human locomotion through natural environments requires the coordination of all levels of the sensorimotor hierarchy, from the cortical areas involved in processing of visual information and high level planning to the subcortical and spinal structures involved in the regulation of the gait and posture. However, despite the complex neural bases of human locomotion, the output is highly regular and well organized around the basic physical dynamics and biomechanics that define the stability and energetic costs of moving a bipedal body through space. There is a rich and growing body of literature describing detailed knowledge each of the individual components of human locomotion, including neural mechanisms, muscular neuromechanics, and biomechanics. However, very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion through a complex and dynamic world. This lack of integrative research not only restricts the breadth of impact of research from these individual disciplines, but also limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to to fill this critical gap in our knowledge about human locomotion, it is necessary to develop an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world. In service of this general goal, this proposal outlines research projects aimed at specific unanswered questions about locomotion over different terrains. This proposal comprises three specific research and training aims on the visual control of locomotion over rough terrain. Aim 1 focuses on the behavioral task itself, Aim 2 investigates the sensory stimulus experienced during real-world locomotion, and Aim 3 examines the motor integration of visually specified goals into the ongoing gait cycle. Aim 1 investigates effects of changing environmental uncertainty and task demands on gaze allocation strategies during locomotion over real-world rough terrain. Aim 2 analyzes and models the visual stimulus experienced during locomotion over real-world rough terrain. Aim 3 determines how visually specified target footholds and targets are integrated into the ongoing preferred steady-state gait. Together these aims will significantly advance our understanding of how humans use vision to control their movement through the natural world, which greatly increase our ability to develop clinical diagnosis and treatment for loss of locomotor function. PROJECT NARRATIVE  Very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion. This lack of integrative research limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to fill this critical gap in our knowledge about human locomotion, this proposal develops an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world.",GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN,9527525,K99EY028229,"['Aging', 'Algorithms', 'Area', 'Attention', 'Behavior', 'Behavioral', 'Biomechanics', 'Body Image', 'Clinical Treatment', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Development', 'Discipline', 'Environment', 'Eye', 'Gait', 'Goals', 'Human', 'Individual', 'Knowledge', 'Link', 'Literature', 'Locomotion', 'Measures', 'Mechanics', 'Mentors', 'Modeling', 'Motion', 'Motor', 'Movement', 'Muscle', 'Musculoskeletal', 'Nature', 'Neuromechanics', 'Output', 'Parkinson Disease', 'Pattern', 'Phase', 'Photic Stimulation', 'Positioning Attribute', 'Postdoctoral Fellow', 'Posture', 'Protocols documentation', 'Regulation', 'Research', 'Research Activity', 'Research Project Grants', 'Research Training', 'Services', 'Signal Transduction', 'Specific qualifier value', 'Spinal', 'Stroke', 'Structure', 'System', 'Training', 'Training Programs', 'Uncertainty', 'Vision', 'Visual', 'Visual Fields', 'Walking', 'Wireless Technology', 'area MST', 'area MT', 'base', 'clinical Diagnosis', 'cost', 'design', 'environmental change', 'experience', 'experimental study', 'foot', 'gaze', 'insight', 'instrument', 'kinematics', 'multidisciplinary', 'neuromechanism', 'neuromuscular', 'novel', 'optic flow', 'programs', 'relating to nervous system', 'response', 'sensory stimulus', 'skills', 'statistics', 'treadmill', 'treatment planning', 'visual control', 'visual information', 'visual processing', 'visual stimulus', 'visual-motor integration']",NEI,"UNIVERSITY OF TEXAS, AUSTIN",K99,2018,121770,-0.001188020647229395
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9440421,R01EY017835,"['Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Industrialization', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2018,585564,0.05500936630181525
"The role of area V4 in the perception and recognition of visual objects ﻿    DESCRIPTION (provided by applicant): The human visual system parses the information that reaches our eyes into a meaningful arrangement of regions and objects. This process, called image segmentation, is one of the most challenging computations accomplished by the primate brain. To discover its neural basis we will study neuronal processes in two brain areas in the macaque monkey-V4, a fundamental stage of form processing along the occipito-temporal pathway, and the prefrontal cortex (PFC), important for executive control. Dysfunctions of both areas impair shape discrimination behavior in displays that require the identification of segmented objects, strongly suggesting that they are important for image segmentation. Our experimental techniques will include single and multielectrode recordings, behavioral manipulations, perturbation methods and computer models. In Aim 1 we will identify the neural signals that reflect segmentation in visual cortex. Using a variety of parametric stimuli with occlusion, clutter and shadows-stimulus features known to challenge segmentation in natural vision-we will evaluate whether segmentation is achieved by grouping regions with similar surface properties, such as surface color, texture and depth, or by grouping contour segments that are likely to form the boundary of an object or some interplay between these two strategies. We will test the hypothesis that contour grouping mechanisms are most effective under low clutter and close to the fovea. In Aim 2, we will investigate how feedback from PFC modulates shape responses in V4 and facilitates segmentation: we will test the longstanding hypothesis that object recognition in higher cortical stages precedes and facilitates segmentation in the midlevels of visual form processing. We will simultaneously study populations of V4 and PFC neurons while animals engage in shape discrimination behavior. We will use single-trial decoding methods and correlation analyses to relate the content and timing of neuronal responses in the two areas. To causally test the role of feedback from PFC, we will reversibly inactivate PFC by cooling and study V4 neurons. Our results will provide the first detailed, analytical models of V4 neuronal response dynamics in the presence of occlusion and clutter and advance our understanding of how complex visual scenes are processed in area V4. They will also reveal how V4 and PFC together mediate performance on a complex shape discrimination task, how executive function and midlevel vision may be coordinated during behavior and how feedback is used in cortical computation. Object recognition is impaired in visual agnosia, a dysfunction of the occipito-temporal pathway, and in dysfunctions of the PFC (e.g. schizophrenia). Results from these experiments will constitute a major advance in our understanding of the brain computations that underlie segmentation and object recognition and will bring us closer to devising strategies to alleviate and treat brain disorders in which these capacities are impaired. PUBLIC HEALTH RELEVANCE: A fundamental capacity of the primate visual system is its ability to segment visual scenes into component objects and then recognize those objects regardless of partial occlusions and clutter. Using a combination of primate neurophysiology experiments, computational modeling, animal behavior and reversible inactivation methods, we hope to achieve a new level of understanding about visual processing in the context of object recognition; these findings will ultimately bring us closer to devising strategies to alleviate and treat brain disorders of impaired object recognition resulting from dysfunctions in the occipito-temporal pathway (e.g. agnosia) and the prefrontal cortex (e.g. schizophrenia).",The role of area V4 in the perception and recognition of visual objects,9462122,R01EY018839,"['Agnosia', 'Animal Behavior', 'Animals', 'Area', 'Back', 'Behavior', 'Behavior Control', 'Behavioral', 'Brain', 'Brain Diseases', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Custom', 'Data', 'Discrimination', 'Eye', 'Feedback', 'Functional disorder', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Lesion', 'Macaca', 'Mediating', 'Methods', 'Modeling', 'Monkeys', 'Neurons', 'Pathway interactions', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Play', 'Population', 'Prefrontal Cortex', 'Primates', 'Process', 'Property', 'Psychology', 'Psychophysics', 'Role', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Stimulus', 'Stream', 'Surface', 'Surface Properties', 'Techniques', 'Testing', 'Texture', 'Time', 'V4 neuron', 'Vision', 'Visual', 'Visual Agnosias', 'Visual Cortex', 'Visual system structure', 'area V4', 'awake', 'base', 'design', 'executive function', 'experimental study', 'fovea centralis', 'imaging Segmentation', 'impaired capacity', 'neurophysiology', 'neurotransmission', 'object recognition', 'public health relevance', 'relating to nervous system', 'response', 'stereoscopic', 'study population', 'visual processing']",NEI,UNIVERSITY OF WASHINGTON,R01,2018,510463,-0.026357774787321965
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,9133939,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Awareness', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Conscious', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Foundations', 'GTP-Binding Protein alpha Subunits, Gs', 'Hand functions', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'Subconscious', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'robot control', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2017,133916,0.00844032981191666
"Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma PROJECT SUMMARY This project aims to apply novel machine learning techniques to recently developed optical imaging measurement to improve the accurate prediction and detection of glaucomatous progression. Complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and advanced pattern recognition/machine learning-based analysis techniques can find and use that hidden information. We will use mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1,800 patient and healthy eyes, available as the result of long-term NIH funding. We also will investigate deep learning and novel statistical techniques for this purpose. The required longitudinal measurements from several newly developed optical imaging techniques were not available to our previously funded NEI- supported work. The proposed work potentially can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care by informing clinical decision-making based on mathematically based, externally validated methods. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. PROJECT NARRATIVE The proposed project will improve machine learning techniques for predicting and detecting glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments and will make use of a very large amount of data, obtained using previously awarded NIH funds, to do so. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neuro- degeneration within the visual pathways at structural and functional levels. The development of a clinically useful novel, empirical system for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and on the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Machine Learning Methods for Detecting Disease-related Functional and Structural Change in Glaucoma,9298423,R21EY027945,"['Address', 'Algorithms', 'Anatomy', 'Award', 'Caring', 'Classification', 'Clinical', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Defect', 'Detection', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Disease', 'Environment', 'Eye', 'Frequencies', 'Funding', 'Future', 'Gaussian model', 'Generations', 'Glaucoma', 'Goals', 'Health Personnel', 'Image', 'Imaging Device', 'Imaging Techniques', 'Instruction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Mathematics', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Performance', 'Perimetry', 'Physiologic Intraocular Pressure', 'Provider', 'Recruitment Activity', 'Reporting', 'Research', 'Scanning', 'Science', 'Series', 'Supervision', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Variant', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'clinical decision-making', 'cost', 'expectation', 'glaucoma test', 'high dimensionality', 'improved', 'independent component analysis', 'instrument', 'learning strategy', 'markov model', 'mathematical model', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'tool']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2017,232500,0.01434596650519646
"Naturalistic Data Collection In The SmartPlayroom PROJECT SUMMARY The aims of this proposal are to fully develop and validate the SmartPlayroom as a powerful automated data collection and analysis tool in developmental research. This room looks like any playroom in a home or school but is designed to naturalistically collect data in real time and simultaneously on all aspects of children's behavior. Behaviors include movement kinematics, language, eye movements, and social interaction while a child performs naturalistic tasks, plays and explores without instruction, walks or crawls, and interacts with a caregiver. The space is equipped with mobile eye tracking, wireless physiological heart rate and galvanic skin response sensors, audio and video recording, and depth sensor technologies. Funding is requested to demonstrate the scientific advantage of naturalistic measurement using an example from visual attention research (Aim 1), and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use in 4-9 year-old children (Aim 2). By combining fine-grained sensor data with high-throughput automated computer vision and machine learning tools, we will be able to automate quantitative data collection and analysis in the SmartPlayroom for use in addressing myriad developmental questions. The SmartPlayroom approach overcomes completely the limitations of task-based experimentation in developmental research, offering quantitative precision in the collection of ecologically valid data. It has the power to magnify both construct validity and measurement reliability in developmental research. The investigators are committed to making freely available our data, computer vision algorithms, and discoveries so that we might move the field forward quickly. NARRATIVE We focus this work on developing and validating a novel and innovative data collection space called the SmartPlayroom, designed to pair naturalistic exploration and action with the precision of computerized automated data collection and analysis. This proposal aims to demonstrate the scientific advantage of naturalistic measurement, and in the process, to provide data to further develop flexible computer vision algorithms for automated behavioral analysis for use with 4-9 year-old children in the SmartPlayroom. !",Naturalistic Data Collection In The SmartPlayroom,9373088,R21MH113870,"['9 year old', 'Address', 'Adult', 'Age', 'Algorithms', 'Attention', 'Automated Annotation', 'Behavior', 'Behavior assessment', 'Behavioral', 'Benchmarking', 'Caregivers', 'Cereals', 'Child', 'Child Behavior', 'Child Development', 'Child Rearing', 'Childhood', 'Cognitive', 'Collection', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Darkness', 'Data', 'Data Analyses', 'Data Collection', 'Development', 'Developmental Process', 'Discipline', 'Education', 'Environment', 'Event', 'Eye', 'Eye Movements', 'Face', 'Funding', 'Galvanic Skin Response', 'Goals', 'Heart Rate', 'Home environment', 'Human', 'Instruction', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Memory', 'Methods', 'Modernization', 'Monitor', 'Movement', 'Neurodevelopmental Disorder', 'Performance', 'Physiological', 'Play', 'Policies', 'Process', 'Research', 'Research Personnel', 'Schools', 'Social Interaction', 'Societies', 'Technology', 'Time', 'Time Study', 'Training', 'Video Recording', 'Vision', 'Visual attention', 'Walking', 'Wireless Technology', 'Work', 'base', 'behavioral study', 'cognitive development', 'computerized', 'cost', 'design', 'eye hand coordination', 'flexibility', 'frontier', 'grasp', 'indexing', 'innovation', 'kinematics', 'novel', 'research and development', 'sample fixation', 'sensor', 'skills', 'tool']",NIMH,BROWN UNIVERSITY,R21,2017,243750,-9.574923133733544e-05
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9390382,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biological Neural Networks', 'Biomedical Engineering', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'base', 'bulk motion', 'capillary', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2017,559269,0.05451976952800462
"Representation of information across the human visual cortex ﻿    DESCRIPTION (provided by applicant): The human visual system is organized as a parallel, hierarchical network, and successive stages of visual processing appear to represent increasingly complicated aspects of shape-related and semantic information. However, the way that shape-related and semantic information is represented across much of the visual hierarchy is still poorly understood. The primary goal of this proposal is to understand how information about object shape and semantic category is represented explicitly across mid- and high-level visual areas. To address this important issue we propose to undertake a series of human functional MRI (fMRI) studies, using both synthetic and natural movies. Data will be analyzed by means of a powerful voxel-wise modeling (VM) approach that has been developed in my laboratory over the past several years. In Aim 1 we propose to measure human brain activity evoked by synthetic naturalistic movies, and to use VM to evaluate and compare several competing theories of shape representation across the entire visual cortex. In Aim 2 we propose to use VM to evaluate and compare competing theories of semantic representation. In Aim 3 we propose to use machine learning and and VM to discover new aspects of shape and semantic representation. These experiments will provide fundamental new insights about the representation of visual information across visual cortex. PUBLIC HEALTH RELEVANCE: Disorders of central vision can severely affect quality of life and the design of treatments and devices for improving visual function will depend critically on understanding the organization of visual cortex. We propose to use functional MRI and sophisticated computational data analysis and modeling procedures to evaluate and compare multiple theories of visual function. The results will reveal how visual information is represented across the several dozen distinct functional areas that constitute human visual cortex.",Representation of information across the human visual cortex,9254553,R01EY019684,"['Address', 'Affect', 'Area', 'Award', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Device Designs', 'Dimensions', 'Disease', 'Elements', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Human', 'Image', 'Individual', 'Laboratories', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Motion', 'Perception', 'Procedures', 'Quality of life', 'Semantics', 'Series', 'Shapes', 'Surface', 'System', 'Testing', 'Training', 'V2 neuron', 'V4 neuron', 'Vision', 'Vision research', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'area V1', 'area striata', 'base', 'data modeling', 'design', 'exhaustion', 'experimental study', 'extrastriate visual cortex', 'high dimensionality', 'improved', 'innovation', 'insight', 'learning strategy', 'movie', 'novel', 'object recognition', 'object shape', 'public health relevance', 'receptive field', 'theories', 'therapy design', 'visual information', 'visual neuroscience', 'visual processing']",NEI,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2017,377994,0.008828729380989775
"Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor Summary The principal goal of this proposal is to increase the accuracy and precision of a low-cost autorefraction device called the QuickSee, in order to improve access to refractive eye care for underserved populations. Poor vision due to a lack of eyeglasses is highly prevalent in low-resource settings throughout the world and significantly reduces quality of life, education, and productivity. The existing QuickSee only extracts the lower- order aberration information contained within a wavefront profile of the eye, to roughly estimate an eyeglass prescription. This proposal will further improve the accuracy of the QuickSee device by exploiting both the lower- and higher-order aberrations contained within the complete wavefront. To realize this goal, we will enroll 300 subjects (600 eyes) in Baltimore, MD, and will obtain subjective refraction and visual acuity (VA) measurements and will use machine learning on this large dataset of wavefront profiles to optimize the wavefront-to-refraction algorithm of the QuickSee device. The main output of this project will be a robust and improved-accuracy next-generation QuickSee device that will increase efficiency of and decrease the training requirements of eye care professionals, and potentially dispense refractive correction that provides similar or better VA than correction from an eye care professional. Successful completion of this work will be an important step towards dramatically improving eyeglass accessibility for health disparity populations in the USA and internationally in low-resource settings. Upon completion of this proposal, we will apply for a Phase II award proposing to work with Wilmer Eye Institute research faculty to assess widespread deployment of the next-generation QuickSee with minimally-trained personnel in order to accurately and reliably provide thousands of pairs of low-cost corrective eyeglasses to underserved communities. Project Narrative This project proposal seeks to develop a novel technology that will disruptively increase the accessibility of refractive eye care for health disparity populations in low-resource settings. Specifically, sophisticated algorithms will be developed that improve the accuracy of the QuickSee device so that it can improve the efficiency of and reduce the training barriers for eye care professionals, and potentially provide refractive correction without the need for refinement by a trained eye care professional. Our goal is to develop a low- cost, easy-to-use, scalable solution to increase accessibility to vision correction globally.","Improving access to vision correction for health disparity populations with the QuickSee: an accurate, low-cost, easy-to-use objective refractor",9312420,R43EB024299,"['Algorithms', 'Award', 'Baltimore', 'Brazil', 'Businesses', 'Caliber', 'Calibration', 'Caring', 'Communities', 'Country', 'Data', 'Data Set', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnostic', 'Education', 'Educational Status', 'Enrollment', 'Eye', 'Eyeglasses', 'Feedback', 'Geometry', 'Goals', 'Gold', 'Guatemala', 'Hospitals', 'Human Resources', 'Impairment', 'Improve Access', 'Income', 'India', 'Institutes', 'International', 'Machine Learning', 'Mali', 'Measurement', 'Measures', 'Modeling', 'Noise', 'Ophthalmic examination and evaluation', 'Optometrist', 'Output', 'Patient Schedules', 'Patients', 'Phase', 'Population', 'Prevalence', 'Procedures', 'Productivity', 'Pupil', 'Quality of life', 'Refractive Errors', 'Research Institute', 'Resources', 'Spottings', 'Testing', 'Time', 'Training', 'Underserved Population', 'Universities', 'Validation', 'Vision', 'Visual Acuity', 'Work', 'base', 'cost', 'faculty research', 'health care disparity', 'health disparity', 'improved', 'lens', 'new technology', 'next generation', 'novel strategies', 'success', 'vector']",NIBIB,"PLENOPTIKA, INC.",R43,2017,200225,-0.005427903516466572
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9394127,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Behavior Therapy', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Health Care Research', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health care quality', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'symptomatic improvement', 'tool']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2017,338637,0.006251159384113965
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability. PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.",Capti Screen Reading Assistant for Goal Directed Web Browsing,9199231,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2017,500000,0.03217743046247355
"Crossmodal Correspondences Between Visual and Auditory Features ﻿    DESCRIPTION (provided by applicant): We live in a multisensory world, in which stimuli of various types constantly compete for our attention. Information about objects or events typically appears on more than one sensory channel, so that integrating inputs across sensory systems (e.g. vision and hearing) can enhance the signal-to-noise ratio and lead to more efficient perception and action. There is increasing interest in studying how stimulus properties in one sensory modality (e.g. vision) correspond to those in another modality (e.g. hearing). For instance, sounds of high pitch are linked to small-sized visual objects whereas sounds of low pitch are linked with large objects; sounds of high/low pitch are associated with, respectively, visual stimuli of high/low elevation; and even aspects of linguistic stimuli such as vowel quality are associated with visual properties such as object size. Such crossmodal correspondences are important factors in multisensory binding. While information has exploded on the kinds of stimulus features that are reliably associated by human observers across modalities, currently there is little neural evidence to allow a mechanistic account of how crossmodal correspondences arise, or how they relate to synesthesia, a phenomenon in which some individuals experience unusual percepts (e.g. colors) triggered by particular stimuli (e.g. letters. Our goal is to address these important gaps in knowledge, by using functional magnetic resonance imaging (fMRI) in humans to investigate the neural mechanisms underlying crossmodal and synesthetic correspondences and thus to distinguish between alternative explanations that have been offered. A number of possible mechanisms have been entertained for crossmodal correspondences. These include: Hypothesis A - learned associations due to statistical co-occurrences, which would predict that the correspondences are based in multisensory or even classic unisensory regions; Hypothesis B - semantic mediation (e.g. the common word ""high"" may mediate the link between high pitch and high elevation); and Hypothesis C - conceptual linking via a high-level property such as magnitude. In a series of eight experiments that comprise three Specific Aims, we propose to examine these competing accounts, recognizing that some or all of them may be operative, and that the mechanisms may vary between different types of crossmodal correspondences. PUBLIC HEALTH RELEVANCE: The proposed systematic study of the brain basis of correspondences between stimulus properties across sensory systems will allow critical insights into the multisensory processing involved in perception and action, illuminate the multisensory basis of language and music, and expand understanding of the phenomenon of synesthesia in relation to normal experience. From a practical standpoint, the proposed work will make significant contributions to the design of sensory substitution approaches for people with visual, auditory and other sensory deficits, and the rehabilitation of individuals with multisensory processing abnormalities, including developmental (autism, dyslexia), neurological (neglect) and psychiatric (schizophrenia) disorders.",Crossmodal Correspondences Between Visual and Auditory Features,9334867,R01EY025978,"['Activation Analysis', 'Address', 'Attention', 'Auditory', 'Auditory pitch', 'Autistic Disorder', 'Behavioral', 'Binding', 'Brain', 'Color', 'Data', 'Development', 'Dimensions', 'Disease', 'Dyslexia', 'Event', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hearing', 'Human', 'Individual', 'Judgment', 'Knowledge', 'Language', 'Lead', 'Letters', 'Linguistics', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Mediation', 'Modality', 'Multivariate Analysis', 'Music', 'Neurologic', 'Noise', 'Pattern', 'Perception', 'Process', 'Property', 'Regression Analysis', 'Rehabilitation therapy', 'Research Personnel', 'Rest', 'Schizophrenia', 'Semantics', 'Sensory', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Time', 'Vision', 'Visual', 'Work', 'base', 'design', 'experience', 'experimental study', 'insight', 'interest', 'multisensory', 'neglect', 'neuroimaging', 'neuromechanism', 'neurophysiology', 'public health relevance', 'relating to nervous system', 'sensory system', 'sound', 'vector', 'visual stimulus']",NEI,EMORY UNIVERSITY,R01,2017,620264,0.00582297664944515
"Characterizing and Modeling Infants' Self-Generated Object Views: Implications for Object Recognition and Language Learning PROJECT SUMMARY/ABSTRACT Human visual object recognition is foundational to many achievements—from object name learning to tool use to real world problem solving. Understanding the developmental processes that underlie visual object recognition is of critical importance because the individual differences that characterize early visual object recognition have clinical, educational, and societal implications. For instance, toddlers with poor visual object recognition skills are more likely to have below-average vocabulary sizes. Toddlers with smaller vocabularies have a greater likelihood of developing language impairments and are more likely to lag in pre-literacy and literacy skills. Deficits in visual object recognition and word learning have also been exhibited by individuals with Autism Spectrum Disorders (ADSs). The overarching goal of the proposed project is to better understand the sensory-motor mechanisms that support visual object recognition. Considerable evidence suggests that active object manipulation relates to better visual object recognition, however little is known about the mechanisms through which object manipulation connects to visual object processing during development. The proposed research tests the hypothesis that one major route through which object manipulation matters is that it generates many different views of the same object, and that the variation within multiple visual instances of the same object facilitates visual object recognition by building more generalizable representations for recognizing unseen instances. This hypothesis is tested by (1) characterizing the properties of object information generated by infants during free play and by (2) evaluating the information in those generated visual streams by feeding them to convolutional neural networks (CNNs) – the first computational models of vision capable of human-like visual recognition. Two additional lines of research motivate the approach. First is evidence showing infants learn from statistical regularities in visual inputs presented briefly in a laboratory setting. Second is research using head-mounted cameras suggesting that object views generated by infant manipulation have unique properties, including views dominated by a single object. What we do not yet know are the visual statistics of the views infants generate in everyday toy play or their value for a statistical learner such as CNNs. The proposed research will address these gaps in the literature by characterizing the visual object inputs infants generate and how these inputs may facilitate visual object recognition. The proposed research will also determine how differences in visual inputs may be linked with individual differences in infant object name learning. This research will lead to a deeper understanding of the early development of visual object recognition, and may also provide a crucial missing link in our understanding of the developmental trajectory of other cognitive functions, including object name learning. Moreover, the knowledge to be gained from the proposed research has the potential to inform (1) individual differences in learning, (2) strategies for identifying learning delays, and (3) construction of interventions to remediate learning delays. PROJECT NARRATIVE The proposed research aims to characterize the nature of infants' early visual experiences of objects and their relation to object manipulation, language abilities, and computational models of object recognition. The period of development to be studied – 18 to 24 months – is a period in which delays in visual object recognition and in vocabulary learning have been connected to future development of language impairments and diagnoses of Autism Spectrum Disorders (ASDs). The proposed research is highly relevant to public health as it will elucidate sources of individual differences in early object recognition and word learning, laying the foundation for aiding diagnosis of early language delay and ASDs, as well as the development of future object recognition and vocabulary/language intervention programs.",Characterizing and Modeling Infants' Self-Generated Object Views: Implications for Object Recognition and Language Learning,9395459,F32HD093280,"['Achievement', 'Address', 'Biological Neural Networks', 'Characteristics', 'Clinical', 'Computer Simulation', 'Data', 'Development', 'Developmental Process', 'Diagnosis', 'Early Diagnosis', 'Exhibits', 'Foundations', 'Future', 'Gap Junctions', 'Goals', 'Head', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Intervention', 'Knowledge', 'Laboratories', 'Language', 'Language Delays', 'Language Development', 'Learning', 'Link', 'Literature', 'Machine Learning', 'Modeling', 'Motor', 'Names', 'Nature', 'Parents', 'Persons', 'Play', 'Problem Solving', 'Property', 'Public Health', 'Recording of previous events', 'Research', 'Role', 'Route', 'Sampling', 'Sensory', 'Shapes', 'Source', 'Specific qualifier value', 'Stream', 'Testing', 'Toddler', 'Toy', 'Training', 'Variant', 'Vision', 'Visual', 'Vocabulary', 'autism spectrum disorder', 'base', 'cognitive function', 'developmental disease', 'experience', 'feeding', 'intervention program', 'language impairment', 'literacy', 'novel', 'object recognition', 'skills', 'statistics', 'tool', 'vision development', 'visual object processing', 'word learning']",NICHD,INDIANA UNIVERSITY BLOOMINGTON,F32,2017,57066,-0.017402646038247944
"Learning and updating internal visual models ﻿    DESCRIPTION (provided by applicant): In line with the strategic plan of the NEI, this project is focused on filling a profound gap in our understanding of neural mechanisms of visual perception. Specifically, we aim to understand how the adaptation of visual cortical circuits contributes to perception. Adaptation is a ubiquitous process by which neural processing and perception are dramatically influenced by recent visual inputs. However, the functional purpose of adaptation is poorly understood. Based on preliminary data, this project tests the hypothesis that visual adaptation instantiates a form of predictive coding, which is used to make unexpected events salient. We posit that cortical circuits learn the statistical structure of visua input in a manner that extends beyond previous fatigue- based descriptions of adaptation effects. This learning is used to discount expected features and signal novel ones. Our project will test this hypothesis through the collaborative effort of three investigators with expertise in human EEG, animal neurophysiology, and computational modeling. Aim 1 will assess the ability of cortical circuits to adapt to temporal sequences of input and to signal deviations from expected sequences. Aim 2 will evaluate the effect of stimulus uncertainty on adaptation and responses to novel events. Aim 3 will determine how adaptation dynamics and responses to novel stimuli are influenced by the temporal constancy of stimulus statistics. Each of these aims involves an experimental manipulation that yields distinct behavior from fatigue- based and predictive coding mechanisms. Thus, together our aims will provide a robust test of our core hypothesis, and provide a much richer understanding of the adaptive properties of cortical circuits. Results from our project will contribute to answering one of the continuing puzzles in visual research, which is to understand the functional purpose of adaptive mechanisms in visual perception. PUBLIC HEALTH RELEVANCE: . This research is relevant to public health because it aims to uncover the function of visual adaptation, a fundamental aspect of visual perception. This work is thus essential to the mission of NEI because it will provide a more detailed understanding of how visual cortical circuits underlie visual perception, which is necessary for developing treatment strategies for individuals with visual processing deficits and for the development of effective prosthetic devices.",Learning and updating internal visual models,9334881,R01EY024858,"['Affect', 'Animals', 'Area', 'Autistic Disorder', 'Behavior', 'Biological', 'Brain', 'Code', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Electroencephalography', 'Elements', 'Environment', 'Event', 'Fatigue', 'Goals', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Machine Learning', 'Mission', 'Modeling', 'Monkeys', 'Neurons', 'Pattern', 'Perception', 'Process', 'Property', 'Prosthesis', 'Public Health', 'Recording of previous events', 'Research', 'Research Personnel', 'Scheme', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Stimulus', 'Strategic Planning', 'Structure', 'Testing', 'Time', 'Uncertainty', 'Update', 'Vision', 'Visual', 'Visual Cortex', 'Visual Perception', 'Work', 'area V4', 'area striata', 'awake', 'base', 'brain machine interface', 'cognitive neuroscience', 'computational neuroscience', 'design', 'discount', 'expectation', 'extrastriate visual cortex', 'human subject', 'improved', 'neuromechanism', 'neurophysiology', 'novel', 'phenomenological models', 'prevent', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'treatment strategy', 'visual adaptation', 'visual processing']",NEI,"ALBERT EINSTEIN COLLEGE OF MEDICINE, INC",R01,2017,447745,0.01538084771428419
"Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions Project Summary/Abstract People with mobility restrictions and limited to no upper extremity use depend on others to position the objects that they rely upon for health, communication, productivity, and general well-being. The technology developed in this project directly increases users’ independence by responding to them without another person’s intervention. The independence resulting from the proposed technology allows a user to perform activities related to self-care and communication. Eye tracking and head tracking access to speech devices, tablets, or computers requires very specific positioning. If the user’s position relative to the device are not aligned precisely, within a narrow operating window, the device will no longer detect the eyes or head, rendering the device inoperable. Auto-positioning technology uses computer vision and robotic positioning to automatically move devices to a usable position. The system moves without assistance from others, ensuring the user has continual access to communication. In a generalized application, the technology can target any area of the user’s body to position relevant equipment, such as for hydration or a head array for power wheelchair control. Research and development of the automatic positioning product will be accomplished through user-centered, iterative design, and design for manufacturing. Input from people with disabilities, their family members, therapists, and assistive technology professionals define the functional requirements of the technology and guide its evolution. Throughout technical development, prototype iterations are demonstrated and user-tested, providing feedback to advance the design. Design for manufacturability influences the outcomes to optimize the product pipeline, ensure high quality and deliver a cost effective product. Phase 1 Aims demonstrate the feasibility of 1) movement and control algorithms 2) face tracking algorithms and logic to maintain device positioning and 3) integration with commercial eye tracking device software development kits (SDK). Phase 2 Aims include technical, usability, and manufacturability objectives leading to development of a product for commercialization. Software development advances computer vision capabilities to recognize facial expressions and gestures. A new sensor module serves as a gesture switch or face tracker. App development provides the user interface, remote monitoring and technical support. Design of scaled down actuators and an enclosed three degree of freedom alignment module reduces the form factor, allowing for smaller footprint applications. Rigorous user testing by people with different diagnoses and end uses will ensure the product addresses a range of needs and is easy to use. Testing involves professionals and family members to evaluate ease of set-up, functionality, and customization. Extended user testing will investigate and measure outcomes and effects on their independence, access and health. Prototype tooling and design for cost-effective manufacturing will produce units for user and regulatory testing, and eventual sale. Project Narrative People who are essentially quadriplegic, with significant mobility impairments and limited to no upper extremity use, are dependent on others to maintain proper positioning for access to devices essential for their health, mobility and communication, such as eye gaze speech devices, call systems, phones, tablets, wheelchair controls, and hydration and suctioning tubes. Auto-positioning technology uses computer vision to detect specific targets, such as facial features, to control a robotic mounting and positioning system; automatically repositioning devices for best access without assistance from others, even when their position changes. This technology enables continuous access to communication, maintains one’s ability to drive a wheelchair and allows a person to drink or suction themselves, providing good hydration, respiratory health and hygiene.",Automatic Positioning of Communication Devices and other Essential Equipment for People with Mobility Restrictions,9407137,R44HD093467,"['Address', 'Advanced Development', 'Algorithms', 'Area', 'Articular Range of Motion', 'Beds', 'Caring', 'Cerebral Palsy', 'Communication', 'Computer Vision Systems', 'Computers', 'Custom', 'Data', 'Dependence', 'Development', 'Devices', 'Diagnosis', 'Disabled Persons', 'Duchenne muscular dystrophy', 'Ensure', 'Equipment', 'Evolution', 'Eye', 'Face', 'Facial Expression', 'Family', 'Family member', 'Feedback', 'Freedom', 'Gestures', 'Goals', 'Head', 'Head and neck structure', 'Health', 'Health Communication', 'Hydration status', 'Hygiene', 'Immunity', 'Impairment', 'Individual', 'Intervention', 'Intuition', 'Joints', 'Lighting', 'Logic', 'Methods', 'Monitor', 'Motor', 'Movement', 'Oral cavity', 'Outcome', 'Outcome Measure', 'Persons', 'Phase', 'Phonation', 'Positioning Attribute', 'Powered wheelchair', 'Productivity', 'Publishing', 'Quadriplegia', 'Quality of life', 'Research Design', 'Robotics', 'Safety', 'Sales', 'Saliva', 'Self Care', 'Self-Help Devices', 'Services', 'Signal Transduction', 'Spastic Tetraplegia', 'Speech', 'Spinal Muscular Atrophy', 'Spinal cord injury', 'Suction', 'System', 'Tablets', 'Technology', 'Telephone', 'Testing', 'Tube', 'Update', 'Upper Extremity', 'Variant', 'Wheelchairs', 'Work', 'application programming interface', 'base', 'body system', 'commercialization', 'communication device', 'cost', 'cost effective', 'cost effectiveness', 'design', 'experience', 'gaze', 'improved', 'iterative design', 'meetings', 'product development', 'prototype', 'psychosocial', 'research and development', 'respiratory health', 'sensor', 'software development', 'tool', 'usability', 'user centered design']",NICHD,"BLUE SKY DESIGNS, INC.",R44,2017,159267,0.017099294478208085
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9238777,R01EY025332,"['Access to Information', 'Adoption', 'Algorithms', 'American', 'Architecture', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image Enhancement', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'experimental study', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2017,416574,0.07793366125332993
"Post-natal development of high-level visual representation in primates View-invariant object recognition is a complex cognitive task that is critical to everyday functioning. A key neural correlate of high-level object recognition is inferior temporal (IT) cortex, a brain area present in both humans and non-human primates. Recent advances in visual systems neuroscience have begun to uncover how images are encoded in the adult IT object representation, however the learning rules by which high level visual areas (especially IT) develop remain mysterious, with both the magnitude and qualitative nature of developmental changes remaining almost completely unknown — in part because, over the last thirty years, there have been practically no studies of spiking neural responses in the higher ventral cortical areas of developing primates. There is thus a significant gap in our understanding of how visual development proceeds.  This exploratory proposal aims to characterize how representation in higher primate visual cortex changes during development. We first aim (Aim 1) to implant chronic electrode arrays to record hundreds of IT neuronal sites in response to thousands of image stimuli in awake behaving juvenile macaques. These data will comprise a snapshot of the developing primate visual representation, and will be particularly powerful because we have already extensively measured adult monkey IT using the same stimuli and methods. By comparing juvenile and adult neuronal responses at both single site and population levels, we will obtain a unprecedentedly large-scale and detailed picture of the neural correlates of high-level visual development (Aim 2).  Aims 1 and 2 are exploratory, but potentially transformative – they will result in publicly available neuronal IT development benchmarks against which any proposed model of high level visual development can be rigorously tested, and will spur the development of those models in our lab and others. In that context, we will also seek (Aim 3) to improve known semi- and un-supervised learning rules from the computer vision and computational neuroscience literature, and to compare them to both recent high-performing (but biologically implausible) supervised models as well to the rich developmental measurements obtained in Aims 1 and 2.  Establishing experimental and surgical procedures for juvenile array recordings will create the future opportunity to observe changes in high level neural visual representations while experience is manipulated in early development, and will enable experiments in other sensory, motor, or decision making domains. If successful, the proposed work will yield a deeper understanding of the principles underlying visual cortex development, understanding which will in turn be helpful for treating neurodevelopmental disorders that implicate cortical circuits, including amblyopia and autism. Project narrative  Visual object recognition is fundamental to our everyday functioning. While the brain is remarkably good at accomplishing these challenging tasks, we do not yet know how it learns this ability during development. The goal of these experiments is to develop new experimental and computational tools to discover the neural learning principles that underlie that visual ability.",Post-natal development of high-level visual representation in primates,9316254,R21EY025863,"['Adolescent', 'Adult', 'Amblyopia', 'Animals', 'Area', 'Autistic Disorder', 'Behavioral', 'Benchmarking', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Chronic', 'Collaborations', 'Complex', 'Computer Vision Systems', 'Conflict (Psychology)', 'Data', 'Data Set', 'Decision Making', 'Development', 'Electrodes', 'Exhibits', 'Eye', 'Face', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Head', 'Human', 'Image', 'Implant', 'Inferior', 'Learning', 'Literature', 'Macaca', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monkeys', 'Motivation', 'Motor', 'Nature', 'Neural Network Simulation', 'Neurodevelopmental Disorder', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Performance', 'Population', 'Primates', 'Procedures', 'Process', 'Research', 'Rewards', 'Sensory', 'Site', 'Stimulus', 'Stream', 'Supervision', 'System', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Variant', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'awake', 'base', 'cognitive task', 'computational neuroscience', 'computerized tools', 'experience', 'experimental study', 'extrastriate visual cortex', 'improved', 'juvenile animal', 'learning network', 'mature animal', 'multi-electrode arrays', 'network models', 'neural correlate', 'nonhuman primate', 'novel', 'object recognition', 'postnatal', 'receptive field', 'relating to nervous system', 'response', 'statistics', 'vision development']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2017,193375,0.013561403434114648
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,9380854,R01EY027023,"['Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2017,403479,0.013035873771238502
"Psychophysics of Reading - Normal and Low Vision DESCRIPTION (provided by applicant):  Psychophysics of Reading - Normal and Low Vision Abstract Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  Difficulty in accessing print imposes obstacles to education, employment, social interaction and recreation.  The ongoing transition to the production and distribution of digital documents brings about new opportunities for people with visual impairment.  Digital documents on computers and mobile devices permit easy manipulation of print size, contrast polarity, font, page layout and other attributes of text.  In short, we now hae unprecedented opportunities to adapt text format to meet the needs of visually impaired readers.  In recent years, our laboratory and others in the vision-science community have made major strides in understanding the impact of different forms of low vision on reading, and the dependence of reading performance on key text properties such as character size and contrast.  But innovations in reading technology have outstripped our knowledge about low-vision reading.  A major gap still exists in translating these laboratory findings into methods for customizing text displays for people with low vision.  The broad aim of the current proposal is to apply our knowledge about the impact of vision impairment on reading to provide tools and methods for enhancing reading accessibility in the modern world of digital reading technology.  Our research plan has three specific goals:   1) To develop and validate an electronic version of the MNREAD test of reading vision, to extend this technology to important text variables in addition to print size, and to develop methods for customizing the selection of text properties for low-vision readers.  MNREAD is the most widely used test of reading in vision research and was originally developed in our laboratory with NIH support.  2) To investigate the ecology of low-vision reading in order to better understand how modern technologies, such as iPad and Kindle are being used by people with low vision.  We plan to evaluate the feasibility of using internet methods to survey low-vision individuals concerning their reading behavior and goals, and of collecting approximate measures of visual function over the internet.  We also plan to develop an ""accessibility checker"" to help low-vision computer users and their families to evaluate the accessibility of specific text displays.  3) To enhance reading accessibility by developing methods for enlarging the visual span (the number of adjacent letters that can be recognized without moving the eyes).  A reduced visual span is thought to be a major factor limiting reading in low vision, especially for people with central-field loss from macular degeneration.  We have already demonstrated methods for enlarging the visual span in peripheral vision.  We plan to develop a more effective perceptual training method for enlarging the visual span, with the goal of improving reading performance for people with central-vision loss. PUBLIC HEALTH RELEVANCE:  Reading difficulty is one of the most disabling consequences of vision loss for five million Americans with low vision.  The ongoing transition to the use of digital documents on computers and mobile devices brings about new opportunities for customizing text for people with visual impairment.  We propose to apply findings from basic vision science on low vision and reading to develop tools and methods for enhancing reading accessibility for digital text.",Psychophysics of Reading - Normal and Low Vision,9292314,R01EY002934,"['American', 'Attention', 'Auditory', 'Behavior', 'Blindness', 'Books', 'Caring', 'Central Scotomas', 'Characteristics', 'Clinical Research', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Custom', 'Dependence', 'Development', 'Devices', 'Ecology', 'Education', 'Employment', 'Eye', 'Family', 'Galaxy', 'Goals', 'Government', 'Guidelines', 'Habits', 'Health', 'Individual', 'Internet', 'Knowledge', 'Laboratories', 'Laboratory Finding', 'Leg', 'Length', 'Letters', 'Life', 'Macular degeneration', 'Mainstreaming', 'Maps', 'Marshal', 'Measures', 'Methods', 'Modernization', 'Optics', 'Paper', 'Participant', 'Patients', 'Perceptual learning', 'Performance', 'Peripheral', 'Play', 'Policies', 'Printing', 'Production', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysics', 'Reader', 'Reading', 'Recreation', 'Reporting', 'Research', 'Resources', 'Role', 'Self-Help Devices', 'Social Interaction', 'Surveys', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Uncertainty', 'United States National Institutes of Health', 'Validation', 'Vision', 'Vision research', 'Visual', 'Visual impairment', 'Work', 'analog', 'base', 'design', 'digital', 'essays', 'handheld mobile device', 'improved', 'innovation', 'invention', 'large print', 'literate', 'public health relevance', 'reading difficulties', 'sound', 'symposium', 'tool', 'vision science', 'web-accessible']",NEI,UNIVERSITY OF MINNESOTA,R01,2017,364423,0.05220409794558505
"The role of area V4 in the perception and recognition of visual objects ﻿    DESCRIPTION (provided by applicant): The human visual system parses the information that reaches our eyes into a meaningful arrangement of regions and objects. This process, called image segmentation, is one of the most challenging computations accomplished by the primate brain. To discover its neural basis we will study neuronal processes in two brain areas in the macaque monkey-V4, a fundamental stage of form processing along the occipito-temporal pathway, and the prefrontal cortex (PFC), important for executive control. Dysfunctions of both areas impair shape discrimination behavior in displays that require the identification of segmented objects, strongly suggesting that they are important for image segmentation. Our experimental techniques will include single and multielectrode recordings, behavioral manipulations, perturbation methods and computer models. In Aim 1 we will identify the neural signals that reflect segmentation in visual cortex. Using a variety of parametric stimuli with occlusion, clutter and shadows-stimulus features known to challenge segmentation in natural vision-we will evaluate whether segmentation is achieved by grouping regions with similar surface properties, such as surface color, texture and depth, or by grouping contour segments that are likely to form the boundary of an object or some interplay between these two strategies. We will test the hypothesis that contour grouping mechanisms are most effective under low clutter and close to the fovea. In Aim 2, we will investigate how feedback from PFC modulates shape responses in V4 and facilitates segmentation: we will test the longstanding hypothesis that object recognition in higher cortical stages precedes and facilitates segmentation in the midlevels of visual form processing. We will simultaneously study populations of V4 and PFC neurons while animals engage in shape discrimination behavior. We will use single-trial decoding methods and correlation analyses to relate the content and timing of neuronal responses in the two areas. To causally test the role of feedback from PFC, we will reversibly inactivate PFC by cooling and study V4 neurons. Our results will provide the first detailed, analytical models of V4 neuronal response dynamics in the presence of occlusion and clutter and advance our understanding of how complex visual scenes are processed in area V4. They will also reveal how V4 and PFC together mediate performance on a complex shape discrimination task, how executive function and midlevel vision may be coordinated during behavior and how feedback is used in cortical computation. Object recognition is impaired in visual agnosia, a dysfunction of the occipito-temporal pathway, and in dysfunctions of the PFC (e.g. schizophrenia). Results from these experiments will constitute a major advance in our understanding of the brain computations that underlie segmentation and object recognition and will bring us closer to devising strategies to alleviate and treat brain disorders in which these capacities are impaired. PUBLIC HEALTH RELEVANCE: A fundamental capacity of the primate visual system is its ability to segment visual scenes into component objects and then recognize those objects regardless of partial occlusions and clutter. Using a combination of primate neurophysiology experiments, computational modeling, animal behavior and reversible inactivation methods, we hope to achieve a new level of understanding about visual processing in the context of object recognition; these findings will ultimately bring us closer to devising strategies to alleviate and treat brain disorders of impaired object recognition resulting from dysfunctions in the occipito-temporal pathway (e.g. agnosia) and the prefrontal cortex (e.g. schizophrenia).",The role of area V4 in the perception and recognition of visual objects,9248377,R01EY018839,"['Agnosia', 'Animal Behavior', 'Animals', 'Area', 'Back', 'Behavior', 'Behavior Control', 'Behavioral', 'Brain', 'Brain Diseases', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Custom', 'Data', 'Discrimination', 'Eye', 'Feedback', 'Functional disorder', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Lesion', 'Macaca', 'Mediating', 'Methods', 'Modeling', 'Monkeys', 'Neurons', 'Pathway interactions', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Play', 'Population', 'Prefrontal Cortex', 'Primates', 'Process', 'Property', 'Psychology', 'Psychophysics', 'Role', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Stimulus', 'Stream', 'Surface', 'Surface Properties', 'Techniques', 'Testing', 'Texture', 'Time', 'V4 neuron', 'Vision', 'Visual', 'Visual Agnosias', 'Visual Cortex', 'Visual system structure', 'area V4', 'awake', 'base', 'design', 'executive function', 'experimental study', 'fovea centralis', 'imaging Segmentation', 'impaired capacity', 'neurophysiology', 'neurotransmission', 'object recognition', 'public health relevance', 'relating to nervous system', 'response', 'stereoscopic', 'study population', 'visual processing']",NEI,UNIVERSITY OF WASHINGTON,R01,2017,479409,-0.026357774787321965
"The role of area V4 in the perception and recognition of visual objects ﻿    DESCRIPTION (provided by applicant): The human visual system parses the information that reaches our eyes into a meaningful arrangement of regions and objects. This process, called image segmentation, is one of the most challenging computations accomplished by the primate brain. To discover its neural basis we will study neuronal processes in two brain areas in the macaque monkey-V4, a fundamental stage of form processing along the occipito-temporal pathway, and the prefrontal cortex (PFC), important for executive control. Dysfunctions of both areas impair shape discrimination behavior in displays that require the identification of segmented objects, strongly suggesting that they are important for image segmentation. Our experimental techniques will include single and multielectrode recordings, behavioral manipulations, perturbation methods and computer models. In Aim 1 we will identify the neural signals that reflect segmentation in visual cortex. Using a variety of parametric stimuli with occlusion, clutter and shadows-stimulus features known to challenge segmentation in natural vision-we will evaluate whether segmentation is achieved by grouping regions with similar surface properties, such as surface color, texture and depth, or by grouping contour segments that are likely to form the boundary of an object or some interplay between these two strategies. We will test the hypothesis that contour grouping mechanisms are most effective under low clutter and close to the fovea. In Aim 2, we will investigate how feedback from PFC modulates shape responses in V4 and facilitates segmentation: we will test the longstanding hypothesis that object recognition in higher cortical stages precedes and facilitates segmentation in the midlevels of visual form processing. We will simultaneously study populations of V4 and PFC neurons while animals engage in shape discrimination behavior. We will use single-trial decoding methods and correlation analyses to relate the content and timing of neuronal responses in the two areas. To causally test the role of feedback from PFC, we will reversibly inactivate PFC by cooling and study V4 neurons. Our results will provide the first detailed, analytical models of V4 neuronal response dynamics in the presence of occlusion and clutter and advance our understanding of how complex visual scenes are processed in area V4. They will also reveal how V4 and PFC together mediate performance on a complex shape discrimination task, how executive function and midlevel vision may be coordinated during behavior and how feedback is used in cortical computation. Object recognition is impaired in visual agnosia, a dysfunction of the occipito-temporal pathway, and in dysfunctions of the PFC (e.g. schizophrenia). Results from these experiments will constitute a major advance in our understanding of the brain computations that underlie segmentation and object recognition and will bring us closer to devising strategies to alleviate and treat brain disorders in which these capacities are impaired. PUBLIC HEALTH RELEVANCE: A fundamental capacity of the primate visual system is its ability to segment visual scenes into component objects and then recognize those objects regardless of partial occlusions and clutter. Using a combination of primate neurophysiology experiments, computational modeling, animal behavior and reversible inactivation methods, we hope to achieve a new level of understanding about visual processing in the context of object recognition; these findings will ultimately bring us closer to devising strategies to alleviate and treat brain disorders of impaired object recognition resulting from dysfunctions in the occipito-temporal pathway (e.g. agnosia) and the prefrontal cortex (e.g. schizophrenia).",The role of area V4 in the perception and recognition of visual objects,9535536,R01EY018839,"['Agnosia', 'Animal Behavior', 'Animals', 'Area', 'Back', 'Behavior', 'Behavior Control', 'Behavioral', 'Brain', 'Brain Diseases', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Custom', 'Data', 'Discrimination', 'Eye', 'Feedback', 'Functional disorder', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Lesion', 'Macaca', 'Mediating', 'Methods', 'Modeling', 'Monkeys', 'Neurons', 'Pathway interactions', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Play', 'Population', 'Prefrontal Cortex', 'Primates', 'Process', 'Property', 'Psychology', 'Psychophysics', 'Role', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Stimulus', 'Stream', 'Surface', 'Surface Properties', 'Techniques', 'Testing', 'Texture', 'Time', 'V4 neuron', 'Vision', 'Visual', 'Visual Agnosias', 'Visual Cortex', 'Visual system structure', 'area V4', 'awake', 'base', 'design', 'executive function', 'experimental study', 'fovea centralis', 'imaging Segmentation', 'impaired capacity', 'neurophysiology', 'neurotransmission', 'object recognition', 'public health relevance', 'relating to nervous system', 'response', 'stereoscopic', 'study population', 'visual processing']",NEI,UNIVERSITY OF WASHINGTON,R01,2017,21050,-0.026357774787321965
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9251284,R01EY017835,"['Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Industrialization', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2017,580671,0.05500936630181525
"Representation of information across the human visual cortex ﻿    DESCRIPTION (provided by applicant): The human visual system is organized as a parallel, hierarchical network, and successive stages of visual processing appear to represent increasingly complicated aspects of shape-related and semantic information. However, the way that shape-related and semantic information is represented across much of the visual hierarchy is still poorly understood. The primary goal of this proposal is to understand how information about object shape and semantic category is represented explicitly across mid- and high-level visual areas. To address this important issue we propose to undertake a series of human functional MRI (fMRI) studies, using both synthetic and natural movies. Data will be analyzed by means of a powerful voxel-wise modeling (VM) approach that has been developed in my laboratory over the past several years. In Aim 1 we propose to measure human brain activity evoked by synthetic naturalistic movies, and to use VM to evaluate and compare several competing theories of shape representation across the entire visual cortex. In Aim 2 we propose to use VM to evaluate and compare competing theories of semantic representation. In Aim 3 we propose to use machine learning and and VM to discover new aspects of shape and semantic representation. These experiments will provide fundamental new insights about the representation of visual information across visual cortex. PUBLIC HEALTH RELEVANCE: Disorders of central vision can severely affect quality of life and the design of treatments and devices for improving visual function will depend critically on understanding the organization of visual cortex. We propose to use functional MRI and sophisticated computational data analysis and modeling procedures to evaluate and compare multiple theories of visual function. The results will reveal how visual information is represented across the several dozen distinct functional areas that constitute human visual cortex.",Representation of information across the human visual cortex,9040948,R01EY019684,"['Address', 'Affect', 'Area', 'Award', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disease', 'Elements', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Health', 'Human', 'Image', 'Individual', 'Laboratories', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Motion', 'Perception', 'Procedures', 'Quality of life', 'Semantics', 'Series', 'Shapes', 'Space Perception', 'Staging', 'Surface', 'System', 'Testing', 'Training', 'V2 neuron', 'V4 neuron', 'Vision', 'Vision research', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'abstracting', 'area V1', 'area striata', 'base', 'data modeling', 'design', 'extrastriate visual cortex', 'improved', 'innovation', 'insight', 'learning strategy', 'movie', 'novel', 'object recognition', 'object shape', 'receptive field', 'research study', 'theories', 'therapy design', 'visual information', 'visual neuroscience', 'visual process', 'visual processing']",NEI,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2016,379313,0.008828729380989775
"Capti Screen Reading Assistant for Goal Directed Web Browsing ﻿    DESCRIPTION (provided by applicant): Web browsing with assistive technologies such as screen readers and magnifiers can often be a frustrating and challenging experience for people with vision impairments, because it entails a lot of searching for content, forms, and links that are required for doing online tasks such as shopping, bill-payment, reservations, etc. This SBIR Phase II project will build on Phase I results and will continue the development and eventual deployment of Capti Screen Reading Assistant - a next-generation assistive technology, enabling goal- directed web browsing for people with visual impairments. With Capti Assistant, users will be able to stay focused on their high-level browsing goals that are expressed in natural language (spoken or typed). The Assistant will lead the users step-by-step towards the fulfillment of these goals by offering suggestions on what action to take at every step of the way and automatically executing the chosen action on behalf of the user. Suggested actions will include operations such as form filling, activating controls (e.g., clicking buttons and links), et. Capti Assistant will dramatically reduce the time spent by people with visual impairments on performing tasks online. The Assistant will significantly improve the speed and efficiency with which they can interact with the Web, thereby, making people with disabilities more productive in today's web-based economy. Given a user browsing goal, expressed in a natural-language form, Capti Assistant will utilize a predictive model to guide the user toward the goal. The unique aspect of the Assistant is that its suggestions will be automatically learned from the user's own history of browsing actions and commands, as well as from the user's demonstration of how to accomplish browsing tasks that have not been done before. The Assistant will process user commands and present the suggested browsing actions to the user on demand, giving the user a choice between following the suggestions or continue browsing normally without accepting the suggestions. The functionality offered by the Assistant will go far beyond popular personal assistant applications such as Siri, which have not been designed specifically for people with vision impairments, and which cannot be used for ad hocweb browsing. Capti Assistant will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the Web. For them, the Assistant will usher in a new era of independence and employability in our global web-based economy. Thus, from a broader perspective, goal- directed browsing will exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", i.e. anyone should be able to reap the benefits of the Web without being constrained by any disability.         PUBLIC HEALTH RELEVANCE: This SBIR Project seeks to do Research and Development on goal-directed web browsing - the next generation accessible technology that will empower people with vision impairments to stay focused on their high-level browsing goals, while the browser will do low-level operations (such as clicking on links and filling forms) necessary to fulfill these goals. Goal-directed browsing will go a long way towards bridging the growing web accessibility divide between the ways people with and without vision impairments browse the web, thus improving independence and employability of the former in our global Web-based economy. From a broader perspective, goal-directed browsing will facilitate rehabilitation of people with disabilities and exemplify the vision of the Universally Accessible Web whose thesis is ""equal access for all"", enabling anyone to reap the benefits of the Web without being constrained by any disability.        ",Capti Screen Reading Assistant for Goal Directed Web Browsing,9048176,R44EY021962,"['Automation', 'Blindness', 'Budgets', 'Businesses', 'Communication', 'Computer software', 'Computers', 'Data', 'Development', 'Disabled Persons', 'Ensure', 'Environment', 'Evaluation', 'FarGo', 'Focus Groups', 'Generations', 'Goals', 'Human Resources', 'In Situ', 'Information Retrieval', 'Internet', 'Internships', 'Laboratory Study', 'Lead', 'Learning', 'Legal patent', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Marketing', 'Mining', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Pattern', 'Phase', 'Probability', 'Process', 'Productivity', 'Publications', 'Publishing', 'Reader', 'Reading', 'Recording of previous events', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Reservations', 'Resources', 'Schedule', 'Scheme', 'Seasons', 'Self-Help Devices', 'Small Business Innovation Research Grant', 'Speed', 'Suggestion', 'System', 'Technology', 'Time', 'Universities', 'Vision', 'Visual impairment', 'Work', 'base', 'collaborative environment', 'commercial application', 'commercialization', 'computer human interaction', 'design', 'disability', 'educational atmosphere', 'empowered', 'experience', 'improved', 'innovation', 'member', 'natural language', 'next generation', 'novel strategies', 'operation', 'payment', 'predictive modeling', 'public health relevance', 'quality assurance', 'query optimization', 'research and development', 'response', 'success', 'technological innovation', 'tool', 'usability', 'web page', 'web-accessible']",NEI,"CHARMTECH LABS, LLC",R44,2016,500000,0.03217743046247355
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,9336584,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,28000,0.04071900617754007
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9136188,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'human-robot interaction', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2016,274076,0.05973965595908843
"Crossmodal Correspondences Between Visual and Auditory Features ﻿    DESCRIPTION (provided by applicant): We live in a multisensory world, in which stimuli of various types constantly compete for our attention. Information about objects or events typically appears on more than one sensory channel, so that integrating inputs across sensory systems (e.g. vision and hearing) can enhance the signal-to-noise ratio and lead to more efficient perception and action. There is increasing interest in studying how stimulus properties in one sensory modality (e.g. vision) correspond to those in another modality (e.g. hearing). For instance, sounds of high pitch are linked to small-sized visual objects whereas sounds of low pitch are linked with large objects; sounds of high/low pitch are associated with, respectively, visual stimuli of high/low elevation; and even aspects of linguistic stimuli such as vowel quality are associated with visual properties such as object size. Such crossmodal correspondences are important factors in multisensory binding. While information has exploded on the kinds of stimulus features that are reliably associated by human observers across modalities, currently there is little neural evidence to allow a mechanistic account of how crossmodal correspondences arise, or how they relate to synesthesia, a phenomenon in which some individuals experience unusual percepts (e.g. colors) triggered by particular stimuli (e.g. letters. Our goal is to address these important gaps in knowledge, by using functional magnetic resonance imaging (fMRI) in humans to investigate the neural mechanisms underlying crossmodal and synesthetic correspondences and thus to distinguish between alternative explanations that have been offered. A number of possible mechanisms have been entertained for crossmodal correspondences. These include: Hypothesis A - learned associations due to statistical co-occurrences, which would predict that the correspondences are based in multisensory or even classic unisensory regions; Hypothesis B - semantic mediation (e.g. the common word ""high"" may mediate the link between high pitch and high elevation); and Hypothesis C - conceptual linking via a high-level property such as magnitude. In a series of eight experiments that comprise three Specific Aims, we propose to examine these competing accounts, recognizing that some or all of them may be operative, and that the mechanisms may vary between different types of crossmodal correspondences. PUBLIC HEALTH RELEVANCE: The proposed systematic study of the brain basis of correspondences between stimulus properties across sensory systems will allow critical insights into the multisensory processing involved in perception and action, illuminate the multisensory basis of language and music, and expand understanding of the phenomenon of synesthesia in relation to normal experience. From a practical standpoint, the proposed work will make significant contributions to the design of sensory substitution approaches for people with visual, auditory and other sensory deficits, and the rehabilitation of individuals with multisensory processing abnormalities, including developmental (autism, dyslexia), neurological (neglect) and psychiatric (schizophrenia) disorders.",Crossmodal Correspondences Between Visual and Auditory Features,9137687,R01EY025978,"['Accounting', 'Address', 'Association Learning', 'Attention', 'Auditory', 'Auditory pitch', 'Autistic Disorder', 'Base of the Brain', 'Behavioral', 'Binding', 'Color', 'Data', 'Development', 'Dimensions', 'Disease', 'Dyslexia', 'Event', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Hearing', 'Human', 'Individual', 'Judgment', 'Knowledge', 'Language', 'Lead', 'Letters', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Mediation', 'Modality', 'Multivariate Analysis', 'Music', 'Neurologic', 'Noise', 'Pattern', 'Perception', 'Process', 'Property', 'Regression Analysis', 'Rehabilitation therapy', 'Research Personnel', 'Rest', 'Schizophrenia', 'Semantics', 'Sensory', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Time', 'Vision', 'Visual', 'Work', 'base', 'design', 'experience', 'insight', 'interest', 'multisensory', 'neglect', 'neuroimaging', 'neuromechanism', 'relating to nervous system', 'research study', 'sensory system', 'sound', 'vector', 'visual stimulus']",NEI,EMORY UNIVERSITY,R01,2016,565302,0.00582297664944515
"Learning and updating internal visual models ﻿    DESCRIPTION (provided by applicant): In line with the strategic plan of the NEI, this project is focused on filling a profound gap in our understanding of neural mechanisms of visual perception. Specifically, we aim to understand how the adaptation of visual cortical circuits contributes to perception. Adaptation is a ubiquitous process by which neural processing and perception are dramatically influenced by recent visual inputs. However, the functional purpose of adaptation is poorly understood. Based on preliminary data, this project tests the hypothesis that visual adaptation instantiates a form of predictive coding, which is used to make unexpected events salient. We posit that cortical circuits learn the statistical structure of visua input in a manner that extends beyond previous fatigue- based descriptions of adaptation effects. This learning is used to discount expected features and signal novel ones. Our project will test this hypothesis through the collaborative effort of three investigators with expertise in human EEG, animal neurophysiology, and computational modeling. Aim 1 will assess the ability of cortical circuits to adapt to temporal sequences of input and to signal deviations from expected sequences. Aim 2 will evaluate the effect of stimulus uncertainty on adaptation and responses to novel events. Aim 3 will determine how adaptation dynamics and responses to novel stimuli are influenced by the temporal constancy of stimulus statistics. Each of these aims involves an experimental manipulation that yields distinct behavior from fatigue- based and predictive coding mechanisms. Thus, together our aims will provide a robust test of our core hypothesis, and provide a much richer understanding of the adaptive properties of cortical circuits. Results from our project will contribute to answering one of the continuing puzzles in visual research, which is to understand the functional purpose of adaptive mechanisms in visual perception. PUBLIC HEALTH RELEVANCE: . This research is relevant to public health because it aims to uncover the function of visual adaptation, a fundamental aspect of visual perception. This work is thus essential to the mission of NEI because it will provide a more detailed understanding of how visual cortical circuits underlie visual perception, which is necessary for developing treatment strategies for individuals with visual processing deficits and for the development of effective prosthetic devices.",Learning and updating internal visual models,9147600,R01EY024858,"['Affect', 'Animals', 'Area', 'Autistic Disorder', 'Behavior', 'Biological', 'Brain', 'Code', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Electroencephalography', 'Elements', 'Environment', 'Event', 'Fatigue', 'Goals', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Machine Learning', 'Mission', 'Modeling', 'Monkeys', 'Neurons', 'Pattern', 'Perception', 'Process', 'Property', 'Prosthesis', 'Public Health', 'Recording of previous events', 'Research', 'Research Personnel', 'Scheme', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Signal Transduction', 'Stimulus', 'Strategic Planning', 'Structure', 'Testing', 'Time', 'Uncertainty', 'Update', 'Visual', 'Visual Cortex', 'Visual Perception', 'Work', 'area V4', 'area striata', 'awake', 'base', 'brain machine interface', 'cognitive neuroscience', 'computational neuroscience', 'design', 'discount', 'expectation', 'extrastriate visual cortex', 'human subject', 'improved', 'meetings', 'neuromechanism', 'neurophysiology', 'novel', 'phenomenological models', 'prevent', 'relating to nervous system', 'response', 'statistics', 'treatment strategy', 'visual adaptation', 'visual process', 'visual processing']",NEI,"ALBERT EINSTEIN COLLEGE OF MEDICINE, INC",R01,2016,561513,0.01538084771428419
"Enabling access to printed text for blind people via assisted mobile OCR DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality. PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.",Enabling access to printed text for blind people via assisted mobile OCR,8989105,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Augmented Reality', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Health', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2016,230563,0.024935100936336815
"CRCNS: Model-driven single-neuron studies of cortical mapping DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale's STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies. n/a",CRCNS: Model-driven single-neuron studies of cortical mapping,9308612,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Electrophysiology (science)', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Software Tools', 'Stream', 'Testing', 'Training', 'Translational Research', 'Underrepresented Groups', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'cortex mapping', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'model building', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'student training', 'symposium', 'tool', 'vector', 'visual motor']",NEI,MONTANA STATE UNIVERSITY - BOZEMAN,R01,2016,180000,-0.000769996043154617
"Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired ﻿    DESCRIPTION (provided by applicant):  Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired Summary CamIO (""Camera Input-Output"") is a novel camera system designed to make physical objects (including documents, maps and 3D objects such as architectural models) fully accessible to people who are blind or visually impaired.  It works by providing real-time audio-haptic feedback in response to the location on an object that the user is pointing to, which is visible to the camera mounted above the workspace.  While exploring the object, the user can move it freely; a gesture such as ""double-tapping"" with a finger signals for the system to provide audio feedback about the location on the object currently pointed to by the finger (or an enhanced image of the selected location for users with low vision).  Compared with other approaches to making objects accessible to people who are blind or visually impaired, CamIO has several advantages:  (a) there is no need to modify or augment existing objects (e.g., with Braille labels or special touch-sensitive buttons), requiring only a low- cost camera and laptop computer; (b) CamIO is accessible even to those who are not fluent in Braille; and (c) it permits natural exploration of the object with all fingers (in contrast with approaches that rely on the use of a special stylus).  Note also that existing approaches to making graphics on touch-sensitive tablets (such as the iPad) accessible can provide only limited haptic feedback - audio and vibration cues - which is severely impoverished compared with the haptic feedback obtained by exploring a physical object with the fingers.  We propose to develop the necessary computer vision algorithms for CamIO to learn and recognize objects, estimate each object's pose to help determine where the fingers are pointing on the object, track the fingers, recognize gestures and perform OCR (optical character recognition) on text printed on the object surface.  The system will be designed with special user interface features that enable blind or visually impaired users to freely explore an object of interest and interact with it naturally to access the information they want, which will be presented in a modality appropriate for their needs (e.g., text-to-speech or enhanced images of text on the laptop screen).  Additional functions will allow sighted assistants (either in person or remote) to contribute object annotation information.  Finally, people who are blind or visually impaired - the target users of the CamIO system - will be involved in all aspects of this proposed research, to maximize the impact of the research effort.  At the conclusion of the grant we plan to release CamIO software as a free and open source (FOSS) project that anyone can download and use, and which can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is insufficient access to a wide range of everyday objects needed for daily activities that require visual inspection on the part of the user.  Such objects include printed documents, maps, infographics, and 3D models used in science, technology, engineering, and mathematics (STEM), and are abundant in schools, the home and the workplace.  The proposed research would result in an inexpensive camera-based assistive technology system to provide increased access to such objects for the approximately 10 million Americans with significant vision impairments or blindness.                ",Enabling Audio-Haptic Interaction with Physical Objects for the Visually Impaired,9030162,R01EY025332,"['3D Print', 'Access to Information', 'Adoption', 'Algorithms', 'American', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Computers', 'Cues', 'Development', 'Economics', 'Employment', 'Ensure', 'Evaluation', 'Feedback', 'Fingers', 'Focus Groups', 'Gestures', 'Goals', 'Grant', 'Home environment', 'Image', 'Information Systems', 'Label', 'Lasers', 'Learning', 'Location', 'Maps', 'Modality', 'Modeling', 'Output', 'Performance', 'Persons', 'Printing', 'Procedures', 'Process', 'Research', 'Rotation', 'Running', 'Schools', 'Science, Technology, Engineering and Mathematics', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Surface', 'System', 'Tablets', 'Tactile', 'Technology', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Translations', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'braille', 'cost', 'design', 'experience', 'haptics', 'heuristics', 'interest', 'laptop', 'novel', 'open source', 'optical character recognition', 'public health relevance', 'research study', 'response', 'three-dimensional modeling', 'vibration']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2016,416574,0.07793366125332993
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8989994,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Imaging Device', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Patient-Focused Outcomes', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'spectrograph', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2016,230028,0.030606867963203277
"Detection of Glaucoma Progression with Macular OCT Imaging DESCRIPTION (provided by applicant): This application is a formal request for a career development award (K23) for an academic glaucoma specialist with a serious interest in the role of imaging in glaucoma using optical coherence tomography (OCT). This will allow the candidate to establish a clinical research program with the main goal of improving detection of glaucoma progression through macular imaging with spectral-domain OCT. By the time the proposed research is accomplished, the candidate will have preliminary data for continuing his research as an independent investigator and will have collected longitudinal structural and functional data in a group of advanced glaucoma patients that will serve as a platform for further improving detection of glaucoma progression with macular OCT imaging. The data will help the candidate provide preliminary results for a subsequent R01 that would potentially allow the PI to continue follow-up of the patients enrolled in the K23 award period.  I have a Master's of Science degree in Clinical Investigation under my belt and intend to deepen my skills in the field of imaging and biostatistics (to be used for enhancing and handling OCT images and for analyzing longitudinal data) by completing the proposed didactic program. By the end of the award period, I expect that I will have gained additional experience, knowledge, and mentorship required to prosper as an independent clinician-scientist in the field of glaucoma. My long-term goal is to carry out longitudinal studies of glaucoma patients where current and upcoming imaging and functional tests can be applied and their utility for detection of glaucoma progression can be investigated. I am confident that the combined skills and experience of my mentors will lead to a successful outcome for the proposed K award. I also envisage myself mentoring candidates like myself in future so that our collective knowledge and wisdom can be passed along to the next generation of aspiring clinician-scientists.  My objectives during the award period are as follows: 1) To develop an individual research program in glaucoma diagnostic imaging; 2) to successfully complete credited coursework in biomathematics, advanced biostatistics, computer vision (image processing), epidemiology, and ethical issues in research.  The main goal of the research component of this proposal is to better delineate the role of macular SD- OCT imaging for detection of glaucoma progression in advanced glaucoma. The specific aims through which this goal will be accomplished are as follows:  (1) To compare the performance of various global and regional macular measures to detect glaucoma.  The potential factors influencing the performance of various macular outcome measures will be explored. Such covariates include age, race, axial length, disc size, central corneal thickness,  OCT signal strength, and outer retinal thickness among others. I hypothesize that the thickness  of the outer retina (outer nuclear layer to retinal pigment epithelium-Bruch's membrane  complex) may be the most important factor explaining the measurement variability of the inner  retinal layer thickness (GCC or ganglion cell/inner plexiform layers).  (2) To determine and compare the utility of the candidate macular measures, detected through the first  aim, for detection of glaucoma progression in moderately advanced to severe glaucoma.  Moderately advanced to severe glaucoma will be defined as eyes with visual field mean  deviation worse than -6 dB or eyes with involvement of the central 10 degrees on the 24-2  visual field. It is widely accepted that measurement of the optic nerve head or RNFL parameters  in advanced glaucoma does not provide clinicians with much useful information. In contrast, the  central macular ganglion cells are the last to die in glaucoma. Macular imaging in advanced  glaucoma is directed towards this area where detection of change may still be possible. I  hypothesize that macular OCT parameters are valid structural outcome measures (biomarkers)  that can be used to follow the course of the disease in advanced glaucoma and that such  measures are significantly correlated with changes in the central visual field. Changes in the  macular measures over time will be first correlated with the corresponding visual field change  (functional progression) over time in eyes with moderately advanced to severe glaucoma. The  utility of the best candidate macular measures for predicting subsequent glaucoma progression  will also be explored and compared. I hypothesize that there may be a lag period between  progressive loss of macular ganglion cells and subsequent visual field progression in advanced  glaucoma, and therefore, detection of worsening in one or more macular outcome measures  can be used as a proxy for subsequent visual field progression.  Collectively, these studies will provide a solid foundation for better understanding and integration of macular OCT imaging in the care of glaucoma patients. Timely detection of glaucoma progression in the later stages can significantly reduce visual disability and blindness through earlier aggressive treatment and will potentially reduce glaucoma's financial burden to society. Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.",Detection of Glaucoma Progression with Macular OCT Imaging,9084580,K23EY022659,"['Age', 'Area', 'Award', 'Biological Markers', 'Biometry', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Caring', 'Clinical Research', 'Complement', 'Complex', 'Computer Vision Systems', 'Cornea', 'Data', 'Detection', 'Diagnosis', 'Diagnostic Imaging', 'Disease', 'Early Diagnosis', 'Enrollment', 'Epidemiology', 'Ethical Issues', 'Evaluation', 'Eye', 'Foundations', 'Functional Imaging', 'Future', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Image Analysis', 'Individual', 'Inner Plexiform Layer', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Legal Blindness', 'Length', 'Longitudinal Studies', 'Master of Science', 'Measurement', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Morbidity - disease rate', 'Nerve Fibers', 'Noise', 'Ophthalmologist', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Process', 'Proxy', 'Quality of life', 'Race', 'Research', 'Research Personnel', 'Retinal', 'Role', 'Scientist', 'Signal Transduction', 'Societies', 'Solid', 'Specialist', 'Staging', 'Structure of retinal pigment epithelium', 'Testing', 'Thick', 'Time', 'Vision', 'Visual', 'Visual Fields', 'advanced disease', 'biomathematics', 'central visual field', 'clinical investigation', 'disability', 'experience', 'field study', 'follow-up', 'ganglion cell', 'image processing', 'improved', 'interest', 'macula', 'next generation', 'programs', 'retina outer nuclear layer', 'retinal nerve fiber layer', 'skills']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,K23,2016,229139,0.06665213663832276
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,9024545,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Fall injury', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Health', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Thinking', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2016,576055,0.05500936630181525
"The role of area V4 in the perception and recognition of visual objects ﻿    DESCRIPTION (provided by applicant): The human visual system parses the information that reaches our eyes into a meaningful arrangement of regions and objects. This process, called image segmentation, is one of the most challenging computations accomplished by the primate brain. To discover its neural basis we will study neuronal processes in two brain areas in the macaque monkey-V4, a fundamental stage of form processing along the occipito-temporal pathway, and the prefrontal cortex (PFC), important for executive control. Dysfunctions of both areas impair shape discrimination behavior in displays that require the identification of segmented objects, strongly suggesting that they are important for image segmentation. Our experimental techniques will include single and multielectrode recordings, behavioral manipulations, perturbation methods and computer models. In Aim 1 we will identify the neural signals that reflect segmentation in visual cortex. Using a variety of parametric stimuli with occlusion, clutter and shadows-stimulus features known to challenge segmentation in natural vision-we will evaluate whether segmentation is achieved by grouping regions with similar surface properties, such as surface color, texture and depth, or by grouping contour segments that are likely to form the boundary of an object or some interplay between these two strategies. We will test the hypothesis that contour grouping mechanisms are most effective under low clutter and close to the fovea. In Aim 2, we will investigate how feedback from PFC modulates shape responses in V4 and facilitates segmentation: we will test the longstanding hypothesis that object recognition in higher cortical stages precedes and facilitates segmentation in the midlevels of visual form processing. We will simultaneously study populations of V4 and PFC neurons while animals engage in shape discrimination behavior. We will use single-trial decoding methods and correlation analyses to relate the content and timing of neuronal responses in the two areas. To causally test the role of feedback from PFC, we will reversibly inactivate PFC by cooling and study V4 neurons. Our results will provide the first detailed, analytical models of V4 neuronal response dynamics in the presence of occlusion and clutter and advance our understanding of how complex visual scenes are processed in area V4. They will also reveal how V4 and PFC together mediate performance on a complex shape discrimination task, how executive function and midlevel vision may be coordinated during behavior and how feedback is used in cortical computation. Object recognition is impaired in visual agnosia, a dysfunction of the occipito-temporal pathway, and in dysfunctions of the PFC (e.g. schizophrenia). Results from these experiments will constitute a major advance in our understanding of the brain computations that underlie segmentation and object recognition and will bring us closer to devising strategies to alleviate and treat brain disorders in which these capacities are impaired. PUBLIC HEALTH RELEVANCE: A fundamental capacity of the primate visual system is its ability to segment visual scenes into component objects and then recognize those objects regardless of partial occlusions and clutter. Using a combination of primate neurophysiology experiments, computational modeling, animal behavior and reversible inactivation methods, we hope to achieve a new level of understanding about visual processing in the context of object recognition; these findings will ultimately bring us closer to devising strategies to alleviate and treat brain disorders of impaired object recognition resulting from dysfunctions in the occipito-temporal pathway (e.g. agnosia) and the prefrontal cortex (e.g. schizophrenia).",The role of area V4 in the perception and recognition of visual objects,9039081,R01EY018839,"['Agnosia', 'Animal Behavior', 'Animals', 'Area', 'Back', 'Behavior', 'Behavior Control', 'Behavioral', 'Brain', 'Brain Diseases', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Custom', 'Data Analyses', 'Discrimination', 'Eye', 'Feedback', 'Functional disorder', 'Goals', 'Grouping', 'Health', 'Human', 'Image', 'Lead', 'Lesion', 'Macaca', 'Mediating', 'Methods', 'Modeling', 'Monkeys', 'Neurons', 'Pathway interactions', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Play', 'Population', 'Prefrontal Cortex', 'Primates', 'Process', 'Property', 'Psychology', 'Psychophysics', 'Role', 'Schizophrenia', 'Shapes', 'Signal Transduction', 'Staging', 'Stimulus', 'Stream', 'Surface', 'Surface Properties', 'Techniques', 'Testing', 'Texture', 'Time', 'V4 neuron', 'Vision', 'Visual', 'Visual Agnosias', 'Visual Cortex', 'Visual system structure', 'area V4', 'awake', 'base', 'design', 'executive function', 'feeding', 'fovea centralis', 'imaging Segmentation', 'impaired capacity', 'neurophysiology', 'neurotransmission', 'object recognition', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'study population', 'visual process', 'visual processing']",NEI,UNIVERSITY OF WASHINGTON,R01,2016,498062,-0.026357774787321965
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning. PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8914675,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Health', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2015,136355,0.00844032981191666
"Representation of information across the human visual cortex ﻿    DESCRIPTION (provided by applicant): The human visual system is organized as a parallel, hierarchical network, and successive stages of visual processing appear to represent increasingly complicated aspects of shape-related and semantic information. However, the way that shape-related and semantic information is represented across much of the visual hierarchy is still poorly understood. The primary goal of this proposal is to understand how information about object shape and semantic category is represented explicitly across mid- and high-level visual areas. To address this important issue we propose to undertake a series of human functional MRI (fMRI) studies, using both synthetic and natural movies. Data will be analyzed by means of a powerful voxel-wise modeling (VM) approach that has been developed in my laboratory over the past several years. In Aim 1 we propose to measure human brain activity evoked by synthetic naturalistic movies, and to use VM to evaluate and compare several competing theories of shape representation across the entire visual cortex. In Aim 2 we propose to use VM to evaluate and compare competing theories of semantic representation. In Aim 3 we propose to use machine learning and and VM to discover new aspects of shape and semantic representation. These experiments will provide fundamental new insights about the representation of visual information across visual cortex.         PUBLIC HEALTH RELEVANCE: Disorders of central vision can severely affect quality of life and the design of treatments and devices for improving visual function will depend critically on understanding the organization of visual cortex. We propose to use functional MRI and sophisticated computational data analysis and modeling procedures to evaluate and compare multiple theories of visual function. The results will reveal how visual information is represented across the several dozen distinct functional areas that constitute human visual cortex.                ",Representation of information across the human visual cortex,8888120,R01EY019684,"['Address', 'Affect', 'Area', 'Award', 'Biological', 'Biological Neural Networks', 'Brain', 'Categories', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disease', 'Elements', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Human', 'Image', 'Individual', 'Laboratories', 'Link', 'Literature', 'Machine Learning', 'Maps', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Motion', 'Perception', 'Procedures', 'Quality of life', 'Semantics', 'Series', 'Shapes', 'Space Perception', 'Staging', 'Surface', 'System', 'Testing', 'Training', 'V2 neuron', 'V4 neuron', 'Vision', 'Vision research', 'Visual', 'Visual Cortex', 'Visual system structure', 'Work', 'abstracting', 'area V1', 'area striata', 'base', 'data modeling', 'design', 'extrastriate visual cortex', 'improved', 'innovation', 'insight', 'movie', 'novel', 'object recognition', 'object shape', 'public health relevance', 'receptive field', 'research study', 'theories', 'therapy design', 'visual information', 'visual neuroscience', 'visual process', 'visual processing']",NEI,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2015,368851,0.008828729380989775
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering. PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8920573,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Health', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'rehabilitation engineering', 'robot rehabilitation', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,3412,0.04071900617754007
"NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired PROJECT SUMMARY (See instructions): The objective of the proposed research is to develop new technology for a Wearable Robotic Object Manipulation Aid (W-ROMA) for the visually impaired. The W-ROMA is a hand-worn assistive device that provides assistance to a visually impaired individual in effectively grasping an object. Thanks to the onboard computer vision methods, the W-ROMA is capable of detecting a target object, determining the hand-object misalignment, and conveying to the wearer, via natural human-device interfaces, the desired hand motion for hand-object alignment. The W-ROMA will contribute to the independent lives of the visually impaired in twofold: First, it helps the visually impaired with independent travel by enabling them to identify a movable obstacle and manipulate the obstacle to make a passage. Second, it assists the visually impaired in effectively grasping an object for non-navigational purpose. The PIs will involve graduate, undergraduate and high school students in the project and use the proposed project activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision and human-robot interaction methods that support accurate and effective object grasping for the visually impaired for their independent daily lives. These methods include: (1) a new real-time object recognition method; (2) an innovative hand-object alignment mechanism; (3) a novel hybrid tactile display system for object shape rendering; and (4) a computationally efficient device localization method. The proposed solutions can be encapsulated in a hand-worn robotic device. The W-ROMA will provide new co-robotic functions for the visually impaired. The PIs have performed proof of concept studies for the computer vision and tactile display methods and the results are promising. The broader impacts include: (1) the research will positively impact the large visually impaired community; (2) the proposed methods can be applied to other small robotic systems that have a wide range of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the PI's university and train graduate and undergraduate students for their future careers in science and engineering. RELEVANCE (See instructions): The project addresses a growing public health care issue--visual impairment. The research fits well into the NEI's Low Vision and Blindness Rehabilitation program that supports development of new technologies for minimizing the impact of visual impairment. The project addresses the NEI's mission by developing new assistive technology that will help the visually impaired to maintain a higher quality of life.",NRI: A Wearable Robotic Object Manipulation Aid for the Visually Impaired,9050942,R01EY026275,"['Address', 'Blindness', 'Canes', 'Code', 'Communities', 'Companions', 'Computer Vision Systems', 'Data', 'Detection', 'Development', 'Devices', 'Encapsulated', 'Engineering', 'Environment', 'Funding', 'Future', 'Goals', 'Grant', 'Graph', 'Hand', 'Healthcare', 'High School Student', 'Human', 'Hybrids', 'Image', 'Independent Living', 'Individual', 'Instruction', 'Law Enforcement', 'Maps', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'Motion', 'Movement', 'National Institute of Biomedical Imaging and Bioengineering', 'Performance', 'Persons', 'Polymers', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Rotation', 'Scheme', 'Science', 'Self-Help Devices', 'Solid', 'Solutions', 'Speech', 'Speed', 'Students', 'System', 'Tactile', 'Testing', 'Thumb structure', 'Time', 'Training', 'Translations', 'Travel', 'United States National Institutes of Health', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'base', 'blind', 'career', 'design', 'experience', 'graduate student', 'grasp', 'improved', 'innovation', 'new technology', 'novel', 'object recognition', 'object shape', 'programs', 'research study', 'robotic device', 'tactile display', 'undergraduate student']",NEI,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2015,280721,0.05973965595908843
"Providing Access to Appliance Displays for Visually Impaired Users DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8916115,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Health', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'contrast enhanced', 'contrast imaging', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2015,368560,0.06680354215959919
"Crossmodal Correspondences Between Visual and Auditory Features ﻿    DESCRIPTION (provided by applicant): We live in a multisensory world, in which stimuli of various types constantly compete for our attention. Information about objects or events typically appears on more than one sensory channel, so that integrating inputs across sensory systems (e.g. vision and hearing) can enhance the signal-to-noise ratio and lead to more efficient perception and action. There is increasing interest in studying how stimulus properties in one sensory modality (e.g. vision) correspond to those in another modality (e.g. hearing). For instance, sounds of high pitch are linked to small-sized visual objects whereas sounds of low pitch are linked with large objects; sounds of high/low pitch are associated with, respectively, visual stimuli of high/low elevation; and even aspects of linguistic stimuli such as vowel quality are associated with visual properties such as object size. Such crossmodal correspondences are important factors in multisensory binding. While information has exploded on the kinds of stimulus features that are reliably associated by human observers across modalities, currently there is little neural evidence to allow a mechanistic account of how crossmodal correspondences arise, or how they relate to synesthesia, a phenomenon in which some individuals experience unusual percepts (e.g. colors) triggered by particular stimuli (e.g. letters. Our goal is to address these important gaps in knowledge, by using functional magnetic resonance imaging (fMRI) in humans to investigate the neural mechanisms underlying crossmodal and synesthetic correspondences and thus to distinguish between alternative explanations that have been offered. A number of possible mechanisms have been entertained for crossmodal correspondences. These include: Hypothesis A - learned associations due to statistical co-occurrences, which would predict that the correspondences are based in multisensory or even classic unisensory regions; Hypothesis B - semantic mediation (e.g. the common word ""high"" may mediate the link between high pitch and high elevation); and Hypothesis C - conceptual linking via a high-level property such as magnitude. In a series of eight experiments that comprise three Specific Aims, we propose to examine these competing accounts, recognizing that some or all of them may be operative, and that the mechanisms may vary between different types of crossmodal correspondences.         PUBLIC HEALTH RELEVANCE: The proposed systematic study of the brain basis of correspondences between stimulus properties across sensory systems will allow critical insights into the multisensory processing involved in perception and action, illuminate the multisensory basis of language and music, and expand understanding of the phenomenon of synesthesia in relation to normal experience. From a practical standpoint, the proposed work will make significant contributions to the design of sensory substitution approaches for people with visual, auditory and other sensory deficits, and the rehabilitation of individuals with multisensory processing abnormalities, including developmental (autism, dyslexia), neurological (neglect) and psychiatric (schizophrenia) disorders.                ",Crossmodal Correspondences Between Visual and Auditory Features,8988153,R01EY025978,"['Accounting', 'Address', 'Association Learning', 'Attention', 'Auditory', 'Auditory pitch', 'Autistic Disorder', 'Base of the Brain', 'Behavioral', 'Binding', 'Color', 'Data', 'Development', 'Dimensions', 'Disease', 'Dyslexia', 'Event', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hearing', 'Human', 'Individual', 'Judgment', 'Knowledge', 'Language', 'Lead', 'Letters', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Measures', 'Mediating', 'Mediation', 'Modality', 'Multivariate Analysis', 'Music', 'Neurologic', 'Noise', 'Pattern', 'Perception', 'Process', 'Property', 'Regression Analysis', 'Rehabilitation therapy', 'Research Personnel', 'Rest', 'Schizophrenia', 'Semantics', 'Sensory', 'Series', 'Shapes', 'Signal Transduction', 'Stimulus', 'Time', 'Vision', 'Visual', 'Work', 'base', 'design', 'experience', 'insight', 'interest', 'multisensory', 'neglect', 'neuroimaging', 'neuromechanism', 'public health relevance', 'relating to nervous system', 'research study', 'sensory system', 'sound', 'vector', 'visual stimulus']",NEI,EMORY UNIVERSITY,R01,2015,586366,0.00582297664944515
"Learning and updating internal visual models ﻿    DESCRIPTION (provided by applicant): In line with the strategic plan of the NEI, this project is focused on filling a profound gap in our understanding of neural mechanisms of visual perception. Specifically, we aim to understand how the adaptation of visual cortical circuits contributes to perception. Adaptation is a ubiquitous process by which neural processing and perception are dramatically influenced by recent visual inputs. However, the functional purpose of adaptation is poorly understood. Based on preliminary data, this project tests the hypothesis that visual adaptation instantiates a form of predictive coding, which is used to make unexpected events salient. We posit that cortical circuits learn the statistical structure of visua input in a manner that extends beyond previous fatigue- based descriptions of adaptation effects. This learning is used to discount expected features and signal novel ones. Our project will test this hypothesis through the collaborative effort of three investigators with expertise in human EEG, animal neurophysiology, and computational modeling. Aim 1 will assess the ability of cortical circuits to adapt to temporal sequences of input and to signal deviations from expected sequences. Aim 2 will evaluate the effect of stimulus uncertainty on adaptation and responses to novel events. Aim 3 will determine how adaptation dynamics and responses to novel stimuli are influenced by the temporal constancy of stimulus statistics. Each of these aims involves an experimental manipulation that yields distinct behavior from fatigue- based and predictive coding mechanisms. Thus, together our aims will provide a robust test of our core hypothesis, and provide a much richer understanding of the adaptive properties of cortical circuits. Results from our project will contribute to answering one of the continuing puzzles in visual research, which is to understand the functional purpose of adaptive mechanisms in visual perception.         PUBLIC HEALTH RELEVANCE: . This research is relevant to public health because it aims to uncover the function of visual adaptation, a fundamental aspect of visual perception. This work is thus essential to the mission of NEI because it will provide a more detailed understanding of how visual cortical circuits underlie visual perception, which is necessary for developing treatment strategies for individuals with visual processing deficits and for the development of effective prosthetic devices.            ",Learning and updating internal visual models,8990935,R01EY024858,"['Affect', 'Animals', 'Area', 'Autistic Disorder', 'Behavior', 'Biological', 'Brain', 'Code', 'Computer Simulation', 'Data', 'Detection', 'Development', 'Disease', 'Electroencephalography', 'Elements', 'Environment', 'Event', 'Fatigue', 'Goals', 'Health', 'Human', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Machine Learning', 'Mission', 'Modeling', 'Monkeys', 'Neurons', 'Pattern', 'Perception', 'Process', 'Property', 'Prosthesis', 'Public Health', 'Recording of previous events', 'Research', 'Research Personnel', 'Scheme', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Signal Transduction', 'Stimulus', 'Strategic Planning', 'Structure', 'Testing', 'Time', 'Uncertainty', 'Update', 'Visual', 'Visual Cortex', 'Visual Perception', 'Work', 'area V4', 'area striata', 'awake', 'base', 'brain machine interface', 'cognitive neuroscience', 'computational neuroscience', 'design', 'discount', 'expectation', 'extrastriate visual cortex', 'human subject', 'improved', 'meetings', 'neuromechanism', 'neurophysiology', 'novel', 'phenomenological models', 'prevent', 'public health relevance', 'relating to nervous system', 'response', 'statistics', 'treatment strategy', 'visual adaptation', 'visual process', 'visual processing']",NEI,"ALBERT EINSTEIN COLLEGE OF MEDICINE, INC",R01,2015,575253,0.01538084771428419
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs. The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8792219,R01EY022039,"['Address', 'Algorithm Design', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2015,379750,0.04167781291567067
"Enabling access to printed text for blind people via assisted mobile OCR     DESCRIPTION (provided by applicant): This application proposes new technology development and user studies aiming to facilitate the use of mobile Optical Character Recognition (OCR) for blind people. Mobile OCR systems, implemented as smartphones apps, have recently appeared on the market. This technology unleashes the power of modern computer vision algorithms to enable a blind person to hear (via synthetic speech) the content of printed text imaged by the smartphone's camera. Unlike traditional OCR, that requires scanning of a document with a flatbed scanner, mobile OCR apps enable access to text anywhere, anytime. Using their own smartphones, blind people can read store receipts, menus, flyers, business cards, utility bills, and many other printed documents of the type normally encountered in everyday life. Unfortunately, current mobile OCR systems suffer from a chicken-and-egg problem, which limits their usability. They require the user to take a well-framed snapshot of the document to be scanned, with the full text in view, and at a close enough distance that each character can be well resolved and thus readable by the machine. However, taking a good picture of a document is difficult without sight, and thus without the ability to look at the scene being imaged by the camera through the smartphone's screen. Anecdotal evidence, supported by results of preliminary studies conducted by the principal investigator's group, confirms that acquisition of an OCR-readable image of a document can indeed by very challenging for some blind users. We plan to address this problem by developing and testing a new technique of assisted mobile OCR. As the user aims the camera at the document, the system analyzes in real time the stream of images acquired by the camera, and determines how the camera position and orientation should be adjusted so that an OCR-readable image of the document can be acquired. This information is conveyed to the user via a specially designed acoustic signal. This acoustic feedback allows users to quickly adjust and reorient the camera or the document, resulting in reduced access time and in more satisfactory user experience. Multiple user studies with blind participants are planned with the purpose of selecting an appropriate acoustic interface and of evaluating the effectiveness of the proposed assisted mobile OCR modality.         PUBLIC HEALTH RELEVANCE: This application is concerned with the development of new technology designed to facilitate use of mobile Optical Character Recognition (OCR) systems to access printed text without sight. Specifically, this exploratory research will develop and test a novel system that, by means of a specially designed acoustic interface, will help a blind person take a well-framed, well-resolved image of a document for OCR processing using a smartphone or wearable camera. If successful, this novel approach to assisted mobile OCR will reduce access time and improve user experience of blind mobile OCR users.                ",Enabling access to printed text for blind people via assisted mobile OCR,8812658,R21EY025077,"['Acoustics', 'Address', 'Algorithms', 'Businesses', 'Cellular Phone', 'Chest', 'Chickens', 'Clothing', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Disabled Persons', 'Education', 'Effectiveness', 'Employment', 'Environment', 'Eyeglasses', 'Feedback', 'Goals', 'Hand', 'Hearing', 'Image', 'Knowledge', 'Life', 'Light', 'Location', 'Marketing', 'Modality', 'Monitor', 'Participant', 'Pattern', 'Positioning Attribute', 'Principal Investigator', 'Printing', 'Process', 'Quality of life', 'Reading', 'Report (document)', 'Research', 'Resolution', 'Restaurants', 'Scanning', 'Series', 'Signal Transduction', 'Solutions', 'Speech', 'Stream', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Technology Development Study', 'Testing', 'Text', 'Time', 'Translating', 'Travel', 'Vision', 'Visual', 'Visually Impaired Persons', 'blind', 'design', 'egg', 'experience', 'handicapping condition', 'improved', 'new technology', 'novel', 'novel strategies', 'object recognition', 'optical character recognition', 'public health relevance', 'research study', 'technology development', 'usability', 'way finding']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2015,191510,0.024935100936336815
"CRCNS: Model-driven single-neuron studies of cortical remapping DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale's STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies. n/a",CRCNS: Model-driven single-neuron studies of cortical remapping,8928626,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Software Tools', 'Stream', 'Students', 'Testing', 'Training', 'Translational Research', 'Underrepresented Minority', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'model building', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'symposium', 'tool', 'vector', 'visual motor']",NEI,YALE UNIVERSITY,R01,2015,203962,-0.00098788761035067
"Neural and Behavioral Interactions Between Attention, Perception, and Learning    DESCRIPTION (provided by applicant): The overarching goal of this research is to characterize how perception and memory interact, in terms of both the learning mechanisms that help transform visual experience into memory, and the intentional mechanisms that regulate this transformation. The specific goal of this proposal is to test the hypothesis that incidental learning about statistical regularities in vision (visual statistical learning) is limited to selectively attend visual information, and that this behavioral interaction arises because of how selective attention modulates neural interactions between human visual and memory systems. We propose a two-stage framework in which selective attention to a high-level visual feature/category increases neural interactions between regions of occipital cortex that represent low-level features and the region of inferior temporal cortex (IT) that represents the attended feature/category, and in turn between this IT region and medial temporal lobe (MTL) sub regions involved in visual learning and memory. In addition to assessing how feature-based selective attention influences learning at a behavioral level, we will use functional magnetic resonance imaging to assess how attention influences evoked neural responses in task-relevant brain regions, as well as neural interactions between these regions in the background of ongoing tasks. We will develop an innovative approach for studying neural interactions in which evoked responses and global noise sources are scrubbed from the data and regional correlations are assessed in the residuals. This background connectivity approach provides a new way to study how intentional goals affect perception and learning. Aim 1 examines the first stage of our framework, testing: how selective attention modulates background connectivity between IT and occipital cortex, where in retinotopic visual cortex this modulation occurs, and how these changes are controlled by frontal and parietal cortex. Aim 2 examines the second stage of our framework, first establishing the role of the MTL in visual statistical learning, and then testing: how selective attention modulates interactions between IT and the MTL, where in cortical and hippocampal sub regions of the MTL this modulation occurs, and how these changes facilitate incidental learning about statistical regularities and later retrieval of this knowledge. In sum, we relate behavioral interactions between selective attention and learning to neural interactions between the mechanisms that represent visual features and those that learn about their relations. This proposal addresses several key issues in the field, including: how attention modulates the MTL, how feature-based attention is controlled, whether different neural mechanisms support rapid versus long-term visual learning, how tasks and goals are represented, and how attention and memory retrieval are related. This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery and rehabilitation of visual function following eye disease, injury, or brain damage.        This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.         ","Neural and Behavioral Interactions Between Attention, Perception, and Learning",8895328,R01EY021755,"['Address', 'Affect', 'Architecture', 'Area', 'Attention', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Categories', 'Cognition', 'Cognitive Science', 'Data', 'Development', 'Diagnosis', 'Disease', 'Event', 'Exhibits', 'Eye diseases', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Inferior', 'Injury', 'Knowledge', 'Lead', 'Learning', 'Light', 'Link', 'Machine Learning', 'Maps', 'Medial', 'Memory', 'Methods', 'Mind', 'Modeling', 'Nature', 'Neurosciences', 'Noise', 'Occipital lobe', 'Parietal', 'Parietal Lobe', 'Perception', 'Physiological', 'Positioning Attribute', 'Process', 'Property', 'Recovery', 'Rehabilitation therapy', 'Research', 'Residual state', 'Resolution', 'Rest', 'Retrieval', 'Role', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Staging', 'Stimulus', 'Sum', 'Synapses', 'System', 'Temporal Lobe', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'cognitive neuroscience', 'cognitive process', 'cognitive task', 'experience', 'extrastriate visual cortex', 'frontal lobe', 'hippocampal subregions', 'improved', 'information processing', 'innovation', 'memory retrieval', 'neuroimaging', 'neuromechanism', 'novel strategies', 'relating to nervous system', 'response', 'retinotopic', 'selective attention', 'transmission process', 'visual information', 'visual learning', 'visual memory', 'visual process', 'visual processing', 'visual stimulus']",NEI,PRINCETON UNIVERSITY,R01,2015,347803,-0.00478888459822869
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8798661,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'spectrograph', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2015,228148,0.030606867963203277
"A Visual Assessment System for Retinal Function/Drug Discovery ﻿    DESCRIPTION (provided by applicant): Preclinical evaluation of treatment strategies for retinal neurodegenerative diseases is highly dependent on mouse models. Classical methods to assess the visual function of animals, such as electroretinogram (ERG), which measures electrical responses in the retina, do not address connections between the eye and brain or visual perception by the visual system. This often raises concerns regarding the functional relevance of the therapeutic benefit. Difficulty in assessing visual perception and related behavior in mice and rats, largely due to their subtle visual behavior cues and the lack of adequate measuring devices, presents a critical barrier to the application of mouse models for evaluating treatment efficacy of new drugs, and for scaling up for behavior phenotyping to screen genetic vision defects. Pupillary light reflex (PLR) and optokinetic reflex (OKR) tests are useful methods in clinics for assessing human visual responses and perception. However, such tests have been difficult to conduct in rodents because current rodent visual testing methods or devices either do not allow accurate quantitative assessment for PLR or OKR or use subjective measures to score visual responses. To address these challenges, we propose to advance the technology by designing an easy-to-use automated platform that employs an eye/pupil tracking device equipped with a computer vision system (chiefly the interactive tracking system) for unambiguous objective scoring of visual responses. Our proposed new device will allow real-time quantitative and accurate assessment of rodent visual function including light responses, visual acuity and contrast sensitivity. The novelty of our system also lies in that it does not require complicated calibration procedures needed in commonly used human eye tracking. Rather than precisely measuring the extent of eye turning (or orientation), we propose to detect the signature eye movement in accordance with the speed and direction of visual stimuli. The system will be validated using normal wildtype mice and mouse models of retinal neurodegeneration known to develop visual behavior changes in the parameters mentioned above. Although rodent eye tracking has been investigated before, this proposed visual assessment system would be the first commercially viable product that uses an eye/pupil tracking device to automatically assess visual perception in rodents. The combined PLR and OKR tests and vastly simplified and automated quantification methods will also provide the first scalable behavior platform for phenotyping and drug discovery in the vision research area. In the future, this technology has the potential of being expanded to measure responses from various visual stimuli. This may translate into broader applications for evaluating brain diseases that afflict the visual pathways. This platform for mouse visual behavior assessment will therefore greatly facilitate drug discovery and development aimed at preventing and slowing vision loss or restoring sight, helping to combat devastating blinding conditions such as age-related macular degeneration (AMD) and glaucoma.         PUBLIC HEALTH RELEVANCE: The objective of the current proposal is to design and develop an automated system for the measure of rodent (mice and rats) light response, visual acuity, and contrast sensitivity. The system will apply human eye/pupil tracking techniques for objective and unambiguous evaluation of light response and visual perception. This platform will provide a powerful tool for phenotypic studies as well as for discovery of new drugs that can prevent or restore sight caused by blinding conditions such as age-related macular degeneration and glaucoma.                ",A Visual Assessment System for Retinal Function/Drug Discovery,8980787,R41EY025913,"['Address', 'Age related macular degeneration', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Behavior', 'Behavior assessment', 'Behavior monitoring', 'Behavioral', 'Biological Assay', 'Blindness', 'Brain', 'Brain Diseases', 'Calibration', 'Clinic', 'Collaborations', 'Coma', 'Computer Vision Systems', 'Contrast Sensitivity', 'Cues', 'Data Analyses', 'Defect', 'Development', 'Devices', 'Disease', 'Electroretinography', 'Evaluation', 'Eye', 'Eye Movements', 'Eye diseases', 'Funding', 'Future', 'Genetic Screening', 'Glaucoma', 'Head', 'Head Movements', 'Human', 'Image', 'Impairment', 'Laboratory Animals', 'Lead', 'Light', 'Marketing', 'Measurement', 'Measures', 'Medical', 'Methods', 'Mus', 'Nerve Degeneration', 'Neurodegenerative Disorders', 'Patients', 'Pattern', 'Perception', 'Performance', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Photic Stimulation', 'Preclinical Drug Evaluation', 'Procedures', 'Pupil', 'Pupil light reflex', 'Rattus', 'Reflex action', 'Research Institute', 'Retina', 'Retinal', 'Retinal Degeneration', 'Retinal Diseases', 'Rodent', 'Small Business Technology Transfer Research', 'Speed', 'System', 'Techniques', 'Technology', 'Technology Transfer', 'Testing', 'Therapeutic', 'Time', 'Training', 'Transgenic Mice', 'Translating', 'Translations', 'Treatment Efficacy', 'Vision', 'Vision research', 'Visual', 'Visual Acuity', 'Visual Pathways', 'Visual Perception', 'Visual system structure', 'approach behavior', 'base', 'behavior change', 'combat', 'commercialization', 'computer generated', 'data acquisition', 'design', 'drug development', 'drug discovery', 'genetic approach', 'innovation', 'instrument', 'liquid crystal', 'mouse model', 'new technology', 'performance tests', 'photoreceptor degeneration', 'preclinical evaluation', 'prevent', 'prototype', 'public health relevance', 'response', 'scale up', 'success', 'tool', 'touchscreen', 'treatment strategy', 'visual performance', 'visual stimulus']",NEI,"AFASCI, INC.",R41,2015,233235,0.04577948049724762
"Detection of Glaucoma Progression with Macular OCT Imaging DESCRIPTION (provided by applicant): This application is a formal request for a career development award (K23) for an academic glaucoma specialist with a serious interest in the role of imaging in glaucoma using optical coherence tomography (OCT). This will allow the candidate to establish a clinical research program with the main goal of improving detection of glaucoma progression through macular imaging with spectral-domain OCT. By the time the proposed research is accomplished, the candidate will have preliminary data for continuing his research as an independent investigator and will have collected longitudinal structural and functional data in a group of advanced glaucoma patients that will serve as a platform for further improving detection of glaucoma progression with macular OCT imaging. The data will help the candidate provide preliminary results for a subsequent R01 that would potentially allow the PI to continue follow-up of the patients enrolled in the K23 award period.  I have a Master's of Science degree in Clinical Investigation under my belt and intend to deepen my skills in the field of imaging and biostatistics (to be used for enhancing and handling OCT images and for analyzing longitudinal data) by completing the proposed didactic program. By the end of the award period, I expect that I will have gained additional experience, knowledge, and mentorship required to prosper as an independent clinician-scientist in the field of glaucoma. My long-term goal is to carry out longitudinal studies of glaucoma patients where current and upcoming imaging and functional tests can be applied and their utility for detection of glaucoma progression can be investigated. I am confident that the combined skills and experience of my mentors will lead to a successful outcome for the proposed K award. I also envisage myself mentoring candidates like myself in future so that our collective knowledge and wisdom can be passed along to the next generation of aspiring clinician-scientists.  My objectives during the award period are as follows: 1) To develop an individual research program in glaucoma diagnostic imaging; 2) to successfully complete credited coursework in biomathematics, advanced biostatistics, computer vision (image processing), epidemiology, and ethical issues in research.  The main goal of the research component of this proposal is to better delineate the role of macular SD- OCT imaging for detection of glaucoma progression in advanced glaucoma. The specific aims through which this goal will be accomplished are as follows:  (1) To compare the performance of various global and regional macular measures to detect glaucoma.  The potential factors influencing the performance of various macular outcome measures will be explored. Such covariates include age, race, axial length, disc size, central corneal thickness,  OCT signal strength, and outer retinal thickness among others. I hypothesize that the thickness  of the outer retina (outer nuclear layer to retinal pigment epithelium-Bruch's membrane  complex) may be the most important factor explaining the measurement variability of the inner  retinal layer thickness (GCC or ganglion cell/inner plexiform layers).  (2) To determine and compare the utility of the candidate macular measures, detected through the first  aim, for detection of glaucoma progression in moderately advanced to severe glaucoma.  Moderately advanced to severe glaucoma will be defined as eyes with visual field mean  deviation worse than -6 dB or eyes with involvement of the central 10 degrees on the 24-2  visual field. It is widely accepted that measurement of the optic nerve head or RNFL parameters  in advanced glaucoma does not provide clinicians with much useful information. In contrast, the  central macular ganglion cells are the last to die in glaucoma. Macular imaging in advanced  glaucoma is directed towards this area where detection of change may still be possible. I  hypothesize that macular OCT parameters are valid structural outcome measures (biomarkers)  that can be used to follow the course of the disease in advanced glaucoma and that such  measures are significantly correlated with changes in the central visual field. Changes in the  macular measures over time will be first correlated with the corresponding visual field change  (functional progression) over time in eyes with moderately advanced to severe glaucoma. The  utility of the best candidate macular measures for predicting subsequent glaucoma progression  will also be explored and compared. I hypothesize that there may be a lag period between  progressive loss of macular ganglion cells and subsequent visual field progression in advanced  glaucoma, and therefore, detection of worsening in one or more macular outcome measures  can be used as a proxy for subsequent visual field progression.  Collectively, these studies will provide a solid foundation for better understanding and integration of macular OCT imaging in the care of glaucoma patients. Timely detection of glaucoma progression in the later stages can significantly reduce visual disability and blindness through earlier aggressive treatment and will potentially reduce glaucoma's financial burden to society. Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.",Detection of Glaucoma Progression with Macular OCT Imaging,8866409,K23EY022659,"['Age', 'Area', 'Award', 'Biological Markers', 'Biometry', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Caring', 'Clinical Research', 'Complement', 'Complex', 'Computer Vision Systems', 'Cornea', 'Data', 'Detection', 'Diagnosis', 'Diagnostic Imaging', 'Disease', 'Early Diagnosis', 'Enrollment', 'Epidemiology', 'Ethical Issues', 'Evaluation', 'Eye', 'Foundations', 'Functional Imaging', 'Future', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Image Analysis', 'Individual', 'Inner Plexiform Layer', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Legal Blindness', 'Length', 'Longitudinal Studies', 'Master of Science', 'Measurement', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Morbidity - disease rate', 'Nerve Fibers', 'Noise', 'Ophthalmologist', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Process', 'Proxy', 'Quality of life', 'Race', 'Research', 'Research Personnel', 'Retinal', 'Role', 'Scientist', 'Signal Transduction', 'Societies', 'Solid', 'Specialist', 'Staging', 'Structure of retinal pigment epithelium', 'Testing', 'Thick', 'Time', 'Vision', 'Visual', 'Visual Fields', 'advanced disease', 'biomathematics', 'central visual field', 'clinical investigation', 'disability', 'experience', 'follow-up', 'ganglion cell', 'image processing', 'improved', 'interest', 'macula', 'next generation', 'programs', 'retina outer nuclear layer', 'retinal nerve fiber layer', 'skills']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,K23,2015,229139,0.06665213663832276
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8795182,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2015,195445,0.047012076164963945
"Designing Visually Accessible Spaces DESCRIPTION (provided by applicant):  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions.  We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area).  This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement.  Our project addresses one of the National Eye Institute's program objectives:  ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals:  1) Empirical:  determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces.  2) Computational:  develop working models to predict low vision visibility and navigability in real-world spaces.  3) Deployment:  translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility.  The key scientific personnel in our partnership come from three institutions:  University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare.  This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare).  We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.",Designing Visually Accessible Spaces,8815314,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Health', 'Height', 'Human', 'Human Resources', 'Indiana', 'Individual', 'Injury', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Physically Handicapped', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'falls', 'hazard', 'imaging system', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2015,550561,0.05500936630181525
"The role of area V4 in the perception and recognition of visual objects ﻿    DESCRIPTION (provided by applicant): The human visual system parses the information that reaches our eyes into a meaningful arrangement of regions and objects. This process, called image segmentation, is one of the most challenging computations accomplished by the primate brain. To discover its neural basis we will study neuronal processes in two brain areas in the macaque monkey-V4, a fundamental stage of form processing along the occipito-temporal pathway, and the prefrontal cortex (PFC), important for executive control. Dysfunctions of both areas impair shape discrimination behavior in displays that require the identification of segmented objects, strongly suggesting that they are important for image segmentation. Our experimental techniques will include single and multielectrode recordings, behavioral manipulations, perturbation methods and computer models. In Aim 1 we will identify the neural signals that reflect segmentation in visual cortex. Using a variety of parametric stimuli with occlusion, clutter and shadows-stimulus features known to challenge segmentation in natural vision-we will evaluate whether segmentation is achieved by grouping regions with similar surface properties, such as surface color, texture and depth, or by grouping contour segments that are likely to form the boundary of an object or some interplay between these two strategies. We will test the hypothesis that contour grouping mechanisms are most effective under low clutter and close to the fovea. In Aim 2, we will investigate how feedback from PFC modulates shape responses in V4 and facilitates segmentation: we will test the longstanding hypothesis that object recognition in higher cortical stages precedes and facilitates segmentation in the midlevels of visual form processing. We will simultaneously study populations of V4 and PFC neurons while animals engage in shape discrimination behavior. We will use single-trial decoding methods and correlation analyses to relate the content and timing of neuronal responses in the two areas. To causally test the role of feedback from PFC, we will reversibly inactivate PFC by cooling and study V4 neurons. Our results will provide the first detailed, analytical models of V4 neuronal response dynamics in the presence of occlusion and clutter and advance our understanding of how complex visual scenes are processed in area V4. They will also reveal how V4 and PFC together mediate performance on a complex shape discrimination task, how executive function and midlevel vision may be coordinated during behavior and how feedback is used in cortical computation. Object recognition is impaired in visual agnosia, a dysfunction of the occipito-temporal pathway, and in dysfunctions of the PFC (e.g. schizophrenia). Results from these experiments will constitute a major advance in our understanding of the brain computations that underlie segmentation and object recognition and will bring us closer to devising strategies to alleviate and treat brain disorders in which these capacities are impaired.         PUBLIC HEALTH RELEVANCE: A fundamental capacity of the primate visual system is its ability to segment visual scenes into component objects and then recognize those objects regardless of partial occlusions and clutter. Using a combination of primate neurophysiology experiments, computational modeling, animal behavior and reversible inactivation methods, we hope to achieve a new level of understanding about visual processing in the context of object recognition; these findings will ultimately bring us closer to devising strategies to alleviate and treat brain disorders of impaired object recognition resulting from dysfunctions in the occipito-temporal pathway (e.g. agnosia) and the prefrontal cortex (e.g. schizophrenia).                ",The role of area V4 in the perception and recognition of visual objects,8893671,R01EY018839,"['Agnosia', 'Animal Behavior', 'Animals', 'Area', 'Back', 'Behavior', 'Behavior Control', 'Behavioral', 'Brain', 'Brain Diseases', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Custom', 'Data Analyses', 'Discrimination', 'Eye', 'Feedback', 'Functional disorder', 'Goals', 'Grouping', 'Human', 'Image', 'Lead', 'Lesion', 'Macaca', 'Mediating', 'Methods', 'Modeling', 'Monkeys', 'Neurons', 'Pathway interactions', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Play', 'Population', 'Population Study', 'Prefrontal Cortex', 'Primates', 'Process', 'Property', 'Psychology', 'Psychophysics', 'Relative (related person)', 'Role', 'Schizophrenia', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Staging', 'Stimulus', 'Stream', 'Surface', 'Surface Properties', 'Techniques', 'Testing', 'Texture', 'Time', 'V4 neuron', 'Vision', 'Visual', 'Visual Agnosias', 'Visual Cortex', 'Visual system structure', 'area V4', 'awake', 'base', 'design', 'executive function', 'feeding', 'fovea centralis', 'imaging Segmentation', 'impaired capacity', 'neurophysiology', 'neurotransmission', 'object recognition', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'visual process', 'visual processing']",NEI,UNIVERSITY OF WASHINGTON,R01,2015,479568,-0.026357774787321965
"NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair     DESCRIPTION (provided by applicant):         The aim of this proposal is to conduct research on the foundational models and algorithms in computer vision and machine learning for an egocentric vision based active learning co-robot wheelchair system to improve the quality of life of elders and disabled who have limited hand functionality or no hand functionality at all, and rely on wheelchairs for mobility. In this co-robt system, the wheelchair users wear a pair of egocentric camera glasses, i.e., the camera is capturing the users' field-of-the-views. This project help reduce the patients' reliance on care-givers. It fits NINR's mission in addressing key issues raised by the Nation's aging population and shortages of healthcare workforces, and in supporting patient-focused research that encourage and enable individuals to become guardians of their own well-beings.  The egocentric camera serves two purposes. On one hand, from vision based motion sensing, the system can capture unique head motion patterns of the users to control the robot wheelchair in a noninvasive way. Secondly, it serves as a unique environment aware vision sensor for the co-robot system as the user will naturally respond to the surroundings by turning their focus of attention, either consciously or subconsciously. Based on the inputs from the egocentric vision sensor and other on-board robotic sensors, an online learning reservoir computing network is exploited, which not only enables the robotic wheelchair system to actively solicit controls from the users when uncertainty is too high for autonomous operation, but also facilitates the robotic wheelchair system to learn from the solicited user controls. This way, the closed- loop co-robot wheelchair system will evolve and be more capable of handling more complicated environment overtime.  The aims ofthe project include: 1) develop an method to harness egocentric computer vision-based sensing of head movements as an alternative method for wheelchair control; 2) develop a method leveraging visual motion from the egocentric camera for category independent moving obstacle detection; and 3) close the loop of the active learning co-robot wheelchair system through uncertainty based active online learning.          PUBLIC HEALTH RELEVANCE:          The project will improve the quality of life of those elders and disabled who rely on wheelchairs for mobility, and reduce their reliance on care-givers. It builds an intelligent wheelchair robot system that can adapt itself to the personalized behavior of the user. The project addresses the need of approximately 1 % of our world's population, including millions of Americans, who rely on wheelchair for mobility.             ",NRI: An Egocentric Computer Vision based Active Learning Co-Robot Wheelchair,8838311,R01NR015371,"['Active Learning', 'Address', 'Algorithms', 'American', 'Attention', 'Behavior', 'Build-it', 'Caregivers', 'Categories', 'Computer Vision Systems', 'Detection', 'Disabled Persons', 'E-learning', 'Elderly', 'Environment', 'Glass', 'Hand', 'Head', 'Head Movements', 'Healthcare', 'Individual', 'Learning', 'Machine Learning', 'Methods', 'Mission', 'Modeling', 'Motion', 'Motivation', 'Patients', 'Pattern', 'Population', 'Principal Investigator', 'Quality of life', 'Reliance', 'Research', 'Robot', 'Robotics', 'System', 'Uncertainty', 'Vision', 'Visual Motion', 'Wheelchairs', 'aging population', 'base', 'improved', 'operation', 'programs', 'public health relevance', 'sensor']",NINR,STEVENS INSTITUTE OF TECHNOLOGY,R01,2014,155663,0.00844032981191666
"CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes DESCRIPTION (provided by applicant): The complexity of natural images is potentially enormous: the number of possible images that can be described by a smallish (100 by 100 pixels) picture is practically infinite (10000256), more than all the images the human race has ever witnessed during its entire existence. How can any system process input data of this magnitude of dimensions and interpret/understand it in terms of the estimated 200,000 objects in the world, their spatial layouts, and scene structures? Yet, this is a task that human visual systems routinely perform in a fraction of a second. The secret must lie in the fact that natural images are highly redundant, living in a restricted space inside this universe of almost infinite possibilities, and that mammalian visual systems have discovered and exploited this fact. In particular, we conjecture that neurons and populations are tuned to the statistical structure of natural images, building on previous work showing, for example, that sparse coding ideas can help predict receptive field properties of 'simple cells' in the visual cortex. This proposal has three stages. Firstly, we will perform a statistical analysis of natural images to classify and model the types of visual patches that appear. This will result in a stimulus dictionary, which will be used as stimuli to investigate the tuning properties of neurons and neuronal populations, and a visual concept dictionary which will be used to make predictions for the tuning properties. Secondly, we will perform multielectrode neurophysiological investigation of the tuning properties of neurons, and neuron populations, at different levels of the visual cortex in response to the stimulus dictionary. Thirdly, we will perform data analysis to model the tuning properties of neurons and populations using a combination of model-driven, which assumes that neurons are tuned to statistical properties of images, and data-driven approaches which can be thought of as learning 'neural visual concepts' directly from the neuron's response to the stimuli. Our theoretical approach - for learning the stimuli dictionary, the visual concepts, and performing data analysis - is based on statistical and machine learning techniques. These assume a hierarchical compositional structure for the data which offers the possibility of taming the complexity of natural images and is also consistent with the known hierarchical structure of the visual cortex. Intellectual merit: This research will help understand the structure of natural images, determine models for the tuning properties of neurons in the visual cortex, and develop novel data analysis techniques. It has the potential to significantly advance our understanding of the statistical structures of natural images and the neural encoding of these structures, including the population level. This will lead to greater understanding of the visual cortex and also help the development of computer vision systems. Broader impacts: This project is interdisciplinary in nature and should have broad impact in multiple disciplines: neuroscience and biological vision, statistical neural data analysis, computer vision, and machine learning. Understanding neuronal properties in the visual cortex is a pre-requisite to the clinical enterprise of developing therapeutic methods and prosthetic devices for the visually impaired. The proposed research program will help facilitate a new graduate program in Computational and Cognitive Neuroscience at UCLA, an inter-college undergraduate minor in Neural Computation at CMU that the investigators are developing at their respective universities. The investigators also plan to organize workshops in NIPS, COSYNE, as well as to integrate their research into both undergraduate and graduate curriculum in their respective universities. This work will also affect undergraduate students at other colleges, by a summer undergraduate training program in Pittsburgh, another at CMU's Qatar campus. In addition, we will propose a workshop and summer school at IPAM (UCLA). We anticipate that this research will lead to invited lectures, peer reviewed publications and, if successful, will have national and international impact. The PIs have good track record in involving undergraduates, including women and minorities, in their NSF-sponsored research, and will continue to endeavor in the training of the next generation of computational neuroscientists. This project will lead to greater understanding of neural mechanisms and coding strategies in the primate  visual cortex. Such knowledge is fundamental to understanding human visual functions and is critical to the  clinical enterprise of developing better diagnostic tools, therapeutic methods, and prosthetic devices for the  visually impaired.",CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes,8731899,R01EY022247,"['Affect', 'Appearance', 'Biological', 'Cells', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Dictionary', 'Dimensions', 'Discipline', 'Educational Curriculum', 'Educational workshop', 'Electrodes', 'Entropy', 'Graph', 'Human', 'Image', 'International', 'Intuition', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Letters', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Minor', 'Minority', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Noise', 'Pattern', 'Peer Review', 'Physiological', 'Population', 'Primates', 'Probability', 'Process', 'Property', 'Prosthesis', 'Publications', 'Qatar', 'Race', 'Research', 'Research Personnel', 'Sampling', 'Schools', 'Staging', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'Universities', 'Vision', 'Visual', 'Visual Cortex', 'Visual system structure', 'Vocabulary', 'Woman', 'Work', 'base', 'cognitive neuroscience', 'college', 'computational neuroscience', 'design', 'devices for the visually impaired', 'isophosphamide mustard', 'lectures', 'neuromechanism', 'neurophysiology', 'next generation', 'novel', 'programs', 'receptive field', 'relating to nervous system', 'research study', 'response', 'statistics', 'tool', 'undergraduate student', 'visual stimulus']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2014,354773,-0.018646730114092697
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8704450,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'High School Student', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2014,51689,0.04071900617754007
"Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices     DESCRIPTION (provided by applicant): The inability to access information on printed signs directly impacts the mobility independence of the over 1.2 million blind persons in the U.S. Many previously proposed technological solutions to this problem either required physical modifications to the environment (talking signs or the placement of coded markers) or required the user to carry around specialized computational equipment, which can be stigmatizing. A recently pursued strategy is to utilize the computational capabilities of smart phones and techniques from computer vision to allow blind persons to read signs at a distance using commercially available, non-stigmatizing, smart- phones. However, despite the fact that sophisticated algorithms exist to recognize and extract sign text from cluttered video input (as evidenced, for example, by mapping services such as Google Maps automatically locating and blurring out only license plate text in street-view maps) current mobile solutions for reading sign text at a distance perform relatively poorly. This poor performance is largely because until recently, smart-phone processors have simply not been able to execute state-of-the-art computer vision text extraction and recognition algorithms at real-time rates, which forced previous mobile sign readers to utilize older, simplistic, less effective algorithms. Next-generation smart-phones run on fundamentally different, hybrid processor architectures (such as the Tegra 4, Snapdragon 800, both released in 2013) with dedicated embedded graphical processing units (GPUs) and multi-core CPUs, which make them ideal for high-performance, vision-heavy computation. In this study, we propose to develop a smart-phone-based system for finding and reading signs at a distance which significantly outperforms previous such readers by implementing state-of-the-art text extraction algorithms on modern smart-phone hybrid GPU/CPU processor architectures. In Phase I, the proposed system will be developed and tested with blind users. In Phase II, feedback from user testing will be integrated into system design and the performance will be improved to permit operation in extremely challenging (such as low light) environments.         PUBLIC HEALTH RELEVANCE: Over 1.2 million people in the US are blind, and lack of safe and independent mobility substantially impacts the quality of life of this population. Printed textual signs, which are ubiquitously used in sighted navigation, are inaccessible to visually impaired persons, and this lack of access to environmental information contributes significantly to the mobility problem. This research would help develop a system whereby blind persons could use commercially available smart-phones to locate and read sign text at a distance.            ",Sign Finding and Reading SFAR on GPU Accelerated Mobile Devices,8779810,R43EY024800,"['Acceleration', 'Access to Information', 'Algorithms', 'Antirrhinum', 'Architecture', 'Back', 'Code', 'Computer Vision Systems', 'Distant', 'Environment', 'Equipment', 'Eye', 'Feedback', 'Hybrids', 'Licensing', 'Light', 'Literature', 'Maps', 'Modification', 'Performance', 'Phase', 'Population', 'Printing', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Research Institute', 'Risk', 'Running', 'SKI gene', 'Self-Help Devices', 'Services', 'Solutions', 'System', 'Techniques', 'Telephone', 'Test Result', 'Testing', 'Text', 'Time', 'Vision', 'Visually Impaired Persons', 'assistive device/technology', 'authority', 'base', 'blind', 'design', 'experience', 'handheld mobile device', 'improved', 'next generation', 'operation', 'phase 1 study', 'public health relevance', 'volunteer']",NEI,"LYNNTECH, INC.",R43,2014,229742,0.02812063532154651
"HHSN261201400054C; TOPIC 308 AUTOMATED COLLECTION, STORAGE, ANALYSIS, AND REPORTING SYSTEMS FOR DIETARY IMAGES 'TITLE: THE MOBILE FOOD INTAKE PHOTO STORAGE & ANALYSIS SYSTEM. PERFORMANCE PERIOD 09/16/ This proposal describes enhancements to Viocare’s Mobile Food Intake Visual and Voice Recognizer (FIVR)  System, a novel combination of innovative technologies including computer vision and speech recognition to  measure dietary intake using a mobile phone. FIVR uses a mobile phone’s camera to capture a short video  of foods to be consumed, which is then verbally-annotated on the mobile phone by the user. These video  and audio files are processed through a real-time backend server speech and image recognition engine for  food recognition and portion size measurement. This project will extend FIVR’s capabilities to analyze more  foods, enhance the analysis and reporting tools, expand system support tools, and develop interfaces to a  diverse set of clinical and research systems. A final evaluation of the FIVR system will be conducted at The  Ohio State University to assess the usability and accuracy of food intake tracking with a group of 100 freeliving  subjects, comparing 4 days of FIVR food intake data to 4 days of 24 hour recalls collected using  ASA24 data. The resulting FIVR product will be a unique food intake tracker that combines selfadministration,  automation (vision), and backend coding to collect food intake records to generate a detailed  nutritional analysis. n/a","HHSN261201400054C; TOPIC 308 AUTOMATED COLLECTION, STORAGE, ANALYSIS, AND REPORTING SYSTEMS FOR DIETARY IMAGES 'TITLE: THE MOBILE FOOD INTAKE PHOTO STORAGE & ANALYSIS SYSTEM. PERFORMANCE PERIOD 09/16/",8947304,61201400054C,"['Architecture', 'Automation', 'Car Phone', 'Clinical Research', 'Code', 'Collection', 'Computer Vision Systems', 'Computerized Medical Record', 'Data', 'Databases', 'Diet', 'Dietary intake', 'Eating', 'Evaluation', 'Food', 'Health', 'Hour', 'Image', 'Individual', 'Location', 'Measurement', 'Measures', 'Methods', 'Nutritional', 'Ohio', 'Output', 'Patients', 'Performance', 'Procedures', 'Process', 'Records', 'Reporting', 'Research Personnel', 'Speech', 'Support System', 'System', 'Systems Analysis', 'Time', 'Universities', 'Vision', 'Visual', 'Voice', 'innovative technologies', 'mobile application', 'novel', 'speech recognition', 'tool', 'usability']",NCI,"VIOCARE, INC.",N44,2014,1000000,0.0030061894621212794
"Providing Access to Appliance Displays for Visually Impaired Users     DESCRIPTION (provided by applicant):  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays.  This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment.  No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents.  Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image.  For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast.  These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view.  Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users.  Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures.  The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software.         PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.",Providing Access to Appliance Displays for Visually Impaired Users,8712492,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2014,368560,0.06680354215959919
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.        The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8601076,R01EY022039,"['Address', 'Algorithm Design', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2014,379750,0.04167781291567067
"CRCNS: Model-driven single-neuron studies of cortical remapping     DESCRIPTION (provided by applicant): During natural vision humans and non-human primates make several saccadic eye movements each second that result in large changes in the retinal input. Despite these often dramatic changes, our visual percept remains remarkably stable and we can readily attend to and direct motor actions towards objects in our visual environment. This project will use a model-driven approach to investigate the neural circuits linking vision, attention and oculomotor planning that stabilize perceptual and attentional representations during natural vision. Experimental data will be collected and used to design a detailed computational model of the visual and oculomotor areas involved in saccade compensation. The proposed collaboration between a computational (DE) and experimental neurophysiological (US) laboratories leverages the power of both disciplines. Biologically accurate models of visually guided behavior and trans-saccadic integration developed in the Hamker lab will guide the design of and interpretation of data obtained from neurophysiological experiments in awake, behaving primates performed in the Mazer lab in an iterative fashion, with experimental results informing model revisions and new model predictions altering experimental designs. The proposed studies will characterize both dorsal and ventral stream visual area contributions to stabilizing visual and attentional representations in the primate brai. Data obtained from these experiments will identify the neural circuits responsible for integrating oculomotor commands, bottom-up visual inputs and top-down attention signals. This approach will yield novel insights into interactions between the dorsal and ventral streams during natural vision and facilitate our understanding of goal-directed, active visual perception, a defining feature of human and non-human primate natural vision. A critical component of this project is the highly collaborative nature of the planned research. We expect great benefits from this interdisciplinary approach, which depends critically on computational models that strictly adhere to the known physiological and anatomical constraints to guide our neurophysiological experiments.     1. Training. The proposal includes a detailed training plan intended to facilitate international training of future modelers and neurophysiologists. Specifically, we will train students and post-doctoral researchers to be experts in both experimental and theoretical approaches in order to advance the field using the hybrid approach outlined in the proposal.    2. Education and Outreach. We plan to organize two in-depth workshops on attention and eye movements. These events (one in Germany and one in the US) will bring together investigators from other institutions and related scientific disciplines to advance the field. In addition investigators will organize and chair 1-2 workshops/symposia at annual meetings (e.g., SFN and COSYNE) during the funding period. Finally, we will participate in science education for underrepresented groups through Yale?s STARS program by providing training, research and mentoring opportunities in the Mazer lab.    3. Data Sharing. The software tools generated and behavioral and neurophysiological data collected during this project will be distributed to the neuroscience community to facilitate data mining and secondary analyses of experimental data.    4. Impact in other scientific fields. Efficient allocation of limited sensor resources is also a important problem faced by computer vision and robotics researchers. Understanding how the primate brain efficiently allocates visual resources using an active-sensing approach will guide development of biologically inspired computer vision algorithms and humanoid cognitive robots.    5. Translational Implications. Although the proposed research is not translational, there is a growing body of evidence suggesting that several clinically important conditions, including Autism Spectrum Disorder, Attention Deficit Hyperactivity Disorder and Schizophrenia, are associated with impaired behavioral links between saccade planning and visual attention. The proposed basic science studies could have significant implications for future translational research potentially leading to improved understanding disease etiology, development of early diagnostic tools and possible interventional strategies.              n/a",CRCNS: Model-driven single-neuron studies of cortical remapping,8837252,R01EY025103,"['Accounting', 'Address', 'Algorithms', 'Animals', 'Area', 'Attention', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Cognitive', 'Collaborations', 'Communities', 'Computer Simulation', 'Computer Vision Systems', 'Data', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Dorsal', 'Education and Outreach', 'Educational workshop', 'Environment', 'Etiology', 'Event', 'Experimental Designs', 'Eye Movements', 'Financial compensation', 'Funding', 'Future', 'Germany', 'Goals', 'Human', 'Hybrids', 'Institution', 'International', 'Intervention', 'Laboratories', 'Link', 'Mentors', 'Modeling', 'Monkeys', 'Motor', 'Nature', 'Neurons', 'Neurosciences', 'Performance', 'Physiological', 'Postdoctoral Fellow', 'Primates', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Retinal', 'Robot', 'Robotics', 'Saccades', 'Schizophrenia', 'Signal Transduction', 'Simulate', 'Software Tools', 'Stream', 'Students', 'Testing', 'Training', 'Translational Research', 'Underrepresented Minority', 'Update', 'Vision', 'Visual', 'Visual Perception', 'Visual attention', 'Work', 'area V4', 'autism spectrum disorder', 'awake', 'base', 'cell type', 'computer framework', 'data mining', 'data sharing', 'design', 'extrastriate visual cortex', 'improved', 'insight', 'interdisciplinary approach', 'meetings', 'neural circuit', 'neurophysiology', 'nonhuman primate', 'novel', 'oculomotor', 'programs', 'relating to nervous system', 'research study', 'science education', 'sensor', 'simulation', 'spatiotemporal', 'symposium', 'tool', 'vector', 'visual motor']",NEI,YALE UNIVERSITY,R01,2014,249750,-0.0008873916669713254
"Neural and Behavioral Interactions Between Attention, Perception, and Learning    DESCRIPTION (provided by applicant): The overarching goal of this research is to characterize how perception and memory interact, in terms of both the learning mechanisms that help transform visual experience into memory, and the intentional mechanisms that regulate this transformation. The specific goal of this proposal is to test the hypothesis that incidental learning about statistical regularities in vision (visual statistical learning) is limited to selectively attend visual information, and that this behavioral interaction arises because of how selective attention modulates neural interactions between human visual and memory systems. We propose a two-stage framework in which selective attention to a high-level visual feature/category increases neural interactions between regions of occipital cortex that represent low-level features and the region of inferior temporal cortex (IT) that represents the attended feature/category, and in turn between this IT region and medial temporal lobe (MTL) sub regions involved in visual learning and memory. In addition to assessing how feature-based selective attention influences learning at a behavioral level, we will use functional magnetic resonance imaging to assess how attention influences evoked neural responses in task-relevant brain regions, as well as neural interactions between these regions in the background of ongoing tasks. We will develop an innovative approach for studying neural interactions in which evoked responses and global noise sources are scrubbed from the data and regional correlations are assessed in the residuals. This background connectivity approach provides a new way to study how intentional goals affect perception and learning. Aim 1 examines the first stage of our framework, testing: how selective attention modulates background connectivity between IT and occipital cortex, where in retinotopic visual cortex this modulation occurs, and how these changes are controlled by frontal and parietal cortex. Aim 2 examines the second stage of our framework, first establishing the role of the MTL in visual statistical learning, and then testing: how selective attention modulates interactions between IT and the MTL, where in cortical and hippocampal sub regions of the MTL this modulation occurs, and how these changes facilitate incidental learning about statistical regularities and later retrieval of this knowledge. In sum, we relate behavioral interactions between selective attention and learning to neural interactions between the mechanisms that represent visual features and those that learn about their relations. This proposal addresses several key issues in the field, including: how attention modulates the MTL, how feature-based attention is controlled, whether different neural mechanisms support rapid versus long-term visual learning, how tasks and goals are represented, and how attention and memory retrieval are related. This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery and rehabilitation of visual function following eye disease, injury, or brain damage.        This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.         ","Neural and Behavioral Interactions Between Attention, Perception, and Learning",8708870,R01EY021755,"['Address', 'Affect', 'Architecture', 'Area', 'Attention', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Cognitive Science', 'Data', 'Development', 'Diagnosis', 'Disease', 'Event', 'Exhibits', 'Eye diseases', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Inferior', 'Injury', 'Knowledge', 'Lead', 'Learning', 'Light', 'Link', 'Machine Learning', 'Maps', 'Medial', 'Memory', 'Methods', 'Mind', 'Modeling', 'Nature', 'Neurosciences', 'Noise', 'Occipital lobe', 'Parietal', 'Parietal Lobe', 'Perception', 'Physiological', 'Positioning Attribute', 'Process', 'Property', 'Recovery', 'Rehabilitation therapy', 'Research', 'Residual state', 'Resolution', 'Rest', 'Retrieval', 'Role', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Staging', 'Stimulus', 'Sum', 'Synapses', 'System', 'Temporal Lobe', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'cognitive neuroscience', 'experience', 'extrastriate visual cortex', 'frontal lobe', 'hippocampal subregions', 'improved', 'information processing', 'innovation', 'memory retrieval', 'neuroimaging', 'neuromechanism', 'novel strategies', 'relating to nervous system', 'response', 'retinotopic', 'selective attention', 'transmission process', 'visual information', 'visual learning', 'visual memory', 'visual process', 'visual processing', 'visual stimulus']",NEI,PRINCETON UNIVERSITY,R01,2014,348504,-0.00478888459822869
"Visual Summary-Statistic Processing in Infancy     DESCRIPTION (provided by applicant): Part of the NICHD's mission is to support basic research in human development. The Developmental Cognitive Psychology, Behavioral Neuroscience, and Psychobiology program supports research to identify links between the developing brain and the environment. The proposed research has been designed to help us understand how the visual environment shapes infants' use of summary statistics to describe the things they see. A growing body of research suggests that adults summarize a great deal of visual information by describing the world with texture-like measurements - instead of measuring exactly what we saw and where we saw it, we often pool information together over large areas by measuring how simple features co-occur and forgetting about exactly where they came from. For some tasks, these kinds of descriptions are useful. In other cases, like recognizing a single object in clutter, they don't work well at all. Understanding the limits of these summary statistic descriptions of our visual world has given us insights into why adults sometimes find visual search difficult (Rosenholtz et al., 2012), why it's hard to recognize single objects in clutter (Balas et al., 2009), and may help us understand conditions like amblyopia or macular degeneration, where we think these kinds of summaries may be nearly all the visual system has to work with. In these studies, we will work with infants and adults to understand how summary-statistic descriptions are shaped by the visual environment. We will use computer graphics techniques to create artificial textures that are matched to natural textures using a model of the early visual system. Our goal is to determine how different artificial textures need to be from natural textures for infants to tell them apart. By using a computer graphics model, we can carefully control what information is available to tell the images apart, which will help us understand what measurements contribute to infants' summary-statistic descriptions as they get older. Further, we plan to measure how well infants can tell real textures apart from artificial ones by measuring how their brain responds to those images. We will use EEG to measure how the brain responds to different kinds of natural and artificial textures. Different neural response reflect different kinds of processing in the brain, and we plan to use the location and the timing of the differences we see in infants' brains to help us understand how summary statistics are applied. These studies support the NICHD's mission and will help us understand the perceptual difficulties that some children and adults have due to visual impairments that force summary statistics to be used all the time.         PUBLIC HEALTH RELEVANCE: The human visual system summarizes a great deal of what we see by measuring what we saw, but forgetting exactly where we saw it. This strategy is good for recognizing textures like wood or stone, but makes it difficult to recognize individual objects that have lots of clutter around them. We plan to study how infants recognize textures so we can understand how their visual system learns to summarize information this way, which will help us, understand some visual impairment that result from the visual system being forced to use summaries like this all the time.            ",Visual Summary-Statistic Processing in Infancy,8687212,R15EY024375,"['Address', 'Adherence', 'Adult', 'Affect', 'Age', 'Algorithms', 'Amblyopia', 'Appearance', 'Area', 'Basic Science', 'Behavior', 'Behavioral', 'Behavioral Paradigm', 'Biological Models', 'Brain', 'Calculi', 'Characteristics', 'Child', 'Childhood', 'Cognitive Science', 'Complement', 'Computer Graphics', 'Computer Simulation', 'Crowding', 'Data', 'Development', 'Discrimination', 'Elderly', 'Electroencephalography', 'Environment', 'Event-Related Potentials', 'Exhibits', 'Exposure to', 'Face', 'Goals', 'Human', 'Human Development', 'Image', 'Individual', 'Infant', 'Learning', 'Life', 'Link', 'Location', 'Machine Learning', 'Macular degeneration', 'Measurement', 'Measures', 'Methods', 'Mission', 'Modeling', 'Nature', 'Neurophysiology - biologic function', 'Neurosciences', 'Paired Comparison', 'Parents', 'Participant', 'Pattern', 'Pattern Recognition', 'Performance', 'Play', 'Process', 'Race', 'Relative (related person)', 'Research', 'Research Support', 'Role', 'Shapes', 'Signal Transduction', 'Stimulus', 'Techniques', 'Testing', 'Texture', 'Time', 'To specify', 'Variant', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Vocabulary', 'Wood material', 'Work', 'base', 'design', 'deviant', 'experience', 'forgetting', 'infancy', 'insight', 'preference', 'programs', 'psychobiology', 'public health relevance', 'relating to nervous system', 'response', 'shape analysis', 'spatial integration', 'statistics', 'vision development', 'visual information', 'visual process', 'visual processing', 'visual search']",NEI,NORTH DAKOTA STATE UNIVERSITY,R15,2014,408066,-0.020798481834530837
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans. PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8786916,R00EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'mathematical sciences', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,UNIVERSITY OF MEMPHIS,R00,2014,248999,0.030606867963203277
"Detection of Glaucoma Progression with Macular OCT Imaging     DESCRIPTION (provided by applicant): This application is a formal request for a career development award (K23) for an academic glaucoma specialist with a serious interest in the role of imaging in glaucoma using optical coherence tomography (OCT). This will allow the candidate to establish a clinical research program with the main goal of improving detection of glaucoma progression through macular imaging with spectral-domain OCT. By the time the proposed research is accomplished, the candidate will have preliminary data for continuing his research as an independent investigator and will have collected longitudinal structural and functional data in a group of advanced glaucoma patients that will serve as a platform for further improving detection of glaucoma progression with macular OCT imaging. The data will help the candidate provide preliminary results for a subsequent R01 that would potentially allow the PI to continue follow-up of the patients enrolled in the K23 award period.  I have a Master's of Science degree in Clinical Investigation under my belt and intend to deepen my skills in the field of imaging and biostatistics (to be used for enhancing and handling OCT images and for analyzing longitudinal data) by completing the proposed didactic program. By the end of the award period, I expect that I will have gained additional experience, knowledge, and mentorship required to prosper as an independent clinician-scientist in the field of glaucoma. My long-term goal is to carry out longitudinal studies of glaucoma patients where current and upcoming imaging and functional tests can be applied and their utility for detection of glaucoma progression can be investigated. I am confident that the combined skills and experience of my mentors will lead to a successful outcome for the proposed K award. I also envisage myself mentoring candidates like myself in future so that our collective knowledge and wisdom can be passed along to the next generation of aspiring clinician-scientists.  My objectives during the award period are as follows: 1) To develop an individual research program in glaucoma diagnostic imaging; 2) to successfully complete credited coursework in biomathematics, advanced biostatistics, computer vision (image processing), epidemiology, and ethical issues in research.  The main goal of the research component of this proposal is to better delineate the role of macular SD- OCT imaging for detection of glaucoma progression in advanced glaucoma. The specific aims through which this goal will be accomplished are as follows:  (1) To compare the performance of various global and regional macular measures to detect glaucoma.  The potential factors influencing the performance of various macular outcome measures will be explored. Such covariates include age, race, axial length, disc size, central corneal thickness,  OCT signal strength, and outer retinal thickness among others. I hypothesize that the thickness  of the outer retina (outer nuclear layer to retinal pigment epithelium-Bruch's membrane  complex) may be the most important factor explaining the measurement variability of the inner  retinal layer thickness (GCC or ganglion cell/inner plexiform layers).  (2) To determine and compare the utility of the candidate macular measures, detected through the first  aim, for detection of glaucoma progression in moderately advanced to severe glaucoma.  Moderately advanced to severe glaucoma will be defined as eyes with visual field mean  deviation worse than -6 dB or eyes with involvement of the central 10 degrees on the 24-2  visual field. It is widely accepted that measurement of the optic nerve head or RNFL parameters  in advanced glaucoma does not provide clinicians with much useful information. In contrast, the  central macular ganglion cells are the last to die in glaucoma. Macular imaging in advanced  glaucoma is directed towards this area where detection of change may still be possible. I  hypothesize that macular OCT parameters are valid structural outcome measures (biomarkers)  that can be used to follow the course of the disease in advanced glaucoma and that such  measures are significantly correlated with changes in the central visual field. Changes in the  macular measures over time will be first correlated with the corresponding visual field change  (functional progression) over time in eyes with moderately advanced to severe glaucoma. The  utility of the best candidate macular measures for predicting subsequent glaucoma progression  will also be explored and compared. I hypothesize that there may be a lag period between  progressive loss of macular ganglion cells and subsequent visual field progression in advanced  glaucoma, and therefore, detection of worsening in one or more macular outcome measures  can be used as a proxy for subsequent visual field progression.  Collectively, these studies will provide a solid foundation for better understanding and integration of macular OCT imaging in the care of glaucoma patients. Timely detection of glaucoma progression in the later stages can significantly reduce visual disability and blindness through earlier aggressive treatment and will potentially reduce glaucoma's financial burden to society.          Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.            ",Detection of Glaucoma Progression with Macular OCT Imaging,8675256,K23EY022659,"['Age', 'Area', 'Award', 'Biological Markers', 'Biometry', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Caring', 'Clinical Research', 'Clinical Trials', 'Complement', 'Complex', 'Computer Vision Systems', 'Cornea', 'Data', 'Detection', 'Diagnosis', 'Diagnostic Imaging', 'Disease', 'Early Diagnosis', 'Enrollment', 'Epidemiology', 'Ethical Issues', 'Evaluation', 'Eye', 'Foundations', 'Functional Imaging', 'Future', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Image Analysis', 'Individual', 'Inner Plexiform Layer', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Legal Blindness', 'Length', 'Longitudinal Studies', 'Master of Science', 'Measurement', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Morbidity - disease rate', 'Nerve Fibers', 'Noise', 'Ophthalmologist', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Process', 'Proxy', 'Quality of life', 'Race', 'Research', 'Research Personnel', 'Retinal', 'Role', 'Scientist', 'Signal Transduction', 'Societies', 'Solid', 'Specialist', 'Staging', 'Structure of retinal pigment epithelium', 'Testing', 'Thick', 'Time', 'Vision', 'Visual', 'Visual Fields', 'advanced disease', 'biomathematics', 'central visual field', 'disability', 'experience', 'follow-up', 'ganglion cell', 'image processing', 'improved', 'interest', 'macula', 'next generation', 'programs', 'retina outer nuclear layer', 'retinal nerve fiber layer', 'skills']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,K23,2014,229139,0.06665213663832276
"Context Understanding Technology to improve internet accessibility for users with DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a  web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization. PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.",Context Understanding Technology to improve internet accessibility for users with,8609036,R44EY020082,"['Advertisements', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Grouping', 'Health', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2014,357073,0.047012076164963945
"Designing Visually Accessible Spaces  Title: Designing Visually Accessible Spaces NIH Program Announcement: 10-234 Bioengineering Research Partnerships (BRP)[R01] Abstract Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals, including those with physical disabilities, and to enhance safety for older people and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool enabling architectural design professionals to assess the visual accessibility of a wide range of environments (such as a hotel lobby, subway station, or eye-clinic reception area). This tool will simulate such environments with sufficient accuracy to predict the visibility of key landmarks or hazards, such as steps or benches, for different levels and types of low vision, and for spaces varying in lighting, surface properties and geometric arrangement. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments."" Our research plan has three specific goals: 1) Empirical: determine factors that influence low-vision accessibility related to hazard detection and navigation in real-world spaces. 2) Computational: develop working models to predict low vision visibility and navigability in real-world spaces. 3) Deployment: translate findings from basic vision science and clinical low vision into much needed industrial usage by producing a set of open source software modules to enhance architectural and lighting design for visual accessibility. The key scientific personnel in our partnership come from three institutions: University of Minnesota -- Gordon Legge and Daniel Kersten; University of Utah -- William Thompson and Sarah Creem-Regehr; and Indiana University -- Robert Shakespeare. This interdisciplinary team has expertise in the necessary areas required for programmatic research on visual accessibility -- empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), computer graphics and photometrically accurate rendering (Thompson & Shakespeare) and architectural lighting design (Shakespeare). We have collaborative arrangements with additional architectural design professionals who will participate in the translation of our research and development into practice. PUBLIC HEALTH RELEVANCE:  Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision.  We define visual accessibility as the use of vision to travel efficiently and safely through an environment, o perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment.  Our BRP partnership, with interdisciplinary expertise in vision science, computer science and lighting design, plans to develop computer-based tools enabling architecture professionals to assess the visual accessibility of their designs.                ",Designing Visually Accessible Spaces,8630772,R01EY017835,"['Accounting', 'Address', 'Age', 'American', 'Architecture', 'Area', 'Biological Models', 'Blindness', 'Central Scotomas', 'Characteristics', 'Clinic', 'Clinical', 'Complement', 'Complex', 'Computer Analysis', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Contrast Sensitivity', 'Cues', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Disorientation', 'Distance Perception', 'Effectiveness', 'Environment', 'Evaluation', 'Eye', 'Geometry', 'Glare', 'Goals', 'Grant', 'Guidelines', 'Height', 'Human', 'Human Resources', 'Image', 'Indiana', 'Individual', 'Injury', 'Institution', 'Investigation', 'Judgment', 'Learning', 'Light', 'Lighting', 'Lobbying', 'Location', 'Minnesota', 'Modeling', 'Modification', 'National Eye Institute', 'Nature', 'Optics', 'Perception', 'Peripheral', 'Phase', 'Photometry', 'Physical environment', 'Positioning Attribute', 'Process', 'Production', 'Property', 'Research', 'Risk', 'Safety', 'Shapes', 'Simulate', 'Source', 'Specific qualifier value', 'Stimulus', 'Structure', 'Subway', 'Surface', 'Surface Properties', 'System', 'Test Result', 'Testing', 'Translating', 'Translations', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual Acuity', 'Visual Fields', 'Visual impairment', 'Work', 'base', 'computer science', 'design', 'disability', 'falls', 'hazard', 'improved', 'knowledge base', 'luminance', 'novel strategies', 'open source', 'programs', 'public health relevance', 'research and development', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2014,595880,0.053538421918144216
"Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation     DESCRIPTION (provided by applicant): Over 3 million emergency intubations are performed in the US every year and failure rates can be as high as 50% (3-5). Success is highly dependent on how frequently the responder performs this life-saving procedure on humans (6). Brio Device, LLC, an airway management medical device company, is addressing the need to decouple the success of the procedure from the experience of the user with their ""smart"" intubation device which integrates anatomic structure recognition algorithms and visual guidance feedback with an articulating stylet. Brio's intubation device is specifically designed fo the needs of emergency responders, such as paramedics, emergency department personnel, code teams in hospitals and military medics, who often arrive at the patient first. The smart intubation device will reduce failure rates by providing the user with visual instruction of the correct path to the trachea as he places the endotracheal tube. The guidance software uses machine learning and computer vision algorithms to recognize the anatomy and determine the path to insert the tube. Ultimately, the intubation device will include both a guidance display on an LCD screen and an optical stylet that has single-axis angulation control of the distal tip. For the purpose of this Phase I study, a laptop or desktop computer will be used for the image processing and the guidance display that accompanies the articulating stylet. The long-term goal is to create a device that is compact, light-weight and portable to suit the needs of ambulances and hospital crash carts.  The hypothesis for this study is that by incorporating a video guidance display with an articulating stylet, inexperienced users will be more successful in correctly placing the endotracheal tube using this device compared to direct laryngoscopy. To achieve this goal, image processing and machine learning algorithms will be developed to recognize key anatomic structures in the airway. Software will also be developed determine the path the tube should follow and to display this information for the user. Finally, the efficacy of the device will be validated in airway simulation mannequins with medical students serving as the inexperienced users. Phase II will focus on integrating the guidance software, articulating optical stylet and display into a portable device with embedded hardware and software contained within the stylet handle. At completion of Phase II, the device will be ready for clinica trials and FDA testing.  Brio will enter the $20 billion airway market with its intubation device. Initial sales will begin with anesthesiologists who are early adopters of new technology to assist with difficult airways. Brio will market its product to ~327,000 clinicians who use intubation devices. The U.S. addressable market for emergency intubation is ~$900M for the 41,000 ambulances and 5,800 emergency departments and hospital code teams.         PUBLIC HEALTH RELEVANCE: In this SBIR Phase I, Brio Device, LLC plans to create and evaluate a device that improves the success rate of emergency intubations by coupling a smart guidance display with a user-controlled single-axis articulating stylet. Emergency intubations are often performed in challenging situations by personnel who do the procedure infrequently. Since failure rates are as high as 50% and approximately 180,000 deaths occur each year from failed pre-hospital intubations, a device is needed to provide visual guidance information to assist the users and increase their success rates in emergency situations.            ",Smart Anatomic Recognition System to Guide Emergency Intubation and Resuscitation,8453607,R43HL114160,"['Accident and Emergency department', 'Address', 'Algorithms', 'Ambulances', 'Anatomic structures', 'Anatomy', 'Brain Death', 'Brain Injuries', 'Cessation of life', 'Clinical Trials', 'Code', 'Computer Vision Systems', 'Computer software', 'Computers', 'Coupling', 'Critical Care', 'Destinations', 'Devices', 'Distal', 'Emergency Situation', 'Failure', 'Feasibility Studies', 'Feedback', 'Goals', 'Hospitals', 'Human', 'Human Resources', 'Image', 'Imagery', 'Instruction', 'Intubation', 'Knowledge', 'Laryngoscopes', 'Laryngoscopy', 'Left', 'Life', 'Light', 'Location', 'Lung', 'Machine Learning', 'Manikins', 'Marketing', 'Medical Device', 'Medical Students', 'Military Hospitals', 'Military Personnel', 'Optics', 'Outcome', 'Outcome Measure', 'Oxygen', 'Paramedical Personnel', 'Patients', 'Phase', 'Physicians', 'Procedures', 'Resuscitation', 'Sales', 'Small Business Innovation Research Grant', 'Structure', 'System', 'Testing', 'Time', 'Trachea', 'Tube', 'Visual', 'commercial application', 'design', 'endotracheal', 'experience', 'flexibility', 'image processing', 'improved', 'information display', 'laptop', 'light weight', 'new technology', 'novel', 'phase 1 study', 'public health relevance', 'secondary outcome', 'simulation', 'success']",NHLBI,"BRIO DEVICE, LLC",R43,2013,244710,0.020794949628681243
"CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes DESCRIPTION (provided by applicant): The complexity of natural images is potentially enormous: the number of possible images that can be described by a smallish (100 by 100 pixels) picture is practically infinite (10000256), more than all the images the human race has ever witnessed during its entire existence. How can any system process input data of this magnitude of dimensions and interpret/understand it in terms of the estimated 200,000 objects in the world, their spatial layouts, and scene structures? Yet, this is a task that human visual systems routinely perform in a fraction of a second. The secret must lie in the fact that natural images are highly redundant, living in a restricted space inside this universe of almost infinite possibilities, and that mammalian visual systems have discovered and exploited this fact. In particular, we conjecture that neurons and populations are tuned to the statistical structure of natural images, building on previous work showing, for example, that sparse coding ideas can help predict receptive field properties of 'simple cells' in the visual cortex. This proposal has three stages. Firstly, we will perform a statistical analysis of natural images to classify and model the types of visual patches that appear. This will result in a stimulus dictionary, which will be used as stimuli to investigate the tuning properties of neurons and neuronal populations, and a visual concept dictionary which will be used to make predictions for the tuning properties. Secondly, we will perform multielectrode neurophysiological investigation of the tuning properties of neurons, and neuron populations, at different levels of the visual cortex in response to the stimulus dictionary. Thirdly, we will perform data analysis to model the tuning properties of neurons and populations using a combination of model-driven, which assumes that neurons are tuned to statistical properties of images, and data-driven approaches which can be thought of as learning 'neural visual concepts' directly from the neuron's response to the stimuli. Our theoretical approach - for learning the stimuli dictionary, the visual concepts, and performing data analysis - is based on statistical and machine learning techniques. These assume a hierarchical compositional structure for the data which offers the possibility of taming the complexity of natural images and is also consistent with the known hierarchical structure of the visual cortex. Intellectual merit: This research will help understand the structure of natural images, determine models for the tuning properties of neurons in the visual cortex, and develop novel data analysis techniques. It has the potential to significantly advance our understanding of the statistical structures of natural images and the neural encoding of these structures, including the population level. This will lead to greater understanding of the visual cortex and also help the development of computer vision systems. Broader impacts: This project is interdisciplinary in nature and should have broad impact in multiple disciplines: neuroscience and biological vision, statistical neural data analysis, computer vision, and machine learning. Understanding neuronal properties in the visual cortex is a pre-requisite to the clinical enterprise of developing therapeutic methods and prosthetic devices for the visually impaired. The proposed research program will help facilitate a new graduate program in Computational and Cognitive Neuroscience at UCLA, an inter-college undergraduate minor in Neural Computation at CMU that the investigators are developing at their respective universities. The investigators also plan to organize workshops in NIPS, COSYNE, as well as to integrate their research into both undergraduate and graduate curriculum in their respective universities. This work will also affect undergraduate students at other colleges, by a summer undergraduate training program in Pittsburgh, another at CMU's Qatar campus. In addition, we will propose a workshop and summer school at IPAM (UCLA). We anticipate that this research will lead to invited lectures, peer reviewed publications and, if successful, will have national and international impact. The PIs have good track record in involving undergraduates, including women and minorities, in their NSF-sponsored research, and will continue to endeavor in the training of the next generation of computational neuroscientists. This project will lead to greater understanding of neural mechanisms and coding strategies in the primate  visual cortex. Such knowledge is fundamental to understanding human visual functions and is critical to the  clinical enterprise of developing better diagnostic tools, therapeutic methods, and prosthetic devices for the  visually impaired.",CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes,8535775,R01EY022247,"['Affect', 'Appearance', 'Biological', 'Cells', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Dictionary', 'Dimensions', 'Discipline', 'Educational Curriculum', 'Educational workshop', 'Electrodes', 'Entropy', 'Graph', 'Human', 'Image', 'International', 'Intuition', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Letters', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Minor', 'Minority', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Noise', 'Pattern', 'Peer Review', 'Physiological', 'Population', 'Primates', 'Probability', 'Process', 'Property', 'Prosthesis', 'Publications', 'Qatar', 'Race', 'Research', 'Research Personnel', 'Sampling', 'Schools', 'Staging', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'Universities', 'Vision', 'Visual', 'Visual Cortex', 'Visual system structure', 'Vocabulary', 'Woman', 'Work', 'base', 'cognitive neuroscience', 'college', 'computational neuroscience', 'design', 'devices for the visually impaired', 'isophosphamide mustard', 'lectures', 'neuromechanism', 'neurophysiology', 'next generation', 'novel', 'programs', 'receptive field', 'relating to nervous system', 'research study', 'response', 'statistics', 'tool', 'undergraduate student', 'visual stimulus']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2013,343780,-0.018646730114092697
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.           The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8389864,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2013,499358,0.07422853864003244
"NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired     DESCRIPTION (provided by applicant): The project's objective is to develop enabling technology for a co-robotic navigation aid, called a Co-Robotic Cane (CRC), for the visually impaired. The CRC is able to collaborate with its user via intuitive Human-Device Interaction (HDl) mechanisms for effective navigation in 3D environments. The CRCs navigational functions include device position estimation, wayfinding, obstacle detection/avoidance, and object recognition. The use of the CRC will improve the visually impaired's independent mobility and thus their quality of life. The proposal's educational plan is to involve graduate, undergraduate and high school students in the project, and use the project's activities to recruit and retain engineering students. The proposal's Intellectual Merit is the development of new computer vision methods that support accurate blind navigation in 3D environments and intuitive HDl interfaces for effective use of device. These methods include: (1) a new robotic pose estimation method that provides accurate device pose by integrating egomotion estimation and visual feature tracking; (2) a pattern recognition method based on the Gaussian Mixture Model that may recognize indoor structures and objects for wayfinding and obstacle manipulation/ avoidance; (3) an innovative mechanism for intuitive conveying of the desired travel direction; and (4) a human intent detection interface for automatic device mode switching. The GPU implementation of the computer vision methods will make real-time execution possible. The proposed blind navigation solution will endow the CRC with advanced navigational functions that are currently unavailable in the existing blind navigation aids. The PI's team has performed proof of concept studies for the computer vision methods and the results are promising. The broader impacts include: (1) the methods' near term applications will impact the large visually impaired community; (2) the methods will improve the autonomy of small robots and portable robotic devices that have a myriad of applications in military surveillance, law enforcement, and search and rescue; and (3) the project will improve the research infrastructure of the Pi's university and train graduate and undergraduate students for their future careers in science and engineering.         PUBLIC HEALTH RELEVANCE:  The co-robotic navigation aid and the related technology address a growing public health care issue -- visual impairment and target improving the life quality of the blind. The research fits well into the Rehabilitation Engineering Program ofthe NIBIB that includes robotics rehabilitation. The proposed research addresses to the NlBlB's mission through developing new computer vision and HDl technologies for blind navigation.            ",NRI: Small: A Co-Robotic Navigation Aid for the Visually Impaired,8650411,R01EB018117,"['Address', 'Canes', 'Communities', 'Computer Vision Systems', 'Detection', 'Development', 'Devices', 'Engineering', 'Environment', 'Future', 'Healthcare', 'Human', 'Law Enforcement', 'Methods', 'Military Personnel', 'Mission', 'Modeling', 'National Institute of Biomedical Imaging and Bioengineering', 'Pattern Recognition', 'Positioning Attribute', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Robot', 'Robotics', 'Science', 'Solutions', 'Structure', 'Students', 'Technology', 'Time', 'Training', 'Travel', 'Universities', 'Visual', 'Visual impairment', 'base', 'blind', 'career', 'graduate student', 'high school', 'improved', 'innovation', 'navigation aid', 'object recognition', 'programs', 'public health relevance', 'rehabilitation engineering', 'robotic device', 'undergraduate student', 'way finding']",NIBIB,UNIVERSITY OF ARKANSAS AT LITTLE ROCK,R01,2013,81362,0.04071900617754007
"Providing Access to Appliance Displays for Visually Impaired Users  Providing Access to Appliance Displays for Visually Impaired Users Summary The goal of this project is to develop a computer vision system that runs on smartphones and tablets to enable blind and visually impaired persons to read appliance displays. This ability is increasingly necessary to use a variety of household and commercial appliances equipped with displays, such as microwave ovens and thermostats, and is essential for independent living, education and employment. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed system runs as software on a standard, off-the-shelf smartphone or tablet and uses computer vision algorithms to analyze video images taken by the user and to detect and read the text within each image. For blind users, this text is either read aloud using synthesized speech, or for low vision users, presented in magnified, contrast- enhanced form (which is most suited to the use of a tablet).  We propose to build on our past work on a prototype display reader using powerful new techniques that will enable the resulting system to read a greater variety of LED/LCD display text fonts, and to better accommodate the conditions that often make reading displays difficult, including glare, reflections and poor contrast. These techniques include novel character recognition techniques adapted specifically to the domain of digital displays; finger detection to allow the user to point to a specific location of interest in the display; the use of display templates as reference images to match to, and thereby help interpret, noisy images of displays; and the integration of multiple views of the display into a single clear view. Special user interface features such as the use of finger detection to help blind users aim the camera towards the display and contrast-enhancement for low vision users address the particular needs of different users. Blind and visually impaired subjects will test the system periodically to provide feedback and performance data, driving the development and continual improvement of the system, which will be assessed with objective performance measures. The resulting display reader system will be released as an app for smartphones and tablets that can be downloaded and used by anybody for free, and also as an open source project that can be freely built on or modified for use in 3rd-party software. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self-sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays.  The proposed research would result in a smartphone/tablet-based assistive technology system (available for free) to provide increased access to such display information for the approximately 10 million Americans with significant vision impairments or blindness.                ",Providing Access to Appliance Displays for Visually Impaired Users,8579051,R01EY018890,"['Access to Information', 'Address', 'Adoption', 'Algorithms', 'American', 'Automobile Driving', 'Blindness', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Detection', 'Development', 'Devices', 'Economics', 'Education', 'Employment', 'Evaluation', 'Feedback', 'Fingers', 'Glare', 'Goals', 'Hand', 'Home environment', 'Household', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging problem', 'Impairment', 'Independent Living', 'Location', 'Mainstreaming', 'Measures', 'Movement', 'Performance', 'Prevalence', 'Printing', 'Procedures', 'Reader', 'Reading', 'Research', 'Research Infrastructure', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'System', 'Tablets', 'Techniques', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'consumer product', 'design', 'digital', 'improved', 'information display', 'interest', 'microwave electromagnetic radiation', 'novel', 'open source', 'optical character recognition', 'prototype', 'public health relevance']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,376082,0.0666545858190996
"Perception of Tactile Graphics    DESCRIPTION (provided by applicant): The broad objective of the proposed research is to answer the following question: why are tactile graphics difficult to understand? People with normal vision can easily recognize line drawings of objects. However, both blind and sighted people find it very difficult to recognize the same drawings when they are presented as tactile images. For blind people, tactile graphics are the only solution for accessing information in visual diagrams and illustrations found in textbooks. Consequently, the results of the proposed research will be used to improve the production of tactile graphics so that they are better understood by blind people. The specific aims of this project are to: 1) explore how the complexity of tactile images affects perception, 2) determine the effects of spatial and temporal integration on perception of tactile images, and 3) investigate how well people can recognize tactile images of objects embedded in backgrounds. The general methodology of the proposed experiments is to present participants with tactile images and to have them draw what they perceive the images to be. Blind individuals will draw tactile images using special paper and a stylus. The experimenters will evaluate the drawings by using a quantitative measure that computes a distance score reflecting the discrepancy between the original image and the participant's drawing. In the first study, participants will feel tactile stimuli of varying complexity, from simple lines in different orientations to complex depictions of objects. The second study will determine the limitations of tactile perceptual integration by limiting either the spatial or temporal window over which participants feel the image. Participants will either view or feel images through apertures of various sizes (spatial window) or they will have a limited amount of time to view or feel the images (temporal window). In the third study, participants will feel tactile images of objects embedded in simple backgrounds. This research will impact several areas of study, including computer vision, human object and scene recognition, and low vision rehabilitation.       PUBLIC HEALTH RELEVANCE: Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.            ",Perception of Tactile Graphics,8417005,F32EY019622,"['Affect', 'Area', 'Categories', 'Child', 'Complex', 'Computer Vision Systems', 'Development', 'Devices', 'Disadvantaged', 'Education', 'Elements', 'Goals', 'Grouping', 'Human', 'Image', 'India', 'Individual', 'Link', 'Measures', 'Methodology', 'Methods', 'Names', 'Nature', 'Paper', 'Participant', 'Perception', 'Population', 'Production', 'Psychophysics', 'Rehabilitation therapy', 'Research', 'Science', 'Sensory', 'Services', 'Shapes', 'Solutions', 'Stimulus', 'Swelling', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Vision', 'Visual', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'blind', 'braille', 'improved', 'object recognition', 'public health relevance', 'research study', 'sight for the blind', 'skills', 'tactile vision substitution system', 'two-dimensional', 'vision development', 'visual process', 'visual processing']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2013,53942,0.011259289265305396
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.       PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8435501,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2013,383018,0.027029026289138475
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.        The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8410578,R01EY022039,"['Address', 'Algorithms', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'design', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2013,368125,0.04167781291567067
"Neural and Behavioral Interactions Between Attention, Perception, and Learning    DESCRIPTION (provided by applicant): The overarching goal of this research is to characterize how perception and memory interact, in terms of both the learning mechanisms that help transform visual experience into memory, and the intentional mechanisms that regulate this transformation. The specific goal of this proposal is to test the hypothesis that incidental learning about statistical regularities in vision (visual statistical learning) is limited to selectively attend visual information, and that this behavioral interaction arises because of how selective attention modulates neural interactions between human visual and memory systems. We propose a two-stage framework in which selective attention to a high-level visual feature/category increases neural interactions between regions of occipital cortex that represent low-level features and the region of inferior temporal cortex (IT) that represents the attended feature/category, and in turn between this IT region and medial temporal lobe (MTL) sub regions involved in visual learning and memory. In addition to assessing how feature-based selective attention influences learning at a behavioral level, we will use functional magnetic resonance imaging to assess how attention influences evoked neural responses in task-relevant brain regions, as well as neural interactions between these regions in the background of ongoing tasks. We will develop an innovative approach for studying neural interactions in which evoked responses and global noise sources are scrubbed from the data and regional correlations are assessed in the residuals. This background connectivity approach provides a new way to study how intentional goals affect perception and learning. Aim 1 examines the first stage of our framework, testing: how selective attention modulates background connectivity between IT and occipital cortex, where in retinotopic visual cortex this modulation occurs, and how these changes are controlled by frontal and parietal cortex. Aim 2 examines the second stage of our framework, first establishing the role of the MTL in visual statistical learning, and then testing: how selective attention modulates interactions between IT and the MTL, where in cortical and hippocampal sub regions of the MTL this modulation occurs, and how these changes facilitate incidental learning about statistical regularities and later retrieval of this knowledge. In sum, we relate behavioral interactions between selective attention and learning to neural interactions between the mechanisms that represent visual features and those that learn about their relations. This proposal addresses several key issues in the field, including: how attention modulates the MTL, how feature-based attention is controlled, whether different neural mechanisms support rapid versus long-term visual learning, how tasks and goals are represented, and how attention and memory retrieval are related. This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery and rehabilitation of visual function following eye disease, injury, or brain damage.        This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.         ","Neural and Behavioral Interactions Between Attention, Perception, and Learning",8515424,R01EY021755,"['Address', 'Affect', 'Architecture', 'Area', 'Attention', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Cognitive Science', 'Data', 'Development', 'Diagnosis', 'Disease', 'Event', 'Exhibits', 'Eye diseases', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Inferior', 'Injury', 'Knowledge', 'Lead', 'Learning', 'Light', 'Link', 'Machine Learning', 'Maps', 'Medial', 'Memory', 'Methods', 'Mind', 'Modeling', 'Nature', 'Neurosciences', 'Noise', 'Occipital lobe', 'Parietal', 'Parietal Lobe', 'Perception', 'Physiological', 'Positioning Attribute', 'Process', 'Property', 'Recovery', 'Rehabilitation therapy', 'Research', 'Residual state', 'Resolution', 'Rest', 'Retrieval', 'Role', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Staging', 'Stimulus', 'Sum', 'Synapses', 'System', 'Temporal Lobe', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'cognitive neuroscience', 'experience', 'extrastriate visual cortex', 'frontal lobe', 'hippocampal subregions', 'improved', 'information processing', 'innovation', 'memory retrieval', 'neuroimaging', 'neuromechanism', 'novel strategies', 'relating to nervous system', 'response', 'retinotopic', 'selective attention', 'transmission process', 'visual information', 'visual learning', 'visual memory', 'visual process', 'visual processing', 'visual stimulus']",NEI,PRINCETON UNIVERSITY,R01,2013,337678,-0.00478888459822869
"Detection of Glaucoma Progression with Macular OCT Imaging     DESCRIPTION (provided by applicant): This application is a formal request for a career development award (K23) for an academic glaucoma specialist with a serious interest in the role of imaging in glaucoma using optical coherence tomography (OCT). This will allow the candidate to establish a clinical research program with the main goal of improving detection of glaucoma progression through macular imaging with spectral-domain OCT. By the time the proposed research is accomplished, the candidate will have preliminary data for continuing his research as an independent investigator and will have collected longitudinal structural and functional data in a group of advanced glaucoma patients that will serve as a platform for further improving detection of glaucoma progression with macular OCT imaging. The data will help the candidate provide preliminary results for a subsequent R01 that would potentially allow the PI to continue follow-up of the patients enrolled in the K23 award period.  I have a Master's of Science degree in Clinical Investigation under my belt and intend to deepen my skills in the field of imaging and biostatistics (to be used for enhancing and handling OCT images and for analyzing longitudinal data) by completing the proposed didactic program. By the end of the award period, I expect that I will have gained additional experience, knowledge, and mentorship required to prosper as an independent clinician-scientist in the field of glaucoma. My long-term goal is to carry out longitudinal studies of glaucoma patients where current and upcoming imaging and functional tests can be applied and their utility for detection of glaucoma progression can be investigated. I am confident that the combined skills and experience of my mentors will lead to a successful outcome for the proposed K award. I also envisage myself mentoring candidates like myself in future so that our collective knowledge and wisdom can be passed along to the next generation of aspiring clinician-scientists.  My objectives during the award period are as follows: 1) To develop an individual research program in glaucoma diagnostic imaging; 2) to successfully complete credited coursework in biomathematics, advanced biostatistics, computer vision (image processing), epidemiology, and ethical issues in research.  The main goal of the research component of this proposal is to better delineate the role of macular SD- OCT imaging for detection of glaucoma progression in advanced glaucoma. The specific aims through which this goal will be accomplished are as follows:  (1) To compare the performance of various global and regional macular measures to detect glaucoma.  The potential factors influencing the performance of various macular outcome measures will be explored. Such covariates include age, race, axial length, disc size, central corneal thickness,  OCT signal strength, and outer retinal thickness among others. I hypothesize that the thickness  of the outer retina (outer nuclear layer to retinal pigment epithelium-Bruch's membrane  complex) may be the most important factor explaining the measurement variability of the inner  retinal layer thickness (GCC or ganglion cell/inner plexiform layers).  (2) To determine and compare the utility of the candidate macular measures, detected through the first  aim, for detection of glaucoma progression in moderately advanced to severe glaucoma.  Moderately advanced to severe glaucoma will be defined as eyes with visual field mean  deviation worse than -6 dB or eyes with involvement of the central 10 degrees on the 24-2  visual field. It is widely accepted that measurement of the optic nerve head or RNFL parameters  in advanced glaucoma does not provide clinicians with much useful information. In contrast, the  central macular ganglion cells are the last to die in glaucoma. Macular imaging in advanced  glaucoma is directed towards this area where detection of change may still be possible. I  hypothesize that macular OCT parameters are valid structural outcome measures (biomarkers)  that can be used to follow the course of the disease in advanced glaucoma and that such  measures are significantly correlated with changes in the central visual field. Changes in the  macular measures over time will be first correlated with the corresponding visual field change  (functional progression) over time in eyes with moderately advanced to severe glaucoma. The  utility of the best candidate macular measures for predicting subsequent glaucoma progression  will also be explored and compared. I hypothesize that there may be a lag period between  progressive loss of macular ganglion cells and subsequent visual field progression in advanced  glaucoma, and therefore, detection of worsening in one or more macular outcome measures  can be used as a proxy for subsequent visual field progression.  Collectively, these studies will provide a solid foundation for better understanding and integration of macular OCT imaging in the care of glaucoma patients. Timely detection of glaucoma progression in the later stages can significantly reduce visual disability and blindness through earlier aggressive treatment and will potentially reduce glaucoma's financial burden to society.          Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.            ",Detection of Glaucoma Progression with Macular OCT Imaging,8529542,K23EY022659,"['Age', 'Area', 'Award', 'Biological Markers', 'Biometry', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Caring', 'Clinical Research', 'Clinical Trials', 'Complement', 'Complex', 'Computer Vision Systems', 'Cornea', 'Data', 'Detection', 'Diagnosis', 'Diagnostic Imaging', 'Disease', 'Early Diagnosis', 'Enrollment', 'Epidemiology', 'Ethical Issues', 'Evaluation', 'Eye', 'Foundations', 'Functional Imaging', 'Future', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Image Analysis', 'Individual', 'Inner Plexiform Layer', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Legal Blindness', 'Length', 'Longitudinal Studies', 'Master of Science', 'Measurement', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Morbidity - disease rate', 'Nerve Fibers', 'Noise', 'Ophthalmologist', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Process', 'Proxy', 'Quality of life', 'Race', 'Research', 'Research Personnel', 'Retinal', 'Role', 'Scientist', 'Signal Transduction', 'Societies', 'Solid', 'Specialist', 'Staging', 'Structure of retinal pigment epithelium', 'Testing', 'Thick', 'Time', 'Vision', 'Visual', 'Visual Fields', 'advanced disease', 'biomathematics', 'central visual field', 'disability', 'experience', 'follow-up', 'ganglion cell', 'image processing', 'improved', 'interest', 'macula', 'next generation', 'programs', 'retina outer nuclear layer', 'retinal nerve fiber layer', 'skills']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,K23,2013,229139,0.06665213663832276
"Context Understanding Technology to improve internet accessibility for users with     DESCRIPTION (provided by applicant): The purpose of this SBIR project is to develop a new tool to improve the ability of blind and visually impaired people to quickly get to the right web pages, to avoid ending up on the wrong web pages, and to reach the desired part of each web page efficiently. The motivation for this effort is that getting a 'big picture' understanding of a web page is one of the biggest challenges facing this population of computer users. Existing tools for blind and visually impaired people, such as screen readers and screen magnifiers, present the entire web page serially, in full detail, either by textual output of the details, or b providing a magnified view of a small part of the page. However, if the user is not already familiar with the website, the resulting lack of context requires a laborious and time-consuming process to scan through sometimes hundreds of details before knowing whether anything of interest even exists on the page in question. In contrast, the proposed technology analyzes web pages in a similar way that sighted users quickly scan pages before reading in detail, to provide a succinct, informative guidance on the context and layout of the page, so that a user quickly gains a much richer ""big-picture"" understanding. The proposed Phase II project builds on a successful Phase I user evaluation of a proof- of-concept of the software, in which users expressed a strong preference for the contextual cues provided by the approach. Phase II includes creating a fully-functional prototype of the software tool. Throughout the technology development, feedback from users and practitioners will guide the path of the R&D for maximum usability. At the conclusion of Phase II, an end-user, in-home evaluation study will verify the effectiveness of the tool, providing the starting point for full-scale commercialization.         PUBLIC HEALTH RELEVANCE: The R&D in this Phase II SBIR project will result in an improved way to use the Internet for individuals with visual disabilities. The technology will help to overcome the lack of accessibility and high level of frustration currently found among blind and visually impaired users. The results of the project will enable such individuals to explore new opportunities and be more productive, thus providing improved employment potential and an increase in the quality of life.                ",Context Understanding Technology to improve internet accessibility for users with,8459121,R44EY020082,"['Advertisements', 'Area', 'Arizona', 'Artificial Intelligence', 'Boxing', 'Characteristics', 'Color', 'Computer software', 'Computers', 'Cues', 'Effectiveness', 'Employment', 'Evaluation', 'Evaluation Studies', 'Feedback', 'Frustration', 'Generations', 'Government', 'Grouping', 'Home environment', 'Individual', 'Interest Group', 'Internet', 'Literature', 'Marketing', 'Motivation', 'Mus', 'Output', 'Persons', 'Phase', 'Population', 'Process', 'Quality of life', 'Reader', 'Reading', 'Research', 'Scanning', 'Small Business Innovation Research Grant', 'Software Tools', 'Speech', 'System', 'Technology', 'Text', 'Time', 'Training', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'base', 'blind', 'commercialization', 'disability', 'experience', 'improved', 'interest', 'natural language', 'phrases', 'preference', 'prototype', 'public health relevance', 'research and development', 'technology development', 'tool', 'usability', 'web page', 'web site']",NEI,"VORTANT TECHNOLOGIES, LLC",R44,2013,371933,0.047012076164963945
"CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes DESCRIPTION (provided by applicant): The complexity of natural images is potentially enormous: the number of possible images that can be described by a smallish (100 by 100 pixels) picture is practically infinite (10000256), more than all the images the human race has ever witnessed during its entire existence. How can any system process input data of this magnitude of dimensions and interpret/understand it in terms of the estimated 200,000 objects in the world, their spatial layouts, and scene structures? Yet, this is a task that human visual systems routinely perform in a fraction of a second. The secret must lie in the fact that natural images are highly redundant, living in a restricted space inside this universe of almost infinite possibilities, and that mammalian visual systems have discovered and exploited this fact. In particular, we conjecture that neurons and populations are tuned to the statistical structure of natural images, building on previous work showing, for example, that sparse coding ideas can help predict receptive field properties of 'simple cells' in the visual cortex. This proposal has three stages. Firstly, we will perform a statistical analysis of natural images to classify and model the types of visual patches that appear. This will result in a stimulus dictionary, which will be used as stimuli to investigate the tuning properties of neurons and neuronal populations, and a visual concept dictionary which will be used to make predictions for the tuning properties. Secondly, we will perform multielectrode neurophysiological investigation of the tuning properties of neurons, and neuron populations, at different levels of the visual cortex in response to the stimulus dictionary. Thirdly, we will perform data analysis to model the tuning properties of neurons and populations using a combination of model-driven, which assumes that neurons are tuned to statistical properties of images, and data-driven approaches which can be thought of as learning 'neural visual concepts' directly from the neuron's response to the stimuli. Our theoretical approach - for learning the stimuli dictionary, the visual concepts, and performing data analysis - is based on statistical and machine learning techniques. These assume a hierarchical compositional structure for the data which offers the possibility of taming the complexity of natural images and is also consistent with the known hierarchical structure of the visual cortex. Intellectual merit: This research will help understand the structure of natural images, determine models for the tuning properties of neurons in the visual cortex, and develop novel data analysis techniques. It has the potential to significantly advance our understanding of the statistical structures of natural images and the neural encoding of these structures, including the population level. This will lead to greater understanding of the visual cortex and also help the development of computer vision systems. Broader impacts: This project is interdisciplinary in nature and should have broad impact in multiple disciplines: neuroscience and biological vision, statistical neural data analysis, computer vision, and machine learning. Understanding neuronal properties in the visual cortex is a pre-requisite to the clinical enterprise of developing therapeutic methods and prosthetic devices for the visually impaired. The proposed research program will help facilitate a new graduate program in Computational and Cognitive Neuroscience at UCLA, an inter-college undergraduate minor in Neural Computation at CMU that the investigators are developing at their respective universities. The investigators also plan to organize workshops in NIPS, COSYNE, as well as to integrate their research into both undergraduate and graduate curriculum in their respective universities. This work will also affect undergraduate students at other colleges, by a summer undergraduate training program in Pittsburgh, another at CMU's Qatar campus. In addition, we will propose a workshop and summer school at IPAM (UCLA). We anticipate that this research will lead to invited lectures, peer reviewed publications and, if successful, will have national and international impact. The PIs have good track record in involving undergraduates, including women and minorities, in their NSF-sponsored research, and will continue to endeavor in the training of the next generation of computational neuroscientists. This project will lead to greater understanding of neural mechanisms and coding strategies in the primate  visual cortex. Such knowledge is fundamental to understanding human visual functions and is critical to the  clinical enterprise of developing better diagnostic tools, therapeutic methods, and prosthetic devices for the  visually impaired.",CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes,8535310,R01EY022247,"['Affect', 'Appearance', 'Biological', 'Cells', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Dictionary', 'Dimensions', 'Discipline', 'Educational Curriculum', 'Educational workshop', 'Electrodes', 'Entropy', 'Graph', 'Human', 'Image', 'International', 'Intuition', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Letters', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Minor', 'Minority', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Noise', 'Pattern', 'Peer Review', 'Physiological', 'Population', 'Primates', 'Probability', 'Process', 'Property', 'Prosthesis', 'Publications', 'Qatar', 'Race', 'Research', 'Research Personnel', 'Sampling', 'Schools', 'Staging', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'Universities', 'Vision', 'Visual', 'Visual Cortex', 'Visual system structure', 'Vocabulary', 'Woman', 'Work', 'base', 'cognitive neuroscience', 'college', 'computational neuroscience', 'design', 'devices for the visually impaired', 'isophosphamide mustard', 'lectures', 'neuromechanism', 'neurophysiology', 'next generation', 'novel', 'programs', 'receptive field', 'relating to nervous system', 'research study', 'response', 'statistics', 'tool', 'undergraduate student', 'visual stimulus']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2012,104373,-0.018646730114092697
"CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes DESCRIPTION (provided by applicant): The complexity of natural images is potentially enormous: the number of possible images that can be described by a smallish (100 by 100 pixels) picture is practically infinite (10000256), more than all the images the human race has ever witnessed during its entire existence. How can any system process input data of this magnitude of dimensions and interpret/understand it in terms of the estimated 200,000 objects in the world, their spatial layouts, and scene structures? Yet, this is a task that human visual systems routinely perform in a fraction of a second. The secret must lie in the fact that natural images are highly redundant, living in a restricted space inside this universe of almost infinite possibilities, and that mammalian visual systems have discovered and exploited this fact. In particular, we conjecture that neurons and populations are tuned to the statistical structure of natural images, building on previous work showing, for example, that sparse coding ideas can help predict receptive field properties of 'simple cells' in the visual cortex. This proposal has three stages. Firstly, we will perform a statistical analysis of natural images to classify and model the types of visual patches that appear. This will result in a stimulus dictionary, which will be used as stimuli to investigate the tuning properties of neurons and neuronal populations, and a visual concept dictionary which will be used to make predictions for the tuning properties. Secondly, we will perform multielectrode neurophysiological investigation of the tuning properties of neurons, and neuron populations, at different levels of the visual cortex in response to the stimulus dictionary. Thirdly, we will perform data analysis to model the tuning properties of neurons and populations using a combination of model-driven, which assumes that neurons are tuned to statistical properties of images, and data-driven approaches which can be thought of as learning 'neural visual concepts' directly from the neuron's response to the stimuli. Our theoretical approach - for learning the stimuli dictionary, the visual concepts, and performing data analysis - is based on statistical and machine learning techniques. These assume a hierarchical compositional structure for the data which offers the possibility of taming the complexity of natural images and is also consistent with the known hierarchical structure of the visual cortex. Intellectual merit: This research will help understand the structure of natural images, determine models for the tuning properties of neurons in the visual cortex, and develop novel data analysis techniques. It has the potential to significantly advance our understanding of the statistical structures of natural images and the neural encoding of these structures, including the population level. This will lead to greater understanding of the visual cortex and also help the development of computer vision systems. Broader impacts: This project is interdisciplinary in nature and should have broad impact in multiple disciplines: neuroscience and biological vision, statistical neural data analysis, computer vision, and machine learning. Understanding neuronal properties in the visual cortex is a pre-requisite to the clinical enterprise of developing therapeutic methods and prosthetic devices for the visually impaired. The proposed research program will help facilitate a new graduate program in Computational and Cognitive Neuroscience at UCLA, an inter-college undergraduate minor in Neural Computation at CMU that the investigators are developing at their respective universities. The investigators also plan to organize workshops in NIPS, COSYNE, as well as to integrate their research into both undergraduate and graduate curriculum in their respective universities. This work will also affect undergraduate students at other colleges, by a summer undergraduate training program in Pittsburgh, another at CMU's Qatar campus. In addition, we will propose a workshop and summer school at IPAM (UCLA). We anticipate that this research will lead to invited lectures, peer reviewed publications and, if successful, will have national and international impact. The PIs have good track record in involving undergraduates, including women and minorities, in their NSF-sponsored research, and will continue to endeavor in the training of the next generation of computational neuroscientists. This project will lead to greater understanding of neural mechanisms and coding strategies in the primate  visual cortex. Such knowledge is fundamental to understanding human visual functions and is critical to the  clinical enterprise of developing better diagnostic tools, therapeutic methods, and prosthetic devices for the  visually impaired.",CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes,8312499,R01EY022247,"['Affect', 'Appearance', 'Biological', 'Cells', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Dictionary', 'Dimensions', 'Discipline', 'Educational Curriculum', 'Educational workshop', 'Electrodes', 'Entropy', 'Graph', 'Human', 'Image', 'International', 'Intuition', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Letters', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Minor', 'Minority', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Noise', 'Pattern', 'Peer Review', 'Physiological', 'Population', 'Primates', 'Probability', 'Process', 'Property', 'Prosthesis', 'Publications', 'Qatar', 'Race', 'Research', 'Research Personnel', 'Sampling', 'Schools', 'Staging', 'Statistical Models', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'Universities', 'Vision', 'Visual', 'Visual Cortex', 'Visual system structure', 'Vocabulary', 'Woman', 'Work', 'base', 'cognitive neuroscience', 'college', 'computational neuroscience', 'design', 'devices for the visually impaired', 'isophosphamide mustard', 'lectures', 'neuromechanism', 'neurophysiology', 'next generation', 'novel', 'programs', 'receptive field', 'relating to nervous system', 'research study', 'response', 'statistics', 'tool', 'undergraduate student', 'visual stimulus']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2012,357385,-0.018646730114092697
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    The technology developed as part of this NIH SBIR project will transform the cell phone camera of visually impaired individuals into a powerful tool capable of identifying the objects they encounter, track the items they own, or navigate complex new environments. Broad access to low-cost visual intelligence technologies developed in this project will improve the independence and capabilities of the visually impaired. There has been tremendous technological progress in computer vision and in the computational power and network bandwidth of and Smartphone platforms. The synergy of these advances stands to revolutionize the way people find information and interact with the physical world. However, these technologies are not yet fully in the hands of the visually impaired, arguably the population that could benefit the most from these developments. Part of the barrier to progress in this area has been that computer vision can accurately handle only a small fraction of the typical images coming from a cell phone camera. To cope with these limitations and make any-image recognition possible, IQ Engines will develop a hybrid system that uses both computer vision and crowdsourcing: if the computer algorithms are not able to understand an image, then the image is sent to a unique crowdsourcing network of people for image analysis. The proposed research includes specific aims to both develop advanced computer vision algorithms for object recognition and advanced crowdsourced networks optimized to the needs of the visually impaired community. This approach combines the speed and accuracy of computer vision with the robustness and understanding of human vision, ultimately providing the user fast and accurate information about the content of any image.      PUBLIC HEALTH RELEVANCE:    The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.                 The image recognition technology developed in this application will enable the visually impaired to access visual information using a mobile phone camera, a device most people already have. Transforming the camera into an intelligent visual sensor will lower the cost of assistance and improve the quality of life of the visually impaired community through increased independence and capabilities.            ",Mobile Search for the Visually Impaired,8198847,R44EY019790,"['Address', 'Algorithms', 'Area', 'Car Phone', 'Cellular Phone', 'Classification', 'Client', 'Clip', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Crowding', 'Data', 'Databases', 'Detection', 'Development', 'Devices', 'Ensure', 'Environment', 'Family', 'Feedback', 'Friends', 'Glosso-Sterandryl', 'Human', 'Hybrid Computers', 'Hybrids', 'Image', 'Image Analysis', 'Individual', 'Intelligence', 'Label', 'Learning', 'Location', 'Modeling', 'Monitor', 'Phase', 'Population', 'Preparation', 'Process', 'Quality of life', 'Research', 'Running', 'Scanning', 'Services', 'Small Business Innovation Research Grant', 'Social Network', 'Source', 'Speed', 'System', 'Technology', 'Telephone', 'Time', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual impairment', 'base', 'blind', 'cell transformation', 'coping', 'cost', 'improved', 'innovation', 'novel', 'object recognition', 'open source', 'sensor', 'tool', 'visual information', 'visual search', 'volunteer']",NEI,"IQ ENGINES, INC.",R44,2012,499358,0.07617880263689936
"OTHER FUNCTIONS SBIR TOPIC 308, PHASE I: THE MOBILE FOOD INTAKE PHOTO STORAGE AN This proposal describes plans to enhance Viocare¿s Mobile Food Intake Visual and Voice Recognizer (FIVR) System. FIVR, an active Genes, Environment and Health Initiative (GEI) project, is a novel combination of innovative technologies including computer vision and speech recognition to measure dietary intake using a mobile phone. Version 1 of FIVR uses a mobile phone¿s embedded camera to capture a short video of food to be consumed. The food to be eaten is annotated verbally on the mobile phone by the user. These video and audio files are sent to a backend server for real-time food recognition and portion size measurement through speech recognition and image analysis. This project will develop specifications to extend FIVR¿s capabilities to standardize, store, and analyze more diverse food images, such as 3D photos; to collect other food data; to enhance the analysis tools; and for interfaces to a variety of clinical/research systems. The FIVR Version 2 functional prototype will be developed to use 3D dietary images as input. A final evaluation of the FIVR V2 prototype will be conducted to assess the accuracy and feasibility of the 3D image diet capture with a group of 9 subjects in a controlled feeding study. n/a","OTHER FUNCTIONS SBIR TOPIC 308, PHASE I: THE MOBILE FOOD INTAKE PHOTO STORAGE AN",8554263,61201200042C,"['Car Phone', 'Clinical Research', 'Computer Vision Systems', 'Data', 'Diet', 'Dietary intake', 'Documentation', 'Eating', 'Environment', 'Evaluation', 'Food', 'Genes', 'Health', 'Image', 'Image Analysis', 'Measurement', 'Measures', 'Phase', 'Reporting', 'Small Business Innovation Research Grant', 'System', 'Three-Dimensional Image', 'Time', 'Visual', 'Voice', 'feeding', 'innovative technologies', 'novel', 'prototype', 'speech recognition', 'tool']",NCI,"VIOCARE, INC.",N43,2012,200000,-0.0007177677199350626
"Vision Without Sight: Exploring the Environment with a Portable Camera  Vision without Sight: Exploring the Environment with a Portable Camera Project Summary As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his colaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specificaly examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.  Vision without Sight: Exploring the Environment with a Portable Camera Project Narrative The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cel phones but are typicaly designed for normaly sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population",Vision Without Sight: Exploring the Environment with a Portable Camera,8334623,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Telephone', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2012,229834,0.06726499122245026
"Perception of Tactile Graphics    DESCRIPTION (provided by applicant): The broad objective of the proposed research is to answer the following question: why are tactile graphics difficult to understand? People with normal vision can easily recognize line drawings of objects. However, both blind and sighted people find it very difficult to recognize the same drawings when they are presented as tactile images. For blind people, tactile graphics are the only solution for accessing information in visual diagrams and illustrations found in textbooks. Consequently, the results of the proposed research will be used to improve the production of tactile graphics so that they are better understood by blind people. The specific aims of this project are to: 1) explore how the complexity of tactile images affects perception, 2) determine the effects of spatial and temporal integration on perception of tactile images, and 3) investigate how well people can recognize tactile images of objects embedded in backgrounds. The general methodology of the proposed experiments is to present participants with tactile images and to have them draw what they perceive the images to be. Blind individuals will draw tactile images using special paper and a stylus. The experimenters will evaluate the drawings by using a quantitative measure that computes a distance score reflecting the discrepancy between the original image and the participant's drawing. In the first study, participants will feel tactile stimuli of varying complexity, from simple lines in different orientations to complex depictions of objects. The second study will determine the limitations of tactile perceptual integration by limiting either the spatial or temporal window over which participants feel the image. Participants will either view or feel images through apertures of various sizes (spatial window) or they will have a limited amount of time to view or feel the images (temporal window). In the third study, participants will feel tactile images of objects embedded in simple backgrounds. This research will impact several areas of study, including computer vision, human object and scene recognition, and low vision rehabilitation.      PUBLIC HEALTH RELEVANCE: Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.              Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.",Perception of Tactile Graphics,8265243,F32EY019622,"['Affect', 'Area', 'Categories', 'Child', 'Complex', 'Computer Vision Systems', 'Development', 'Devices', 'Disadvantaged', 'Education', 'Elements', 'Goals', 'Grouping', 'Human', 'Image', 'India', 'Individual', 'Link', 'Measures', 'Methodology', 'Methods', 'Names', 'Nature', 'Paper', 'Participant', 'Perception', 'Population', 'Production', 'Psychophysics', 'Rehabilitation therapy', 'Research', 'Science', 'Sensory', 'Services', 'Shapes', 'Solutions', 'Stimulus', 'Swelling', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Vision', 'Visual', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'blind', 'braille', 'improved', 'object recognition', 'public health relevance', 'research study', 'sight for the blind', 'skills', 'tactile vision substitution system', 'two-dimensional', 'vision development', 'visual process', 'visual processing']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2012,52190,0.0239273149150125
"Predicting and Detecting Glaucomatous Progression Using Pattern Recognition    DESCRIPTION (provided by applicant): This project aims to improve glaucoma management by applying novel pattern recognition techniques to improve the accurate prediction and detection of glaucomatous progression. The premise is that complex functional and structural tests in daily use by eye care providers contain hidden information that is not fully used in current analyses, and that advanced pattern recognition techniques can find and use that hidden information. The primary goals involve the use of mathematically rigorous techniques to discover patterns of defects and to track their changes in longitudinal series of perimetric and optical imaging data from up to 1800 glaucomatous and healthy eyes, available as the result of long-term NIH funding. With the interdisciplinary team of glaucoma and pattern recognition experts we have assembled, with our extensive NIH-supported database of eyes, and with the knowledge we have acquired in the optimal use of pattern recognition methods from previous NIH support, we believe the proposed work can enhance significantly the medical and surgical treatment of glaucoma and reduce the cost of glaucoma care. Moreover, improved techniques for predicting and detecting glaucomatous progression can be used for refined subject recruitment and to define endpoints for clinical trials of intraocular pressure-lowering and neuroprotective drugs.      PUBLIC HEALTH RELEVANCE: The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.              The proposed project will develop and demonstrate the usefulness of pattern recognition techniques for predicting and detecting patterns of glaucomatous change in patient eyes tested longitudinally by visual field and optical imaging instruments. This proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset and characterize glaucomatous neurodegeneration within the visual pathways at structural and functional levels. The development/use of novel, empirical techniques for predicting and detecting glaucomatous progression can have a significant impact on the future of clinical care and the future of clinical trials designed to investigate IOP lowering and neuroprotective drugs.            ",Predicting and Detecting Glaucomatous Progression Using Pattern Recognition,8216617,R01EY022039,"['Address', 'Algorithms', 'California', 'Caring', 'Clinic', 'Clinical Trials', 'Clinical Trials Design', 'Complex', 'Data', 'Databases', 'Defect', 'Detection', 'Development', 'Diagnostic', 'Disease', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Goals', 'Grant', 'Image', 'Imaging Device', 'Informatics', 'Knowledge', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Modeling', 'National Eye Institute', 'Nerve Degeneration', 'Neuroprotective Agents', 'Noise', 'Onset of illness', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Patients', 'Pattern', 'Pattern Recognition', 'Perimetry', 'Physiologic Intraocular Pressure', 'Physiological', 'Provider', 'Scanning', 'Science', 'Series', 'Signal Transduction', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'Translational Research', 'Treatment Effectiveness', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual Fields', 'Visual Pathways', 'Work', 'base', 'clinical care', 'cost', 'design', 'heuristics', 'improved', 'independent component analysis', 'instrument', 'novel', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'retinal nerve fiber layer', 'skills']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2012,386771,0.0434604606126537
"Neural and Behavioral Interactions Between Attention, Perception, and Learning    DESCRIPTION (provided by applicant): The overarching goal of this research is to characterize how perception and memory interact, in terms of both the learning mechanisms that help transform visual experience into memory, and the intentional mechanisms that regulate this transformation. The specific goal of this proposal is to test the hypothesis that incidental learning about statistical regularities in vision (visual statistical learning) is limited to selectively attend visual information, and that this behavioral interaction arises because of how selective attention modulates neural interactions between human visual and memory systems. We propose a two-stage framework in which selective attention to a high-level visual feature/category increases neural interactions between regions of occipital cortex that represent low-level features and the region of inferior temporal cortex (IT) that represents the attended feature/category, and in turn between this IT region and medial temporal lobe (MTL) sub regions involved in visual learning and memory. In addition to assessing how feature-based selective attention influences learning at a behavioral level, we will use functional magnetic resonance imaging to assess how attention influences evoked neural responses in task-relevant brain regions, as well as neural interactions between these regions in the background of ongoing tasks. We will develop an innovative approach for studying neural interactions in which evoked responses and global noise sources are scrubbed from the data and regional correlations are assessed in the residuals. This background connectivity approach provides a new way to study how intentional goals affect perception and learning. Aim 1 examines the first stage of our framework, testing: how selective attention modulates background connectivity between IT and occipital cortex, where in retinotopic visual cortex this modulation occurs, and how these changes are controlled by frontal and parietal cortex. Aim 2 examines the second stage of our framework, first establishing the role of the MTL in visual statistical learning, and then testing: how selective attention modulates interactions between IT and the MTL, where in cortical and hippocampal sub regions of the MTL this modulation occurs, and how these changes facilitate incidental learning about statistical regularities and later retrieval of this knowledge. In sum, we relate behavioral interactions between selective attention and learning to neural interactions between the mechanisms that represent visual features and those that learn about their relations. This proposal addresses several key issues in the field, including: how attention modulates the MTL, how feature-based attention is controlled, whether different neural mechanisms support rapid versus long-term visual learning, how tasks and goals are represented, and how attention and memory retrieval are related. This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery and rehabilitation of visual function following eye disease, injury, or brain damage.        This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.         ","Neural and Behavioral Interactions Between Attention, Perception, and Learning",8306867,R01EY021755,"['Address', 'Affect', 'Architecture', 'Area', 'Attention', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Cognitive Science', 'Data', 'Development', 'Diagnosis', 'Disease', 'Event', 'Exhibits', 'Eye diseases', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Inferior', 'Injury', 'Knowledge', 'Lead', 'Learning', 'Light', 'Link', 'Machine Learning', 'Maps', 'Medial', 'Memory', 'Methods', 'Mind', 'Modeling', 'Nature', 'Neurosciences', 'Noise', 'Occipital lobe', 'Parietal', 'Parietal Lobe', 'Perception', 'Physiological', 'Positioning Attribute', 'Process', 'Property', 'Recovery', 'Rehabilitation therapy', 'Research', 'Residual state', 'Resolution', 'Rest', 'Retrieval', 'Role', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Staging', 'Stimulus', 'Sum', 'Synapses', 'System', 'Temporal Lobe', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'cognitive neuroscience', 'experience', 'extrastriate visual cortex', 'frontal lobe', 'hippocampal subregions', 'improved', 'information processing', 'innovation', 'memory retrieval', 'neuroimaging', 'neuromechanism', 'novel strategies', 'relating to nervous system', 'response', 'retinotopic', 'selective attention', 'transmission process', 'visual information', 'visual learning', 'visual memory', 'visual process', 'visual processing', 'visual stimulus']",NEI,PRINCETON UNIVERSITY,R01,2012,355624,-0.00478888459822869
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              Public Health Relevance The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8227997,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'public health relevance', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2012,403177,0.04441157971030666
"Perceptual Learning: Human vs. Optimal Bayesian Neural plasticity and perceptual learning are fundamental in the developmental stages of vision,  in attaining expertise in specialized perceptual tasks, and in recovery from brain injuries and low-  vision disorders. One important process in perceptual learning is the improvement in humans'  ability to use task-relevant (signal) information. Although there have been advances in the  understanding of the dynamics and algorithms mediating how humans optimize the selection of  task relevant visual information, little is known about how eye movement patterns vary with  practice and their impact in optimizing perceptual performance. Yet, in real world environments,  eye movements are a critical component of active vision as humans explore the visual scene to  make perceptual judgments. Understanding perceptual learning in human daily life requires  studying the mechanisms mediating the changes in the planning of eye movements with learning  and their contributions to optimizing perceptual performance. We hypothesize that two new  experimental paradigms with digitally designed visual stimuli, in conjunction with eye position  recording, and a newly developed foveated ideal observer and Bayesian learner will help  elucidate how humans learn to strategize their eye movements and the contributions of the  optimized sampling of the images to improvements in perceptual learning. The proposed work  will address the following questions: 1) Do humans use learned information about the statistical  properties of the visual stimuli and the requirements of the task at hand to strategize their eye  movements to optimize the foveal sampling of the visual scene and perceptual performance?; 2)  Do humans use knowledge of the varying resolution of their foveated visual system to optimally  learn to plan eye movements for a given set of visual stimuli and task?; 3) What are the  contributions of learning to strategize eye movements to the overall improvements in perceptual  performance in ecologically important tasks such as face recognition, object identification and  visual search?; 4) How do human fixation patterns and performance benefits from strategizing  eye movements compare to an optimal foveated observer and learner? The proposed work will  improve our understanding of the human neural algorithms mediating the dynamics of adult  perceptual learning during active vision for ecologically important tasks. The proposed  experimental protocols and theoretical developments will also provide a novel, powerful and  flexible framework with which other researchers can study eye movements and learning of  humans undergoing visual loss recovery as well as patients with learning disabilities. PUBLIC HEALTH RELEVANCE  The proposed work benefits public health by increasing our understanding of how  humans learn to move their eyes to potentially informative regions of the visual scene in  important daily tasks such as identifying faces or searching for objects. Thorough  understanding of these mechanisms in normal humans will allow identification of  learning anomalies in patients recovering from visual-loss or learning disabilities and  potentially develop tests to assess treatments.",Perceptual Learning: Human vs. Optimal Bayesian,8323947,R01EY015925,"['Accounting', 'Address', 'Adult', 'Algorithms', 'Amblyopia', 'Animals', 'Area', 'Behavior', 'Blindness', 'Brain Injuries', 'Brain imaging', 'Cells', 'Chin', 'Complex', 'Data', 'Detection', 'Development', 'Discrimination', 'Emotional', 'Emotions', 'Environment', 'Eye', 'Eye Movements', 'Face', 'Frequencies', 'Funding', 'Goals', 'Gold', 'Health', 'Human', 'Image', 'Individual', 'Infant', 'Instruction', 'Investigation', 'Judgment', 'Knowledge', 'Learning', 'Learning Disabilities', 'Life', 'Literature', 'Location', 'Machine Learning', 'Macular degeneration', 'Maps', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Monitor', 'Nature', 'Neuronal Plasticity', 'Nose', 'Oral cavity', 'Patients', 'Pattern', 'Perceptual learning', 'Performance', 'Positioning Attribute', 'Process', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysiology', 'Public Health', 'Recording of previous events', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Retinal', 'Retinitis Pigmentosa', 'Role', 'Sampling', 'Signal Transduction', 'Source', 'Spatial Distribution', 'Staging', 'Stimulus', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Work', 'active vision', 'area striata', 'design', 'experience', 'flexibility', 'gaze', 'ideal observer (Bayesian)', 'improved', 'interest', 'neurophysiology', 'novel', 'object recognition', 'oculomotor', 'prevent', 'relating to nervous system', 'sample fixation', 'tumor', 'visual information', 'visual performance', 'visual process', 'visual processing', 'visual search', 'visual stimulus']",NEI,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R01,2012,281126,-0.07818649845728383
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma  ABSTRACT  This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Comptutational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans.  PROJECT NARRATIVE  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,8209138,K99EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2012,89325,0.030606867963203277
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8473426,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2012,407051,0.04934044453097952
"Detection of Glaucoma Progression with Macular OCT Imaging     DESCRIPTION (provided by applicant): This application is a formal request for a career development award (K23) for an academic glaucoma specialist with a serious interest in the role of imaging in glaucoma using optical coherence tomography (OCT). This will allow the candidate to establish a clinical research program with the main goal of improving detection of glaucoma progression through macular imaging with spectral-domain OCT. By the time the proposed research is accomplished, the candidate will have preliminary data for continuing his research as an independent investigator and will have collected longitudinal structural and functional data in a group of advanced glaucoma patients that will serve as a platform for further improving detection of glaucoma progression with macular OCT imaging. The data will help the candidate provide preliminary results for a subsequent R01 that would potentially allow the PI to continue follow-up of the patients enrolled in the K23 award period.  I have a Master's of Science degree in Clinical Investigation under my belt and intend to deepen my skills in the field of imaging and biostatistics (to be used for enhancing and handling OCT images and for analyzing longitudinal data) by completing the proposed didactic program. By the end of the award period, I expect that I will have gained additional experience, knowledge, and mentorship required to prosper as an independent clinician-scientist in the field of glaucoma. My long-term goal is to carry out longitudinal studies of glaucoma patients where current and upcoming imaging and functional tests can be applied and their utility for detection of glaucoma progression can be investigated. I am confident that the combined skills and experience of my mentors will lead to a successful outcome for the proposed K award. I also envisage myself mentoring candidates like myself in future so that our collective knowledge and wisdom can be passed along to the next generation of aspiring clinician-scientists.  My objectives during the award period are as follows: 1) To develop an individual research program in glaucoma diagnostic imaging; 2) to successfully complete credited coursework in biomathematics, advanced biostatistics, computer vision (image processing), epidemiology, and ethical issues in research.  The main goal of the research component of this proposal is to better delineate the role of macular SD- OCT imaging for detection of glaucoma progression in advanced glaucoma. The specific aims through which this goal will be accomplished are as follows:  (1) To compare the performance of various global and regional macular measures to detect glaucoma.  The potential factors influencing the performance of various macular outcome measures will be explored. Such covariates include age, race, axial length, disc size, central corneal thickness,  OCT signal strength, and outer retinal thickness among others. I hypothesize that the thickness  of the outer retina (outer nuclear layer to retinal pigment epithelium-Bruch's membrane  complex) may be the most important factor explaining the measurement variability of the inner  retinal layer thickness (GCC or ganglion cell/inner plexiform layers).  (2) To determine and compare the utility of the candidate macular measures, detected through the first  aim, for detection of glaucoma progression in moderately advanced to severe glaucoma.  Moderately advanced to severe glaucoma will be defined as eyes with visual field mean  deviation worse than -6 dB or eyes with involvement of the central 10 degrees on the 24-2  visual field. It is widely accepted that measurement of the optic nerve head or RNFL parameters  in advanced glaucoma does not provide clinicians with much useful information. In contrast, the  central macular ganglion cells are the last to die in glaucoma. Macular imaging in advanced  glaucoma is directed towards this area where detection of change may still be possible. I  hypothesize that macular OCT parameters are valid structural outcome measures (biomarkers)  that can be used to follow the course of the disease in advanced glaucoma and that such  measures are significantly correlated with changes in the central visual field. Changes in the  macular measures over time will be first correlated with the corresponding visual field change  (functional progression) over time in eyes with moderately advanced to severe glaucoma. The  utility of the best candidate macular measures for predicting subsequent glaucoma progression  will also be explored and compared. I hypothesize that there may be a lag period between  progressive loss of macular ganglion cells and subsequent visual field progression in advanced  glaucoma, and therefore, detection of worsening in one or more macular outcome measures  can be used as a proxy for subsequent visual field progression.  Collectively, these studies will provide a solid foundation for better understanding and integration of macular OCT imaging in the care of glaucoma patients. Timely detection of glaucoma progression in the later stages can significantly reduce visual disability and blindness through earlier aggressive treatment and will potentially reduce glaucoma's financial burden to society.        PUBLIC HEALTH RELEVANCE: Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.              Detection of glaucoma progression remains a challenging task in eyes demonstrating significant damage. Even small amounts of progression in advanced glaucoma can have important consequences with regard to patient's visual function and quality of life. The results of the proposed study will potentially lead to more effective and earlier detection of glaucoma progression and will allow ophthalmologists to step up treatment in a timely manner. This will in turn result in less visual morbidity and reduced blindness from glaucoma, which is projected to cause more than 10 million cases of legal blindness around the world in 2020.            ",Detection of Glaucoma Progression with Macular OCT Imaging,8353379,K23EY022659,"['Age', 'Area', 'Award', 'Biological Markers', 'Biometry', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Caring', 'Clinical Research', 'Clinical Trials', 'Complement', 'Complex', 'Computer Vision Systems', 'Cornea', 'Data', 'Detection', 'Diagnosis', 'Diagnostic Imaging', 'Disease', 'Early Diagnosis', 'Enrollment', 'Epidemiology', 'Ethical Issues', 'Evaluation', 'Eye', 'Foundations', 'Functional Imaging', 'Future', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Image Analysis', 'Individual', 'Inner Plexiform Layer', 'K-Series Research Career Programs', 'Knowledge', 'Lead', 'Legal Blindness', 'Length', 'Longitudinal Studies', 'Master of Science', 'Measurement', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Morbidity - disease rate', 'Nerve Fibers', 'Noise', 'Ophthalmologist', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Outcome Measure', 'Patients', 'Performance', 'Process', 'Proxy', 'Quality of life', 'Race', 'Research', 'Research Personnel', 'Retinal', 'Role', 'Scientist', 'Signal Transduction', 'Societies', 'Solid', 'Specialist', 'Staging', 'Structure of retinal pigment epithelium', 'Testing', 'Thick', 'Time', 'Vision', 'Visual', 'Visual Fields', 'advanced disease', 'biomathematics', 'central visual field', 'disability', 'experience', 'follow-up', 'ganglion cell', 'image processing', 'improved', 'interest', 'macula', 'next generation', 'programs', 'retina outer nuclear layer', 'retinal nerve fiber layer', 'skills']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,K23,2012,229139,0.07053918201806905
"CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes DESCRIPTION (provided by applicant): The complexity of natural images is potentially enormous: the number of possible images that can be described by a smallish (100 by 100 pixels) picture is practically infinite (10000256), more than all the images the human race has ever witnessed during its entire existence. How can any system process input data of this magnitude of dimensions and interpret/understand it in terms of the estimated 200,000 objects in the world, their spatial layouts, and scene structures? Yet, this is a task that human visual systems routinely perform in a fraction of a second. The secret must lie in the fact that natural images are highly redundant, living in a restricted space inside this universe of almost infinite possibilities, and that mammalian visual systems have discovered and exploited this fact. In particular, we conjecture that neurons and populations are tuned to the statistical structure of natural images, building on previous work showing, for example, that sparse coding ideas can help predict receptive field properties of 'simple cells' in the visual cortex. This proposal has three stages. Firstly, we will perform a statistical analysis of natural images to classify and model the types of visual patches that appear. This will result in a stimulus dictionary, which will be used as stimuli to investigate the tuning properties of neurons and neuronal populations, and a visual concept dictionary which will be used to make predictions for the tuning properties. Secondly, we will perform multielectrode neurophysiological investigation of the tuning properties of neurons, and neuron populations, at different levels of the visual cortex in response to the stimulus dictionary. Thirdly, we will perform data analysis to model the tuning properties of neurons and populations using a combination of model-driven, which assumes that neurons are tuned to statistical properties of images, and data-driven approaches which can be thought of as learning 'neural visual concepts' directly from the neuron's response to the stimuli. Our theoretical approach - for learning the stimuli dictionary, the visual concepts, and performing data analysis - is based on statistical and machine learning techniques. These assume a hierarchical compositional structure for the data which offers the possibility of taming the complexity of natural images and is also consistent with the known hierarchical structure of the visual cortex. Intellectual merit: This research will help understand the structure of natural images, determine models for the tuning properties of neurons in the visual cortex, and develop novel data analysis techniques. It has the potential to significantly advance our understanding of the statistical structures of natural images and the neural encoding of these structures, including the population level. This will lead to greater understanding of the visual cortex and also help the development of computer vision systems. Broader impacts: This project is interdisciplinary in nature and should have broad impact in multiple disciplines: neuroscience and biological vision, statistical neural data analysis, computer vision, and machine learning. Understanding neuronal properties in the visual cortex is a pre-requisite to the clinical enterprise of developing therapeutic methods and prosthetic devices for the visually impaired. The proposed research program will help facilitate a new graduate program in Computational and Cognitive Neuroscience at UCLA, an inter-college undergraduate minor in Neural Computation at CMU that the investigators are developing at their respective universities. The investigators also plan to organize workshops in NIPS, COSYNE, as well as to integrate their research into both undergraduate and graduate curriculum in their respective universities. This work will also affect undergraduate students at other colleges, by a summer undergraduate training program in Pittsburgh, another at CMU's Qatar campus. In addition, we will propose a workshop and summer school at IPAM (UCLA). We anticipate that this research will lead to invited lectures, peer reviewed publications and, if successful, will have national and international impact. The PIs have good track record in involving undergraduates, including women and minorities, in their NSF-sponsored research, and will continue to endeavor in the training of the next generation of computational neuroscientists. This project will lead to greater understanding of neural mechanisms and coding strategies in the primate  visual cortex. Such knowledge is fundamental to understanding human visual functions and is critical to the  clinical enterprise of developing better diagnostic tools, therapeutic methods, and prosthetic devices for the  visually impaired.",CRCNS: Neural Reprensentation of Hierarchical Visual Concepts in Natural Scenes,8255663,R01EY022247,"['Affect', 'Appearance', 'Biological', 'Cells', 'Clinical', 'Code', 'Computer Vision Systems', 'Data', 'Data Analyses', 'Development', 'Diagnostic', 'Dictionary', 'Dimensions', 'Discipline', 'Educational Curriculum', 'Educational workshop', 'Electrodes', 'Entropy', 'Graph', 'Human', 'Image', 'International', 'Intuition', 'Investigation', 'Knowledge', 'Lead', 'Learning', 'Letters', 'Life', 'Machine Learning', 'Measures', 'Methods', 'Minor', 'Minority', 'Modeling', 'Nature', 'Neurons', 'Neurosciences', 'Noise', 'Pattern', 'Peer Review', 'Physiological', 'Population', 'Primates', 'Probability', 'Process', 'Property', 'Prosthesis', 'Publications', 'Qatar', 'Race', 'Research', 'Research Personnel', 'Sampling', 'Schools', 'Staging', 'Statistical Models', 'Stimulus', 'Structure', 'Students', 'System', 'Techniques', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Training', 'Training Programs', 'Universities', 'Vision', 'Visual', 'Visual Cortex', 'Visual system structure', 'Vocabulary', 'Woman', 'Work', 'base', 'cognitive neuroscience', 'college', 'computational neuroscience', 'design', 'devices for the visually impaired', 'isophosphamide mustard', 'lectures', 'neuromechanism', 'neurophysiology', 'next generation', 'novel', 'programs', 'receptive field', 'relating to nervous system', 'research study', 'response', 'statistics', 'tool', 'visual stimulus']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2011,372452,-0.018646730114092697
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.          PROJECT NARRATIVE:  There is a considerable need for improved educational software for mathematics in general, but the problem of  quality educational software materials for the blind and visually impaired is particularly acute. A weak  mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or  even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of  science, technology, engineering and mathematics. Through previous federally-supported research, Quantum  Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI)  tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power  and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,8055353,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Health', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2011,394165,0.009656924022307693
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,8142000,R01EY016093,"['Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Peripheral', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,1141143,0.05554573358229701
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,8139754,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2011,219694,0.032187052097077076
"Perceptual Learning: Human vs. Optimal Bayesian    DESCRIPTION (provided by applicant): Neural plasticity and perceptual learning are fundamental in the developmental stages of vision, in attaining expertise in specialized perceptual tasks, and in recovery from brain injuries and low- vision disorders. One important process in perceptual learning is the improvement in humans' ability to use task-relevant (signal) information. Although there have been advances in the understanding of the dynamics and algorithms mediating how humans optimize the selection of task relevant visual information, little is known about how eye movement patterns vary with practice and their impact in optimizing perceptual performance. Yet, in real world environments, eye movements are a critical component of active vision as humans explore the visual scene to make perceptual judgments. Understanding perceptual learning in human daily life requires studying the mechanisms mediating the changes in the planning of eye movements with learning and their contributions to optimizing perceptual performance. We hypothesize that two new experimental paradigms with digitally designed visual stimuli, in conjunction with eye position recording, and a newly developed foveated ideal observer and Bayesian learner will help elucidate how humans learn to strategize their eye movements and the contributions of the optimized sampling of the images to improvements in perceptual learning. The proposed work will address the following questions: 1) Do humans use learned information about the statistical properties of the visual stimuli and the requirements of the task at hand to strategize their eye movements to optimize the foveal sampling of the visual scene and perceptual performance?; 2) Do humans use knowledge of the varying resolution of their foveated visual system to optimally learn to plan eye movements for a given set of visual stimuli and task?; 3) What are the contributions of learning to strategize eye movements to the overall improvements in perceptual performance in ecologically important tasks such as face recognition, object identification and visual search?; 4) How do human fixation patterns and performance benefits from strategizing eye movements compare to an optimal foveated observer and learner? The proposed work will improve our understanding of the human neural algorithms mediating the dynamics of adult perceptual learning during active vision for ecologically important tasks. The proposed experimental protocols and theoretical developments will also provide a novel, powerful and flexible framework with which other researchers can study eye movements and learning of humans undergoing visual loss recovery as well as patients with learning disabilities.      PUBLIC HEALTH RELEVANCE: The proposed work benefits public health by increasing our understanding of how humans learn to move their eyes to potentially informative regions of the visual scene in important daily tasks such as identifying faces or searching for objects. Thorough understanding of these mechanisms in normal humans will allow identification of learning anomalies in patients recovering from visual-loss or learning disabilities and potentially develop tests to assess treatments.          PUBLIC HEALTH RELEVANCE  The proposed work benefits public health by increasing our understanding of how  humans learn to move their eyes to potentially informative regions of the visual scene in  important daily tasks such as identifying faces or searching for objects. Thorough  understanding of these mechanisms in normal humans will allow identification of  learning anomalies in patients recovering from visual-loss or learning disabilities and  potentially develop tests to assess treatments.",Perceptual Learning: Human vs. Optimal Bayesian,8123224,R01EY015925,"['Accounting', 'Address', 'Adult', 'Algorithms', 'Amblyopia', 'Animals', 'Area', 'Behavior', 'Blindness', 'Brain Injuries', 'Brain imaging', 'Cells', 'Chin', 'Complex', 'Data', 'Detection', 'Development', 'Discrimination', 'Emotional', 'Emotions', 'Environment', 'Eye', 'Eye Movements', 'Face', 'Frequencies', 'Funding', 'Goals', 'Gold', 'Human', 'Image', 'Individual', 'Infant', 'Instruction', 'Investigation', 'Judgment', 'Knowledge', 'Learning', 'Learning Disabilities', 'Life', 'Literature', 'Location', 'Machine Learning', 'Macular degeneration', 'Maps', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Monitor', 'Nature', 'Neuronal Plasticity', 'Nose', 'Oral cavity', 'Patients', 'Pattern', 'Perceptual learning', 'Performance', 'Positioning Attribute', 'Process', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysiology', 'Public Health', 'Recording of previous events', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Retinal', 'Retinitis Pigmentosa', 'Role', 'Sampling', 'Signal Transduction', 'Source', 'Spatial Distribution', 'Staging', 'Stimulus', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Work', 'active vision', 'area striata', 'design', 'experience', 'flexibility', 'gaze', 'ideal observer (Bayesian)', 'improved', 'interest', 'neurophysiology', 'novel', 'object recognition', 'oculomotor', 'prevent', 'public health relevance', 'relating to nervous system', 'sample fixation', 'tumor', 'visual information', 'visual performance', 'visual process', 'visual processing', 'visual search', 'visual stimulus']",NEI,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R01,2011,281126,-0.09127728657335672
"Vision Without Sight: Exploring the Environment with a Portable Camera    DESCRIPTION (provided by applicant): As computer vision object recognition algorithms improve in accuracy and speed, and computers become more powerful and compact, it is becoming increasingly practical to implement such algorithms on portable devices such as camera-enabled cell phones. This ""mobile vision"" approach allows normally sighted users to identify objects, signs, places and other features in the environment simply by snapping a photo and waiting a few seconds for the results of the object recognition analysis. The approach holds great promise for blind or visually impaired (VI) users, who may have no other means of identifying important features that are undetectable by non-visual cues. However, in order for the approach to be practical for VI users, the interaction between the user and the environment using the camera must be properly facilitated. For instance, since the user may not know in advance which direction to point the camera towards a desired target, he or she must be able to pan the camera left and right to search for it, and receive rapid feedback whenever it is detected. Drawing on past experience of the PI and his collaborators on object recognition systems for VI users, we propose to study the use of mobile vision technologies for exploring features in the environment, specific examining the process of discovering these features and obtaining guidance towards them. Our main objectives are to investigate the strategies adopted by users of these technologies to expedite the exploration process, devise and test maximally effective user interfaces consistent with these strategies, and to assess and benchmark the efficiency of the technologies. The result will be a set of minimum design standards that will specify the system performance parameters, the user interface functionality and the operational strategies necessary for any mobile vision object recognition system for VI users.      PUBLIC HEALTH RELEVANCE: The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population              The ability to locate and identify objects, places and other features in the environment is taken for granted every day by the sighted, but this fundamental capability is missing or severely degraded in the approximately 10 million Americans with significant vision impairments and a million who are legally blind. The proposed research would investigate how computer vision object recognition technologies, which are now being implemented on mobile vision devices such as cell phones but are typically designed for normally sighted users, could be modified and harnessed to meet the special needs of blind and visually impaired persons. Such research could lead to new technologies to dramatically improve independence for this population            ",Vision Without Sight: Exploring the Environment with a Portable Camera,8097202,R21EY021643,"['Address', 'Adopted', 'Algorithms', 'American', 'Benchmarking', 'Cellular Phone', 'Computer Hardware', 'Computer Vision Systems', 'Computers', 'Cues', 'Development', 'Devices', 'Environment', 'Feedback', 'Glosso-Sterandryl', 'Goals', 'Goggles', 'Grant', 'Image Analysis', 'Impairment', 'Lead', 'Learning', 'Left', 'Location', 'Performance', 'Population', 'Printing', 'Process', 'Research', 'Self-Help Devices', 'Specific qualifier value', 'Speed', 'System', 'Task Performances', 'Technology', 'Testing', 'Time', 'Touch sensation', 'Training', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'blind', 'design', 'experience', 'improved', 'insight', 'interest', 'legally blind', 'meetings', 'new technology', 'object recognition', 'operation', 'usability', 'visual feedback']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R21,2011,205070,0.07460426566593067
"New Techniques for Measuring Volumetric Structural Changes in Glaucoma    DESCRIPTION (provided by applicant) This K99/R00 application supports additional research training in computational mathematics and computer vision which will enable Dr. Madhusudhanan Balasubramanian-the applicant, to become an independent multidisciplinary investigator in computational ophthalmology. Specifically, in the K99 training phase of this grant, Dr. Balasubramanian will train at UC San Diego under the direction of Linda Zangwill PhD, an established glaucoma clinical researcher in the Department of Ophthalmology, as well as a team of co- mentors, including, Dr. Michael Holst from the Department of Mathematics and co-director for the Center for Computational Mathematics, and co-director of the Computational Science, Mathematics and Engineering and Dr. David Kriegman from Computer Science and Engineering. Training will be conducted via formal coursework, hands-on lab training, mentored research, and progress review by an advisory committee, visiting collaborating researchers and regular attendance at seminars and workshops. The subsequent R00 independent research phase involves applying Dr. Balasubramanian's newly acquired computational techniques to the difficult task of identifying glaucomatous change over time from optical images of the optic nerve head and retinal nerve fiber layer.  A documented presence of progressive optic neuropathy is the best gold standard currently available for glaucoma diagnosis. Confocal Scanning Laser Ophthalmoscope (CSLO) and Spectral Domain Optical Coherence Tomography (SD-OCT) are two of the optical imaging instruments available for monitoring the optic nerve head health in glaucoma diagnosis and management. Currently, several statistical and computational techniques are available for detecting localized glaucomatous changes from the CSLO exams. SD-OCT is a new generation ophthalmic imaging instrument based on the principle of optical interferometry. In contrast to the CSLO technology, SDOCT can resolve retinal layers from the internal limiting membrane (ILM) through the Bruch's membrane and can capture the 3-D architecture of the optic nerve head at a very high resolution. These high-resolution, high-dimensional volume scans introduce a new level of data complexity not seen in glaucoma progression analysis before and therefore, powerful (high-performance) computational techniques are required to fully utilize the high precision retinal measurements for glaucoma diagnosis. The central focus of this application in the K99 mentored phase of the application will be in 1) developing computational and statistical techniques for detecting structural glaucomatous changes in various retinal layers from the SDOCT scans, and 2) developing a new avenue of research in glaucoma management where in strain in retinal layers will be estimated non-invasively to characterize glaucomatous progression. In the R00 independent phase, the specific aims focus on developing 1) statistical and computational techniques for detecting volumetric glaucomatous change over time using 3-D SD-OCT volume scans and 2) a computational framework to estimate full-field 3-D volumetric strain from the standard SD-OCT scans.      PUBLIC HEALTH RELEVANCE:  Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.            Detecting the onset and progression of glaucomatous changes in the eye is central to glaucoma diagnosis and management. This multidisciplinary project focuses on developing powerful (high-performance) computational, mathematical, and statistical techniques for detecting volumetric glaucomatous changes from the Confocal Scanning Laser Ophthalmoscopy (CSLO) scans and volumetric Spectral Domain Optical Coherence Tomography (SD-OCT) scans of the optic nerve head. In addition, the Principal Investigator of this proposal will receive extensive training in the areas of computational mathematics and computer vision to augment and strengthen his multidisciplinary expertise essential to execute the proposed specific aims.         ",New Techniques for Measuring Volumetric Structural Changes in Glaucoma,7871151,K99EY020518,"['3-Dimensional', 'Address', 'Advisory Committees', 'Affect', 'Architecture', 'Area', 'Biology', 'Blindness', 'Brain imaging', 'Bruch&apos', 's basal membrane structure', 'Cardiology', 'Clinic', 'Clinical', 'Complex', 'Computational Science', 'Computational Technique', 'Computer Vision Systems', 'Confocal Microscopy', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease Progression', 'Doctor of Philosophy', 'Educational workshop', 'Elements', 'Engineering', 'Eye', 'Functional disorder', 'Gastroenterology', 'Generations', 'Genetic screening method', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Health', 'Image', 'Interferometry', 'Lasers', 'Lead', 'Left', 'Mathematics', 'Measurement', 'Measures', 'Medicine', 'Membrane', 'Mentors', 'Methodology', 'Modeling', 'Monitor', 'National Eye Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'Onset of illness', 'Ophthalmology', 'Ophthalmoscopes', 'Ophthalmoscopy', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Patients', 'Performance', 'Phase', 'Principal Investigator', 'Recommendation', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Retinal', 'Scanning', 'Science', 'Sensitivity and Specificity', 'Structure', 'Surface', 'Techniques', 'Technology', 'Time', 'Training', 'Treatment Effectiveness', 'Treatment Protocols', 'Validation', 'Vision', 'Vision research', 'Visit', 'analytical tool', 'base', 'bioimaging', 'blood flow measurement', 'cancer imaging', 'computer framework', 'computer science', 'cost', 'diagnostic accuracy', 'improved', 'instrument', 'medical specialties', 'multidisciplinary', 'optic nerve disorder', 'optical imaging', 'prevent', 'programs', 'retinal nerve fiber layer', 'symposium', 'time use']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2011,89922,0.030768584602221802
"Perception of Tactile Graphics    DESCRIPTION (provided by applicant): The broad objective of the proposed research is to answer the following question: why are tactile graphics difficult to understand? People with normal vision can easily recognize line drawings of objects. However, both blind and sighted people find it very difficult to recognize the same drawings when they are presented as tactile images. For blind people, tactile graphics are the only solution for accessing information in visual diagrams and illustrations found in textbooks. Consequently, the results of the proposed research will be used to improve the production of tactile graphics so that they are better understood by blind people. The specific aims of this project are to: 1) explore how the complexity of tactile images affects perception, 2) determine the effects of spatial and temporal integration on perception of tactile images, and 3) investigate how well people can recognize tactile images of objects embedded in backgrounds. The general methodology of the proposed experiments is to present participants with tactile images and to have them draw what they perceive the images to be. Blind individuals will draw tactile images using special paper and a stylus. The experimenters will evaluate the drawings by using a quantitative measure that computes a distance score reflecting the discrepancy between the original image and the participant's drawing. In the first study, participants will feel tactile stimuli of varying complexity, from simple lines in different orientations to complex depictions of objects. The second study will determine the limitations of tactile perceptual integration by limiting either the spatial or temporal window over which participants feel the image. Participants will either view or feel images through apertures of various sizes (spatial window) or they will have a limited amount of time to view or feel the images (temporal window). In the third study, participants will feel tactile images of objects embedded in simple backgrounds. This research will impact several areas of study, including computer vision, human object and scene recognition, and low vision rehabilitation.      PUBLIC HEALTH RELEVANCE: Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.              Relevance The proposed research seeks to improve the translation of visual graphics into tactile graphics for use by visually-impaired individuals. By making the information in visual graphics more accessible, this research will improve educational services for people with low vision. This work will also facilitate the development of better visual-to-tactile substitution technologies.            ",Perception of Tactile Graphics,8060259,F32EY019622,"['Affect', 'Area', 'Categories', 'Child', 'Complex', 'Computer Vision Systems', 'Development', 'Devices', 'Disadvantaged', 'Education', 'Elements', 'Goals', 'Grouping', 'Human', 'Image', 'India', 'Individual', 'Link', 'Measures', 'Methodology', 'Methods', 'Names', 'Nature', 'Paper', 'Participant', 'Perception', 'Population', 'Production', 'Psychophysics', 'Rehabilitation therapy', 'Research', 'Science', 'Sensory', 'Services', 'Shapes', 'Solutions', 'Stimulus', 'Swelling', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Vision', 'Visual', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'blind', 'braille', 'improved', 'object recognition', 'research study', 'sight for the blind', 'skills', 'tactile vision substitution system', 'two-dimensional', 'vision development', 'visual process', 'visual processing']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2011,48398,0.0239273149150125
"Neural and Behavioral Interactions Between Attention, Perception, and Learning    DESCRIPTION (provided by applicant): The overarching goal of this research is to characterize how perception and memory interact, in terms of both the learning mechanisms that help transform visual experience into memory, and the intentional mechanisms that regulate this transformation. The specific goal of this proposal is to test the hypothesis that incidental learning about statistical regularities in vision (visual statistical learning) is limited to selectively attend visual information, and that this behavioral interaction arises because of how selective attention modulates neural interactions between human visual and memory systems. We propose a two-stage framework in which selective attention to a high-level visual feature/category increases neural interactions between regions of occipital cortex that represent low-level features and the region of inferior temporal cortex (IT) that represents the attended feature/category, and in turn between this IT region and medial temporal lobe (MTL) sub regions involved in visual learning and memory. In addition to assessing how feature-based selective attention influences learning at a behavioral level, we will use functional magnetic resonance imaging to assess how attention influences evoked neural responses in task-relevant brain regions, as well as neural interactions between these regions in the background of ongoing tasks. We will develop an innovative approach for studying neural interactions in which evoked responses and global noise sources are scrubbed from the data and regional correlations are assessed in the residuals. This background connectivity approach provides a new way to study how intentional goals affect perception and learning. Aim 1 examines the first stage of our framework, testing: how selective attention modulates background connectivity between IT and occipital cortex, where in retinotopic visual cortex this modulation occurs, and how these changes are controlled by frontal and parietal cortex. Aim 2 examines the second stage of our framework, first establishing the role of the MTL in visual statistical learning, and then testing: how selective attention modulates interactions between IT and the MTL, where in cortical and hippocampal sub regions of the MTL this modulation occurs, and how these changes facilitate incidental learning about statistical regularities and later retrieval of this knowledge. In sum, we relate behavioral interactions between selective attention and learning to neural interactions between the mechanisms that represent visual features and those that learn about their relations. This proposal addresses several key issues in the field, including: how attention modulates the MTL, how feature-based attention is controlled, whether different neural mechanisms support rapid versus long-term visual learning, how tasks and goals are represented, and how attention and memory retrieval are related. This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery and rehabilitation of visual function following eye disease, injury, or brain damage.      PUBLIC HEALTH RELEVANCE: This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.           This research will improve our understanding of how humans learn from visual experience, and how visual processing is in turn influenced by learning. These advances will shed light on the plasticity that occurs during development and during the recovery from visual impairment caused by eye disease, injury, or brain damage. The behavioral tasks that we develop to enhance learning with attention will inform practices for rehabilitating visual function, and the methods that we develop to study neural interactions during tasks will lead to new approaches for diagnosing disorders of visual processing.         ","Neural and Behavioral Interactions Between Attention, Perception, and Learning",8162573,R01EY021755,"['Address', 'Affect', 'Architecture', 'Area', 'Attention', 'Behavioral', 'Brain', 'Brain Injuries', 'Brain region', 'Categories', 'Cognition', 'Cognitive', 'Cognitive Science', 'Data', 'Development', 'Diagnosis', 'Disease', 'Event', 'Exhibits', 'Eye diseases', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Hippocampus (Brain)', 'Human', 'Individual', 'Inferior', 'Injury', 'Knowledge', 'Lead', 'Learning', 'Light', 'Link', 'Machine Learning', 'Maps', 'Medial', 'Memory', 'Methods', 'Mind', 'Modeling', 'Nature', 'Neurosciences', 'Noise', 'Occipital lobe', 'Parietal', 'Parietal Lobe', 'Perception', 'Physiological', 'Positioning Attribute', 'Process', 'Property', 'Recovery', 'Rehabilitation therapy', 'Research', 'Residual state', 'Resolution', 'Rest', 'Retrieval', 'Role', 'Sensory Process', 'Short-Term Memory', 'Signal Transduction', 'Source', 'Staging', 'Stimulus', 'Sum', 'Synapses', 'System', 'Temporal Lobe', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'base', 'cognitive neuroscience', 'experience', 'extrastriate visual cortex', 'frontal lobe', 'hippocampal subregions', 'improved', 'information processing', 'innovation', 'memory retrieval', 'neuroimaging', 'neuromechanism', 'novel strategies', 'relating to nervous system', 'response', 'retinotopic', 'selective attention', 'transmission process', 'visual information', 'visual learning', 'visual memory', 'visual process', 'visual processing', 'visual stimulus']",NEI,PRINCETON UNIVERSITY,R01,2011,362154,-0.0022490311457033915
"A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians    DESCRIPTION (provided by applicant): A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians Abstract Project Summary Urban intersections are among the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard cell phone, to provide real-time feedback. Building on our past work on a prototype ""Crosswatch"" system that uses computer vision algorithms to find crosswalks and Walk lights, we will greatly enhance the functionality of the system with information about the intersection layout and the identity of its connecting streets, the presence of stop signs, one-way signs and other controls indicating right-of-way, and timing information integrated from Walk/Don't Walk lights, countdown timers and other traffic lights. The system will convey intersection information, and will actively guide the user to align himself/herself with crosswalks, using a combination of synthesized speech and audio tones. We will conduct human factors studies to optimize the system functionality and the configuration of the user interface, as well as develop interactive training applications to equip users with the skills to better use the system. These training applications, implemented as additional cell phone software to complement the intersection system, will train users to hold the camera horizontal and forward and to minimize veer when traversing a crosswalk. The intersection analysis and training software will be made freely available for download onto popular cell phones (such as iPhone, Android or Symbian models). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function.      PUBLIC HEALTH RELEVANCE: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.              The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.            ",A Cell Phone-Based Street Intersection Analyzer for Visually Impaired Pedestrians,8042468,R01EY018345,"['Address', 'Adoption', 'Algorithms', 'American', 'Cellular Phone', 'Complement', 'Complement component C4', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Crowding', 'Custom', 'Data', 'Development', 'Devices', 'Equipment', 'Face', 'Feedback', 'Glosso-Sterandryl', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Length', 'Light', 'Location', 'Mainstreaming', 'Modeling', 'Modification', 'Names', 'Process', 'Reading', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Running', 'Safety', 'Self-Help Devices', 'Signal Transduction', 'Speech', 'Speed', 'System', 'Techniques', 'Technology', 'Telephone', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'abstracting', 'base', 'blind', 'consumer product', 'cost', 'design', 'improved', 'interest', 'legally blind', 'meter', 'prototype', 'sensor', 'skills', 'trafficking', 'volunteer', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2011,403177,0.044524994233852824
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,8133823,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'digital', 'experience', 'falls', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition', 'web services']",NEI,BLINDSIGHT CORPORATION,R44,2011,776548,0.04934044453097952
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8109271,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2011,205267,-0.014745570191327353
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,8037680,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2011,513228,0.05343166431183283
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.           PROJECT NARRATIVE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,8043275,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'public health relevance', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'stem', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2010,394165,0.009656924022307693
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7904837,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,1196495,0.05554573358229701
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7911700,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2010,226559,0.032187052097077076
"Towards cortical visual prosthetics    Description (provided by applicant): Visual object recognition is crucial for most everyday tasks including face identification, reading and navigation. In spite of the massive increase in computational power over the last two decades, a 3-year-old still outperforms the most sophisticated algorithms even in simple recognition tasks. Understanding the computations performed by the human visual system to recognize objects will have profound implications not only to understand the functions (and malfunction) of the cerebral cortex but also for developing visual prosthetic devices for the visually impaired. We combine neurophysiology, electrical stimulation and tools from machine learning to further our understanding of the neuronal circuits, algorithms and computations performed by the human visual system to perform visual pattern recognition. In the vast majority of visually impaired or blind people, the problems originate at the level of the retina while the visual cortex remains unimpaired. Our proposal constitutes a proof- of-principle approach towards developing visual prosthetic devices that rely on electrical stimulation of visual cortex. The specific aims of this proposal are designed to test the possibility of decoding and recoding information in visual cortex: (1) Read-out of visual information from human visual cortex on line (2) Write-in of visual information in human visual cortex. We take advantage of a rare opportunity to study the human brain at high spatial and temporal resolution by studying patients who have electrodes implanted for clinical reasons. Our electrophysiological recordings provide us with a unique view of the human temporal lobe circuitry and allow us to test the feasibility of cortical visual prosthetics in behaving human subjects. PUBLIC HEALTH RELEVANCE: Towards cortical visual prosthetics one of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.           7. Project Narrative: Towards cortical visual prosthetics  One of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.",Towards cortical visual prosthetics,7903931,R21EY019710,"['3 year old', 'Action Potentials', 'Algorithms', 'Animals', 'Auditory', 'Brain', 'Categories', 'Cerebral cortex', 'Clinical', 'Code', 'Computer software', 'Data', 'Data Quality', 'Detection', 'Development', 'Devices', 'Electric Stimulation', 'Electrodes', 'Epilepsy', 'Face', 'Goals', 'Human', 'Implanted Electrodes', 'Inferior', 'Limb structure', 'Macaca', 'Machine Learning', 'Methodology', 'Methods', 'Monkeys', 'Neurons', 'Output', 'Patients', 'Perception', 'Physiological', 'Prosthesis', 'Reading', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resolution', 'Retina', 'Sensory', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'System', 'Temporal Lobe', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Visual Pattern Recognition', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Wireless Technology', 'Writing', 'base', 'brain machine interface', 'design', 'devices for the visually impaired', 'human data', 'human subject', 'improved', 'interest', 'neural prosthesis', 'neurophysiology', 'object recognition', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'retinal prosthesis', 'tool', 'vision development', 'visual information']",NEI,BOSTON CHILDREN'S HOSPITAL,R21,2010,212644,0.0561359569535982
"Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually There are more than 10 million blind and visually impaired people living in America today. Recent technology developments in computer vision, digital cameras, and portable computers make it possible to assist these individuals by developing camera-based products that combine computer vision technology with other existing products.  Although a number of reading assistants have been designed specifically for people who are blind or visually impaired, reading text from complex backgrounds or non-flat surfaces is very challenging and has not yet been successfully addressed. Many everyday tasks involve these challenging conditions, such as reading instructions on vending machines, titles of books aligned on a shelf, instructions on medicine bottles or labels on soup cans.  This proposal focuses on the development of new computer vision algorithms to recognize text from complex backgrounds: 1) from backgrounds with multiple different colors (e.g .. the titles of books lined up on a shelf) and 2) from non-flat surfaces (e.g .. labels on medicine bottles or soup cans). The newly developed computer vision techniques will be integrated with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed by a portable computer (PDA or cell phone), while the speech display will be outputted via mini speakers, earphones, or Bluetooth device. A practical reading system prototype will be produced to read text from complex backgrounds and non-flat surfaces. The system will be cost-effective since it requires only a head mounted camera (<US$100 for 1M resolution), a wearable computer (<US$300), and two mini-speakers or earphones. The price of ""ReadIRlS"" [74] OCR software is under $150 and the ""TextAloud"" speech synthesis software is about $30 [75].  This project will be executed over two years at the City College of New York (CCNY) and Lighthouse International, New York. CCNY, located in the Harlem neighborhood of New York City, is designated as both a Minority Institution and a Hispanic-serving Institution (37% Hispanic and 27% African American). Lighthouse International is a leading non-profit organization dedicated to preserving vision and to providing critically needed vision and rehabilitation services to help people of all ages overcome the challenges of vision loss. During the two years, we will 1) develop new algorithms to recognize text from backgrounds with multiple different colors; 2) develop new algorithms to recognize text from non-flat surfaces; and 3) develop a cost-effective prototype reading system for blind users by integrating with off-the-shelf optical character recognition (OCR) and speech-synthesis software products. The effectiveness of the prototype and algorithms will be evaluated by people with normal vision and people with vision impairment. A database of text on complex backgrounds (multiple colors and non-flat surfaces) will be created for algorithm and system evaluation. The database will be made available to research communities in the areas of computer vision and vision rehabilitation science. In summary, this effort will provide a research-based foundation to inform the design of next generation reading assistants for blind persons, as well as produce a practical prototype to help the blind user read text from complex backgrounds in real-world environments. PROJECT NARRATIVE  The goal of the proposed research is to develop new computer vision algorithms for camera-based text recognition from complex backgrounds and non-flat surfaces, as well as produce a practical reading system prototype in combination with off-the-shelf  optical character recognition (OCR) and speech-synthesis software products, to help blind or visually impaired people read instructions on vending machines, titles of books aligned on a shelf, labels on medicine bottles or soup cans, etc. Visual information will be captured via a head-mounted camera (on sunglasses or hat) and analyzed in realtime through a portable computer, such as a mini laptop or a personal digital assistant (PDA). The speech display will be outputted via mini speakers, earphones, or Bluetooth device.",Camera-based Text Recognition from Complex Backgrounds for the Blind or Visually,7977496,R21EY020990,"['Address', 'African American', 'Age', 'Algorithms', 'Americas', 'Area', 'Blindness', 'Books', 'Cellular Phone', 'Cities', 'Color', 'Communities', 'Complex', 'Computer Systems Development', 'Computer Vision Systems', 'Computer software', 'Computers', 'Databases', 'Development', 'Devices', 'Effectiveness', 'Environment', 'Evaluation', 'Event', 'Facial Expression Recognition', 'Foundations', 'Goals', 'Grant', 'Head', 'Hispanics', 'Image', 'Impairment', 'Individual', 'Institution', 'Instruction', 'International', 'Label', 'Letters', 'Life', 'Mails', 'Marketing', 'Medicine', 'Methods', 'Minority', 'Neighborhoods', 'New York', 'New York City', 'Nonprofit Organizations', 'Output', 'Personal Digital Assistant', 'Price', 'Printing', 'Reading', 'Rehabilitation therapy', 'Research', 'Research Project Grants', 'Resolution', 'Running', 'Scientist', 'Shapes', 'Solutions', 'Speech', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Thick', 'Time', 'United States National Institutes of Health', 'Vertebral column', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Writing', 'base', 'blind', 'college', 'computer generated', 'computer human interaction', 'cost', 'design', 'digital', 'experience', 'laptop', 'next generation', 'optical character recognition', 'prototype', 'rehabilitation science', 'rehabilitation service', 'research and development', 'sunglasses', 'technology development', 'visual information']",NEI,CITY COLLEGE OF NEW YORK,R21,2010,190000,0.05502914670116199
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7799708,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,427932,0.06426981181700273
"Mobile Search for the Visually Impaired    DESCRIPTION (provided by applicant):    IQ Engines' mobile visual search technology will enable the visually impaired to access real-time information about physical objects using their mobile phone camera. The mobile phone provides a visually-driven hyperlink between the physical and digital world: point the camera at an object and get information (for example product information or navigation information). The mobile phone camera is a powerful yet underutilized tool for the visually impaired. Our proposal has two specific aims. Working directly with the visually impaired community, we will build a prototype mobile visual search application that meets their accessibility and use requirements. Our second aim is to improve upon the state of the art for 3D object recognition. We will investigate a novel combination of sparse image representation, feature matching algorithm, and geometric verification in order to advance the performance of 3D object matching. While state-of-the-art image intelligence is robust enough to enable rapid and accurate image search of flat feature-rich objects, current computer vision pales in comparison to the abilities of biological vision systems to recognize 3-dimensional objects. Our underlying goal is to bring inspiration from recent advances in theoretical neuroscience and apply them to image and video search solutions.      PUBLIC HEALTH RELEVANCE:    Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.           Mobile visual search, using a cell phone camera to retrieve object information, enables a mobile phone camera to become an artificial 'eye' with object recognition intelligence. Implemented on a cell phone, a mobile visual search tool can be a low cost visual aid for the blind.",Mobile Search for the Visually Impaired,7909025,R43EY019790,"['3-Dimensional', 'Algorithms', 'Arts', 'Biological', 'Breathing', 'Car Phone', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Databases', 'Feedback', 'Funding', 'Future', 'Goals', 'Image', 'Intelligence', 'Internet', 'Letters', 'Modeling', 'Neurosciences', 'Ocular Prosthesis', 'Performance', 'Research', 'Solutions', 'Speech Synthesizers', 'System', 'Technology', 'Text', 'Time', 'Vision', 'Visual', 'Visual Aid', 'Visual impairment', 'Work', 'base', 'blind', 'cost', 'digital', 'improved', 'meetings', 'novel', 'object recognition', 'prototype', 'public health relevance', 'technology development', 'tool', 'visual search']",NEI,"IQ ENGINES, INC.",R43,2010,138770,0.04841021316340316
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7911722,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'operation', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2010,423145,0.05287116230647698
"Origins of Object Knowledge    DESCRIPTION (provided by applicant): Twelve experiments investigate the early development, in human infants, of perception of the unity of partly occluded surfaces. The experiments focus on a time during ontogeny when there may be evidence of visual sensitivity to information specifying object properties, but limited ability to perceive occlusion, with the goal of observing real-time processes by which the infant assembles visible parts of a stimulus into a coherent whole. This approach stipulates that onset of sensitivity to motion and orientation information, development of the oculomotor system, and experience viewing objects undergoing occlusion and disocclusion, play a direct, foundational role in the ontogeny of object perception. That is, there is an hypothesized period, 2 to 4 months of age, during which infants come to use newly-emerged visual skills to perceive objects accurately.   The experiments follow a similar strategy: explorations of individual and group differences in both basic visual processing skills and perception of the unity of partly occluded surfaces. Infant perception is assessed with two methods: (a) recording of eye movements, to measure improvements in pickup of important visual .information, and (b) habituation/dishabituation, to ascertain perception of object unity as well as to determine the extent of sensitivity to available visual information. It is expected that the detailed analysis of individual differences afforded by this approach provide the opportunity for exceptionally sensitive measures of the emergence of visual skills and object knowledge.   The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the context of the classic nature-nurture debate. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, and how perceptual skills impact knowledge acquisition and structure. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.         n/a",Origins of Object Knowledge,7783780,R01HD040432,"['Academy', 'Adult', 'Age', 'Age-Months', 'Birth', 'Budgets', 'Categories', 'Child Development', 'Cognition', 'Cognitive', 'Cognitive Science', 'Corpus striatum structure', 'Dependence', 'Dependency', 'Detection', 'Development', 'Diagnostic', 'Drug Formulations', 'Elderly', 'Event', 'Exhibits', 'Eye', 'Eye Movements', 'Failure', 'Fostering', 'Frequencies', 'Funding', 'Future', 'Goals', 'Grant', 'Growth', 'Heart', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Infant Development', 'Investigation', 'Journals', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Legal patent', 'Light', 'Link', 'Literature', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Methods', 'Motion', 'Nature', 'Neurobiology', 'Pattern', 'Perception', 'Performance', 'Plant Roots', 'Play', 'Preparation', 'Process', 'Process Measure', 'Property', 'Psychology', 'Relative (related person)', 'Reporting', 'Research', 'Research Support', 'Resources', 'Role', 'Rotation', 'Saccades', 'Sampling', 'Scanning', 'Science', 'Side', 'Specific qualifier value', 'Speed', 'Stimulus', 'Stress', 'Structure', 'Suggestion', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Vision', 'Visual', 'Visual attention', 'Visual system structure', 'Work', 'Writing', 'abstracting', 'base', 'developmental disease', 'experience', 'gaze', 'graduate student', 'indexing', 'infancy', 'information processing', 'meter', 'object perception', 'oculomotor', 'psychologic', 'research study', 'sequence learning', 'skills', 'spatiotemporal', 'symposium', 'theories', 'tool', 'trend', 'visual information', 'visual process', 'visual processing']",NICHD,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2010,270932,-0.019787915841621404
"A Texture Analysis/Synthesis Model of Visual Crowding    DESCRIPTION (provided by applicant): Identifying a visual stimulus can be substantially impaired by the mere presence of additional stimuli in the immediate vicinity. This phenomenon is called ""crowding,"" and it powerfully limits visual perception in many circumstances, especially in the peripheral visual field. There is a rich body of literature detailing the phenomenology of crowding, but we do not know why crowding occurs. We lack a computational model that can predict what information will be available to an observer in an arbitrary crowded display. A popular hypothesis is that crowding results from obligatory ""texture processing,"" but there have been few efforts to formalize and test what this might mean, despite broad agreement that crowding reflects some form of ""excessive integration."" Dr. Rosenholtz has extensive experience with computational models of texture processing, which are a powerful means of defining the exact nature of ""texture processing"" and testing the ability of such models to explain and predict visual behavior. The proposed research has 3 aims: (1) To clarify and formalize the hypothesis that crowding is due to a ""texture"" - i.e. statistical -- representation of the crowded stimuli. (2) To collect behavioral data from a wider variety of displays and tasks than is typically studied in crowding. (3) To develop and validate the first general-purpose model of visual crowding. To achieve these aims, Dr. Rosenholtz will apply state-of-the-art computational tools for texture synthesis to ""crowded"" stimuli. ""Texturizing"" crowded arrays of stimuli affords a tool for visualizing the information available in a crowded display and a vocabulary for describing its representational content. Thus, Dr. Rosenholtz will attack the problem of crowding through a useful synthesis of computer graphics, computer vision, and psychophysics. PUBLIC HEALTH RELEVANCE: Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.            Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.",A Texture Analysis/Synthesis Model of Visual Crowding,7903934,R21EY019366,"['Age related macular degeneration', 'Agreement', 'Amblyopia', 'Area', 'Arts', 'Attention', 'Behavior', 'Behavioral', 'Binding', 'Cells', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Crowding', 'Data', 'Databases', 'Discrimination', 'Disease', 'Elderly', 'Eye Movements', 'Face', 'Failure', 'Functional disorder', 'Gender', 'Goals', 'Gray unit of radiation dose', 'Human', 'Imagery', 'Individual', 'Joints', 'Lesion', 'Letters', 'Literature', 'Location', 'Masks', 'Methods', 'Modeling', 'Nature', 'Patients', 'Perception', 'Performance', 'Peripheral', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Resolution', 'Saccades', 'Severities', 'Stimulus', 'Techniques', 'Testing', 'Texture', 'Time', 'Training', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Vocabulary', 'Work', 'base', 'clinically significant', 'computerized tools', 'design', 'experience', 'improved', 'neglect', 'novel', 'object recognition', 'public health relevance', 'research study', 'response', 'statistics', 'theories', 'tool', 'vision aid', 'visual information', 'visual search', 'visual stimulus']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2010,166320,0.020499259072113488
"Perceptual Learning: Human vs. Optimal Bayesian    DESCRIPTION (provided by applicant): Neural plasticity and perceptual learning are fundamental in the developmental stages of vision, in attaining expertise in specialized perceptual tasks, and in recovery from brain injuries and low- vision disorders. One important process in perceptual learning is the improvement in humans' ability to use task-relevant (signal) information. Although there have been advances in the understanding of the dynamics and algorithms mediating how humans optimize the selection of task relevant visual information, little is known about how eye movement patterns vary with practice and their impact in optimizing perceptual performance. Yet, in real world environments, eye movements are a critical component of active vision as humans explore the visual scene to make perceptual judgments. Understanding perceptual learning in human daily life requires studying the mechanisms mediating the changes in the planning of eye movements with learning and their contributions to optimizing perceptual performance. We hypothesize that two new experimental paradigms with digitally designed visual stimuli, in conjunction with eye position recording, and a newly developed foveated ideal observer and Bayesian learner will help elucidate how humans learn to strategize their eye movements and the contributions of the optimized sampling of the images to improvements in perceptual learning. The proposed work will address the following questions: 1) Do humans use learned information about the statistical properties of the visual stimuli and the requirements of the task at hand to strategize their eye movements to optimize the foveal sampling of the visual scene and perceptual performance?; 2) Do humans use knowledge of the varying resolution of their foveated visual system to optimally learn to plan eye movements for a given set of visual stimuli and task?; 3) What are the contributions of learning to strategize eye movements to the overall improvements in perceptual performance in ecologically important tasks such as face recognition, object identification and visual search?; 4) How do human fixation patterns and performance benefits from strategizing eye movements compare to an optimal foveated observer and learner? The proposed work will improve our understanding of the human neural algorithms mediating the dynamics of adult perceptual learning during active vision for ecologically important tasks. The proposed experimental protocols and theoretical developments will also provide a novel, powerful and flexible framework with which other researchers can study eye movements and learning of humans undergoing visual loss recovery as well as patients with learning disabilities.      PUBLIC HEALTH RELEVANCE: The proposed work benefits public health by increasing our understanding of how humans learn to move their eyes to potentially informative regions of the visual scene in important daily tasks such as identifying faces or searching for objects. Thorough understanding of these mechanisms in normal humans will allow identification of learning anomalies in patients recovering from visual-loss or learning disabilities and potentially develop tests to assess treatments.          PUBLIC HEALTH RELEVANCE  The proposed work benefits public health by increasing our understanding of how  humans learn to move their eyes to potentially informative regions of the visual scene in  important daily tasks such as identifying faces or searching for objects. Thorough  understanding of these mechanisms in normal humans will allow identification of  learning anomalies in patients recovering from visual-loss or learning disabilities and  potentially develop tests to assess treatments.",Perceptual Learning: Human vs. Optimal Bayesian,7988249,R01EY015925,"['Accounting', 'Address', 'Adult', 'Algorithms', 'Amblyopia', 'Animals', 'Area', 'Behavior', 'Blindness', 'Brain Injuries', 'Brain imaging', 'Cells', 'Chin', 'Complex', 'Data', 'Detection', 'Development', 'Discrimination', 'Emotional', 'Emotions', 'Environment', 'Eye', 'Eye Movements', 'Face', 'Frequencies', 'Funding', 'Goals', 'Gold', 'Hand', 'Human', 'Image', 'Individual', 'Infant', 'Instruction', 'Investigation', 'Judgment', 'Knowledge', 'Learning', 'Learning Disabilities', 'Life', 'Literature', 'Location', 'Machine Learning', 'Macular degeneration', 'Maps', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Monitor', 'Nature', 'Neuronal Plasticity', 'Nose', 'Oral cavity', 'Patients', 'Pattern', 'Perceptual learning', 'Performance', 'Positioning Attribute', 'Process', 'Progress Reports', 'Property', 'Protocols documentation', 'Psychophysiology', 'Public Health', 'Recording of previous events', 'Recovery', 'Research', 'Research Personnel', 'Resolution', 'Retinal', 'Retinitis Pigmentosa', 'Role', 'Sampling', 'Signal Transduction', 'Source', 'Spatial Distribution', 'Staging', 'Stimulus', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Work', 'active vision', 'area striata', 'design', 'experience', 'flexibility', 'gaze', 'ideal observer (Bayesian)', 'improved', 'interest', 'neurophysiology', 'novel', 'object recognition', 'oculomotor', 'prevent', 'public health relevance', 'relating to nervous system', 'sample fixation', 'tumor', 'visual information', 'visual performance', 'visual process', 'visual processing', 'visual search', 'visual stimulus']",NEI,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R01,2010,277974,-0.09127728657335672
"Handsight: Mobile Services for Low Vision    DESCRIPTION (provided by applicant): Handsight is a mobile phone service that offers an affordable, extensible set of automated sight-assistant functions to millions of blind and low-vision persons at $30/month atop standard voice and data fees. To the user, Handsight is simply a mobile phone application, requiring no special equipment or updating. By aiming a mobile telephone<s camera in roughly the right direction and pressing just one button, a Handsight user can snap a picture, send it to the Handsight computer center along with location data, and have a response within seconds. Moving the key computation and data from the handset to web servers cut cost, eases technology upgrades, and enables numerous data and technology partnerships needed to rapidly solve a broad spectrum of tasks. To allow widespread adoption Handsight uses voice, keypad, and vibration for user interaction; and for extensibility it is built on open-source software both on the handset and on the web application infrastructure. The range of tasks encountered by the blind and low-vision requires an array of components to solve: some are more heavily text-oriented and involve no location/navigational feedback (distinguishing and identifying different medicines; finding the phone bill in a stack of letters); some are more specifically navigational (locating the exact store entrance, finding the bus stop); yet others are informational (finding out when the next bus is due). Since we aim to provide a broadly useful tool, Handsight has to accommodate the whole range of task types. We are therefore proposing to build an architecture that integrates a set of components addressing the various task types, from text-detection and recognition software to navigational databases. Handsight<s application programming interface (API) will enable third parties to add capabilities to solve new tasks. As a web service, Handsight can evolve as computer vision and navigation technologies advance without requiring users to upgrade handsets or buy new versions of software. Phase I of this project was funded by the Dept. of Education / NIDRR and completed successfully between 10/01/2007 and 04/01/2008 under grant # H133S070044. It demonstrated not just the viability of the proposed project but also likely demand for such a service. (The NIDRR<s immutable deadlines precluded us from pursuing a Phase II with that agency.) Phase II consists in building the cell phone application and remote processing infrastructure with APIs to enable plug-in of the basic and future features. Subcontractor Smith-Kettlewell Institute for Eye Research will assure usability by blind and low-vision users; data partner NAVTEQ will provide a range of leading-edge digital map data. Blindsight already owns fast, accurate text detection and recognition software, and has experience in building scalable web services. A minimum set of features that make for a system with widespread appeal will be developed and/or licensed, and integrated into the service.      PUBLIC HEALTH RELEVANCE: The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today<s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, and shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.        Re: Project Title: Handsight: Mobile Services for Low Vision  FOA: PHS 2009-02 Omnibus Solicitation of the NIH, CDC, FDA and ACF for Small Business  Innovation Research Grant Applications (Parent SBIR [R43/R44])  Proposed project period: April 1, 2010 - March 31, 2012  Principal Investigator: Hallinan, Peter W. SF424 Research and Related Other Project Information: Project Narrative The proposed Handsight service falls squarely in the NEI mission to support research and programs with respect to visual disorders and the special health problems and requirements of the blind. It aims to provide a maximum of mobility and independence to the blind and low-vision using today�s mobile phone infrastructure via automated web services, using a minimum of specialized hardware and training. The 3 million-plus blind and low-vision persons in the U.S. encounter barriers to such activities of daily living as travel, navigation, shopping, reading and social interactions. The difficulty of recognizing when a friend is nearby, of finding a product in a grocery store, of making change or locating a destination building often require sighted friends or assistants to make these tasks possible. This is expensive, difficult to arrange, and increases dependence. The Handsight system will simplify or make possible to many a new set of tasks previously requiring human assistance or special-purpose devices.",Handsight: Mobile Services for Low Vision,7913126,R44EY020707,"['Activities of Daily Living', 'Address', 'Adoption', 'Architecture', 'Area', 'Businesses', 'Car Phone', 'Cellular Phone', 'Centers for Disease Control and Prevention (U.S.)', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Dependence', 'Destinations', 'Detection', 'Devices', 'Education', 'Eye', 'Feedback', 'Fees', 'Focus Groups', 'Friends', 'Funding', 'Future', 'Grant', 'Health', 'Human', 'Individual', 'Institutes', 'Internet', 'Letters', 'Licensing', 'Location', 'Maps', 'Medicine', 'Methods', 'Mission', 'Parents', 'Persons', 'Phase', 'Plug-in', 'Principal Investigator', 'Process', 'Provider', 'Reading', 'Research', 'Research Design', 'Research Infrastructure', 'Research Institute', 'Research Project Grants', 'Research Support', 'Services', 'Simulate', 'Small Business Innovation Research Grant', 'Social Interaction', 'Special Equipment', 'System', 'Target Populations', 'Technology', 'Telephone', 'Text', 'Touch sensation', 'Training', 'Travel', 'United States National Institutes of Health', 'Update', 'Vision', 'Vision Disorders', 'Visual impairment', 'Voice', 'Work', 'blind', 'computer center', 'cost', 'design', 'digital', 'experience', 'falls', 'innovation', 'open source', 'programs', 'public health relevance', 'response', 'success', 'tool', 'usability', 'vibration', 'voice recognition']",NEI,BLINDSIGHT CORPORATION,R44,2010,656703,0.04934044453097952
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),8136874,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,125017,0.029334244726083585
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),8143048,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,76123,0.029334244726083585
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7876805,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2010,676574,0.029334244726083585
"Computational Methods for Analysis of Mouth Shapes in Sign Languages    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hand) and by the nonmanual components (the face). These facial articulations perform significant semantic, prosodic, pragmatic, and syntactic functions. This proposal will systematically study mouth positions in ASL. Our hypothesis is that ASL mouth positions are more extensive than those used in speech. To study this hypothesis, this project is divided into three aims. In our first aim, we hypothesize that mouth positions are fundamental for the understanding of signs produced in context because they are very distinct from signs seen in isolation. To study this we have recently collected a database of ASL sentences and nonmanuals in over 3600 video clips from 20 Deaf native signers. Our experiments will use this database to identify potential mappings from visual to linguistic features. To successfully do this, our second aim is to design a set of shape analysis and discriminant analysis algorithms that can efficiently analyze the large number of frames in these video clips. The goal is to define a linguistically useful model, i.e., the smallest model that contains the main visual features from which further predictions can be made. Then, in our third aim, we will explore the hypothesis that the linguistically distinct mouth positions are also visually distinct. In particular, we will use the algorithms defined in the second aim to determine if distinct visual features are used to define different linguistic categories. This result will show whether linguistically meaningful mouth positions are not only necessary in ASL (as hypothesized in aim 1), but whether they are defined using non-overlapping visual features (as hypothesized in aim 3). These aims address a critical need. At present, the study of nonmanuals must be carried out manually, that is, the shape and position of each facial feature in each frame must be recorded by hand. Furthermore, to be able to draw conclusive results for the design of a linguistic model, it is necessary to study many video sequences of related sentences as produced by different signers. It has thus proven nearly impossible to continue this research manually. The algorithms designed in the course of this grant will facilitate this analysis of ASL nonmanuals and lead to better teaching materials.      PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.           Project Narrative Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the non-manuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for Analysis of Mouth Shapes in Sign Languages,8101448,R21DC011081,"['Academic achievement', 'Access to Information', 'Address', 'Adult', 'Algorithms', 'Applications Grants', 'Arts', 'Categories', 'Child', 'Clip', 'Communication', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Databases', 'Devices', 'Discriminant Analysis', 'Educational process of instructing', 'Emotions', 'Excision', 'Eye', 'Face', 'Funding', 'Goals', 'Grant', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Joints', 'Knowledge', 'Language', 'Lead', 'Learning', 'Life', 'Linguistics', 'Manuals', 'Modeling', 'Oral cavity', 'Parents', 'Pattern Recognition', 'Positioning Attribute', 'Process', 'Regulation', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Science', 'Scientist', 'Semantics', 'Shapes', 'Sign Language', 'Social Interaction', 'Specific qualifier value', 'Speech', 'Teaching Materials', 'Technology', 'Testing', 'Training', 'United States National Institutes of Health', 'Visual', 'Work', 'computerized tools', 'deafness', 'design', 'experience', 'innovation', 'instructor', 'interest', 'novel', 'prevent', 'public health relevance', 'research study', 'shape analysis', 'success', 'syntax', 'teacher', 'tool', 'visual map']",NIDCD,OHIO STATE UNIVERSITY,R21,2010,187999,-0.014745570191327353
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7777764,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2010,527186,0.05343166431183283
"Perceptual bases of visual concepts    DESCRIPTION (provided by applicant): Our general aim is to discover how the brain processes visual information, how neural activity is related to visual perception, and how visual processing interacts with other brain systems underlying cognition. Our specific aim is to elucidate how pigeons recognize and conceptualize visual stimuli. By studying the pigeon-a highly visual animal which can readily learn, but which does not have language or a mammalian neocortex and whose history can be carefully controlled and systematically varied-the processes of visual recognition and conceptualization may be more quickly and readily discovered. If the visual discrimination behavior of pigeons resembles that of humans, then the processes of conceptualization may be mediated by common neurobiological mechanisms which do not depend on linguistic competence or the human brain. The pigeon may become a powerful model system for both behavioral and biological studies of complex visual processing. Our proposal aims to see whether the perceptual processes of recognition and conceptualization are similar in humans and pigeons. Pigeons will be trained with several different operant conditioning procedures to discriminate line drawings and computer renderings of natural and artificial stimuli. The pigeons will later be tested with stimuli that: (1) degrade the training stimuli, (2) rearrange its parts, and (3) rotate the image in depth. These test stimuli produce highly specific effects in humans, which encourage the idea that object recognition is mediated by a structural description specifying a neural representation of the object's parts and the relations among those parts. If people and pigeons similarly process these various visual stimuli, then the results of our experiments with pigeons should parallel those with people. Empirical convergence would attest to the economy of nature and to the superfluity of language for visual recognition and conceptualization. Empirical divergence would imply that different neurobiological or linguistic mechanisms mediate visual recognition and conceptualization in people and pigeons. In either case, the results of this research project should shed considerable light on the basic mechanisms of visual recognition and conceptualization. Beyond the scientific significance of our proposed research, its health relevancy is considerable. Developing sound animal models of object recognition and conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising nonverbal diagnostic tests for assessing visual performance. In addition, comparing how pigeons and people recognize objects could provide new insights for computer and cognitive scientists to construct artificial devices which can recognize complex stimuli in the real world. Discovering how different biological systems accomplish the same adaptive feat might greatly help those attempting to create different artificial and prosthetic systems of object recognition. PUBLIC HEALTH RELEVANCE: The development of animal models of object recognition and visual conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising practical diagnostic tests for assessing visual performance; particularly important here is the fact that verbal behavior need not participate in such performance assessments, making nonverbal tests especially useful with infants, young children, and clinical populations. Our results should also advance our understanding of how the visual system develops and how to promote its regeneration after disease or injury.           Relevance to Public Health The development of animal models of object recognition and visual conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising practical diagnostic tests for assessing visual performance; particularly important here is the fact that verbal behavior need not participate in such performance assessments, making nonverbal tests especially useful with infants, young children, and clinical populations. Our results should also advance our understanding of how the visual system develops and how to promote its regeneration after disease or injury.",Perceptual bases of visual concepts,7924548,R01EY019781,"['Adaptive Behaviors', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Binding', 'Biological', 'Biological Models', 'Brain', 'Categories', 'Child', 'Classification', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Columbidae', 'Communities', 'Competence', 'Complex', 'Computer Vision Systems', 'Computers', 'Concept Formation', 'Development', 'Devices', 'Diagnostic tests', 'Discrimination', 'Discrimination Learning', 'Disease', 'Expert Systems', 'Health', 'Human', 'Image', 'Infant', 'Injury', 'Investigation', 'Iowa', 'Knowledge', 'Language', 'Learning', 'Light', 'Linguistics', 'Mediating', 'Methods', 'Natural regeneration', 'Nature', 'Neocortex', 'Neurobiology', 'Operant Conditioning', 'Organism', 'Perception', 'Performance', 'Phylogeny', 'Play', 'Population', 'Primates', 'Procedures', 'Process', 'Prosthesis', 'Psychological reinforcement', 'Public Health', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Retinal', 'Role', 'Scientist', 'Shapes', 'Specific qualifier value', 'Stimulus', 'Stress', 'System', 'Teaching Method', 'Technology', 'Testing', 'Training', 'Universities', 'Variant', 'Verbal Behavior', 'Visual', 'Visual Perception', 'Visual system structure', 'Work', 'animal model development', 'base', 'biological systems', 'cognitive neuroscience', 'cooking', 'coping', 'experience', 'information processing', 'innovation', 'insight', 'member', 'neurobiological mechanism', 'novel', 'object recognition', 'programs', 'public health relevance', 'relating to nervous system', 'research study', 'sound', 'theories', 'two-dimensional', 'visual information', 'visual performance', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF IOWA,R01,2010,330471,0.031057971060160876
"Accessible Artificial Intelligence Tutoring Software for Mathematics    DESCRIPTION (provided by applicant): This Fast-Track application focuses on developing the first artificial intelligence (AI) educational software to teach developmental mathematics to the blind and visually impaired. This project responds to the National Eye Institute's General Research Topics for ""teaching tools"" and the Visual Impairment and Blindness Program for ""other devices that meet the rehabilitative and everyday living needs of persons who are blind or have low vision."" The intervention being developed will place a comprehensive set of AI mathematics tutoring systems with integrated AI assessment capabilities in the hands of the blind K-12, college and adult student, for use on demand during study at home and at school. The formulation of an advanced AI tutoring methodology with accessibility inherent to its design will have broad implications for development in many subject areas beyond mathematics. Project objectives include: Horizontal Expansion of Accessible Curriculum Content Coverage (Ratio and Proportion, Percentages, Linear Equations, Metric Units, Scientific Notation) 1) Conduct initial accessibility review and analysis of AI tutor's existing user interface. 2) Implement accessibility requirements and recommendations from NFB, instructors and other partners. 3) Conduct final review to gain NFB accessibility certification after implementation of requirements. 4) Develop and issue survey of instructors on mathematics pedagogy and technology. Vertical Expansion of Accessible Features and Technological Capability 5) Implement Braille support in AI technology. 6) Develop additional AI tutor on Fractions that is automatically accessible from first principles using accessible AI framework. Evaluation of Accessible AI Educational Technology 7) Field evaluation of accessible AI technology with blind students and their instructors. 8) Continued demonstration and review of accessible AI technology by partners and other stakeholders. Preparation for success in Phase III has already been undertaken by involving partners that are important commercially as well as technically, such as the National Federation of the Blind and the American Printing House for the Blind (APH). In addition, Quantum already has long-term partnerships established with McGraw- Hill and Holt, Rinehart and Winston, two of the country's leading educational publishers, as well as a major science education catalog company, CyberEd, Inc., a PLATO Learning Company. PUBLIC HEALTH RELEVANCE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.           PROJECT NARRATIVE: There is a considerable need for improved educational software for mathematics in general, but the problem of quality educational software materials for the blind and visually impaired is particularly acute. A weak mathematics background can cause unnecessary limitations in daily living activities and seriously hinder or even preclude effective pursuit of more advanced mathematics education and careers in the STEM fields of science, technology, engineering and mathematics. Through previous federally-supported research, Quantum Simulations, Inc. has successfully developed, tested and brought to the classroom artificial intelligence (AI) tutoring systems for developmental mathematics. The goal of this Fast-Track project is to bring the full power and benefit of this cutting-edge educational technology to students who are blind and visually impaired.",Accessible Artificial Intelligence Tutoring Software for Mathematics,7608855,R44EY019414,"['Activities of Daily Living', 'Acute', 'Address', 'Adult', 'American', 'Area', 'Artificial Intelligence', 'Blindness', 'Businesses', 'Cataloging', 'Catalogs', 'Categories', 'Certification', 'Chemicals', 'Chemistry', 'Collaborations', 'Computer software', 'Country', 'Development', 'Development Plans', 'Devices', 'Dimensions', 'Drug Formulations', 'Dyslexia', 'Education', 'Educational Curriculum', 'Educational Technology', 'Educational process of instructing', 'Elements', 'Engineering', 'Ensure', 'Equation', 'Equilibrium', 'Evaluation', 'Feedback', 'Future', 'Goals', 'Hand', 'Home environment', 'Housing', 'Individual', 'Institution', 'Instruction', 'Internet', 'Intervention', 'Language', 'Learning', 'Letters', 'Life', 'Mathematics', 'Measures', 'Mediation', 'Methodology', 'Metric', 'Mission', 'Modeling', 'National Eye Institute', 'Nature', 'Outcome', 'Performance', 'Persons', 'Phase', 'Philosophy', 'Play', 'Preparation', 'Printing', 'Process', 'Publishing', 'Reader', 'Reading Disabilities', 'Recommendation', 'Rehabilitation therapy', 'Research', 'Research Infrastructure', 'Research Support', 'Role', 'Schools', 'Science', 'Small Business Innovation Research Grant', 'Software Tools', 'Students', 'Surveys', 'System', 'Techniques', 'Technology', 'Testing', 'Textbooks', 'Visual impairment', 'Work', 'blind', 'braille', 'career', 'college', 'commercial application', 'commercialization', 'design', 'disability', 'high school', 'improved', 'innovation', 'innovative technologies', 'instructor', 'meetings', 'middle school', 'programs', 'prospective', 'prototype', 'public health relevance', 'quantum', 'quantum chemistry', 'remediation', 'research and development', 'science education', 'simulation', 'stem', 'success', 'teacher', 'technological innovation', 'tool']",NEI,"QUANTUM SIMULATIONS, INC.",R44,2009,164486,0.009656924022307693
"Causal Perceptual Processing    DESCRIPTION (provided by applicant): The broad, long-term objectives of this research are to make original contributions to scientific understanding of biological perception. The specific aims are to: 1) develop and test a causal model of touch sensations' integration with visual sensations for perception, and to generalize this model for application to a broader class of sensory integration phenomena, 2) apply causal models to investigate how humans perceive cause-and-effect events, 3) increase the applicant's technical proficiency with causal modeling and applied machine learning methods. The proposal includes two main projects; the first will measure how humans judge the size of objects when their distances are uncertain. Specifically the first project examines the theory that human perception uses knowledge about how size and distance sensations are caused to integrate related sensations. Human experimental participants will view objects while touching them and report their perceptions of the objects' sizes, which will be used to evaluate theoretical predictions. The second project investigates how humans perceive cause-and-effect chains of events by examining the theory that the brain uses built-in knowledge of rudimentary physical behaviors, like momentum in collisions, to interpret such simple events. Human experimental participants will view colliding objects and report what occurred, which will again be used to evaluate theoretical predictions. PUBLIC HEALTH RELEVANCE: The public health relevance of this research is to increase scientific and medical understanding of the neural communication and processing strategies the brain employs to create perceptual experiences, so that people with perceptual impairments can be provided with effective rehabilitation programs and biotechnological substitutes for diminished capabilities. Specific impairments include blindness and low-vision, traumatic brain and nervous system injuries, and strokes. In particular, modern sensory prostheses are now using computerized components that can interface with neural pathways to more comprehensively and effectively restore normal abilities in patients, and whose development faces significant obstacles establishing effective communication channels with the biological nervous system.          n/a",Causal Perceptual Processing,7778287,F32EY019228,"['Algorithms', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological', 'Biological Process', 'Blindness', 'Brain', 'Cognition', 'Cognitive', 'Cognitive Science', 'Communication', 'Complex', 'Computational Technique', 'Computer Simulation', 'Decision Making', 'Development', 'Distance Perception', 'Economics', 'Elements', 'Esthesia', 'Etiology', 'Event', 'Face', 'Glass', 'Goals', 'Human', 'Image', 'Impairment', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Lifting', 'Machine Learning', 'Masks', 'Measures', 'Medical', 'Memory', 'Methods', 'Modeling', 'Nature', 'Nervous System Trauma', 'Nervous system structure', 'Neural Pathways', 'Neurosciences', 'Participant', 'Patients', 'Pattern', 'Perception', 'Plague', 'Preparation', 'Process', 'Property', 'Psychologist', 'Psychology', 'Rehabilitation therapy', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Science', 'Scientific Advances and Accomplishments', 'Semantics', 'Sensory', 'Sorting - Cell Movement', 'Space Perception', 'Stimulus', 'Stroke', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Visual', 'Visual impairment', 'Water', 'Work', 'abstracting', 'analytical method', 'computer monitor', 'computer science', 'computerized', 'experience', 'falls', 'haptics', 'insight', 'object perception', 'phrases', 'predictive modeling', 'programs', 'public health relevance', 'relating to nervous system', 'sensory integration', 'sensory prosthesis', 'skills', 'statistics', 'theories', 'tool']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2009,47210,0.005238139310222435
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7668573,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,1187062,0.05554573358229701
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7922310,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'efficacy testing', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'meetings', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,152260,0.05554573358229701
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7692268,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Cues', 'Data', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'meetings', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2009,228847,0.032187052097077076
"Towards cortical visual prosthetics    Description (provided by applicant): Visual object recognition is crucial for most everyday tasks including face identification, reading and navigation. In spite of the massive increase in computational power over the last two decades, a 3-year-old still outperforms the most sophisticated algorithms even in simple recognition tasks. Understanding the computations performed by the human visual system to recognize objects will have profound implications not only to understand the functions (and malfunction) of the cerebral cortex but also for developing visual prosthetic devices for the visually impaired. We combine neurophysiology, electrical stimulation and tools from machine learning to further our understanding of the neuronal circuits, algorithms and computations performed by the human visual system to perform visual pattern recognition. In the vast majority of visually impaired or blind people, the problems originate at the level of the retina while the visual cortex remains unimpaired. Our proposal constitutes a proof- of-principle approach towards developing visual prosthetic devices that rely on electrical stimulation of visual cortex. The specific aims of this proposal are designed to test the possibility of decoding and recoding information in visual cortex: (1) Read-out of visual information from human visual cortex on line (2) Write-in of visual information in human visual cortex. We take advantage of a rare opportunity to study the human brain at high spatial and temporal resolution by studying patients who have electrodes implanted for clinical reasons. Our electrophysiological recordings provide us with a unique view of the human temporal lobe circuitry and allow us to test the feasibility of cortical visual prosthetics in behaving human subjects. PUBLIC HEALTH RELEVANCE: Towards cortical visual prosthetics one of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.           7. Project Narrative: Towards cortical visual prosthetics  One of the key challenges for the visually impaired and blind people is the lack of visual object recognition capabilities. Visual recognition is crucial for most everyday tasks including navigation and face identification. Our proposal is a proof-of-principle approach towards the development of visual prosthetics devices based on electrical stimulation in visual cortex.",Towards cortical visual prosthetics,7701276,R21EY019710,"['3 year old', 'Action Potentials', 'Algorithms', 'Animals', 'Auditory', 'Brain', 'Categories', 'Cerebral cortex', 'Clinical', 'Code', 'Computer software', 'Data', 'Data Quality', 'Detection', 'Development', 'Devices', 'Electric Stimulation', 'Electrodes', 'Epilepsy', 'Face', 'Goals', 'Human', 'Implanted Electrodes', 'Inferior', 'Limb structure', 'Macaca', 'Machine Learning', 'Methodology', 'Methods', 'Monkeys', 'Neurons', 'Output', 'Patients', 'Perception', 'Physiological', 'Prosthesis', 'Reading', 'Recording of previous events', 'Reporting', 'Research Personnel', 'Resolution', 'Retina', 'Sensory', 'Signal Transduction', 'Sorting - Cell Movement', 'Specificity', 'System', 'Temporal Lobe', 'Testing', 'Time', 'Visual', 'Visual Cortex', 'Visual Pattern Recognition', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Wireless Technology', 'Writing', 'base', 'brain machine interface', 'design', 'devices for the visually impaired', 'human data', 'human subject', 'improved', 'interest', 'neural prosthesis', 'neurophysiology', 'object recognition', 'public health relevance', 'relating to nervous system', 'research study', 'response', 'retinal prosthesis', 'tool', 'vision development', 'visual information']",NEI,BOSTON CHILDREN'S HOSPITAL,R21,2009,247290,0.0561359569535982
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7663288,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Computer Systems Development', 'Computer software', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Source', 'Structure', 'System', 'Systems Integration', 'Technology', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'meetings', 'models and simulation', 'open source', 'outreach', 'protein complex', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2009,437938,0.014847958863040368
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7589644,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'image processing', 'meetings', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'public health relevance', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,426946,0.06426981181700273
"A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons    DESCRIPTION (provided by applicant): We propose to develop and evaluate a cell-phone-based system to enable blind and visually impaired individuals to find and read street signs and other signs relevant to wayfinding. Using the built-in camera and computing power of a standard cell phone, the system will process images captured by the user to find and analyze signs, and speak their contents. This will provide valuable assistance for blind or visually impaired pedestrians in finding and reading street signs, as well as locating and identifying addresses and store names, without requiring them to carry any special-purpose hardware. The sign finding and reading software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). We will build on our prior and ongoing work in applying computer vision techniques to practical problem-solving for blind persons, including cell-phone implementation of algorithms for indoor wayfinding and for reading digital appliance displays. We will develop, refine and transfer to the cell phone platform a new belief propagation-based algorithm that has shown preliminary success in finding and analyzing signs under difficult real-world conditions including partial shadow coverage. Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. Access to environmental labels, signs or landmarks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.          n/a",A Cell Phone-based Sign Reader for Blind & Visually Impaired Persons,7373002,R01EY018210,"['Accidents', 'Address', 'Algorithms', 'American', 'Belief', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Databases', 'Detection', 'Development', 'Devices', 'Effectiveness', 'Evaluation', 'Face', 'Figs - dietary', 'Generations', 'Grant', 'Human', 'Image', 'Impairment', 'Individual', 'Label', 'Left', 'Mainstreaming', 'Marketing', 'Modification', 'Names', 'Operating System', 'Operative Surgical Procedures', 'Performance', 'Problem Solving', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Sampling', 'Self-Help Devices', 'Series', 'Shadowing (Histology)', 'Signal Transduction', 'Speech', 'System', 'Target Populations', 'Techniques', 'Testing', 'Text', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'base', 'blind', 'consumer product', 'cost', 'design', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'open source', 'prevent', 'programs', 'prototype', 'skills', 'success', 'tool', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,417177,0.05287116230647698
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7681076,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding', 'web site']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,346662,0.007179627974862272
"A Texture Analysis/Synthesis Model of Visual Crowding    DESCRIPTION (provided by applicant): Identifying a visual stimulus can be substantially impaired by the mere presence of additional stimuli in the immediate vicinity. This phenomenon is called ""crowding,"" and it powerfully limits visual perception in many circumstances, especially in the peripheral visual field. There is a rich body of literature detailing the phenomenology of crowding, but we do not know why crowding occurs. We lack a computational model that can predict what information will be available to an observer in an arbitrary crowded display. A popular hypothesis is that crowding results from obligatory ""texture processing,"" but there have been few efforts to formalize and test what this might mean, despite broad agreement that crowding reflects some form of ""excessive integration."" Dr. Rosenholtz has extensive experience with computational models of texture processing, which are a powerful means of defining the exact nature of ""texture processing"" and testing the ability of such models to explain and predict visual behavior. The proposed research has 3 aims: (1) To clarify and formalize the hypothesis that crowding is due to a ""texture"" - i.e. statistical -- representation of the crowded stimuli. (2) To collect behavioral data from a wider variety of displays and tasks than is typically studied in crowding. (3) To develop and validate the first general-purpose model of visual crowding. To achieve these aims, Dr. Rosenholtz will apply state-of-the-art computational tools for texture synthesis to ""crowded"" stimuli. ""Texturizing"" crowded arrays of stimuli affords a tool for visualizing the information available in a crowded display and a vocabulary for describing its representational content. Thus, Dr. Rosenholtz will attack the problem of crowding through a useful synthesis of computer graphics, computer vision, and psychophysics. PUBLIC HEALTH RELEVANCE: Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.            Understanding crowding, besides elucidating representations and performance of normal human vision, is crucial for disorders like age-related macular degeneration, for which, without foveal vision, virtually all perception is essentially crowded. In addition, percepts under crowding may be related to percepts under other visual dysfunctions where there is ""excessive integration"", such as amblyopia and simultagnosia. Successfully predicting crowding severity would also advance the design of low-vision aids for older adults and improve our ability to design for the visually-impaired.",A Texture Analysis/Synthesis Model of Visual Crowding,7740891,R21EY019366,"['Age related macular degeneration', 'Agreement', 'Amblyopia', 'Area', 'Arts', 'Attention', 'Behavior', 'Behavioral', 'Binding', 'Cells', 'Classification', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Crowding', 'Data', 'Databases', 'Discrimination', 'Disease', 'Elderly', 'Eye Movements', 'Face', 'Failure', 'Functional disorder', 'Gender', 'Goals', 'Gray unit of radiation dose', 'Human', 'Imagery', 'Individual', 'Joints', 'Lesion', 'Letters', 'Literature', 'Location', 'Masks', 'Methods', 'Modeling', 'Nature', 'Patients', 'Perception', 'Performance', 'Peripheral', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Resolution', 'Saccades', 'Severities', 'Stimulus', 'Techniques', 'Testing', 'Texture', 'Time', 'Training', 'Vision', 'Vision Disorders', 'Visual', 'Visual Fields', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Vocabulary', 'Work', 'base', 'clinically significant', 'computerized tools', 'design', 'experience', 'improved', 'neglect', 'novel', 'object recognition', 'public health relevance', 'research study', 'response', 'statistics', 'theories', 'tool', 'vision aid', 'visual information', 'visual search', 'visual stimulus']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,R21,2009,168000,0.020499259072113488
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7915039,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2009,168580,0.029334244726083585
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7643324,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'population based', 'programs', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2009,815277,0.029334244726083585
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7586102,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Central Scotomas', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Contrast Sensitivity', 'Data', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'depressed', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2009,522802,0.05343166431183283
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7904674,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Theoretical model', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'information processing', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2009,92207,-0.041671127548373176
"Perceptual bases of visual concepts    DESCRIPTION (provided by applicant): Our general aim is to discover how the brain processes visual information, how neural activity is related to visual perception, and how visual processing interacts with other brain systems underlying cognition. Our specific aim is to elucidate how pigeons recognize and conceptualize visual stimuli. By studying the pigeon-a highly visual animal which can readily learn, but which does not have language or a mammalian neocortex and whose history can be carefully controlled and systematically varied-the processes of visual recognition and conceptualization may be more quickly and readily discovered. If the visual discrimination behavior of pigeons resembles that of humans, then the processes of conceptualization may be mediated by common neurobiological mechanisms which do not depend on linguistic competence or the human brain. The pigeon may become a powerful model system for both behavioral and biological studies of complex visual processing. Our proposal aims to see whether the perceptual processes of recognition and conceptualization are similar in humans and pigeons. Pigeons will be trained with several different operant conditioning procedures to discriminate line drawings and computer renderings of natural and artificial stimuli. The pigeons will later be tested with stimuli that: (1) degrade the training stimuli, (2) rearrange its parts, and (3) rotate the image in depth. These test stimuli produce highly specific effects in humans, which encourage the idea that object recognition is mediated by a structural description specifying a neural representation of the object's parts and the relations among those parts. If people and pigeons similarly process these various visual stimuli, then the results of our experiments with pigeons should parallel those with people. Empirical convergence would attest to the economy of nature and to the superfluity of language for visual recognition and conceptualization. Empirical divergence would imply that different neurobiological or linguistic mechanisms mediate visual recognition and conceptualization in people and pigeons. In either case, the results of this research project should shed considerable light on the basic mechanisms of visual recognition and conceptualization. Beyond the scientific significance of our proposed research, its health relevancy is considerable. Developing sound animal models of object recognition and conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising nonverbal diagnostic tests for assessing visual performance. In addition, comparing how pigeons and people recognize objects could provide new insights for computer and cognitive scientists to construct artificial devices which can recognize complex stimuli in the real world. Discovering how different biological systems accomplish the same adaptive feat might greatly help those attempting to create different artificial and prosthetic systems of object recognition. PUBLIC HEALTH RELEVANCE: The development of animal models of object recognition and visual conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising practical diagnostic tests for assessing visual performance; particularly important here is the fact that verbal behavior need not participate in such performance assessments, making nonverbal tests especially useful with infants, young children, and clinical populations. Our results should also advance our understanding of how the visual system develops and how to promote its regeneration after disease or injury.           Relevance to Public Health The development of animal models of object recognition and visual conceptualization might better enable us to understand the behavioral and biological mechanisms of the human visual system in both health and disease. Effective animal models may also help pave the way for devising practical diagnostic tests for assessing visual performance; particularly important here is the fact that verbal behavior need not participate in such performance assessments, making nonverbal tests especially useful with infants, young children, and clinical populations. Our results should also advance our understanding of how the visual system develops and how to promote its regeneration after disease or injury.",Perceptual bases of visual concepts,7727727,R01EY019781,"['Adaptive Behaviors', 'Animal Model', 'Animals', 'Behavior', 'Behavioral', 'Binding', 'Biological', 'Biological Models', 'Brain', 'Categories', 'Child', 'Classification', 'Clinical', 'Cognition', 'Cognitive', 'Collaborations', 'Columbidae', 'Communities', 'Competence', 'Complex', 'Computer Vision Systems', 'Computers', 'Concept Formation', 'Development', 'Devices', 'Diagnostic tests', 'Discrimination', 'Discrimination Learning', 'Disease', 'Expert Systems', 'Health', 'Human', 'Image', 'Infant', 'Injury', 'Investigation', 'Iowa', 'Knowledge', 'Language', 'Learning', 'Light', 'Linguistics', 'Mediating', 'Methods', 'Natural regeneration', 'Nature', 'Neocortex', 'Neurobiology', 'Operant Conditioning', 'Organism', 'Perception', 'Performance', 'Phylogeny', 'Play', 'Population', 'Primates', 'Procedures', 'Process', 'Prosthesis', 'Psychological reinforcement', 'Public Health', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Retinal', 'Role', 'Scientist', 'Shapes', 'Specific qualifier value', 'Stimulus', 'Stress', 'System', 'Teaching Method', 'Technology', 'Testing', 'Training', 'Universities', 'Variant', 'Verbal Behavior', 'Visual', 'Visual Perception', 'Visual system structure', 'Work', 'animal model development', 'base', 'biological systems', 'cognitive neuroscience', 'cooking', 'coping', 'experience', 'information processing', 'innovation', 'insight', 'member', 'neurobiological mechanism', 'novel', 'object recognition', 'programs', 'public health relevance', 'relating to nervous system', 'research study', 'sound', 'theories', 'two-dimensional', 'visual information', 'visual performance', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF IOWA,R01,2009,328156,0.031057971060160876
"Causal Perceptual Processing    DESCRIPTION (provided by applicant): The broad, long-term objectives of this research are to make original contributions to scientific understanding of biological perception. The specific aims are to: 1) develop and test a causal model of touch sensations' integration with visual sensations for perception, and to generalize this model for application to a broader class of sensory integration phenomena, 2) apply causal models to investigate how humans perceive cause-and-effect events, 3) increase the applicant's technical proficiency with causal modeling and applied machine learning methods. The proposal includes two main projects; the first will measure how humans judge the size of objects when their distances are uncertain. Specifically the first project examines the theory that human perception uses knowledge about how size and distance sensations are caused to integrate related sensations. Human experimental participants will view objects while touching them and report their perceptions of the objects' sizes, which will be used to evaluate theoretical predictions. The second project investigates how humans perceive cause-and-effect chains of events by examining the theory that the brain uses built-in knowledge of rudimentary physical behaviors, like momentum in collisions, to interpret such simple events. Human experimental participants will view colliding objects and report what occurred, which will again be used to evaluate theoretical predictions. PUBLIC HEALTH RELEVANCE: The public health relevance of this research is to increase scientific and medical understanding of the neural communication and processing strategies the brain employs to create perceptual experiences, so that people with perceptual impairments can be provided with effective rehabilitation programs and biotechnological substitutes for diminished capabilities. Specific impairments include blindness and low-vision, traumatic brain and nervous system injuries, and strokes. In particular, modern sensory prostheses are now using computerized components that can interface with neural pathways to more comprehensively and effectively restore normal abilities in patients, and whose development faces significant obstacles establishing effective communication channels with the biological nervous system.          n/a",Causal Perceptual Processing,7545242,F32EY019228,"['Algorithms', 'Artificial Intelligence', 'Behavior', 'Behavioral', 'Biological', 'Biological Process', 'Blindness', 'Brain', 'Class', 'Cognition', 'Cognitive', 'Cognitive Science', 'Communication', 'Complex', 'Computational Technique', 'Computer Simulation', 'Decision Making', 'Development', 'Distance Perception', 'Economics', 'Elements', 'Esthesia', 'Etiology', 'Event', 'Face', 'Glass', 'Goals', 'Human', 'Image', 'Impairment', 'Judgment', 'Knowledge', 'Laboratories', 'Language', 'Lead', 'Learning', 'Lifting', 'Machine Learning', 'Masks', 'Measures', 'Medical', 'Memory', 'Methods', 'Modeling', 'Nature', 'Nervous System Trauma', 'Nervous system structure', 'Neural Pathways', 'Neurosciences', 'Participant', 'Patients', 'Pattern', 'Perception', 'Plague', 'Preparation', 'Process', 'Property', 'Psychologist', 'Psychology', 'Public Health', 'Rehabilitation therapy', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Science', 'Scientific Advances and Accomplishments', 'Semantics', 'Sensory', 'Sorting - Cell Movement', 'Space Perception', 'Stimulus', 'Stroke', 'Testing', 'Text', 'Time', 'Touch sensation', 'Training', 'Visual', 'Visual impairment', 'Water', 'Work', 'abstracting', 'analytical method', 'computer monitor', 'computer science', 'computerized', 'experience', 'falls', 'haptics', 'insight', 'object perception', 'predictive modeling', 'programs', 'relating to nervous system', 'sensory integration', 'sensory prosthesis', 'size', 'skills', 'statistics', 'theories', 'tool']",NEI,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,F32,2008,44846,0.005238139310222435
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7500697,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,1146026,0.05554573358229701
"Cue Reliability and Depth Calibration During Space Perception    DESCRIPTION (provided by applicant): The long-term objective of the proposed work is to understand how learning by the visual system helps it to represent the immediate environment during perception. Because perception is accurate, we can know spatial layout: the shapes, orientations, sizes, and spatial locations of the objects and surfaces around us. But this accuracy requires that the visual system learn over time how best to interpret visual ""cues"". These cues are the signals from the environment that the visual system extracts from the retinal images that are informative about spatial layout. Known cues include binocular disparity, texture gradients, occlusion relations, motion parallax, and familiar size, to name a few. How do these cues come to be interpreted correctly? A fundamental problem is that visual cues are ambiguous. Even if cues could be measured exactly (which they cannot, the visual system being a physical device) there would still be different possible 3D interpretations for a given set of cues. As a result, the visual system is forced to operate probabilistically: the way things ""look"" to us reflects an implicit guess as to which interpretation of the cues is most likely to be correct. Each additional cue helps improve the guess. For example, the retinal image of a door could be interpreted as a vertical rectangle or as some other quadrilateral at a non-vertical orientation in space, and the shadow cues at the bottom of the door helps the system know that it's a vertical rectangle. What mechanisms do the visual system use to discern which cues are available for interpreting images correctly? The proposed work aims to answer this fundamental question about perceptual learning. It was recently shown that the visual system can detect and start using new cues for perception. This phenomenon can be studied in the laboratory using classical conditioning procedures that were previously developed to study learning in animals. In the proposed experiments, a model system is used to understand details about when this learning occurs and what is learned. The data will be compared to predictions based on older, analogous studies in the animal learning literature, and interpreted in the context of Bayesian statistical inference, especially machine learning theory. The proposed work benefits public health by characterizing the brain mechanisms that keep visual perception accurate. These mechanisms are at work in the many months during which a person with congenital cataracts learns to use vision after the cataracts are removed, and it is presumably these mechanisms that go awry when an individual with a family history of synesthesia or autism develops anomalous experience-dependent perceptual responses. Neurodegenerative diseases may disrupt visual learning, in which case visual learning tests could be used to detect disease; understanding the learning of new cues in human vision could lead to better computerized aids for the visually impaired; and knowing what causes a new cue to be learned could lead to new technologies for training people to perceive accurately in novel work environments.          n/a",Cue Reliability and Depth Calibration During Space Perception,7388324,R01EY013988,"['Address', 'Adult', 'Animal Behavior', 'Animals', 'Appearance', 'Autistic Disorder', 'Binocular Vision', 'Biological Models', 'Brain', 'Calibration', 'Cataract', 'Computer Vision Systems', 'Condition', 'Cues', 'Data', 'Depth', 'Devices', 'Diagnosis', 'Disease', 'Environment', 'Experimental Designs', 'Family history of', 'Food', 'Funding', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Laboratories', 'Lead', 'Learning', 'Learning Disabilities', 'Literature', 'Location', 'Longevity', 'Machine Learning', 'Measures', 'Memory', 'Motion', 'Motion Perception', 'Names', 'Neurodegenerative Disorders', 'Neuronal Plasticity', 'Pathology', 'Perception', 'Perceptual learning', 'Persons', 'Positioning Attribute', 'Primates', 'Procedures', 'Process', 'Public Health', 'Rate', 'Recruitment Activity', 'Research', 'Retinal', 'Reversal Learning', 'Rotation', 'Shadowing (Histology)', 'Shapes', 'Signal Transduction', 'Source', 'Space Perception', 'Stimulus', 'Surface', 'System', 'Testing', 'Texture', 'Time', 'Training', 'Translations', 'Trust', 'Ursidae Family', 'Vision', 'Vision Disparity', 'Visual', 'Visual Perception', 'Visual impairment', 'Visual system structure', 'Work', 'Workplace', 'area MT', 'base', 'classical conditioning', 'clinical application', 'computerized', 'concept', 'congenital cataract', 'design', 'devices for the visually impaired', 'experience', 'improved', 'neuromechanism', 'new technology', 'novel', 'programs', 'relating to nervous system', 'research study', 'response', 'size', 'stereoscopic', 'theories', 'tool', 'visual information', 'visual learning', 'visual process', 'visual processing']",NEI,STATE COLLEGE OF OPTOMETRY,R01,2008,228847,0.032187052097077076
"Diagnostic Innovations in Glaucoma: Clinical Electrophysiology    DESCRIPTION (provided by applicant): This application proposes to investigate the diagnostic precision for detecting glaucoma of clinical electrophysiological measurement (pattern electroretinogram, PERG; and multifocal visual evoked potentials, mfVEP), a technique identified as an important recent glaucoma- related development in eye research by the 2004 National Eye Institute (NEI) National Plan. Aim 1: Electrophysiological responses will be characterized in glaucoma, suspect and healthy eyes and the diagnostic accuracy of these commercially available techniques will be compared to current reference standards (evaluation of stereoscopic photographs of the optic disc and standard automated perimetry) and to recently developed diagnostic techniques including optical imaging of the optic disc and retinal nerve fiber layer (RNFL) (confocal scanning laser ophthalmoscopy, optical coherence tomography, and scanning laser polarimetry) and visual function-specific perimetry (short-wavelength automated perimetry and frequency doubling technology perimetry). Aim 2: Novel use of machine learning classifier techniques (e.g. relevance vector machines, support vector machines, mixture of Gaussian techniques, independent components analysis) will be applied to electrophysiological data to improve its diagnostic accuracy and data from different diagnostic techniques (named above) will be combined to improve overall diagnostic accuracy. Aim 3: Electrophysiological measurements will be validated as functional indicators of optic nerve damage by examining the relationship between electrophysiological abnormality and optic disc and RNFL damage in glaucoma and glaucoma suspect patients. 210 patients (105 glaucoma's, 105 glaucoma suspects) and 105 healthy participants will be enrolled and studied cross-sectionally. The specific aims of this proposal address the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset, determining functional correlates of optic nerve damage, and characterizing glaucomatous neurodegeneration within the visual pathways at structural and functional levels. Information about the relative usefulness of electrophysiological measurement, optical imaging techniques, and ganglion cell-specific perimetry for glaucoma detection is important to the clinical community for determining future evidence-based changes in standard of care for glaucoma diagnosis and monitoring. These studies will demonstrate the relative usefulness of electrophysiological measurement (pattern electroretinogram, PERG; multi-focal visual evoked potential, mfVEP) compared to optical imaging techniques (confocal scanning ophthalmoscopy, optical coherence tomography, scanning laser polarimetry) and ganglion cell-specific perimetry (short wavelength and frequency doubling perimetry) for glaucoma detection. The proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset, determining functional correlates of optic nerve damage, and characterizing glaucomatous neurodegeneration within the visual pathways at structural and functional levels. Findings will be important to the clinical community for determining future evidence-based changes in standard of care for glaucoma diagnosis and monitoring.                n/a",Diagnostic Innovations in Glaucoma: Clinical Electrophysiology,7452327,R21EY018190,"['Address', 'California', 'Caring', 'Clinical', 'Communities', 'Data', 'Defect', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Electrophysiology (science)', 'Electroretinography', 'Enrollment', 'Evaluation', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Image', 'Imaging Techniques', 'Individual', 'Investigation', 'Lasers', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Names', 'National Eye Institute', 'Nerve Degeneration', 'Onset of illness', 'Ophthalmoscopy', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Patients', 'Pattern', 'Perimetry', 'Peripheral', 'Personal Satisfaction', 'Photography', 'Psychophysiology', 'Range', 'Reference Standards', 'Relative (related person)', 'Research', 'Retinal', 'Scanning', 'Scotoma', 'Sensitivity and Specificity', 'Severities', 'Standards of Weights and Measures', 'Structure', 'Suspect Glaucomas', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual', 'Visual Pathways', 'Visual evoked cortical potential', 'base', 'design', 'diagnostic accuracy', 'ganglion cell', 'improved', 'independent component analysis', 'innovation', 'novel', 'novel diagnostics', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'response', 'retinal nerve fiber layer', 'stereoscopic', 'vector']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2008,189263,0.04499988482050782
"A Non-Document Text and Display Reader for Visually Impaired Persons    DESCRIPTION (provided by applicant): The goal of this project is to develop a computer vision system based on standard camera cell phones to give blind and visually impaired persons the ability to read appliance displays and similar forms of non-document visual information. This ability is increasingly necessary to use everyday appliances such as microwave ovens and DVD players, and to perform many daily activities such as counting paper money. No access to this information is currently afforded by conventional text reading systems such as optical character recognition (OCR), which is intended for reading printed documents. Our proposed software runs on a standard, off-the- shelf camera phone and uses computer vision algorithms to analyze images taken by the user, to detect and read the text within each image, and to then read it aloud using synthesized speech. Preliminary feasibility studies indicate that current cellular phones easily exceed the minimum processing power required for these tasks. Initially, the software will read out three categories of symbols: LED/LCD appliance displays, product or user-defined barcodes, and denominations of paper money. Ultimately these functions will be integrated with other capabilities being developed under separate funding, such as reading a broad range of printed text (including signs), recognizing objects, and analyzing photographs and graphics, etc., all available as free or low-cost software downloads for any cell phone user. Our specific goals are to (1) gather a database of real images taken by blind and visually impaired persons of a variety of LED/LCD appliance displays, barcodes and US paper currency; (2) develop algorithms to process the images and extract the desired information; (3) implement the algorithms on a camera phone; and (4) conduct user testing to establish design parameters and optimize the human interface. PUBLIC HEALTH RELEVANCE:  For blind and visually impaired persons, one of the most serious barriers to employment, economic self sufficiency and independence is insufficient access to the ever-increasing variety of devices and appliances in the home, workplace, school or university that incorporate visual LED/LCD displays, and to other types of text and symbolic information hitherto unaddressed by rehabilitation technology. The proposed research would result in an assistive technology system (with zero or minimal cost to users) to provide increased access to such display and non- document text information for the approximately 10 million Americans with significant vision impairments or blindness.          n/a",A Non-Document Text and Display Reader for Visually Impaired Persons,7446299,R01EY018890,"['Access to Information', 'Acoustics', 'Address', 'Algorithms', 'American', 'Applications Grants', 'Auditory', 'Blindness', 'Categories', 'Cellular Phone', 'Code', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Count', 'Custom', 'Daily', 'Data', 'Databases', 'Development', 'Devices', 'Economics', 'Electronics', 'Employment', 'Evaluation', 'Feasibility Studies', 'Figs - dietary', 'Funding', 'Goals', 'Hand', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Lavandula', 'Mainstreaming', 'Marketing', 'Memory', 'Modification', 'Paper', 'Printing', 'Process', 'Public Health', 'Range', 'Reader', 'Reading', 'Research', 'Resolution', 'Running', 'Schools', 'Self-Help Devices', 'Speech', 'Standards of Weights and Measures', 'Surveys', 'System', 'Telephone', 'Testing', 'Text', 'Training', 'Universities', 'Vision', 'Visit', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Voice', 'Workplace', 'base', 'blind', 'consumer product', 'cost', 'design', 'desire', 'image processing', 'microwave electromagnetic radiation', 'optical character recognition', 'prevent', 'programs', 'rehabilitation technology', 'response', 'time interval', 'tool', 'trafficking', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,421791,0.06426981181700273
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7457647,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2008,437938,0.014847958863040368
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7490458,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'Standards of Weights and Measures', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'day', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,335542,0.007179627974862272
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7486800,R44EY014487,"['Algorithms', 'Arts', 'Cataract', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Macular degeneration', 'Marketing', 'Melissa', 'Optics', 'Persons', 'Phase', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2008,434041,0.06296656060263456
"Origins of Object Knowledge    DESCRIPTION (provided by applicant): Twelve experiments investigate the early development, in human infants, of perception of the unity of partly occluded surfaces. The experiments focus on a time during ontogeny when there may be evidence of visual sensitivity to information specifying object properties, but limited ability to perceive occlusion, with the goal of observing real-time processes by which the infant assembles visible parts of a stimulus into a coherent whole. This approach stipulates that onset of sensitivity to motion and orientation information, development of the oculomotor system, and experience viewing objects undergoing occlusion and disocclusion, play a direct, foundational role in the ontogeny of object perception. That is, there is an hypothesized period, 2 to 4 months of age, during which infants come to use newly-emerged visual skills to perceive objects accurately. The experiments follow a similar strategy: explorations of individual and group differences in both basic visual processing skills and perception of the unity of partly occluded surfaces. Infant perception is assessed with two methods: (a) recording of eye movements, to measure improvements in pickup of important visual .information, and (b) habituation/dishabituation, to ascertain perception of object unity as well as to determine the extent of sensitivity to available visual information. It is expected that the detailed analysis of individual differences afforded by this approach provide the opportunity for exceptionally sensitive measures of the emergence of visual skills and object knowledge. The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the context of the classic nature-nurture debate. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, and how perceptual skills impact knowledge acquisition and structure. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.         n/a",Origins of Object Knowledge,7355592,R01HD040432,"['Academy', 'Adult', 'Age', 'Age-Months', 'Birth', 'Budgets', 'Categories', 'Child Development', 'Cognition', 'Cognitive', 'Cognitive Science', 'Computer information processing', 'Condition', 'Corpus striatum structure', 'Dependence', 'Dependency', 'Detection', 'Development', 'Diagnostic', 'Drug Formulations', 'Elderly', 'Event', 'Exhibits', 'Eye', 'Eye Movements', 'Failure', 'Fostering', 'Frequencies', 'Funding', 'Future', 'Goals', 'Grant', 'Growth', 'Heart', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Infant Development', 'Investigation', 'Journals', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Legal patent', 'Light', 'Link', 'Literature', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Methods', 'Motion', 'Nature', 'Neurobiology', 'None or Not Applicable', 'Pattern', 'Perception', 'Performance', 'Plant Roots', 'Play', 'Preparation', 'Process', 'Process Measure', 'Property', 'Psychology', 'Range', 'Relative (related person)', 'Reporting', 'Research', 'Resources', 'Role', 'Rotation', 'Saccades', 'Sampling', 'Scanning', 'Science', 'Side', 'Specific qualifier value', 'Speed', 'Stimulus', 'Stress', 'Structure', 'Students', 'Suggestion', 'Support of Research', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Vision', 'Visual', 'Visual attention', 'Visual system structure', 'Work', 'Writing', 'abstracting', 'base', 'concept', 'developmental disease', 'experience', 'follow-up', 'gaze', 'indexing', 'infancy', 'meter', 'object perception', 'oculomotor', 'psychologic', 'research study', 'sequence learning', 'size', 'skills', 'spatiotemporal', 'symposium', 'theories', 'tool', 'trend', 'visual information', 'visual process', 'visual processing']",NICHD,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2008,269804,-0.019787915841621404
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7665248,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2008,80289,0.029334244726083585
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7489821,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2008,1042528,0.029334244726083585
"Webcam Interface for Audio/touch Graphics Access by Blind People    DESCRIPTION (provided by applicant):  The goal of this project is to develop a compact inexpensive alternative to the bulky expensive touchpads now required by blind people for audio/touch access to graphical information. Audio/touch is known to provide excellent access to computer-literate blind people as well as people with dyslexia or other severe print disabilities. Preparing Audio/touch materials was very expensive until ViewPlus introduced the IVEO Scalable Vector Graphic (SVG) Authoring/conversion software in 2005. IVEO permits virtually any graphical information to be created or converted/imported easily to a well- structured highly accessible SVG format. Tactile copy was also very expensive before 2000 when ViewPlus introduced the Tiger embossing Windows printers that ""print"" by embossing. The new ViewPlus Emprint printer/embossers emboss and also print color images, creating color tactile images particularly useful for people with dyslexia and a number of other print disabilities. An audio/touch user reads an IVEO SVG graphic using the free IVEO Viewer, a tactile copy of the image, and a touchpad. The user places the tactile graphic on the touchpad and presses a point of interest. The touchpad communicates the position of that point back to the computer, and the IVEO Viewer speaks the appropriate information. Tactile text made from mainstream graphics has a distinctive pattern. When a user presses, that text is spoken by the IVEO Viewer. When the user presses a graphic object having a SVG title within the file, that title will be spoken. Objects may also have arbitrarily long description fields that can be spoken and browsed. All spoken information can be displayed on an attached braille display if desired. Graphical information is ubiquitous today, but almost none is accessible to blind people. Government agencies, libraries, companies, and agencies serving people with disabilities could easily send highly accessible IVEO graphics files and tactile graphic copies to clients with disabilities, but there is a ""chicken and egg"" dilemma that must be overcome before they are likely to do so. Few blind people have a touchpad (which cost $500 or more), so few could use that information. The specific aim of this Phase I proposal is to develop an affordable webcam-based prototype as an alternative to touchpads. It is based on an inexpensive webcam that is focused on the graphic and follows a finger. A touchpad press is emulated in this prototype by pressing some computer key with the other hand. This project could be the key to bringing accessible graphics to all blind computer users and is clearly of interest to NEI whose mission statement includes mental health and quality of life of blind people. PUBLIC HEALTH RELEVANCE:  This proposal is relevant to the mission of the National Eye Institute, because it could be the key to making nearly all graphical information easily accessible to people who are blind or have other severe print disabilities. Graphical information is ubiquitous in the world today but is not presently accessible to blind people except through expensive and time-consuming conversion by trained transcribers. Making all graphical information accessible would have an obviously highly beneficial direct effect on education and professional opportunities, mental health, and quality of life of blind people. Mental health and quality of life issues for blind people are parts of the mission of the National Eye Institute.          n/a",Webcam Interface for Audio/touch Graphics Access by Blind People,7480812,R43EY018973,"['Back', 'Braille Display', 'Businesses', 'Chickens', 'Client', 'Color', 'Communities', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consultations', 'Development', 'Devices', 'Disabled Persons', 'Dyslexia', 'Event', 'Fingers', 'Goals', 'Government Agencies', 'Hand', 'Home environment', 'Image', 'Information Systems', 'Institution', 'Internet', 'Libraries', 'Link', 'Mainstreaming', 'Marketing', 'Mental Health', 'Methods', 'Mission', 'Modeling', 'Mus', 'National Eye Institute', 'Numbers', 'Oregon', 'Pattern', 'Phase', 'Positioning Attribute', 'Printing', 'Professional Education', 'Public Health', 'Publications', 'Quality of life', 'Range', 'Reading', 'Site', 'Structure', 'Structure of nail of finger', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Tigers', 'Time', 'Title', 'Today', 'Touch sensation', 'Training', 'Universities', 'Visual', 'Visually Impaired Persons', 'base', 'blind', 'braille', 'cost', 'desire', 'digital', 'disability', 'egg', 'interest', 'literate', 'print disabilities', 'programs', 'prototype', 'research and development', 'tool', 'touchpad', 'vector']",NEI,"VIEWPLUS TECHNOLOGIES, INC.",R43,2008,100001,0.02857051811572357
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7477498,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2008,365248,0.03207154418291173
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7351808,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Condition', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Range', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Models', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'concept', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2008,482736,0.05343166431183283
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7477064,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Condition', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Experimental Models', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2008,250129,-0.041671127548373176
"Computer Vision Methods for the Real Time Assessment of Dietary Intake    DESCRIPTION (provided by applicant): Obesity is a leading cause of preventable death and disability in the U.S. Self- monitoring of all foods and beverages consumed is central to weight loss and maintenance efforts; however, this places a heavy burden on the user. These same burdens also impede nutritional research. The proposed research is for the testing of a semi-automated, objective, near real-time computer vision and pattern recognition approach to the measurement of dietary intake. In the proposed product, cell phone pictures of meals and snacks will be analyzed by software in an attempt to automatically recognize as many items as possible. A small number of intelligent yes/no questions will help provide additional information when necessary in order to meet the accuracy demands of the target application. Following identification of the items, the software will estimate the portion sizes of all identified items. The experiments comprising this Phase I SBIR are (a) extract the most informative sets of features using a large number of food and beverage items taken from an existing database of real world meal images, (b) compare the accuracy of candidate pattern recognition approaches to identify items based on the extracted features, (c) identify the most feasible algorithms for estimating portion size, and (d) test usability and user acceptance with a simulated version of the product. Phase II will (a) apply the approach to a greater variety of food and beverage items, (b) improve automated analysis, and (c) compare the approach to existing assessment instruments. This research will extend defense- and security-related technologies to the assessment and treatment of obesity.          n/a",Computer Vision Methods for the Real Time Assessment of Dietary Intake,7405586,R43CA124265,"['Address', 'Adherence', 'Algorithms', 'Area', 'Behavior', 'Behavioral', 'Biological Neural Networks', 'Biometry', 'Body Weight decreased', 'Calculi', 'Cellular Phone', 'Cessation of life', 'Class', 'Coin', 'Computer Vision Systems', 'Computer software', 'Data', 'Databases', 'Decision Trees', 'Diabetic Diet', 'Diet Records', 'Dietary intake', 'Disease', 'Eating', 'Eating Behavior', 'Face', 'Feedback', 'Fingerprint', 'Food', 'Food and Beverages', 'Goals', 'Habits', 'Health', 'Image', 'Individual', 'Information Theory', 'Intake', 'Iris', 'Life', 'Life Style', 'Lighting', 'Machine Learning', 'Maintenance', 'Marketing', 'Mathematics', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Numbers', 'Nutritional', 'Nutritionist', 'Obesity', 'Obesity associated disease', 'Pattern Recognition', 'Phase', 'Placement', 'Principal Investigator', 'Public Health', 'Research', 'Research Personnel', 'Security', 'Shapes', 'Simulate', 'Small Business Funding Mechanisms', 'Small Business Innovation Research Grant', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Three-Dimensional Image', 'Time', 'Training', 'Treatment Protocols', 'United States', 'Wing', 'base', 'design', 'digital imaging', 'disability', 'improved', 'innovation', 'instrument', 'interest', 'obesity treatment', 'research study', 'size', 'usability']",NCI,"MEDIABALANCE, INC.",R43,2007,191710,0.01603967908034603
"Mid-Level Vision Systems for Low Vision    DESCRIPTION (provided by applicant): Low vision is a significant reduction of visual function that cannot be fully corrected by ordinary lenses, medical treatment, or surgery. Aging, injuries, and diseases can cause low vision. Its leading causes and those of blindness, include cataracts and age-related macular degeneration (AMD), both of which are more prevalent in the elderly population. Our overarching goal is to develop devices to aid people with low vision. We propose to use techniques of computer vision and computational neuroscience to build systems that enhance natural images. We plan test these systems on normal people and visually impaired older adults, with and without AMD. We have three milestones to reach: 1) Our first milestone will be to develop a system for low-noise image-contrast enhancement, which should help with AMD, because it causes lower contrast sensitivity. 2) Our second milestone will be a system that extracts the main contours of images in a cortical-like manner. Superimposing these contours on images should help with contrast-sensitivity problems at occlusion boundaries. Diffusing regions inside contours should help with crowding problems prominent in AMD. 3) Our third milestone is to probe whether these systems can help people with low vision people. For this purpose, we plan to use a battery of search and recognition psychophysical tests tailor-made for AMD. We put together an interdisciplinary team. The Principal Investigator is Dr. Norberto Grzywacz from Biomedical Engineering at USC. Drs. Gerard Medioni from Computer Science and Bartlett Mel from Biomedical Engineering at USC will lead the efforts in Specific Aims 1 and 2 respectively. Drs. Bosco Tjan from USC Psychology, Susana Chung from the University of Houston, and Eli Peli from Harvard Medical School will lead Specific Aim 3. Other scientists are from USC. They include Dr. Irving Biederman from Psychology, an object-recognition expert and Dr. Mark Humayun, from Ophthalmology, an AMD expert. They also include Dr. lone Fine, from Ophthalmology, an expert in people who recover vision after a prolonged period without it and Dr. Zhong-Lin Lu, an expert on motion perception and perceptual learning.           n/a",Mid-Level Vision Systems for Low Vision,7172503,R01EY016093,"['Age', 'Age related macular degeneration', 'Aging', 'Algorithms', 'Biological', 'Biomedical Engineering', 'Blindness', 'California', 'Cataract', 'Clutterings', 'Cognitive', 'Color Visions', 'Complex', 'Computer Vision Systems', 'Consultations', 'Contrast Sensitivity', 'Crowding', 'Development', 'Devices', 'Diffuse', 'Disease', 'Effectiveness', 'Elderly', 'Engineering', 'Esthetics', 'Evaluation', 'Face', 'Faculty', 'Goals', 'Human', 'Image', 'Image Analysis', 'Inferior', 'Injury', 'Lead', 'Learning', 'Machine Learning', 'Masks', 'Measurement', 'Medical', 'Minor', 'Modeling', 'Motion Perception', 'Nature', 'Noise', 'Operative Surgical Procedures', 'Ophthalmology', 'Optometry', 'Patients', 'Perceptual learning', 'Population', 'Principal Investigator', 'Property', 'Psychologist', 'Psychology', 'Psychophysiology', 'Purpose', 'Range', 'Reading', 'Retina', 'Schools', 'Scientist', 'Shapes', 'Skiing', 'Surface', 'Surface Properties', 'System', 'Techniques', 'Technology', 'Testing', 'Texture', 'Universities', 'Vision', 'Visual', 'Visual Acuity', 'Visual Aid', 'Visual Fields', 'Visual impairment', 'Visual system structure', 'Visually Impaired Persons', 'Work', 'computational neuroscience', 'computer science', 'fovea centralis', 'human subject', 'improved', 'lens', 'medical schools', 'member', 'object recognition', 'symposium', 'tool', 'vision science', 'visual performance']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2007,1159531,0.05554573358229701
"Diagnostic Innovations in Glaucoma: Clinical Electrophysiology    DESCRIPTION (provided by applicant): This application proposes to investigate the diagnostic precision for detecting glaucoma of clinical electrophysiological measurement (pattern electroretinogram, PERG; and multifocal visual evoked potentials, mfVEP), a technique identified as an important recent glaucoma- related development in eye research by the 2004 National Eye Institute (NEI) National Plan. Aim 1: Electrophysiological responses will be characterized in glaucoma, suspect and healthy eyes and the diagnostic accuracy of these commercially available techniques will be compared to current reference standards (evaluation of stereoscopic photographs of the optic disc and standard automated perimetry) and to recently developed diagnostic techniques including optical imaging of the optic disc and retinal nerve fiber layer (RNFL) (confocal scanning laser ophthalmoscopy, optical coherence tomography, and scanning laser polarimetry) and visual function-specific perimetry (short-wavelength automated perimetry and frequency doubling technology perimetry). Aim 2: Novel use of machine learning classifier techniques (e.g. relevance vector machines, support vector machines, mixture of Gaussian techniques, independent components analysis) will be applied to electrophysiological data to improve its diagnostic accuracy and data from different diagnostic techniques (named above) will be combined to improve overall diagnostic accuracy. Aim 3: Electrophysiological measurements will be validated as functional indicators of optic nerve damage by examining the relationship between electrophysiological abnormality and optic disc and RNFL damage in glaucoma and glaucoma suspect patients. 210 patients (105 glaucoma's, 105 glaucoma suspects) and 105 healthy participants will be enrolled and studied cross-sectionally. The specific aims of this proposal address the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset, determining functional correlates of optic nerve damage, and characterizing glaucomatous neurodegeneration within the visual pathways at structural and functional levels. Information about the relative usefulness of electrophysiological measurement, optical imaging techniques, and ganglion cell-specific perimetry for glaucoma detection is important to the clinical community for determining future evidence-based changes in standard of care for glaucoma diagnosis and monitoring. These studies will demonstrate the relative usefulness of electrophysiological measurement (pattern electroretinogram, PERG; multi-focal visual evoked potential, mfVEP) compared to optical imaging techniques (confocal scanning ophthalmoscopy, optical coherence tomography, scanning laser polarimetry) and ganglion cell-specific perimetry (short wavelength and frequency doubling perimetry) for glaucoma detection. The proposal addresses the current NEI Glaucoma and Optic Neuropathies Program objectives of developing improved diagnostic measures to characterize and detect optic nerve disease onset, determining functional correlates of optic nerve damage, and characterizing glaucomatous neurodegeneration within the visual pathways at structural and functional levels. Findings will be important to the clinical community for determining future evidence-based changes in standard of care for glaucoma diagnosis and monitoring.                n/a",Diagnostic Innovations in Glaucoma: Clinical Electrophysiology,7242398,R21EY018190,"['Address', 'California', 'Caring', 'Clinical', 'Communities', 'Data', 'Defect', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Procedure', 'Disease', 'Electrophysiology (science)', 'Electroretinography', 'Enrollment', 'Evaluation', 'Eye', 'Frequencies', 'Funding', 'Future', 'Glaucoma', 'Image', 'Imaging Techniques', 'Individual', 'Investigation', 'Lasers', 'Longitudinal Studies', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Monitor', 'Names', 'National Eye Institute', 'Nerve Degeneration', 'Onset of illness', 'Ophthalmoscopy', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Patients', 'Pattern', 'Perimetry', 'Peripheral', 'Personal Satisfaction', 'Photography', 'Psychophysiology', 'Range', 'Reference Standards', 'Relative (related person)', 'Research', 'Retinal', 'Scanning', 'Scotoma', 'Sensitivity and Specificity', 'Severities', 'Standards of Weights and Measures', 'Structure', 'Suspect Glaucomas', 'Techniques', 'Technology', 'Testing', 'Thick', 'Time', 'United States National Institutes of Health', 'Universities', 'Vision', 'Vision research', 'Visual', 'Visual Pathways', 'Visual evoked cortical potential', 'base', 'design', 'diagnostic accuracy', 'ganglion cell', 'improved', 'independent component analysis', 'innovation', 'novel', 'novel diagnostics', 'optic nerve disorder', 'optical imaging', 'polarimetry', 'programs', 'response', 'retinal nerve fiber layer', 'stereoscopic', 'vector']",NEI,UNIVERSITY OF CALIFORNIA,R21,2007,218125,0.04499988482050782
"Visant-Predictome: A System for Integration, Mining Visualization and Analysis    DESCRIPTION (provided by applicant): Recent and continuing technological advances are producing large amounts of disparate data about cell structure, function and activity. This is driving the development of tools for storing, mining, analyzing, visualizing and integrating data. This proposal describes the VisANT system: a tool for visual data mining that operates on a local database which includes results from our lab, as well as automatically updated proteomics data from web accessible databases such as MIPS and BIND. In addition to accessing its own database, a name normalization table (i.e. a dictionary of identifiers), permits the system to seamlessly retrieve sequence, disease and other data from sources such as GenBank and OMIM. The visualization tool is able to reversibly group related sets of nodes, and display and duplicate their internal structure, providing an approach to hierarchical representation and modeling. We propose to build further on these unique features by including capabilities for mining and representing chemical reactions, orthologous networks, combinatorially regulated transcriptional networks, splice variants and functional hierarchies. Software is open source, and the system also allows users to exchange and integrate the networks that they discover with those of others.           n/a","Visant-Predictome: A System for Integration, Mining Visualization and Analysis",7287965,R01RR022971,"['Address', 'Archives', 'Automobile Driving', 'Bayesian Method', 'Binding', 'Binding Sites', 'Biological', 'Cell physiology', 'Cellular Structures', 'Chemicals', 'Communication', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Condition', 'Data', 'Data Sources', 'Databases', 'Dependence', 'Dependency', 'Development', 'Dictionary', 'Disease', 'Educational workshop', 'Electronic Mail', 'Facility Construction Funding Category', 'Genbank', 'Genes', 'Goals', 'Imagery', 'Information Systems', 'Link', 'Machine Learning', 'Maintenance', 'Methods', 'Mining', 'Modeling', 'Names', 'Network-based', 'Numbers', 'Online Mendelian Inheritance In Man', 'Phylogenetic Analysis', 'Proteomics', 'RNA Splicing', 'Reaction', 'Reporting', 'Score', 'Software Tools', 'Source', 'Structure', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Tertiary Protein Structure', 'Update', 'Ursidae Family', 'Variant', 'Visual', 'Weight', 'base', 'chemical reaction', 'data mining', 'improved', 'models and simulation', 'open source', 'outreach', 'protein protein interaction', 'software development', 'statistics', 'tool', 'tool development', 'web-accessible', 'wiki']",NCRR,BOSTON UNIVERSITY (CHARLES RIVER CAMPUS),R01,2007,446875,0.014847958863040368
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,7194202,R01EY015888,"['Agnosia', 'Area', 'Awareness', 'Color', 'Communication', 'Computer Vision Systems', 'Condition', 'Conscious', 'Cues', 'Development', 'Disease', 'Distant', 'Dyslexia', 'Elements', 'Goals', 'Grouping', 'Image', 'Lateral', 'Lead', 'Literature', 'Location', 'Measurable', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Motion', 'Nature', 'Neighborhoods', 'Paint', 'Perception', 'Process', 'Rate', 'Research', 'Research Personnel', 'Source', 'Staging', 'Stimulus', 'Structure', 'Textbooks', 'Texture', 'Time', 'Vision', 'Visual', 'Visual Fields', 'Visual system structure', 'base', 'design', 'interest', 'millisecond', 'neurophysiology', 'novel', 'perceptual organization', 'receptive field', 'relating to nervous system', 'research study', 'visual process', 'visual processing']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2007,216928,0.03388686462766864
"A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians    DESCRIPTION (provided by applicant): Urban intersections are the most dangerous parts of a blind person's travel. They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult. To alleviate this problem, we propose to develop and evaluate a cell phone-based system to analyze images of street intersections, taken by a blind or visually impaired person using a standard camera cell phone, to provide real-time feedback. Drawing on our recent work on computer vision algorithms that help a blind person find crosswalks and other important features in a street intersection, as well as our ongoing work on cell phone implementations of algorithms for indoor wayfinding and for reading digital appliance displays, we will refine these algorithms and implement them on a cell phone. The information extracted by the algorithms will be communicated to the user with a combination of synthesized speech, audio tones and/or tactile feedback (using the cell phone's built-in vibrator). Human factors studies will help determine how to configure the system and its user controls for maximum effectiveness and ease of use, and provide an evaluation of the overall system. The street intersection analysis software will be made freely available for download into any camera-equipped cell phone that uses the widespread Symbian operating system (such as the popular Nokia cell phone series). The cell phone will not need any hardware modifications or add-ons to run this software. Ultimately a user will be able to download an entire suite of such algorithms for free onto the cell phone he or she is already likely to be carrying, without having to carry a separate piece of equipment for each function. Relevance: The ability to walk safely and confidently along sidewalks and traverse crosswalks is taken for granted every day by the sighted, but approximately 10 million Americans with significant vision impairments and a million who are legally blind face severe difficulties in this task. The proposed research would result in a highly accessible system (with zero or minimal cost to users) to augment existing wayfinding techniques, which could dramatically improve independent travel for blind and visually impaired persons.           n/a",A Cell Phone-Based Street Intersection Analyzer for Visually Impared Pedestrians,7298706,R01EY018345,"['Accidents', 'Address', 'Algorithms', 'American', 'Cellular Phone', 'Complex', 'Computer Vision Systems', 'Computer software', 'Cosmetics', 'Custom', 'Detection', 'Devices', 'Effectiveness', 'Equipment', 'Evaluation', 'Face', 'Feedback', 'Figs - dietary', 'Grant', 'Human', 'Image', 'Image Analysis', 'Impairment', 'Left', 'Light', 'Mainstreaming', 'Marketing', 'Modification', 'Operating System', 'Pattern', 'Performance', 'Reading', 'Research', 'Research Infrastructure', 'Resolution', 'Running', 'Self-Help Devices', 'Series', 'Signal Transduction', 'Speech', 'Standards of Weights and Measures', 'System', 'Tactile', 'Target Populations', 'Techniques', 'Testing', 'Time', 'Training', 'Travel', 'Vision', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'Zebra', 'base', 'blind', 'consumer product', 'cost', 'day', 'digital', 'experience', 'image processing', 'improved', 'legally blind', 'novel', 'prevent', 'programs', 'skills', 'tool', 'trafficking', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2007,338243,0.007179627974862272
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7295688,R21EY017003,"['Access to Information', 'Address', 'Algorithms', 'Auditory', 'Bar Codes', 'Canes', 'Canis familiaris', 'Cellular Phone', 'Clutterings', 'Cognitive', 'Color', 'Complement component C1s', 'Computer Vision Systems', 'Computer software', 'Condition', 'Consultations', 'Databases', 'Detection', 'Development', 'Devices', 'Education', 'Elderly', 'Employment', 'Environment', 'Exhibits', 'Feedback', 'Future', 'Goals', 'Home environment', 'Housing', 'Image', 'Individual', 'Instruction', 'Label', 'Localized', 'Location', 'Modality', 'Museums', 'Paper', 'Pattern', 'Persons', 'Population', 'Printing', 'Psychoacoustics', 'Quality of life', 'Range', 'Rate', 'Reading', 'Research', 'Running', 'Scanning', 'Semantics', 'Shapes', 'Source', 'Speech', 'Standards of Weights and Measures', 'Stress', 'System', 'Tactile', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States', 'Vision', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'age group', 'base', 'blind', 'braille', 'concept', 'cost', 'design', 'interest', 'legally blind', 'meter', 'novel', 'optical character recognition', 'programs', 'prototype', 'research study', 'size', 'skills', 'sound', 'success', 'symposium', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2007,193398,0.050111130497719605
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to build and test a ""Smart Telescope,"" a device for persons with low vision that uses computer vision algorithms to search for, detect and enhance targets such as text and faces to aid in everyday tasks such as travel, navigation and social interactions. The practical, cosmetically acceptable packaging will consist of a miniature camera and visual display discreetly mounted on spectacles or a hat, and a compact computing device and set of controls that fit into a pocket. The Smart Telescope advances today's state of the art in assistive devices for low vision by automatically searching for, detecting and enhancing target objects even when they fill only a small portion of the device's field of view, without the user having to point the device directly or accurately at the target as with optical telescopes. The Smart Telescope is small and lightweight, but large enough for the elderly to handle and control; simple to operate and easy to carry, store, recharge, don and remove. Advanced options are hidden during day-to-day use, but easy to access when necessary. In Phase I, we developed and evaluated a working prototype and received enthusiastic feedback from subjects in our target population. In Phase II we propose to prototype a commercially viable consumer version of the Smart Telescope. The Phase II work plan has four tracks: 1) User interaction and interface design, 2) physical design and configuration, 3) software design and development, and 4) hardware design and development. Smith-Kettlewell's Rehabilitation Engineering Research Center (RERC) will provide expertise for the human factors portions of the project. Blindsight will design and build the device hardware from off-the-shelf components with the help of Bolton Engineering. Low vision experts Drs. Don Fletcher, Melissa Chun and Ian Bailey will work with the RERC to guarantee a practical product for the target audience. The overall aim is to create a commercial version of the proposed device for persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems, increasing mobility and independence for those with acuity between approximately 20/200 and 20/600. At under $1,000, the total market for such a device is estimated at up to 300,000, i.e., 10% of low vision persons in the United States. The commercial version of the Smart Telescope will significantly increase mobility and independence for persons with visual acuity between approximately 20/200 and 20/600, aiding them in everyday tasks such as travel, navigation, and social interactions. It will advance today's state of the art in assistive devices for low vision by improving on and surpassing the capabilities of the traditional optical telescope, greatly benefiting persons with reduced visual acuity, reduced contrast sensitivity, or other loss of visual function caused by macular degeneration, diabetic retinopathy, glaucoma, cataracts, and other eye problems.          n/a",A Smart Telescope for Low Vision,7327116,R44EY014487,"['Algorithms', 'Arts', 'Back', 'Cataract', 'Clutterings', 'Computer Vision Systems', 'Contrast Sensitivity', 'Development', 'Devices', 'Diabetic Retinopathy', 'Elderly', 'Engineering', 'Eye', 'Eyeglasses', 'Face', 'Feedback', 'Glaucoma', 'Human', 'Lighting', 'Location', 'Macular degeneration', 'Marketing', 'Melissa', 'Motion', 'Optics', 'Peripheral', 'Persons', 'Phase', 'Reading', 'Research', 'Self-Help Devices', 'Social Interaction', 'Software Design', 'Target Populations', 'Testing', 'Text', 'Today', 'Travel', 'United States', 'Vision', 'Visual', 'Visual Acuity', 'Visual impairment', 'Work', 'day', 'design', 'improved', 'low vision telescope', 'monocular', 'prototype', 'rehabilitation engineering']",NEI,BLINDSIGHT CORPORATION,R44,2007,448477,0.06296656060263456
"A General Model of Saccadic Selectivity in Visual Search    DESCRIPTION (provided by applicant): Many of our everyday tasks, such as spotting a friend in a crowd or picking a bottle of soda from the refrigerator, require us to perform visual search. Due to this ubiquity of visual search, its study promises to shed light on the fundamental processes that control our visual attention so efficiently in natural tasks. To quantitatively assess search behavior, previous research using simple, artificial displays has employed eye-movement recording to analyze saccadic selectivity, that is, the bias of saccadic endpoints (""landing points"" of eye movements) towards display items that share certain features with the search target. Recently, saccadic selectivity in natural, complex displays has been examined as well (Pomplun, 2006), giving a first insight into eye-movement control as it is performed during everyday tasks. The aim of this project is to devise, implement, and evaluate a general, computational model of saccadic selectivity in visual search tasks. Due to its quantitative nature, absence of freely adjustable parameters, and support from empirical research results, the Area Activation Model (Pomplun, Shen, & Reingold, 2003) is a promising starting point for developing such a model. Its basic assumption is that eye movements in visual search tasks tend to target display areas that provide a maximum amount of task-relevant information for processing. To advance this model towards a general model of saccadic selectivity in visual search, additional eye-movement studies are performed to provide detailed information on the influence of color and target size on saccadic selectivity. Based on the data obtained, various aspects of the influence of display and target features on eye- movement patterns are quantified. These data are used to devise the advanced version of the Area Activation Model. The crucial improvements include the elimination of required empirical a-priori information, the consideration of bottom-up activation, and the applicability of the model to search displays beyond artificial images with discrete items and features. Ideally, the resulting model will be straightforward, consistent with natural principles, and carefully avoiding any freely adjustable model parameters to qualify it as a streamlined and general approach to eye-movement control in visual search. Such a model will be important for understanding the functionality of the visual system and will also have significant impact on the fields of computer vision, human-computer interaction, and cognitive modeling. The project will deepen our understanding of visual attention in general and the visual factors underlying saccade programming in particular. Such understanding will advance the possibilities for surgical and therapeutic treatment of visual illnesses. Moreover, the results of the study can directly be applied to improve current human-computer interfaces for computer-assisted surgery and x-ray image analysis.          n/a",A General Model of Saccadic Selectivity in Visual Search,7305151,R15EY017988,"['Appetitive Behavior', 'Area', 'Attention', 'Cognitive', 'Collection', 'Color', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Computer-Assisted Surgery', 'Crowding', 'Data', 'Data Analyses', 'Dimensions', 'Empirical Research', 'End Point', 'Eye Movements', 'Frequencies', 'Friends', 'Image', 'Image Analysis', 'Light', 'Measurement', 'Modeling', 'Modification', 'Nature', 'Numbers', 'Operative Surgical Procedures', 'Pattern', 'Performance', 'Personal Satisfaction', 'Pliability', 'Positioning Attribute', 'Process', 'Qualifying', 'Research', 'Resources', 'Saccades', 'Scanning', 'Spottings', 'Stimulus', 'Testing', 'Therapeutic', 'User-Computer Interface', 'Visual', 'Visual attention', 'Visual system structure', 'Work', 'base', 'computer human interaction', 'design', 'improved', 'insight', 'programs', 'sample fixation', 'size', 'visual process', 'visual processing', 'visual search']",NEI,UNIVERSITY OF MASSACHUSETTS BOSTON,R15,2007,209463,-0.007157536085422964
"Origins of Object Knowledge    DESCRIPTION (provided by applicant): Twelve experiments investigate the early development, in human infants, of perception of the unity of partly occluded surfaces. The experiments focus on a time during ontogeny when there may be evidence of visual sensitivity to information specifying object properties, but limited ability to perceive occlusion, with the goal of observing real-time processes by which the infant assembles visible parts of a stimulus into a coherent whole. This approach stipulates that onset of sensitivity to motion and orientation information, development of the oculomotor system, and experience viewing objects undergoing occlusion and disocclusion, play a direct, foundational role in the ontogeny of object perception. That is, there is an hypothesized period, 2 to 4 months of age, during which infants come to use newly-emerged visual skills to perceive objects accurately. The experiments follow a similar strategy: explorations of individual and group differences in both basic visual processing skills and perception of the unity of partly occluded surfaces. Infant perception is assessed with two methods: (a) recording of eye movements, to measure improvements in pickup of important visual .information, and (b) habituation/dishabituation, to ascertain perception of object unity as well as to determine the extent of sensitivity to available visual information. It is expected that the detailed analysis of individual differences afforded by this approach provide the opportunity for exceptionally sensitive measures of the emergence of visual skills and object knowledge. The short-term objectives of the present proposal are to elucidate fundamental developmental mechanisms in the context of the classic nature-nurture debate. The long-term goals are to shed light on the larger question of how knowledge is acquired and structured in the human, and how perceptual skills impact knowledge acquisition and structure. In the future, such understanding may aid in the formulation of diagnostics and treatments for some developmental disorders.         n/a",Origins of Object Knowledge,7212153,R01HD040432,"['Academy', 'Adult', 'Age', 'Age-Months', 'Birth', 'Budgets', 'Categories', 'Child Development', 'Cognition', 'Cognitive', 'Cognitive Science', 'Computer information processing', 'Condition', 'Corpus striatum structure', 'Dependence', 'Dependency', 'Detection', 'Development', 'Diagnostic', 'Drug Formulations', 'Elderly', 'Event', 'Exhibits', 'Eye', 'Eye Movements', 'Failure', 'Fostering', 'Frequencies', 'Funding', 'Future', 'Goals', 'Grant', 'Growth', 'Heart', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Infant Development', 'Investigation', 'Journals', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Legal patent', 'Light', 'Link', 'Literature', 'Location', 'Machine Learning', 'Manuscripts', 'Measures', 'Methods', 'Motion', 'Nature', 'Neurobiology', 'None or Not Applicable', 'Pattern', 'Perception', 'Performance', 'Plant Roots', 'Play', 'Preparation', 'Process', 'Process Measure', 'Property', 'Psychology', 'Range', 'Relative (related person)', 'Reporting', 'Research', 'Resources', 'Role', 'Rotation', 'Saccades', 'Sampling', 'Scanning', 'Science', 'Side', 'Specific qualifier value', 'Speed', 'Stimulus', 'Stress', 'Structure', 'Students', 'Suggestion', 'Support of Research', 'Surface', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Translating', 'Universities', 'Vision', 'Visual', 'Visual attention', 'Visual system structure', 'Work', 'Writing', 'abstracting', 'base', 'concept', 'developmental disease', 'experience', 'follow-up', 'gaze', 'indexing', 'infancy', 'meter', 'object perception', 'oculomotor', 'psychologic', 'research study', 'sequence learning', 'size', 'skills', 'spatiotemporal', 'symposium', 'theories', 'tool', 'trend', 'visual information', 'visual process', 'visual processing']",NICHD,NEW YORK UNIVERSITY,R01,2007,272700,-0.019787915841621404
"Mobile Food Intake Visualization and Voice Recognize (FIVR) Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives. n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7490204,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2007,3000,0.02860279315686732
"Mobile Food Intake Visualization and Voice Recognize (FIVR)    DESCRIPTION (provided by applicant):   Inadequate dietary intake assessment tools hamper studying relationships between diet and disease. Methods suitable for use in large epidemiologic studies (e.g., dietary recall, food diaries, and food frequency questionnaires) are subject to considerable inaccuracy, and more accurate methods (e.g., metabolic ward studies, doubly-labeled water) are prohibitively costly and/or labor-intensive for use in population-based studies. A simple, inexpensive and convenient, yet valid, dietary measurement tool is needed to provide more accurate determination of dietary intake in populations. We therefore propose a three-phase project to develop and test a new assessment tool called FIVR (Food Intake Visualization and Voice Recognizer) that uses a novel combination of innovative technologies: advanced voice recognition, visualization techniques, and adaptive user modeling in an electronic system to automatically record and evaluate food intake. FIVR uses cell phones to capture both voice recordings and photographs of dietary intake in real-time. These dual sources of data are sent to a database server for recognition processing for real-time food recognition and portion size measurement through speech recognition and image analysis. The user model will allow for enhanced identification of food and method of preparation in situations that images alone might not produce accurate results as the system learns through experience and adapts to the individual's food patterns. Objectives are to fuse existing voice and image recognition techniques into a system that will recognize foods by food type and unique characteristics and determine volume by film clip showing an image from at least two angles. The item identified will be matched to an appropriate food item and amount within a food composition database and nutrient intake computed. Researchers will be able to view the resulting analysis through an adapted version of the dietary analysis program, ProNutra. The proposed protocol will incorporate three discrete phases: 1) technology development, integration, and testing; 2) validity testing with a controlled diet (metabolic ward study); and 3) real-world utility testing. Validity of the data collected will be judged by how closely the nutrient calculations match the known composition of the metabolic ward diets consumed. FIVR has the potential to establish a method of highly accurate dietary intake assessment suitable for cost-effective use at the population level, and thereby advance crucial public health objectives.             n/a",Mobile Food Intake Visualization and Voice Recognize (FIVR),7340845,U01HL091738,"['Accounting', 'Algorithms', 'Cellular Phone', 'Characteristics', 'Clip', 'Computer Vision Systems', 'Data', 'Data Sources', 'Databases', 'Detection', 'Diet', 'Diet Records', 'Diet Research', 'Dietary intake', 'Disease', 'Eating', 'Electronics', 'Energy Intake', 'Environment', 'Epidemiologic Studies', 'Film', 'Food', 'Food Patterns', 'Frequencies', 'Image', 'Image Analysis', 'Imagery', 'Individual', 'Intake', 'Internet', 'Label', 'Learning', 'Life', 'Measurement', 'Measures', 'Metabolic', 'Methods', 'Modeling', 'Multimedia', 'Nutrient', 'Nutritional Study', 'Outcome', 'Patient Self-Report', 'Phase', 'Population', 'Preparation', 'Process', 'Protocols documentation', 'Public Health', 'Qualitative Evaluations', 'Quantitative Evaluations', 'Questionnaires', 'Reliance', 'Reminder Systems', 'Research', 'Research Personnel', 'Standards of Weights and Measures', 'Surveys', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Visual', 'Voice', 'Water', 'base', 'cost', 'experience', 'feeding', 'graphical user interface', 'innovative technologies', 'novel', 'programs', 'size', 'speech recognition', 'technology development', 'tool', 'voice recognition', 'ward']",NHLBI,"VIOCARE, INC.",U01,2007,1039742,0.029334244726083585
"Indoor Magnetic Wayfinding For The Visually Impaired    DESCRIPTION (provided by applicant): Advanced Medical Electronics (AME) proposes the development of an indoor way-finding device utilizing the unique magnetic anomaly patterns that exist in modern, man-made structures. The proposed system will record the magnitude of magnetic field strength from sensors in three orthogonal axes. The time history of these magnetic data points can be continuously compared with an electronic map of magnetic anomalies (or, ""signature"") to determine current position within a building. The phase I developed prototype system tracked in feasibility experiments with an accuracy of 1 foot (radius). Magnetic anomalies render a magnetic compass useless for finding a directional bearing. However, these same invisible anomalies represent valuable, unique indoor terrain features measurable by magnetic sensors located inside a small, portable device. Such a device would be able to provide low-vision users with a valuable indoor low-cost way-finding tool analogous to a Global Positioning System (GPS) device used outdoors. About 3.7 million Americans are visually disabled. Of these, 200,000 are blind, and the rest have low vision. The key advantage of the way-finding concept presented in this proposal, over other methods, is that the benefits are made available to the visually impaired community without requiring expensive building infrastructure investments. This is of particular advantage to large government buildings and educational campuses. The proposed approach allows a cost effective solution to way-finding within these buildings.          n/a",Indoor Magnetic Wayfinding For The Visually Impaired,7326673,R44EY015616,"['American', 'Appointment', 'Building Codes', 'Cognition', 'Communities', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Devices', 'Disabled Persons', 'Doctor of Philosophy', 'Education', 'Electronics', 'Engineering', 'Fee-for-Service Plans', 'Funding', 'Government', 'Hand', 'Housing', 'Human', 'Indoor Magnetic Wayfinding', 'Investments', 'Joints', 'Label', 'Location', 'Magnetism', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Medical Electronics', 'Minnesota', 'Modeling', 'Modification', 'Neurosciences', 'Numbers', 'Oceans', 'Pattern', 'Pattern Recognition', 'Pennsylvania', 'Persons', 'Phase', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Production', 'Psychology', 'Recording of previous events', 'Records', 'Reporting', 'Research', 'Research Infrastructure', 'Rest', 'Services', 'Silicon Dioxide', 'Solutions', 'Somatotype', 'Speech', 'Steel', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Today', 'Universities', 'Vision', 'Visual Perception', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Wireless Technology', 'World Health Organization', 'base', 'blind', 'college', 'computer science', 'concept', 'cost', 'cost effective', 'court', 'design', 'digital', 'foot', 'human subject', 'innovation', 'interest', 'magnetic field', 'miniaturize', 'motor control', 'performance tests', 'professor', 'prototype', 'radius bone structure', 'research study', 'sensor', 'sensory integration', 'tool', 'way finding']",NEI,ADVANCED MEDICAL ELECTRONICS CORPORATION,R44,2007,386674,0.03207154418291173
"Designing Visually Accessible Spaces    DESCRIPTION (provided by applicant): Reduced mobility is one of the most debilitating consequences of vision loss for more than three million Americans with low vision. We define visual accessibility as the use of vision to travel efficiently and safely through an environment, to perceive the spatial layout of key features in the environment, and to keep track of one's location in the environment. Our long-term goal is to create tools to enable the design of safe environments for the mobility of low-vision individuals and to enhance safety for the elderly and others who may need to operate under low lighting and other visually challenging conditions. We plan to develop a computer-based design tool in which environments (such as a hotel lobby, large classroom, or hospital reception area), could be simulated with sufficient accuracy to predict the visibility of key landmarks or obstacles, such as steps or benches, under differing lighting conditions. Our project addresses one of the National Eye Institute's program objectives: ""Develop a knowledge base of design requirements for architectural structures, open spaces, and parks and the devices necessary for optimizing the execution of navigation and other everyday tasks by people with visual impairments"". Our research plan has four specific goals: 1) Develop methods for predicting the physical levels of light reaching the eye in existing or planned architectural spaces. 2) Acquire performance data for normally sighted subjects with visual restrictions and people with low vision to investigate perceptual capabilities critical to visually-based mobility. 3) Develop models that can predict perceptual competence on tasks critical to visually-based mobility. 4) Demonstrate a proof-of-concept software tool that operates on design models from existing architectural design systems and is able to highlight potential obstacles to visual accessibility. The lead investigators in our partnership come from three institutions: University of Minnesota Gordon Legge, Daniel Kersten; University of Utah William Thompson, Peter Shirley, Sarah Creem-Regehr; and Indiana University Robert Shakespeare. This interdisciplinary team has expertise in the four areas required for programmatic research on visual accessibility empirical studies of normal and low vision (Legge, Kersten, Creem-Regehr, and Thompson), computational modeling of perception (Legge, Kersten, and Thompson), photometrically correct computer graphics (Shirley), and architectural design and lighting (Shakespeare).           n/a",Designing Visually Accessible Spaces,7172766,R01EY017835,"['Accounting', 'Address', 'American', 'Area', 'Arts', 'Blindness', 'Characteristics', 'Classification', 'Competence', 'Complex', 'Computer Graphics', 'Computer Simulation', 'Computer Vision Systems', 'Computers', 'Condition', 'Contrast Sensitivity', 'Data', 'Depressed mood', 'Detection', 'Development', 'Devices', 'Elderly', 'Engineering', 'Environment', 'Evaluation', 'Eye', 'Goals', 'Hospitals', 'Indiana', 'Individual', 'Institution', 'Lead', 'Light', 'Lighting', 'Lobbying', 'Location', 'Measurement', 'Methods', 'Minnesota', 'Modeling', 'National Eye Institute', 'Pattern', 'Perception', 'Performance', 'Peripheral', 'Photometry', 'Published Comment', 'Range', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Research Priority', 'Safety', 'Simulate', 'Software Tools', 'Space Models', 'Space Perception', 'Specialist', 'Specific qualifier value', 'Structure', 'Surface', 'System', 'Testing', 'Travel', 'Universities', 'Utah', 'Vision', 'Visual', 'Visual impairment', 'base', 'concept', 'design', 'innovation', 'knowledge base', 'luminance', 'model design', 'model development', 'physical model', 'predictive modeling', 'programs', 'tool', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2007,487230,0.05343166431183283
"CRCNS: Where to look next? Modeling eye movements in normal and impaired vision    DESCRIPTION (provided by applicant): The goal of this proposal is to gain a better understanding of the information processing and decision strategies that underlie eye movement planning in both the normal and diseased state. In patients with age-related macular degeneration (AMD), central areas of the retina are damaged, creating a large blind spot that forces them to rely solely on residual vision in the periphery. Rehabilitation outcomes for these patients can be successful, but are often inconsistent. Despite similar retinopathies, some patients learn to use their residual vision more effectively than others. We have developed an information-theoretic model and experimental paradigm which will allow us to objectively measure human scanning efficiency. The development of the model has naturally motivated fundamental experimental questions about eye movements and neural decision making. The answers to these questions will be used to refine the model and enhance our understanding of the system in general. We will then apply the model framework to investigate differences in eye movement behavior between AMD patients and normally-sighted individuals. The interplay of model development and experimental investigation will significantly increase our knowledge of how humans use prior knowledge and task demands to direct their gaze, and how new visual information is incorporated into an eye movement plan. The results will have broad relevance to understanding neural decision making in general.   Relevance to Public Health. The application of the model to a clinical population will bring much-needed objective measures to understanding the extent of impairment in individuals with AMD. With this understanding comes great potential for improving rehabilitation training strategies that will enhance the quality of life for these patients and their families.          n/a",CRCNS: Where to look next? Modeling eye movements in normal and impaired vision,7271209,R01EY018004,"['Age related macular degeneration', 'Algorithms', 'Architecture', 'Area', 'Behavior', 'California', 'Clinic', 'Clinical', 'Computer Simulation', 'Computer Vision Systems', 'Computer information processing', 'Condition', 'Decision Making', 'Doctor of Philosophy', 'Ensure', 'Experimental Models', 'Eye Movements', 'Family', 'Goals', 'Human', 'Impairment', 'Individual', 'Information Theory', 'Investigation', 'Knowledge', 'Learning', 'Measures', 'Medical center', 'Modeling', 'Movement', 'Ophthalmologist', 'Outcome', 'Patients', 'Pattern', 'Population', 'Positioning Attribute', 'Principal Investigator', 'Process', 'Psychophysics', 'Psychophysiology', 'Public Health', 'Quality of life', 'Recruitment Activity', 'Rehabilitation Outcome', 'Rehabilitation therapy', 'Research', 'Research Personnel', 'Residual state', 'Retina', 'Retinal', 'Retinal Diseases', 'Retinal blind spot', 'Saccades', 'Scanning', 'Signal Detection Analysis', 'Statistical Models', 'System', 'Techniques', 'Training', 'Training Programs', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'behavior prediction', 'design', 'disease characteristic', 'experience', 'gaze', 'image processing', 'improved', 'model development', 'predictive modeling', 'relating to nervous system', 'research study', 'sample fixation', 'success', 'visual information']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2007,240641,-0.041671127548373176
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,7287568,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2006,38558,0.024261345929877765
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,7004518,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2006,457462,-0.014969553986069025
"Wayfinding for the blind & visually impaired using passive environmental labels    DESCRIPTION (provided by applicant): The objective of this proposal is to tackle the problem of way finding (finding one's way in an environment), faced by blind and severely visually impaired persons who are unable to find or read signs, landmarks and locations. We propose a novel and very inexpensive environmental labeling system to provide this population with access to information needed for indoor way finding (where GPS is not available). The system uses simple passive landmark symbols printed on paper or other material, placed next to text, Braille signs or barcode at locations of interest (offices, bathrooms, etc.) in an environment such as an office building. These printed patterns contain spatial and semantic information that is detected using computer vision algorithms running on a standard camera cell phone. By scanning the environment with the device, which detects all landmark symbols in its line of sight up to distances of 10 meters, the user can determine his or her approximate location in the environment as well as the information encoded near each landmark symbol. The system extracts this information in real-time and communicates it to the user by sound, synthesized speech and/or tactile feedback. This information includes spatial (e.g. audio tones to indicate the presence and direction of a label in the camera's field of view) and semantic information (""Mr. Johnson's office, room 429, at 11 o'clock""). The research proposed here will produce a prototype system that will be tested by blind and low vision subjects. Our team includes a blind expert on psychoacoustics (and other in-house blind staff) and an expert consultant on low-vision way finding and navigation to help optimize the user interface and guide development into a practical, easy-to-use system.              n/a",Wayfinding for the blind & visually impaired using passive environmental labels,7143942,R21EY017003,"['clinical research', 'computers', 'reading', 'semantics', 'touch', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,224601,0.050111130497719605
"Perceptual Organization and Attention: Behavior & fMRI    DESCRIPTION (provided by applicant): Visual perception is selective, and visual attention is the mechanism by which salient or high-priority objects are prioritized for awareness and action. Recent work has shown that attentional selection often operates on perceptual objects and not simply on spatial locations. In a multi-object scene involving partial occlusion, image regions must be grouped into coherent object representations prior to attentional deployment. The visual system employs a set of heuristics that serve to constrain object recognition based on principles of perceptual organization. Despite their importance to human navigation and behavior, knowledge of these heuristics and the neural mechanisms that underlie this process remain poorly understood. The proposed project will examine the grouping principles that are critical to object-based attentional selection, and will enumerate the dominance relations among these principles when they are consistent with competing groupings within a scene. The project also explores the neural implementation of these processes to extend our knowledge of the constraints built into the object recognition and visual attention systems. The combination of behavioral and neuroimaging methods outlined in this application will result in a better understanding of the neural circuitry that is responsible for efficient, goal-directed object recognition.           n/a",Perceptual Organization and Attention: Behavior & fMRI,7112773,F31NS055664,"['artificial intelligence', 'attention', 'behavior', 'clinical research', 'cues', 'visual perception']",NINDS,JOHNS HOPKINS UNIVERSITY,F31,2006,37861,-0.09477754422832546
"Spatial Modeling in Glaucoma DESCRIPTION (provided by applicant): This career training proposal is to train Michael D. Twa, OD, MS as an independent clinician-scientist. A five year training program is proposed, consisting of formal coursework in vision science, specific training in computer science and image processing, and mentoring in the application of these skills to clinical outcomes research in glaucoma. In September 2003, NIH announced a new ""Roadmap"" to accelerate advances in biomedical research for the 21st century. Three areas listed in this Roadmap are relevant to this research proposal: (1) Interdisciplinary research training. (2) Clinical research informatics. (3) Development of enabling technologies for improved assessment of clinical outcomes. The Roadmap emphasizes coordinated strategies to develop both technological and human resources to take full advantage of multidisciplinary and translational research opportunities. This proposal addresses the stated training objectives at an individual level.  Glaucoma is a leading cause of blindness. Visual field assessment and optic nerve head imaging (confocal scanning laser tomography) are commonly used to diagnose the disease and monitor its progression, yet there is considerable controversy about how to interpret and make best use of this information. Currently, raw data from these observations are reduced to statistical indices that are meant to summarize clinically meaningful features and provide a basis for classifying test results as normal or not. Unfortunately, these indices may sacrifice other relevant features in the data for interpretability.  We will use mathematical modeling methods (polynomial modeling, spline fitting and wavelet analysis) to quantify patterns in visual field data and topographic images of the optic nerve head. We will use features derived from these modeling methods to apply novel pattern recognition techniques from computer and information sciences-decision trees and non-linear regression analysis-and then compare these techniques to current methods to identify glaucoma. By improving current methods of analysis we can provide a more quantitative basis for clinical decisions, and offer greater consistency and objectivity on data interpretation. The long-term objective of this proposal is to translate advances in computer and information sciences to the analysis of clinical outcomes research in glaucoma and other eye diseases. n/a",Spatial Modeling in Glaucoma,7015012,K23EY016225,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer assisted diagnosis', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human data', 'image processing', 'mathematical model', 'model design /development', 'neuroimaging', 'optic nerve', 'patient oriented research', 'tomography', 'visual fields']",NEI,OHIO STATE UNIVERSITY,K23,2006,142642,0.03695191109343796
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,7096566,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2006,214738,0.03197703143484375
"Diagnostic aid software for the visual field test Visual field (VF) test is a widely used, noninvasive technique for evaluating pathology or dysfunction in the visual pathways. The VF test, in conjunction with other diagnostics, is used for detection of  laucoma and for following its progression. Early detection is critical as blindness from glaucoma is preventable in nearly all cases, provided treatment is administered early in the progression. There is a need for an automated decision aid tool that will facilitate and standardize the interpretation task. Following a successful Phase I feasibility demonstration, Phase II will apply novel machine learning approaches to the problem of glaucoma diagnosis via an automated analysis of visual field and ancillary data. IAC will develop an integrated, user friendly software program that will provide a reliable detailed classification of glaucomatous and non-glaucomatous defects with the main emphasis on glaucomatous defects and early detection. The aim is to achieve classification accuracy close to that of a highly skilled human expert. The diagnosis suggested by the software will be supported by a set of comprehensive rules extracted from the classification algorithm. Optionally, the program will provide measures of visual field and glaucoma progression. n/a",Diagnostic aid software for the visual field test,7120029,R44EY014077,"['computer human interaction', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease /disorder classification', 'early diagnosis', 'eye disorder diagnosis', 'glaucoma', 'human data', 'mathematics', 'model design /development', 'pathologic process', 'visual fields']",NEI,"BIOFORMATIX, INC.",R44,2006,325411,0.060002801923794195
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,7057358,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2006,180672,0.03960396795020501
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,7037390,R01EY015888,"['clinical research', 'computational neuroscience', 'cues', 'form /pattern perception', 'human subject', 'mathematical model', 'mental process', 'motion perception', 'neural information processing', 'neuropsychological tests', 'neuropsychology', 'psychophysics', 'space perception', 'statistics /biometry', 'time perception', 'vision tests', 'visual depth perception', 'visual stimulus', 'visual tracking']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2006,215583,0.03388686462766864
"MobileEye OCR for the Visually Impaired    DESCRIPTION (provided by applicant): In this SBIR we propose to demonstrate the technical feasibility of Mobile OCR, a portable software system which makes use of existing personal devices to provide access to textual materials for the elderly or the visually impaired. The system will help these low vision individuals with basic daily activities, such as shopping, preparing meals, taking medication, and reading traffic signs. It will step beyond our proposed MobileEyes vision enhancement system to apply cutting edge recognition technology for mobile devices. The system will use common camera phone hardware to capture and enhance textual information, perform Optical Character Recognition (OCR) and provide audio or visual feedback. Our research will focus on implementing and integrating new vision enhancement and analysis techniques on limited resource mobile devices. Specifically, we will develop algorithms for detection and rectification of text on planes and generalized cylinders subject to perspective distortions, implement more robust and efficient algorithms and systems for stabilization and enhancement of text blocks, provide mobile OCR on complex textured backgrounds, and implement these techniques on small devices across a variety of platforms. The recognized text will be presented through Text-to-Speech (TTS), or displayed on the device with enhanced quality which can be easily read by low vision users. Phase I will focus on demonstrating the technical feasibility of our approach, and will incorporate a performance measurement methodology to quantitatively evaluate progress and evaluate our system against other approaches. In comparison to existing vision enhancement devices, such as magnifying glasses, telescopes, and text reading devices such as scanner-based OCR, our solution has several advantages: 1) it makes use of a single, portable device (camera cell phone) that is commonly available and typically already carried for its telecommunications capabilities; 2) it can be used selectively by users so they will not be overwhelmed by irrelevant information; and 3) it can be integrated directly with other applications for specialized tasks. Our research results will impact the millions of low-vision individuals and the blind, as well as vision and computer vision researchers. Our team is uniquely qualified to explore the feasibility of extending visual applications to these devices, and provide a platform for integrating future vision algorithms.         n/a",MobileEye OCR for the Visually Impaired,7053650,R43EY017216,"['reading', 'solutions', 'vision']",NEI,"APPLIED MEDIA ANALYSIS, LLC",R43,2006,104935,0.06769777108117309
"Accessible Artificial Intelligence Tutoring Software DESCRIPTION (provided by applicant): Quantum has successfully developed, tested and brought to the classroom the first artificial intelligence (Al) tutoring systems in chemistry education. This work successfully addressed several longstanding, clearly articulated needs for improved interactive educational software. A leading distributor for the U.S. and Canada, Science Kit & Boreal Laboratories, as well as prominent textbook publisher, Holt, Rinehart and Winston, have entered into long-term contracts with Quantum, resulting in rapid dissemination to an established end user base. The aim of this Phase I SBIR proposal is to bring the full power and benefits of this cutting-edge new educational technology to students who are blind and visually impaired. There is a considerable need for improved educational software for science education in general, but the problem of quality educational software materials for the blind is known to be particularly acute. Certain unique attributes of the Quantum Al Tutors make them potentially very well suited for full accessibility to the blind using Internet-capable screen reader technology. The potential technological innovation here is the development of advanced Al tutoring technology that has accessibility built into its framework design. If successful, an immediate outcome will be the first Al tutoring systems that are accessible to blind students, delivered through the Internet. A formulation of an Al tutoring methodology with accessibility inherent to the design will have broad implications for the prospect of developing sophisticated accessible educational software in all content areas, beyond chemistry. This project can only be accomplished by working intimately with experts in education for the blind, and Quantum has arranged a number of important partnerships in this respect, for research as well as commercialization of the resulting technology, including: the National Federation of the Blind, the American Printing House for the Blind, Pearson Learning Group, Bartimaeus Group and Henter Mathematics. n/a",Accessible Artificial Intelligence Tutoring Software,6880607,R43EY016251,"['Internet', 'artificial intelligence', 'blind aid', 'chemistry', 'computer assisted instruction', 'computer human interaction', 'computer program /software', 'educational resource design /development', 'science education', 'technology /technique development']",NEI,"QUANTUM SIMULATIONS, INC.",R43,2005,100721,-0.00024122409867496968
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6910621,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2005,245768,0.024261345929877765
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6995047,R43EY014487,"['artificial intelligence', 'biomedical equipment development', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection', 'digital imaging', 'functional ability', 'human subject', 'image processing', 'medical rehabilitation related tag', 'patient oriented research', 'portable biomedical equipment', 'questionnaires', 'vision aid', 'vision disorders', 'visual fields', 'visual perception', 'visual threshold', 'visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2005,144106,0.013735046471733249
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6832762,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2005,461157,-0.014969553986069025
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6879624,R01HD041386,"['Internet', 'artificial intelligence', 'attitude', 'behavior prediction', 'behavior test', 'behavioral /social science research tag', 'clinical research', 'computer human interaction', 'cues', 'data collection methodology /evaluation', 'human subject', 'imagery', 'interactive multimedia', 'mathematics', 'population survey', 'questionnaires', 'space perception', 'visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2005,206550,0.018112886733702906
"Spatial Modeling in Glaucoma DESCRIPTION (provided by applicant): This career training proposal is to train Michael D. Twa, OD, MS as an independent clinician-scientist. A five year training program is proposed, consisting of formal coursework in vision science, specific training in computer science and image processing, and mentoring in the application of these skills to clinical outcomes research in glaucoma. In September 2003, NIH announced a new ""Roadmap"" to accelerate advances in biomedical research for the 21st century. Three areas listed in this Roadmap are relevant to this research proposal: (1) Interdisciplinary research training. (2) Clinical research informatics. (3) Development of enabling technologies for improved assessment of clinical outcomes. The Roadmap emphasizes coordinated strategies to develop both technological and human resources to take full advantage of multidisciplinary and translational research opportunities. This proposal addresses the stated training objectives at an individual level.  Glaucoma is a leading cause of blindness. Visual field assessment and optic nerve head imaging (confocal scanning laser tomography) are commonly used to diagnose the disease and monitor its progression, yet there is considerable controversy about how to interpret and make best use of this information. Currently, raw data from these observations are reduced to statistical indices that are meant to summarize clinically meaningful features and provide a basis for classifying test results as normal or not. Unfortunately, these indices may sacrifice other relevant features in the data for interpretability.  We will use mathematical modeling methods (polynomial modeling, spline fitting and wavelet analysis) to quantify patterns in visual field data and topographic images of the optic nerve head. We will use features derived from these modeling methods to apply novel pattern recognition techniques from computer and information sciences-decision trees and non-linear regression analysis-and then compare these techniques to current methods to identify glaucoma. By improving current methods of analysis we can provide a more quantitative basis for clinical decisions, and offer greater consistency and objectivity on data interpretation. The long-term objective of this proposal is to translate advances in computer and information sciences to the analysis of clinical outcomes research in glaucoma and other eye diseases. n/a",Spatial Modeling in Glaucoma,6863529,K23EY016225,"['artificial intelligence', 'bioimaging /biomedical imaging', 'clinical research', 'computer assisted diagnosis', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human data', 'image processing', 'mathematical model', 'model design /development', 'neuroimaging', 'optic nerve', 'patient oriented research', 'tomography', 'visual fields']",NEI,OHIO STATE UNIVERSITY,K23,2005,143096,0.03695191109343796
"Medical Advice from Glaucoma Informatics (MAGI)  DESCRIPTION (provided by applicant):  The project, Medical Advice from Glaucoma Informatics (MAGI), seeks to improve glaucoma diagnosis and management with state-of-the-art machine learning classifiers. These classifiers will automate the interpretation of standard automated perimetry (SAP), newer visual field tests, and structural tests for glaucoma in the general population and in stratified glaucoma populations. Phase 1 will complete the feasibility testing already underway. Phase 2 will apply the refined methods to a wider set of glaucoma testing problems.The management of glaucoma depends on a series of classifications. The glaucoma provider classifies tests as normal or indicative of glaucoma. The clinician then determines whether an eye has glaucoma or has had progression. Assembling these classifications, the provider makes decisions about management. Automated test interpreters, either as part of the testing machine or as a computer-based resource, can aid glaucoma providers with real-time interpretations. The research we propose takes advantage of our extensive data sets and builds on the ongoing research in our laboratories.Statistical classifiers, Bayesian nets, machine learning classifiers, and expert systems represent different types of classifiers with diverse properties. Machine learning classifiers can perform exceptionally well at identifying classes, even when the data are complex and have dependencies. We will test and select the optimal machine learning classifier for diagnosis. We will further improve classifier performance and determine feature utility by optimizing the feature set visual field tests are time consuming and stressful. We will streamline the tests by removing unimportant test points.Even with decades of experience, there is uncertainty with regard to the evaluation of the SAP. There is less accumulated knowledge about non-standard tests, such as short-wavelength automated perimetry, nerve fiber layer thickness, or optic nerve head topography. Machine classifiers may learn how to interpret nonstandard tests better. We will go beyond STATPAC's capabilities with classifiers that have learned to interpret SAP, nonstandard visual field tests, structural glaucoma tests, and STATPAC plots in the general population and in patients stratified by race, family history, and other information available at the time of the test.Conversion of suspects to glaucoma and progression of glaucoma cannot yet be predicted from tests. We will develop classifiers for these predictions. Classifiers will be designed to diagnose early glaucoma, detect early progression, and identify glaucomatous eyes at risk of progression.Unsupervised learning provides cluster analysis that can determine distinct groups with members in some way similar from the test data. In an effort to discover new and use useful information with unsupervised learning, we will mine our data in visual function and structural tests for glaucoma  and in specific combinations of population groups. n/a",Medical Advice from Glaucoma Informatics (MAGI),6937074,R33EY013928,"['clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'early diagnosis', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human subject', 'neural information processing', 'neuropathology', 'visual fields']",NEI,UNIVERSITY OF CALIFORNIA SAN DIEGO,R33,2005,606534,0.06358857418637343
"Traffic Intersection Analysis Algorithms for the Blind DESCRIPTION (provided by applicant): This project aims to explore, develop and test computer vision algorithms to analyze images of street intersections from a camera worn by a blind person.  Urban intersections are the most dangerous parts of a blind person's travel.  They are becoming increasingly complex, making safe crossing using conventional blind orientation and mobility techniques ever more difficult.  We will explore computer vision algorithms to help a blind person find the crosswalk, find the pedestrian signal button, determine when the ""walk"" light is on, and alert him/her to any veering out of the crosswalk.  We will emphasize the development of completely novel methods of analyzing non-ideal images including shadows, occlusions and other irregularities using spatial grouping techniques based on Bayesian inference.  The resulting algorithms are intended for eventual integration as modules for a computer vision system we are already developing to help blind persons with travel tasks such as finding and reading aloud printed signs and negotiating street crossings.  The combined system would have potential for a radical advance in independent travel for blind persons.  In this exploratory project, we aim to: (1) Explore and test alternative approaches to algorithm design to process intersection images and extract the information about the crosswalk, crossing signal, etc., using a database of real-world images taken by blind persons at a variety of different kinds of intersections.  (2) Test the algorithms using a portable camera connected to a notebook computer with speech output. n/a",Traffic Intersection Analysis Algorithms for the Blind,6920594,R21EY015187,"['blind aid', 'blindness', 'clinical research', 'computer simulation', 'computer system design /evaluation', 'gait', 'human subject', 'injury prevention', 'mathematical model', 'statistics /biometry', 'transportation /recreation safety', 'urban area']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R21,2005,255198,0.03197703143484375
"Diagnostic aid software for the visual field test Visual field (VF) test is a widely used, noninvasive technique for evaluating pathology or dysfunction in the visual pathways. The VF test, in conjunction with other diagnostics, is used for detection of  laucoma and for following its progression. Early detection is critical as blindness from glaucoma is preventable in nearly all cases, provided treatment is administered early in the progression. There is a need for an automated decision aid tool that will facilitate and standardize the interpretation task. Following a successful Phase I feasibility demonstration, Phase II will apply novel machine learning approaches to the problem of glaucoma diagnosis via an automated analysis of visual field and ancillary data. IAC will develop an integrated, user friendly software program that will provide a reliable detailed classification of glaucomatous and non-glaucomatous defects with the main emphasis on glaucomatous defects and early detection. The aim is to achieve classification accuracy close to that of a highly skilled human expert. The diagnosis suggested by the software will be supported by a set of comprehensive rules extracted from the classification algorithm. Optionally, the program will provide measures of visual field and glaucoma progression. n/a",Diagnostic aid software for the visual field test,6882489,R44EY014077,"['computer human interaction', 'computer program /software', 'computer system design /evaluation', 'diagnosis design /evaluation', 'disease /disorder classification', 'early diagnosis', 'eye disorder diagnosis', 'glaucoma', 'human data', 'mathematics', 'model design /development', 'pathologic process', 'visual fields']",NEI,"BIOFORMATIX, INC.",R44,2005,324664,0.060002801923794195
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,6891373,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2005,175413,0.03960396795020501
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6910762,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2005,611743,0.03372195171264315
"The formation of visual objects    DESCRIPTION (provided by applicant): Perceptual grouping is the process by which the initially raw and inchoate visual image is organized into perceptual ""objects"". What spatial factors induce perceptual grouping? What is the sequence of computations whereby the image is progressively organized? One source of difficulty in modeling this process is that, unlike many aspects of early vision, perceptual grouping inherently involves non-local: computations - integration of cues from potentially distant locations in the image. Another difficulty in understanding perceptual grouping has been the lack of objective and temporally precise methods for actually measuring the observer's subjective organization of an image. This proposal seeks to combine (a) recent advances in understanding the non-local computations involved in perceptual grouping with (b) novel experimental methods for determining subjective organization. The experimental methods are based on the finding that perceptual objects enjoy certain objectively measurable benefits, including more efficient visual comparisons within them than between distinct objects. This proposal seeks to use this effect to discover what the visual system in fact treats as a perceptual object, and how this percept develops over the course of processing. Most of the proposed experiments involve carefully constructed artificial stimuli with various grouping cues in force, designed to allow detailed comparisons of the strength, interaction, and time-course of each potential grouping cue. In addition, several experiments involve natural images, in order to uncover how perceptual organization proceeds under more naturalistic conditions. This research may lead to technological advancement in the area of computer vision, as well as to better understanding of disorders of perceptual organization such as visual agnosia and dyslexia.         n/a",The formation of visual objects,6924971,R01EY015888,"['clinical research', 'computational neuroscience', 'cues', 'form /pattern perception', 'human subject', 'mathematical model', 'mental process', 'motion perception', 'neural information processing', 'neuropsychological tests', 'neuropsychology', 'psychophysics', 'space perception', 'statistics /biometry', 'time perception', 'vision tests', 'visual depth perception', 'visual stimulus', 'visual tracking']",NEI,RUTGERS THE ST UNIV OF NJ NEW BRUNSWICK,R01,2005,222488,0.03388686462766864
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6774688,R01EY014162,"['artificial intelligence', 'bioimaging /biomedical imaging', 'computer assisted diagnosis', 'computer program /software', 'computer simulation', 'computer system design /evaluation', 'diagnosis design /evaluation', 'eye disorder diagnosis', 'eye refractometry', 'human data', 'image processing', 'ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2004,242058,0.024261345929877765
"Sign Finder: Computer Vision to Find and Read Signs DESCRIPTION (provided by applicant): We propose to build a device that enhances the mobility of visually impaired persons by finding and reading signs aloud without the need for infrastructure beyond ordinary signs. Using new computer vision techniques, it will detect and read text in images captured by a camera worn like a pendant around the user's neck.      We will build two commercially viable, self-contained consumer versions, a $1,500 device using consumer computers and cameras, and a $750 proprietary device.      In typical use, a wearer will select a mode (city street, supermarket) by pressing buttons and optionally speaking commands, and then either point the device at a scene, or scan the scene using auto-repeat image capture. The device will find and read signs, but only output audio for signs relevant to the mode.      The Phase II work plan has three tracks: 1) Computer vision software development and testing, 2) Human interface design and development, and 3) development and testing of the two forms of the device.       Continuing our collaboration from Phase I, we use Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for human factors. Bolton Engineering will design and build the proprietary device hardware. n/a",Sign Finder: Computer Vision to Find and Read Signs,6739928,R44EY011821,"['assistive device /technology', 'blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer system design /evaluation', 'functional ability', 'human subject', 'medical rehabilitation related tag', 'questionnaires']",NEI,BLINDSIGHT CORPORATION,R44,2004,462571,-0.014969553986069025
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6743701,R01HD041386,"['Internet', 'artificial intelligence', 'attitude', 'behavior prediction', 'behavior test', 'behavioral /social science research tag', 'clinical research', 'computer human interaction', 'cues', 'data collection methodology /evaluation', 'human subject', 'imagery', 'interactive multimedia', 'mathematics', 'population survey', 'questionnaires', 'space perception', 'visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2004,201780,0.018112886733702906
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6801171,R01EY013875,"['blind aid', 'blindness', 'clinical research', 'computer human interaction', 'computer program /software', 'cues', 'human subject', 'reading', 'vision aid', 'vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,329706,0.057791081376277996
"Medical Advice from Glaucoma Informatics (MAGI)  DESCRIPTION (provided by applicant):  The project, Medical Advice from Glaucoma Informatics (MAGI), seeks to improve glaucoma diagnosis and management with state-of-the-art machine learning classifiers. These classifiers will automate the interpretation of standard automated perimetry (SAP), newer visual field tests, and structural tests for glaucoma in the general population and in stratified glaucoma populations. Phase 1 will complete the feasibility testing already underway. Phase 2 will apply the refined methods to a wider set of glaucoma testing problems.The management of glaucoma depends on a series of classifications. The glaucoma provider classifies tests as normal or indicative of glaucoma. The clinician then determines whether an eye has glaucoma or has had progression. Assembling these classifications, the provider makes decisions about management. Automated test interpreters, either as part of the testing machine or as a computer-based resource, can aid glaucoma providers with real-time interpretations. The research we propose takes advantage of our extensive data sets and builds on the ongoing research in our laboratories.Statistical classifiers, Bayesian nets, machine learning classifiers, and expert systems represent different types of classifiers with diverse properties. Machine learning classifiers can perform exceptionally well at identifying classes, even when the data are complex and have dependencies. We will test and select the optimal machine learning classifier for diagnosis. We will further improve classifier performance and determine feature utility by optimizing the feature set visual field tests are time consuming and stressful. We will streamline the tests by removing unimportant test points.Even with decades of experience, there is uncertainty with regard to the evaluation of the SAP. There is less accumulated knowledge about non-standard tests, such as short-wavelength automated perimetry, nerve fiber layer thickness, or optic nerve head topography. Machine classifiers may learn how to interpret nonstandard tests better. We will go beyond STATPAC's capabilities with classifiers that have learned to interpret SAP, nonstandard visual field tests, structural glaucoma tests, and STATPAC plots in the general population and in patients stratified by race, family history, and other information available at the time of the test.Conversion of suspects to glaucoma and progression of glaucoma cannot yet be predicted from tests. We will develop classifiers for these predictions. Classifiers will be designed to diagnose early glaucoma, detect early progression, and identify glaucomatous eyes at risk of progression.Unsupervised learning provides cluster analysis that can determine distinct groups with members in some way similar from the test data. In an effort to discover new and use useful information with unsupervised learning, we will mine our data in visual function and structural tests for glaucoma  and in specific combinations of population groups. n/a",Medical Advice from Glaucoma Informatics (MAGI),6830123,R33EY013928,"['clinical research', 'diagnosis design /evaluation', 'diagnosis quality /standard', 'early diagnosis', 'eye disorder diagnosis', 'glaucoma', 'glaucoma test', 'human subject', 'neural information processing', 'neuropathology', 'visual fields']",NEI,UNIVERSITY OF CALIFORNIA SAN DIEGO,R33,2004,589323,0.06358857418637343
"Instrument development & fabrication for vision research    DESCRIPTION (provided by applicant):  The objective of this proposal is to enhance the research capabilities and collaborative efforts of the vision researchers at the Columbia University Medical Center.  State-of-the-art vision research often requires the custom fabrication of mechanical instruments to support the research. Support is requested for a single module to renovate and support the machine shop in the Harkness Eye Institute at Columbia University, to be shared primarily between the Department of Ophthalmology and the Mahoney Center for Brain and Behavior.  The module will have 10 users, 7 of whom have NEI-funded RO1 grants, and 3 of whom perform research in the area of visual systems neuroscience on grants funded by the NIMH. All of the investigators are also mentors on an NEI-funded training grant. The current systems projects include studies of the neurophysiology   and psychophysics of spatial vision, visual attention, early cortical processing, visual emotional association, and visual motion; the cellular and molecular projects include studies of fluid transport across corneal epithelium, retinal axon guidance, ocular wound healing, and the impact of the lipofuscin fluorophores on retinal pigmented epithelial cell function and viability. All of these projects require the development and fabrication of devices primarily designed for a given project. A great number of these can, when perfected, be shared among a number of projects. Examples of such devices include custom-made nanoliter injection devices, recording chambers, multiple-microdrive platforms, dual recording-iontophoretic devices, illumination devices, and recording gdds. The PI has extensive experience collaborating with machinists, and several of the devices in whose development he participated have been marketed commercially.  Currently the Department of Ophthalmology has a fully-equipped machine shop the machines of which are all fine old Bridgeport and Hardinge manual machines. This proposal is to upgrade the machine shop, with a computer-controlled lathe and a computer-controlled milling machine, and to support the salary of the machinist who was hired in June, 2003, using university startup funds. The availability of an in-house professionally certified machinist will significantly speed the process of design and fabrication of custom instruments.  The use of computer controlled machine tools will facilitate duplication of instruments usable in multiple laboratories.            n/a",Instrument development & fabrication for vision research,6795629,R24EY015634,"['biomedical equipment', 'biomedical resource', 'clinical research', 'computers', 'neurosciences', 'vision']",NEI,COLUMBIA UNIVERSITY HEALTH SCIENCES,R24,2004,367302,0.03960396795020501
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6766751,P30EY006883,"['biomedical facility', 'health science research', 'vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2004,593925,0.03372195171264315
"Computer-Aided Interpretation of Oculometric Data    DESCRIPTION (provided by applicant): The primary goal of the proposed research program is to develop computer software tools with embedded artificial intelligence (AI) that can perform instantaneous, automated analysis and clinical interpretation of wavefront error measurements of the human eye and cornea. Secondary goals are to improve the overall design of oculometric data visualization tools, provide information that will help to establish clinical and scientific standards for ocular measurements and procedures, and improve our understanding of the fundamental relationship between optical performance and visual performance. We hypothesize that a) AI-based algorithms will detect complex patterns of wavefront errors; b) these patterns are specific to and significantly correlated with certain diseases and disorders; and c) AI-based interpretation of complex data will be superior to that performed by expert humans, who are the gold standard for interpreting clinical data. Specifically, we will (1) develop, train, and test AI-based algorithms (Bayesian and neural networks) to interpret the significance of complex wavefront error data obtained retrospectively from examination records of patients with various ocular diseases, disorders, or surgical interventions, as well as normal eyes; (2) simulate wavefront error data using computer models based on statistical distributions of actual ocular aberrations from patient population samples for the purpose of investigating the importance of individual higher order aberrations to retinal image formation and potential visual performance, as well as to generate new data that will enhance the overall AI training and testing process, and (3) establish standard methods to acquire and analyze wavefront error data. AI-based tools will assist vision scientists to efficiently develop study databases and analyze aberration data. Clinicians will diagnose patients faster, more accurately, and with a greater degree of confidence. For patients, refractive surgery outcomes will be more predictable, and they will benefit from earlier detection of diseases such as cataracts and corneal ectasias.         n/a",Computer-Aided Interpretation of Oculometric Data,6617187,R01EY014162,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer assisted diagnosis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' diagnosis design /evaluation', ' eye disorder diagnosis', ' eye refractometry', ' human data', ' image processing', ' ophthalmoscopy']",NEI,LOUISIANA STATE UNIV HSC NEW ORLEANS,R01,2003,241570,0.024261345929877765
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6710523,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,139234,0.013735046471733249
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6665322,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2003,245656,0.013735046471733249
"Diagnostic Aid Software for Visual Field Test    DESCRIPTION (provided by applicant): Visual Field (VF) test is a widely used, noninvasive technique for evaluating pathology or dysfunction in the visual pathways. The VF test, in conjunction with other diagnostics, is used for detection of early stages of glaucoma and for following its progression. Early detection is critical as blindness from glaucoma is preventable in nearly all cases, provided treatment is administered early in the progression. However, the inherent subjectivity of the VF test makes it often difficult to interpret even for a skilled practitioner. There is a need for automated decision aid tool that will facilitate and standardize the interpretation task.   In Phase 1 of this project, IAC will design and implement novel software algorithms to automate the interpretation of VF test data for detection of glaucoma. The software will classify VF test data into normal, borderline glaucomatous, glaucomatous and unknown (not normal or glaucomatous). The aim is to provide classification performance close to that of a highly skilled human expert. The emphasis will be on the detection of early stages of glaucoma. In addition to the classification output, the software will produce a set of comprehensive rules that will explain the decision path leading to the suggested diagnosis.         n/a",Diagnostic Aid Software for Visual Field Test,6582662,R43EY014077,"['artificial intelligence', ' computer assisted diagnosis', ' computer program /software', ' computer system design /evaluation', ' diagnosis design /evaluation', ' early diagnosis', ' glaucoma', ' glaucoma test', ' human data', ' noninvasive diagnosis', ' vision tests', ' visual fields']",NEI,INTELLIGENT AUTOMATION CORPORATION,R43,2003,99681,0.04861361997831894
"Visual & Interactive Issues in the Design of Web Surveys    DESCRIPTION (provided by applicant): The rapid acceptance of the Worldwide Web as a vehicle for survey data collection raises important questions about how the new method works. Key features of Web surveys include the use of rich visual presentation of questions and the capability of interaction with the respondent. The rapid growth of the Web makes a close examination of these issues even more urgent. Neither set of features has been explored thoroughly even with earlier modes and the Web offers widely increased resources for both visual display (Web questionnaires can readily incorporate still pictures or video clips) and interaction (such as, floating screens and scrolling for help with definitions). Our application outlines a set of studies designed to address key questions about these issues. The studies focus on Web surveys, but we believe that the results would generalize to other modes of data collection that rely on visual presentation or incorporate interactive design features.   Experiments 1-5 examine how respondents interpret the visual cues in Web questionnaires. These studies test the general proposition that incidental features of the presentation of the questions (for example, the spacing of the response options, the color assigned to different response options) can give rise to unintended inferences about their meaning. These studies test predictions derived from a theoretical framework that assumes respondents use simple interpretive heuristics to assign meaning to visual features of the questions. The next two experiments examine the effects of including images as a supplement to the text of the question. Images are necessarily concrete, and Experiment 6 tests the hypothesis that this concreteness may lead respondents to interpret the questions more narrowly when they are accompanied by images. Experiment 7 tests the idea that the item depicted in an image may serve as a standard of comparison for respondents' judgments. Again, the results of these studies will lead to practical guidelines about the dangers involved in using images as an adjunct to verbal questions. The final series of studies examines when respondents are likely to take advantage of interactive features of a questionnaire. These experiments test three general hypotheses; respondents are more likely to utilize the information available to them interactively when 1) the information is easy to obtain, 2) it is clearly helpful, and 3) respondents are highly motivated to seek help. These six experiments would yield a better understanding of methods for getting respondents to use features that could yield better survey data.         n/a",Visual & Interactive Issues in the Design of Web Surveys,6629976,R01HD041386,"['Internet', ' artificial intelligence', ' attitude', ' behavior prediction', ' behavior test', ' behavioral /social science research tag', ' clinical research', ' computer human interaction', ' cues', ' data collection methodology /evaluation', ' human subject', ' imagery', ' interactive multimedia', ' mathematics', ' population survey', ' questionnaires', ' space perception', ' visual perception']",NICHD,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2003,205876,0.018112886733702906
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6666671,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2003,327524,0.057791081376277996
"Medical Advice from Glaucoma Informatics (MAGI)  DESCRIPTION (provided by applicant):  The project, Medical Advice from Glaucoma Informatics (MAGI), seeks to improve glaucoma diagnosis and management with state-of-the-art machine learning classifiers. These classifiers will automate the interpretation of standard automated perimetry (SAP), newer visual field tests, and structural tests for glaucoma in the general population and in stratified glaucoma populations. Phase 1 will complete the feasibility testing already underway. Phase 2 will apply the refined methods to a wider set of glaucoma testing problems.The management of glaucoma depends on a series of classifications. The glaucoma provider classifies tests as normal or indicative of glaucoma. The clinician then determines whether an eye has glaucoma or has had progression. Assembling these classifications, the provider makes decisions about management. Automated test interpreters, either as part of the testing machine or as a computer-based resource, can aid glaucoma providers with real-time interpretations. The research we propose takes advantage of our extensive data sets and builds on the ongoing research in our laboratories.Statistical classifiers, Bayesian nets, machine learning classifiers, and expert systems represent different types of classifiers with diverse properties. Machine learning classifiers can perform exceptionally well at identifying classes, even when the data are complex and have dependencies. We will test and select the optimal machine learning classifier for diagnosis. We will further improve classifier performance and determine feature utility by optimizing the feature set visual field tests are time consuming and stressful. We will streamline the tests by removing unimportant test points.Even with decades of experience, there is uncertainty with regard to the evaluation of the SAP. There is less accumulated knowledge about non-standard tests, such as short-wavelength automated perimetry, nerve fiber layer thickness, or optic nerve head topography. Machine classifiers may learn how to interpret nonstandard tests better. We will go beyond STATPAC's capabilities with classifiers that have learned to interpret SAP, nonstandard visual field tests, structural glaucoma tests, and STATPAC plots in the general population and in patients stratified by race, family history, and other information available at the time of the test.Conversion of suspects to glaucoma and progression of glaucoma cannot yet be predicted from tests. We will develop classifiers for these predictions. Classifiers will be designed to diagnose early glaucoma, detect early progression, and identify glaucomatous eyes at risk of progression.Unsupervised learning provides cluster analysis that can determine distinct groups with members in some way similar from the test data. In an effort to discover new and use useful information with unsupervised learning, we will mine our data in visual function and structural tests for glaucoma  and in specific combinations of population groups. n/a",Medical Advice from Glaucoma Informatics (MAGI),6788651,R33EY013928,"['clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' early diagnosis', ' eye disorder diagnosis', ' glaucoma', ' glaucoma test', ' human subject', ' neural information processing', ' neuropathology', ' visual fields']",NEI,UNIVERSITY OF CALIFORNIA SAN DIEGO,R33,2003,591603,0.06358857418637343
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6635595,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2003,576626,0.03372195171264315
"A Smart Telescope for Low Vision    DESCRIPTION (provided by applicant): This is a proposal to test the feasibility of a ""Smart Telescope"" for use to improve the ability of visually impaired persons in tasks such as travel, navigation and social interactions. Advances in low-power high-speed portable computers combined with novel computer vision algorithms enable us to build an affordable, portable, and cosmetically acceptable digital telescope that can enable visually impaired persons to perform these tasks with greater ease than with current telescopes.        The computer vision algorithms first detect regions of interest in an image where targets are likely to be, even if these targets occupy only a small portion of the visual field, obviating the need for a user to scan or search a scene as would be necessary with an ordinary telescope. Next, novel object-specific super-resolution enhancement algorithms use target-specific knowledge to magnify and enhance these regions so that users can interpret them, similar to pointing a telescope at those regions. Algorithms can then track the targets as the observer moves, and indicate their relative locations. Finally, like today's digital telescopes for   the low vision community, the Smart Telescope will output either to a monocular viewfinder display or to a bioptic display.      The project process involves: (1) Data collection and analysis (Iow-vision persons will be used to collect image data); (2) Detection, enhancement and tracking algorithm development; (3) Integration of algorithms with user interface, and (4) Testing human factors.      Our field prototypes will range in cost from approximately $3,000 to $5,000 each, while the target cost of a commercial version is under $1,000. Our price estimates are conservative, and we anticipate that the rapid development of computer technology will lower these costs substantially in the next few years.         n/a",A Smart Telescope for Low Vision,6580977,R43EY014487,"['artificial intelligence', ' biomedical equipment development', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection', ' digital imaging', ' functional ability', ' human subject', ' image processing', ' medical rehabilitation related tag', ' patient oriented research', ' portable biomedical equipment', ' questionnaires', ' vision aid', ' vision disorders', ' visual fields', ' visual perception', ' visual threshold', ' visual tracking']",NEI,BLINDSIGHT CORPORATION,R43,2002,246164,0.013735046471733249
"Locating and Reading Informational Signs  DESCRIPTION (provided by applicant): Our goal is to construct computer vision systems to enable the blind and severely visually impaired to detect and read informational text in city scenes. The informational text can be street signs, bus numbers hospital signs, supermarket signs, and names of products (eg. Kellogg's cornflakes). We will construct portable prototype computer vision systems implemented by digital cameras attached to personal, or hand held, computers. The camera need only be pointed in the general direction of the text (so the text is only one percent of the image). A speech synthesizer will read the text to the user. Blind and visually impaired users will test the device in the field (under supervision) and give feedback to improve the algorithms. We argue that this work will make a significant contribution to improving human health (rehabilitation). Computer vision is a rapidly maturing technology with immense potential to help the blind and visually impaired. Reports suggest that detecting and reading informational text is one of the main unsatisfied desires of these groups. Written signs and information in the environment are used for navigation, shopping, operating equipment, identifying buses, and many other purposes (to which a blind person does not otherwise have independent access). The blind and severely visually impaired make up a large fraction of the US population (3 million). Moreover, this proportion is expected to increase by a factor of two in the next ten years due to increased life expectancy. Our proposal is design-driven. It uses a new class of computer vision algorithms known as Data Driven Monte Carlo (DDMCMC). The algorithms are used to: (i) search for text, and (ii) to read it. Recent developments in digital cameras and portable/handheld computers make it practical to implement these algorithms in portable prototype systems. The three scientists in this proposal have the necessary expertise to accomplish it. Dr.'s Yuille and Zhu have backgrounds in computer vision and Dr. Brabyn has experience in developing and testing engineering systems to help the blind and visually impaired. Our proposal falls within the scope of the Bioengineering initiative because we are applying techniques from the mathematical/engineering sciences to develop informatic approaches for patient rehabilitation. More specifically, our work will facilitate the development of portable devices to help the blind and visually impaired.   n/a",Locating and Reading Informational Signs,6547549,R01EY013875,"['blind aid', ' blindness', ' clinical research', ' computer human interaction', ' computer program /software', ' cues', ' human subject', ' reading', ' vision aid', ' vision disorders']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2002,338540,0.057791081376277996
"Medical Advice from Glaucoma Informatics (MAGI)  DESCRIPTION (provided by applicant):  The project, Medical Advice from Glaucoma Informatics (MAGI), seeks to improve glaucoma diagnosis and management with state-of-the-art machine learning classifiers. These classifiers will automate the interpretation of standard automated perimetry (SAP), newer visual field tests, and structural tests for glaucoma in the general population and in stratified glaucoma populations. Phase 1 will complete the feasibility testing already underway. Phase 2 will apply the refined methods to a wider set of glaucoma testing problems.The management of glaucoma depends on a series of classifications. The glaucoma provider classifies tests as normal or indicative of glaucoma. The clinician then determines whether an eye has glaucoma or has had progression. Assembling these classifications, the provider makes decisions about management. Automated test interpreters, either as part of the testing machine or as a computer-based resource, can aid glaucoma providers with real-time interpretations. The research we propose takes advantage of our extensive data sets and builds on the ongoing research in our laboratories.Statistical classifiers, Bayesian nets, machine learning classifiers, and expert systems represent different types of classifiers with diverse properties. Machine learning classifiers can perform exceptionally well at identifying classes, even when the data are complex and have dependencies. We will test and select the optimal machine learning classifier for diagnosis. We will further improve classifier performance and determine feature utility by optimizing the feature set visual field tests are time consuming and stressful. We will streamline the tests by removing unimportant test points.Even with decades of experience, there is uncertainty with regard to the evaluation of the SAP. There is less accumulated knowledge about non-standard tests, such as short-wavelength automated perimetry, nerve fiber layer thickness, or optic nerve head topography. Machine classifiers may learn how to interpret nonstandard tests better. We will go beyond STATPAC's capabilities with classifiers that have learned to interpret SAP, nonstandard visual field tests, structural glaucoma tests, and STATPAC plots in the general population and in patients stratified by race, family history, and other information available at the time of the test.Conversion of suspects to glaucoma and progression of glaucoma cannot yet be predicted from tests. We will develop classifiers for these predictions. Classifiers will be designed to diagnose early glaucoma, detect early progression, and identify glaucomatous eyes at risk of progression.Unsupervised learning provides cluster analysis that can determine distinct groups with members in some way similar from the test data. In an effort to discover new and use useful information with unsupervised learning, we will mine our data in visual function and structural tests for glaucoma  and in specific combinations of population groups. n/a",Medical Advice from Glaucoma Informatics (MAGI),6551796,R21EY013928,"['clinical research', ' diagnosis design /evaluation', ' diagnosis quality /standard', ' early diagnosis', ' eye disorder diagnosis', ' glaucoma', ' glaucoma test', ' human subject', ' neural information processing', ' neuropathology', ' visual fields']",NEI,UNIVERSITY OF CALIFORNIA SAN DIEGO,R21,2002,144319,0.06358857418637343
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6518379,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2002,559831,0.03372195171264315
"Advanced Vision Intervention Algorithm(AVIA)   Description (from the investigator's abstract): The objective of this                application is to implement an iterative, nine-step advanced vision                  intervention algorithm (AVIA) in software to optimize the predictability of          virtually any current or anticipated customized human vision intervention            method. The software program will use the investigator's Visual Optics class         library, as well as new software for the ray transfer element, database              analysis routines, and the ray tracing surface optimization algorithm. The           program will allow, but not require, exam data from commercially available           ophthalmic instruments such as corneal topography and wavefront aberration for       input in the optical modeling of an individual's eye. This algorithm is, to the      investigator's knowledge, the only formal framework designed specifically to         optimize the predictability of surgical and non-surgical correction methods. It      is not only a technological innovation in its own right, it also makes the most      of the current and future vision correction methods to which it is applied.          PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                     n/a",Advanced Vision Intervention Algorithm(AVIA),6403968,R43EY013666,"['artificial intelligence', ' computer assisted patient care', ' computer program /software', ' computer system design /evaluation', ' eye surgery', ' laser therapy', ' ophthalmoscopy', ' statistics /biometry', ' vision disorders']",NEI,"SARVER AND ASSOCIATES, INC.",R43,2001,99785,0.023500992754972052
"Core Grant for Vision Research The Smith-Kettlewell Eye Research Institute is a private non-profit organization, founded to encourage a productive collaboration between the clinical and basic research communities. To further this objective, the Institutes incorporates visual scientists from diverse medical and scientific backgrounds: ophthalmology visual scientific backgrounds. In the past, research at this Institute was focused on topics related to strabismus and amblyopia (oculomoter processing, binocular vision, cortical development of t he visual pathways). While these research areas are still growing, other distinct specialties have emerged in recent years Vision in the aging eye, motion and long-range processing, analysis of retinal functioning, retinal development, object recognition and computer vision are among the new research interests. Given the small scale of Smith-Kettlewell, the proximity of the scientists, and their common research interests, collaboration among scientist is an important aspect of the research milieu. Principal Investigators share resources with little difficulty. For more than twenty years, the Core Grant has formed the central component of the most important shared research services, chiefly electronic hardware design and maintenance, and computer communication and support. The technical expertise of our electronic and computer services group greatly benefits the rapid development of new research agendas-a tremendous advantage for new principal investigators and postdoctoral fellows, and a major factor in our high productivity. The Computer Services Module has evolved from providing predominantly software services in the past to establishing central computer services, including Internet, email, data transfer, central back-up and Intranet capabilities. We therefore are requesting renewal of these valuable Core Facilities.  n/a",Core Grant for Vision Research,6346620,P30EY006883,"['biomedical facility', ' health science research', ' vision']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,P30,2001,527694,0.03372195171264315
"RECOGNITION OF OBJECTS UNDER VARIABLE ILLUMINATION The goal of the proposed research is to develop and test a computational theory for the human ability to recognize objects under variable illumination (including extreme shadowing) and viewpoint changes. The ability to recognize objects is of fundamental importance in everyday life and the loss of this ability, due to a stroke or Alzheimer's disease, is a serious handicap to the person involved. The computational theory is based on a new paradigm for object representation --generative modeling - - in which an image-based model of an object is ""generated"" from a small set of training images. This theory has been demonstrated to successfully recognize objects from real images under extreme lighting variations. This gives a reality check on the theory and can be thought of as making it an ecological theory (in the sense that it yields good results on the types of images that humans encounter in the real world and not just on the visual stimuli occurring in laboratories). We have assembled a team of researchers with interdisciplinary skills in computer and biological vision. who will divide their efforts on the project based on their expertise. It is our explicit intent that the algorithms and psychophysical studies develop in tandem, with each group verifying the other's results. Indeed, as reviewed below, the computer vision theory, when applied to human performance, makes a number of predictions. some of which have already been partially confirmed by our preliminary experimental work. Our proposal is organized into three main areas. The psychophysical work parallels the computational issues in three series of experiments in which we investigate: (I) How human observers learn and recognize objects, given variable lighting conditions, from a single fixed viewpoint. (II) How illumination and viewpoint interact in human object recognition. (III) The role of class-specific knowledge in recognition.  n/a",RECOGNITION OF OBJECTS UNDER VARIABLE ILLUMINATION,6384831,R01EY012691,"['computer simulation', ' form /pattern perception', ' human subject', ' light intensity', ' psychophysics', ' visual perception']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2001,356485,0.01892669630613214
"VISUAL OBJECT RECOGNITION OF CATEGORIES AND EXEMPLARS Fundamental to visual object recognition is the ability to recognize abstract categories of objects (e.g., cups versus pens) as well as specific exemplars within those categories (e.g., individual pens).  Interestingly, this ability poses a dilemma for the visual system: How can it recognize that two shapes should be considered the same (i.e., belong to the same abstract category) yet also different (i.e., correspond to different exemplars)?  The aim of the proposed work is to investigate how the human brain may implement a solution to this problems.  In particular, the goal is to uncover the architecture of functionally defined, neurally dissociable subsystems that underlie object recognition, and to determine whether this architecture reflects a solution to the abstract/specific dilemma.  Understanding the structure of component subsystems has primary importance, as it must be addressed before satisfactory answers can be offered for contemporary questions in this field; different answers may apply to different subsystems.  Preliminary studies indicate that dissociable subsystems learn to operate in parallel to accomplish abstract-category and specific-exemplar recognition of visual objects.  However, it is difficult to produce fail-safe dissociations of functionally specified subsystems, and other architectures remain viable as alternative theories (e.g., dissociable subsystems operating in sequence, a single general-purpose mechanism, attention to different information within a single subsystem).  Thus, the proposed research will further test and refine these theories, using a converging evidence attack to draw strong conclusions.  The research will integrate analyses of the goals of the visual system with evidence of the neural implementation of dissociable subsystems to constrain such theories.  Divided-visual-field studies will test whether abstract-category and specific-exemplar recognition subsystems operate in parallel (rather than in sequence) and with different relative efficiencies in the left and right cerebral hemispheres.  They also will test the particular levels of categorization performed by abstract and specific subsystems, whether stimulus and task demands influence the relative contributions of these subsystems in predictable ways, as well as whether these subsystems utilize contradictory processing strategies (e.g., features-based versus whole-based processing).  Overall, should evidence for dissociable parallel subsystems be observed, object recognition theories that attempt to account for performance through different architectures would have to be significantly revised.  In any case, this research should lead to a greater understanding of the component subsystems underlying visual object recognition, with implications for addressing why neurological damage can produce selective visual recognition impairments and for suggesting useful architectures in computer vision systems.  n/a",VISUAL OBJECT RECOGNITION OF CATEGORIES AND EXEMPLARS,6392656,R03MH060442,"['behavioral /social science research tag', ' classification', ' clinical research', ' human subject', ' neural information processing', ' vision', ' visual perception']",NIMH,UNIVERSITY OF MINNESOTA TWIN CITIES,R03,2001,67221,0.01482063210039939
"SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS In this Phase l proposal we plan to develop and test a new vision technology to locat and read general informational signs (street names, building directories, office door plates) and location and directional signs (EXIT, Information, aisle signs in supermarkets). To strengthen feasibility, we will target a restricted class of signs: those consisting primarily of one- color text on a different one-color background, and whose shape falls within a prescribed set. The intended market is for people who are blind or whose sight is impaired and hence cannot read these signs unaided. Our approach makes extensive use of recently developed computer vision recognition algorithms. We also make use of the Smith-Kettlewell's Rehabilitation Engineering Research Center's expertise for determining what the potential users will require from such a system. The ultimate goal, for Phase II, is to build and test a highly portable PC- based device implementing this vision technology using a CCD camera as input and a voice-generator as output. The user would scan/point the device at a scene and it would locate and read one or more signs. Given the pace of increase in power and decrease in size of computing devices, a hand-held Sign-Finder system may be plausible to build entirely with commercial, off-the-shelf hardware in two to three years. PROPOSED COMMERCIAL APPLICATION: The potential utility to blind and visually impaired individuals is great; a commercial product could have a market potential of 500,000.  n/a",SIGN FINDER: COMPUTER VISION TO FIND AND READ SIGNS,2720318,R43EY011821,"['artificial intelligence', ' blind aid', ' charge coupled device camera', ' computer graphics /printing', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' information display', ' portable biomedical equipment', ' symbolism', ' technology /technique development', ' vision aid']",NEI,BLINDSIGHT CORPORATION,R43,2000,100000,0.026039372701420597
"RECOGNITION OF OBJECTS UNDER VARIABLE ILLUMINATION The goal of the proposed research is to develop and test a computational theory for the human ability to recognize objects under variable illumination (including extreme shadowing) and viewpoint changes. The ability to recognize objects is of fundamental importance in everyday life and the loss of this ability, due to a stroke or Alzheimer's disease, is a serious handicap to the person involved. The computational theory is based on a new paradigm for object representation --generative modeling - - in which an image-based model of an object is ""generated"" from a small set of training images. This theory has been demonstrated to successfully recognize objects from real images under extreme lighting variations. This gives a reality check on the theory and can be thought of as making it an ecological theory (in the sense that it yields good results on the types of images that humans encounter in the real world and not just on the visual stimuli occurring in laboratories). We have assembled a team of researchers with interdisciplinary skills in computer and biological vision. who will divide their efforts on the project based on their expertise. It is our explicit intent that the algorithms and psychophysical studies develop in tandem, with each group verifying the other's results. Indeed, as reviewed below, the computer vision theory, when applied to human performance, makes a number of predictions. some of which have already been partially confirmed by our preliminary experimental work. Our proposal is organized into three main areas. The psychophysical work parallels the computational issues in three series of experiments in which we investigate: (I) How human observers learn and recognize objects, given variable lighting conditions, from a single fixed viewpoint. (II) How illumination and viewpoint interact in human object recognition. (III) The role of class-specific knowledge in recognition.  n/a",RECOGNITION OF OBJECTS UNDER VARIABLE ILLUMINATION,6179288,R01EY012691,"['computer simulation', ' form /pattern perception', ' human subject', ' light intensity', ' psychophysics', ' visual perception']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2000,332626,0.01892669630613214
"VISUAL OBJECT RECOGNITION OF CATEGORIES AND EXEMPLARS Fundamental to visual object recognition is the ability to recognize abstract categories of objects (e.g., cups versus pens) as well as specific exemplars within those categories (e.g., individual pens).  Interestingly, this ability poses a dilemma for the visual system: How can it recognize that two shapes should be considered the same (i.e., belong to the same abstract category) yet also different (i.e., correspond to different exemplars)?  The aim of the proposed work is to investigate how the human brain may implement a solution to this problems.  In particular, the goal is to uncover the architecture of functionally defined, neurally dissociable subsystems that underlie object recognition, and to determine whether this architecture reflects a solution to the abstract/specific dilemma.  Understanding the structure of component subsystems has primary importance, as it must be addressed before satisfactory answers can be offered for contemporary questions in this field; different answers may apply to different subsystems.  Preliminary studies indicate that dissociable subsystems learn to operate in parallel to accomplish abstract-category and specific-exemplar recognition of visual objects.  However, it is difficult to produce fail-safe dissociations of functionally specified subsystems, and other architectures remain viable as alternative theories (e.g., dissociable subsystems operating in sequence, a single general-purpose mechanism, attention to different information within a single subsystem).  Thus, the proposed research will further test and refine these theories, using a converging evidence attack to draw strong conclusions.  The research will integrate analyses of the goals of the visual system with evidence of the neural implementation of dissociable subsystems to constrain such theories.  Divided-visual-field studies will test whether abstract-category and specific-exemplar recognition subsystems operate in parallel (rather than in sequence) and with different relative efficiencies in the left and right cerebral hemispheres.  They also will test the particular levels of categorization performed by abstract and specific subsystems, whether stimulus and task demands influence the relative contributions of these subsystems in predictable ways, as well as whether these subsystems utilize contradictory processing strategies (e.g., features-based versus whole-based processing).  Overall, should evidence for dissociable parallel subsystems be observed, object recognition theories that attempt to account for performance through different architectures would have to be significantly revised.  In any case, this research should lead to a greater understanding of the component subsystems underlying visual object recognition, with implications for addressing why neurological damage can produce selective visual recognition impairments and for suggesting useful architectures in computer vision systems.  n/a",VISUAL OBJECT RECOGNITION OF CATEGORIES AND EXEMPLARS,6131974,R03MH060442,"['behavioral /social science research tag', ' classification', ' clinical research', ' human subject', ' neural information processing', ' vision', ' visual perception']",NIMH,UNIVERSITY OF MINNESOTA TWIN CITIES,R03,2000,67687,0.01482063210039939
"A hybrid artificial intelligence framework for glaucoma monitoring Glaucoma is a complex neurodegenerative disease that results in degeneration of retinal ganglion cells and their axons. With older people making up the fastest growing part of the US population, glaucoma will become even more prevalent in the US in the coming decades. Due to the complex interaction of multiple factors in glaucoma, better structural and functional predictors are needed for its progression. The main impediments are massive health record data and sophisticated computational models. Our overall goal is to leverage the power of big data and rapidly evolving machine learning approaches. The NEI's “Big Data to Knowledge (BD2K)” initiative and the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) registry are all efforts to exploit the power of data and to better understand diseases and to provide improved prevention and treatment.  In this multi-PI proposal, we offer to assemble over 1 million optical coherence tomography (OCT) and visual fields (VFs) from the glaucoma research network (GRN). We propose to develop a hybrid artificial intelligence (AI) algorithm that synthesizes Gaussian mixture model expectation maximization (GEM) and archetypal machine learning approach to identify glaucoma progression and its monitoring using VFs and retinal nerve fiber layer (RNFL) thickness measurements. We will make these tools openly available to the vision and ophthalmology research communities.  Our proposed studies could offer substantial improvements in the prognosis of glaucoma as well as potentially providing OCT and joint VF/OCT surrogate endpoints to be used in glaucoma clinical trials. Leveraging big data in eye care is challenging. This study uses big functional and structural glaucoma data and develops hybrid machine learning models to identify glaucoma progression and its monitoring. Results could offer substantial improvements in prognosis of glaucoma and may provide surrogate endpoints for use in glaucoma clinical trials.",A hybrid artificial intelligence framework for glaucoma monitoring,9892013,R21EY030142,"['Academy', 'Address', 'American', 'Artificial Intelligence', 'Axon', 'Bayesian Modeling', 'Big Data', 'Big Data to Knowledge', 'Blindness', 'Caring', 'Clinical', 'Clinical Trials', 'Communities', 'Complex', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Detection', 'Devices', 'Disease', 'Disease Progression', 'Evolution', 'Eye', 'Gaussian model', 'Glaucoma', 'Goals', 'Hybrids', 'Institutes', 'Intelligence', 'Joints', 'Judgment', 'Knowledge', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Monitor', 'Neurodegenerative Disorders', 'Ophthalmology', 'Optic Nerve', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Pattern', 'Population', 'Prevention', 'Public Health', 'Registries', 'Research', 'Retina', 'Severity of illness', 'Source', 'Structural Models', 'Structure', 'Surrogate Endpoint', 'Testing', 'Thick', 'Thinness', 'Time', 'Training', 'Vision', 'Visual Fields', 'analytical method', 'archetypal analysis', 'clinical Diagnosis', 'data space', 'design', 'diagnostic accuracy', 'early onset', 'evidence base', 'expectation', 'field study', 'health record', 'high dimensionality', 'improved', 'intelligent algorithm', 'machine learning algorithm', 'multidimensional data', 'novel', 'open source', 'optical imaging', 'outcome forecast', 'programs', 'retinal ganglion cell degeneration', 'retinal nerve fiber layer', 'tool']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R21,2020,220808,0.05777723383336114
"Improved Glaucoma Monitoring Using Artificial-Intelligence Enabled Dashboard Detecting functional and structural loss due to glaucoma is critical to making treatment decisions with the goal of preserving vision and maintaining quality of life. However, most of the approaches for glaucoma assessment through visual fields (VFs) or optical coherence tomography (OCT) measurements have several limitations that poses critical challenge to their clinical utility. Identifying glaucoma-induced changes from a sequence of VF or OCT data is challenging either if the patients is in the early stages of the disease with subtle manifested structural and functional signs or if the patients are in the later stages of the disease with significant VF variability and OCT flooring effect. A major limitation of the current glaucoma monitoring techniques is that they generate a binary outcome of whether the glaucoma is worsening or not while current high-throughput data (e.g., OCT) has more information than a binary outcome. Another major drawback of some of these approaches is that they rely on traditional paradigms for progression detection such as linear regression. However, rates of glaucomatous progression may be non-linear and rapid, particularly during the later stages of the disease. Another limitation is that ad-hoc rules are adopted to define glaucoma progression while objective criteria are required to define thresholds for progression. Finally, a major deficiency of most of these methods is that they lack advanced visualization and interpretation. We propose to address these limitations by developing artificial intelligence (AI)-enabled visualization tools for effectively monitoring the functional and structural loss in patients with glaucoma. This approach provides qualitative and quantitative means to monitor 1) global visual functional and structural worsening, 2) extent of loss in hemifields, and 3) local patterns of functional and structural loss on advanced 2-D visualization tools. To achieve these objectives, we have assembled a team of interdisciplinary experts with access to large clinically annotated glaucoma data. The central hypothesis of this proposal is that advanced interpretable machine learning applied to a complete profile of VFs in all test locations (e.g., 54 in 24-2 system) and OCT-derived measurements of retinal nerve fiber layer (RNFL) (e.g., 768 A-scans around the optic disc and 7 global sectoral regions) can objectively and automatically learn and quantify the most important features, yielding a more specific and sensitive means for monitoring of glaucoma worsening than current subjectively-specified or statistically-identified approaches. We also hypothesize that machine learning can provide interpretable models with several layers of glaucoma knowledge that may provide a promising complement to current glaucoma assessment tests. Our proposed studies may offer substantial improvements in prognosis and management of glaucoma through effective use of analysis and visualization to improve glaucoma management and making more informed treatment options. Current glaucoma assessment is hampered by several limitations including lack of visualization and interpretation, providing binary rather than more-informed results, utilizing traditional approaches for data analysis, and adopting ad-hoc assessment criteria. Glaucoma is best managed and treated if both functional and structural data is utilized and mined using advanced computational tools to generate more-informative quantitative results. We propose developing artificial intelligence (AI)-enabled visualization dashboards for qualitative and quantitative monitoring of global visual functional and structural worsening, extent of loss in hemifields, and local patterns of functional and structural loss on advanced interpretable 2-D and 3-D visualization maps.",Improved Glaucoma Monitoring Using Artificial-Intelligence Enabled Dashboard,10043768,R21EY031725,"['3-Dimensional', 'Address', 'Adopted', 'Affect', 'Algorithms', 'Artificial Intelligence', 'Axon', 'Big Data to Knowledge', 'Clinical', 'Clinical Trials', 'Communities', 'Complement', 'Computer Systems', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Diagnosis', 'Disease', 'Early Diagnosis', 'Eye', 'Floor', 'Glaucoma', 'Goals', 'Health', 'High Performance Computing', 'Image', 'Incidence', 'Knowledge', 'Learning', 'Linear Regressions', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Pattern', 'Principal Component Analysis', 'Quality of life', 'Reproducibility', 'Research', 'Retinal Ganglion Cells', 'Savings', 'Scanning', 'Severities', 'Specific qualifier value', 'Specificity', 'Structure', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Treatment Cost', 'United States National Institutes of Health', 'Vision', 'Visual', 'Visual Fields', 'Visualization', 'Visualization software', 'base', 'computerized tools', 'dashboard', 'field study', 'glaucoma test', 'hands-on learning', 'improved', 'large datasets', 'longitudinal dataset', 'multidimensional data', 'open source', 'outcome forecast', 'preservation', 'retinal nerve fiber layer', 'structured data', 'three-dimensional visualization', 'tool', 'treatment strategy', 'unsupervised learning']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R21,2020,253379,0.06588605700673884
"Deep learning to quantify glaucomatous damage on fundus photographs for teleophthalmology PROJECT SUMMARY/ABSTRACT Candidate: Atalie Carina Thompson, MD, MPH is a current glaucoma fellow and Heed fellow with a long-term career goal of becoming an independent clinician-scientist and leader in the field of glaucoma and public health. She has a long-standing interest in addressing healthcare disparities in medicine, and in improving the diagnosis of glaucoma and other ophthalmic diseases through imaging technology. While obtaining a medical degree at Stanford, she received a fellowship to complete a master’s degree in public health with additional higher-level coursework in biostatistics and epidemiology. Her immediate goal in this proposal is to refine and validate a deep learning (DL) algorithm capable of quantifying neuroretinal damage on optic disc photographs and then to apply it in a pilot teleophthalmology program. With a K23 Mentored Patient-Oriented Research Career Development Award, she will acquire additional didactic training and mentored research experience in glaucoma imaging, machine learning, biostatistics, clinical research, and the responsible conduct of research. Environment: The mentorship and expertise of the advisory committee, the extensive resources at the Duke Eye Center and Departments of Biostatistics and Biomedical Engineering, and the significant institutional commitment will provide her with the support needed to transition successfully into an independent clinician-scientist. Research: This proposal will test the hypothesis that a DL algorithm trained with SDOCT detects glaucoma on optic disc photographs with greater accuracy than human graders. In Specific Aim 1, a DL algorithm that quantifies neuroretinal damage on optic disc photographs will be refined. The main hypothesis is that the quantitative output provided by the DL algorithm will allow accurate discrimination of eyes at different stages of the disease according to standard automated perimetry, and will generate cut-offs suitable for use in a screening setting. In Specific Aim 2, the short-term repeatability and reproducibility of the DL algorithm in optic disc photographs acquired over a time period of several weeks will be determined. The hypothesis is that the test-retest variability of the predictions from the DL algorithm will be similar to the original measurements acquired by SDOCT. In Specific Aim 3, the DL algorithm will be applied to optic disc photographs obtained during a pilot screening teleophthalmology program in primary care clinics and assisted living facilities. The hypothesis is that the DL algorithm will be more accurate than human graders when a full ophthalmic examination is used as the gold standard. This work will constitute the basis of an R01 grant and will advance our understanding of the application of deep learning algorithms in glaucoma and teleophthalmology. PROJECT NARRATIVE Glaucoma is the leading cause of irreversible blindness in the world. However, since the disease can be asymptomatic until later stages, many patients with glaucoma will not know they have glaucoma until they suffer substantial and irreversible visual field loss. This study seeks to refine and validate a deep learning algorithm for early diagnosis of glaucoma on optic disc photographs and subsequently test it in a pilot teleophthalmology program.",Deep learning to quantify glaucomatous damage on fundus photographs for teleophthalmology,9868507,K23EY030897,"['Address', 'Adult', 'Advisory Committees', 'Agreement', 'Algorithms', 'Artificial Intelligence', 'Assisted Living Facilities', 'Biomedical Engineering', 'Biometry', 'Blindness', 'Clinic', 'Clinical', 'Clinical Research', 'Consumption', 'Data', 'Dependence', 'Detection', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Disease', 'Early Diagnosis', 'Effectiveness', 'Environment', 'Epidemiology', 'Evaluation', 'Eye', 'Eye diseases', 'Fellowship', 'Frequencies', 'Fundus', 'Fundus photography', 'Glaucoma', 'Goals', 'Gold', 'Grant', 'Human', 'Image', 'Imaging technology', 'Improve Access', 'Individual', 'Label', 'Machine Learning', 'Manuals', 'Masks', 'Master&apos', 's Degree', 'Measurement', 'Medical', 'Medicine', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Nature', 'Optic Disk', 'Optical Coherence Tomography', 'Output', 'Patients', 'Perimetry', 'Primary Health Care', 'Public Health', 'Reference Standards', 'Reproducibility', 'Research', 'Research Priority', 'Research Proposals', 'Resources', 'Scientist', 'Screening procedure', 'Sensitivity and Specificity', 'Severity of illness', 'Specialist', 'Suspect Glaucomas', 'Technology', 'Testing', 'Thick', 'Time', 'Training', 'Validation', 'Visual Fields', 'Visual impairment', 'Width', 'Work', 'algorithm training', 'career', 'carina', 'cohort', 'cost', 'cost effective', 'deep learning', 'deep learning algorithm', 'deep neural network', 'demographics', 'experience', 'eye center', 'health care disparity', 'high risk', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'learning network', 'neural network', 'novel', 'novel diagnostics', 'population based', 'programs', 'prospective', 'public health intervention', 'responsible research conduct', 'retinal nerve fiber layer', 'screening', 'tool']",NEI,DUKE UNIVERSITY,K23,2020,195131,0.03544948456526786
"Deep Learning Approaches for Personalized Modeling and Forecasting of Glaucomatous Changes Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at managing the optimal care for individual cases due to difficulties of accurately assessing the potential progression and its speed and magnitude. These difficulties are due to a variety of causes that change over the course of the disease, including large inter-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we propose novel agnostic data-driven deep learning approaches to detect glaucoma and accurately forecast its progression that are optimized to each individual case. We will use state- of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. Instead of relying on the conventional knowledge-based approaches (e.g. quantifying tissues known to be significantly associated with glaucoma such as retinal nerve fiber layer), the proposed cutting-edge agnostic deep learning approaches determine the features responsible for future structural and functional changes out of thousands of features autonomously by learning from the provided large longitudinal dataset. This program will advance the use of structural and functional information obtained in the clinics with a substantial impact on the clinical management of subjects with glaucoma. Furthermore, the developed methods have potentials to be applied to various clinical applications beyond glaucoma and ophthalmology. Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies using agnostic deep learning approaches that will substantially improve detection of glaucoma and its progression forecasting and monitoring in order to prevent blindness.",Deep Learning Approaches for Personalized Modeling and Forecasting of Glaucomatous Changes,9864905,R01EY030929,"['3-Dimensional', 'Area', 'Atlases', 'Blindness', 'Brain', 'Caring', 'Clinic', 'Clinic Visits', 'Clinical', 'Clinical Management', 'Collaborations', 'Color', 'Complex', 'Cross-Sectional Studies', 'Custom', 'Data', 'Data Set', 'Decision Making', 'Detection', 'Development', 'Disease', 'Disease Progression', 'Disease model', 'Early Diagnosis', 'Eye', 'Future', 'Glaucoma', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Intervention', 'Investments', 'Knowledge', 'Learning', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Measurable', 'Measurement', 'Medical Imaging', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Ophthalmology', 'Optical Coherence Tomography', 'Outcome', 'Patients', 'Performance', 'Research', 'Research Proposals', 'Retina', 'Sampling', 'Series', 'Speed', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Thick', 'Thinness', 'Time', 'Tissues', 'Training', 'Vision', 'Visit', 'Visual Fields', 'analytical method', 'base', 'case-by-case basis', 'clinical application', 'clinical practice', 'cohort', 'computerized', 'cost', 'deep learning', 'falls', 'feature selection', 'follow-up', 'image processing', 'imaging modality', 'improved', 'in vivo', 'individual patient', 'innovation', 'insight', 'knowledge base', 'longitudinal analysis', 'longitudinal dataset', 'machine learning method', 'novel', 'ocular imaging', 'personalized approach', 'personalized medicine', 'personalized predictions', 'predictive modeling', 'preservation', 'prevent', 'programs', 'retinal nerve fiber layer', 'theories', 'tool', 'treatment planning', 'trend']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,400056,0.025272176755777996
"Deep Learning Approaches to Detect Glaucoma and Predict Progression from Spectral Domain Optical Coherence Tomography Project Abstract / Summary Primary open angle glaucoma (POAG) is a leading cause of blindness in the United States and worldwide. It is estimated that over 2.2 million Americans suffer from POAG and that over 130,000 are legally blind from the disease. As the population ages, the number of people with POAG in the United States will increase to over 3.3 million in 2020 and worldwide to an estimated 111.8 million by 2040. POAG is a progressive disease associated with characteristic functional and structural changes that clinicians use to diagnose and monitor the disease. Over the past several years, spectral domain optical coherent tomography (SDOCT) has become the standard tool for measuring structure in POAG. This 3D imaging modality provides a wealth of information about retinal structure and POAG-related retinal layers. This large amount of data is hard for clinicians to interpret and use effectively to help guide treatment decisions. Instead, summary metrics such as average layer thicknesses are used to reduce SDOCT images to a handful of values. While these metrics are useful, they can be difficult to interpret and they throwaway important information regarding voxel intensity and texture, relationships across retinal layers, and the overall 3D structure of the retina. Relying too heavily on these metrics limits our ability to gain a deeper understanding structural contributions to POAG, the relationship between structure and visual function, and how structural (and functional) changes progress in POAG. Recent advances in artificial intelligence and deep learning, however, offer new data-driven tools and techniques to interpret 3D SDOCT images and learn from the large SDOCT datasets being collected in clinics around the world. This proposal will apply state-of-the-art deep learning techniques to 3D SDOCT data in order to (1) develop more accurate POAG detection tools, (2) reveal structure-function relationships, and (3) predict structural and functional progression in POAG. This proposal also details a training plan to help the PI transition from a postdoctoral scholar to an independent researcher. The mentored phase of this award will be supervised by the primary mentor, Dr. Linda Zangwill, and a multidisciplinary mentoring team including Dr. Robert Weinreb (Ophthalmology), Dr. David Kriegman (Computer Science and Engineering), and Dr. Armin Schwartzman (Biostatistics). Performing the proposed research, formal coursework, and mentored career development will the provide the PI with highly sought- after skills and experience to help ensure a successful transition into independence. Project Narrative Three-dimensional imaging techniques such as optical coherence tomography have become an essential tool in the clinical care of glaucoma and other eye diseases. These imaging techniques provide clinicians with huge amounts of structural information, but interpreting the data and using it effectively to improve outcomes remains challenging in clinical glaucoma management. This project will improve patient care by applying powerful deep learning techniques to provide clinicians with critical decision support information to more accurately detect glaucoma, reveal associations between structure and visual function, and predict glaucoma progression.",Deep Learning Approaches to Detect Glaucoma and Predict Progression from Spectral Domain Optical Coherence Tomography,10055661,K99EY030942,"['3-Dimensional', 'Affect', 'Age', 'American', 'Artificial Intelligence', 'Award', 'Biometry', 'Blindness', 'Caring', 'Characteristics', 'Clinic', 'Clinical', 'Computational Technique', 'Cornea', 'Data', 'Data Set', 'Decision Making', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Disease Progression', 'Engineering', 'Ensure', 'Evaluation', 'Eye', 'Eye diseases', 'Frequencies', 'Glaucoma', 'Image', 'Imaging Techniques', 'Individual', 'Learning', 'Length', 'Measurement', 'Measures', 'Medicine', 'Mentors', 'Modeling', 'Monitor', 'Ophthalmology', 'Optic Disk', 'Optical Coherence Tomography', 'Optics', 'Participant', 'Patient Care', 'Patients', 'Performance', 'Phase', 'Population', 'Primary Open Angle Glaucoma', 'Probability', 'Progressive Disease', 'Race', 'Research', 'Research Personnel', 'Retina', 'Scanning', 'Severities', 'Severity of illness', 'South Korea', 'Standardization', 'Structure', 'Structure-Activity Relationship', 'Supervision', 'Techniques', 'Texture', 'Thick', 'Thinness', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Training', 'Translating', 'United States', 'Universities', 'Vision', 'Visual Fields', 'Visualization', 'Width', 'Work', 'base', 'career development', 'clinical care', 'college', 'computer science', 'deep learning', 'experience', 'field study', 'imaging modality', 'improved', 'improved outcome', 'individual patient', 'large datasets', 'legally blind', 'macula', 'multidisciplinary', 'predictive modeling', 'preservation', 'research clinical testing', 'retinal nerve fiber layer', 'sex', 'skills', 'standard measure', 'three dimensional structure', 'tomography', 'tool']",NEI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2020,117347,0.0068575271827647465
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Summary The goal of this project is to develop a smartphone-based wayfinding app designed to help people with visual impairments navigate indoor environments more easily and independently. It harnesses computer vision and smartphone sensors to estimate and track the user’s location in real time relative to a map of the indoor environment, providing audio-based turn-by-turn directions to guide the user to a desired destination. An additional option is to provide audio or tactile alerts to the presence of nearby points of interest in the environment, such as exits, elevators, restrooms and meeting rooms. The app estimates the user’s location by recognizing standard informational signs present in the environment, tracking the user’s trajectory and relating it to a digital map that has been annotated with information about signs and landmarks. Compared with other indoor wayfinding approaches, our computer vision and sensor-based approach has the advantage of requiring neither physical infrastructure to be installed and maintained (such as iBeacons) nor precise prior calibration (such as the spatially referenced radiofrequency signature acquisition process required for Wi-Fi-based systems), which are costly measures that are likely to impede widespread adoption. Our proposed system has the potential to greatly expand opportunities for safe, independent navigation of indoor spaces for people with visual impairments. Towards the end of the grant period, the wayfinding software (including documentation) will be released as free and open source software (FOSS). Health Relevance For people who are blind or visually impaired, a serious barrier to employment, economic self- sufficiency and independence is the ability to navigate independently, efficiently and safely in a variety of environments, including large public spaces such as medical centers, schools and office buildings. The proposed research would result in a new smartphone-based wayfinding app that could greatly increase travel independence for the approximately 10 million Americans with significant vision impairments or blindness.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,9899994,R01EY029033,"['Adoption', 'Algorithms', 'American', 'Blindness', 'Calibration', 'Cellular Phone', 'Computer Vision Systems', 'Computer software', 'Cost Measures', 'Destinations', 'Development', 'Documentation', 'Economics', 'Elevator', 'Employment', 'Ensure', 'Environment', 'Evaluation', 'Floor', 'Focus Groups', 'Goals', 'Grant', 'Health', 'Indoor environment', 'Infrastructure', 'Location', 'Maps', 'Measures', 'Medical center', 'Needs Assessment', 'Performance', 'Process', 'Research', 'Schools', 'System', 'Systems Integration', 'Tactile', 'Testing', 'Time', 'Travel', 'Visual', 'Visual impairment', 'base', 'blind', 'design', 'digital', 'improved', 'interest', 'lens', 'meetings', 'open source', 'radio frequency', 'sensor', 'way finding', 'wireless fidelity']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,416374,0.050803426385949035
"Virtual prototyping for retinal prosthesis patients Project Summary/Abstract Retinal dystrophies such as retinitis pigmentosa and macular degeneration induce progressive loss of photoreceptors, resulting in profound visual impairment in more than ten million people worldwide. Visual neuroprostheses (‘bionic eyes’) aim to restore functional vision by electrically stimulating remaining cells in the retina, analogous to cochlear implants. A wide variety of neuroprostheses are either in development (e.g. optogenetics, cortical) or are being implanted in patients (e.g. subretinal or epiretinal electrical). A limiting factor that affects all device types are perceptual distortions and subsequent loss of information, caused by interactions between the implant technology and the underlying neurophysiology. Understanding the causes of these distortions and finding ways to alleviate them is critically important to the success of current and future sight restoration technologies. In this proposal, human visual psychophysics, computational modeling, data-driven approaches, and virtual reality (VR) will be combined to develop and experimentally validate optimized stimulation protocols for epiretinal prostheses. This approach is analogous to virtual prototyping for airplanes and other complex systems: to use a high-quality model of both the implant electronics and the visual system in order to generate a ‘virtual patient’. Retinal electrophysiological and visual behavioral data will be used to develop and validate a computational model of the expected visual experience of patients when electrically stimulated. One way of using this model will be to generate simulations of the expected perceptual outcome of electrical stimulation across a wide variety of electrical stimulation patterns. These will be used as a training set for machine learning algorithms that will invert the input-output function of the model to find the electrical stimulation protocol that best replicates any desired perceptual experience. The model can also be used to simulate the expected perceptual experience of real patients by using sighted subjects in a VR environment – ‘VR virtual patients’. These virtual patients will be used to discover preprocessing methods (e.g., edge enhancement, retargeting, decluttering) that improve behavioral performance in VR. Although current retinal prostheses have been implanted in over 250 patients worldwide, experimentation with improved stimulation protocols remains challenging and expensive. Implementing ‘virtual patients’ in VR offers an affordable and practical alternative for high-throughput experiments to test new stimulation protocols. Stimulation protocols that result in good VR performance will be experimentally validated in real prosthesis patients in collaboration with Second Sight Medical Products Inc. and Pixium Vision, two leading device manufacturers in the field. This work has the potential to significantly improve the effectiveness of visual neuroprostheses as a treatment option for individuals suffering from blinding retinal diseases. Project Narrative Inadequate stimulation paradigms are currently one of the main factors limiting the effectiveness of visual prostheses as a treatment option for individuals suffering from blinding retinal diseases. My goal is to develop and validate novel stimulation protocols for visual prosthesis patients that minimize perceptual distortions and thereby improve behavioral performance. Developing methods for generating better stimulation protocols through a combination of behavioral testing, virtual reality, computational modeling, and machine learning, has the potential to provide a transformative improvement of this device technology.",Virtual prototyping for retinal prosthesis patients,10200240,R00EY029329,"['Affect', 'Behavioral', 'Bionics', 'Cells', 'Clinical Trials', 'Cochlear Implants', 'Collaborations', 'Complex', 'Computer Models', 'Computer Vision Systems', 'Data', 'Development', 'Devices', 'Effectiveness', 'Electric Stimulation', 'Electrodes', 'Electronics', 'Electrophysiology (science)', 'Eye', 'Eye Movements', 'Family', 'Financial compensation', 'Future', 'Goals', 'Head', 'Human', 'Implant', 'In Vitro', 'Individual', 'Knowledge', 'Learning', 'Letters', 'Machine Learning', 'Macular degeneration', 'Manufacturer Name', 'Medical', 'Medicare', 'Methods', 'Modeling', 'Motion', 'Neurons', 'Ocular Prosthesis', 'Online Systems', 'Outcome', 'Output', 'Patients', 'Pattern', 'Perceptual distortions', 'Performance', 'Photoreceptors', 'Prosthesis', 'Prosthesis Design', 'Protocols documentation', 'Psychophysics', 'Rehabilitation therapy', 'Reporting', 'Retina', 'Retinal Diseases', 'Retinal Dystrophy', 'Retinitis Pigmentosa', 'Schedule', 'Severities', 'Shapes', 'Specialist', 'Stimulus', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Vision', 'Visual', 'Visual Psychophysics', 'Visual impairment', 'Visual system structure', 'Visualization', 'Work', 'base', 'behavior measurement', 'behavior test', 'deep neural network', 'design', 'experience', 'experimental study', 'gaze', 'implantation', 'improved', 'machine learning algorithm', 'neurophysiology', 'neuroprosthesis', 'novel', 'object recognition', 'optogenetics', 'predictive modeling', 'prototype', 'regression algorithm', 'restoration', 'retinal prosthesis', 'simulation', 'spatiotemporal', 'success', 'virtual', 'virtual reality', 'virtual reality environment']",NEI,UNIVERSITY OF CALIFORNIA SANTA BARBARA,R00,2020,247272,0.014962473180550493
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,10018924,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'segmentation algorithm', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,190044,-0.0122933876772248
"CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision   To understand and navigate the environment, sensory systems must solve simultaneously two competing and challenging tasks: the segmentation of a sensory scene into individual objects and the grouping of elementary sensory features to build these objects. Understanding perceptual grouping and segmentation is therefore a major goal of sensory neuroscience, and it is central to advancing artificial perceptual systems that can help restore impaired vision. To make progress in understanding image segmentation and improving algorithms, this project combines two key components. First, a new experimental paradigm that allows for well-controlled measurements of perceptual segmentation of natural images. This addresses a major limitation of existing data that are either restricted to artificial stimuli, or, for natural images, rely on manual labeling and conflate perceptual, motor, and cognitive factors. Second, this project involves developing and testing a computational framework that accommodates bottom-up information about image statistics and top-down information about objects and behavioral goals. This is in contrast with the paradigmatic view of visual processing as a feedforward cascade of feature detectors, that has long dominated computer vision algorithms and our understanding of visual processing. The proposed approach builds instead on the influential theory that perception requires probabilistic inference to extract meaning from ambiguous sensory inputs. Segmentation is a prime example of inference on ambiguous inputs: the pixels of an image often cannot be labeled with certainty as grouped or segmented. This project will test the hypothesis that human visual segmentation is a process of hierarchical probabilistic inference. Specific Aim 1 will determine whether the measured variability of human segmentations reflects the uncertainty predicted by the model, as required for well-calibrated probabilistic inference. Specific Aim 2 addresses how feedforward and feedback processing in human segmentation contribute to efficient integration of visual features across different levels of complexity, from small contours to object parts. Specific Aim 3 will determine reciprocal interactions between perceptual segmentation and top-down influences including: semantic scene content; visual texture discrimination; and expectations reflecting environmental statistics. The proposed approach models these influences as Bayesian priors, and thus, if supported by the proposed experiments, will offer a unified framework to understand the integration of bottom-up and top- down influences in human segmentation of natural inputs. RELEVANCE (See instructions): This project aims to provide a unified understanding of perceptual segmentation and grouping of visual inputs encountered in the natural environment, through correct integration of the information contained in the visual inputs with top-down information about objects and behavioral goals. This understanding is central to advancing artificial perceptual systems that can help restore impaired vision in patient populations. n/a",CRCNS: Probabilistic models of perceptual grouping/segmentation in natural vision  ,10135248,R01EY031166,"['Address', 'Algorithms', 'Behavioral', 'Cognitive', 'Computer Vision Systems', 'Cues', 'Data', 'Data Set', 'Discrimination', 'Environment', 'Experimental Designs', 'Feedback', 'Goals', 'Grouping', 'Human', 'Image', 'Impairment', 'Individual', 'Influentials', 'Instruction', 'Label', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Motor', 'Neurodevelopmental Disorder', 'Neurons', 'Participant', 'Perception', 'Process', 'Protocols documentation', 'Recurrence', 'Semantics', 'Sensory', 'Statistical Models', 'Stimulus', 'System', 'Testing', 'Texture', 'Uncertainty', 'Vision', 'Visual', 'Visual Cortex', 'Visual impairment', 'Work', 'base', 'behavior influence', 'computer framework', 'deep learning', 'detector', 'expectation', 'experimental study', 'flexibility', 'imaging Segmentation', 'improved', 'object recognition', 'patient population', 'predictive modeling', 'segmentation algorithm', 'sensory input', 'sensory integration', 'sensory neuroscience', 'sensory system', 'statistics', 'theories', 'vision science', 'visual processing']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,7300,-0.0122933876772248
"Functional and Structural Optical Coherence Tomography for Glaucoma PROJECT SUMMARY Glaucoma is a leading cause of blindness. Early diagnosis and close monitoring of glaucoma are important because the onset is insidious and the damage is irreversible. Advanced imaging modalities such as optical coherence tomography (OCT) have been used in the past 2 decades to improve the objective evaluation of glaucoma. OCT has higher axial spatial resolution than other posterior eye imaging modalities and can precisely measure neural structures. However, structural imaging alone has limited sensitivity for detecting early glaucoma and only moderate correlation with visual field (VF) loss. Using high-speed OCT systems, we have developed novel OCT angiography technologies to image vascular plexuses that supply the retinal nerve fibers and ganglion cells damaged by glaucoma. Our results showed that OCT angiographic parameters have better correlation with VF parameters. We have also found that measurement of focal and sectoral glaucoma damage using high-definition volumetric OCT angiographic and structural parameters improves diagnostic performance. The goal of the proposed project is to further improve the diagnosis and monitoring of glaucoma using ultrahigh-speed OCT and artificial intelligence machine learning techniques. The specific aims are: 1. Develop quantitative wide-field OCT angiography. We will develop a swept-source OCT prototype that  is 4 times faster than current commercial OCT systems. The higher speed will be used to fully sample the  neural structures and associated capillary plexuses damaged by glaucoma. 2. Simulate VF by combining structural and angiographic OCT. Preliminary results showed that both  structural and angiographic OCT parameters have high correlation with VF on a sector basis. It may be  possible to accurately simulate VF results by combining these parameters using an artificial neural  network. The simulated VF may be more precise and reliable than subjective VF testing. 3. Longitudinal clinical study in glaucoma diagnosis and monitoring. Our novel OCT structural and  angiographic parameters have high accuracy in diagnosing glaucoma. Neural network analysis of structural  and angiographic data from a larger clinical study could further improve diagnostic accuracy. Longitudinal  follow-up will assess if simulated VF could monitor disease progression as well as actual VF. 4. Clinical study to assess the effects of glaucoma treatments. Preliminary results suggest that OCT  angiography could detect the improvement in capillary density after glaucoma surgery and the effects of  drugs. These intriguing effects will be tested in before-and-after comparison studies. If successful, we will have an OCT diagnostic system that in minutes provides objective information on the location and severity of glaucoma damage. This approach could replace time-consuming and unreliable VF testing. Measuring the improvement in retinal circulation could be a quicker way to detect the benefit of glaucoma therapies that work through neuroprotection or regeneration, compared to monitoring VF. PROJECT NARRATIVE Optical coherence tomography is a high-resolution imaging technology that can non-invasively measure both the eye structures and small blood vessels that are damaged by glaucoma, a leading cause of blindness. The proposed research will further improve this technology so that it can provide detailed measurement over wider areas inside the eye, detect earlier stages of glaucoma, evaluate the location and severity of glaucoma damage, monitor disease progression, and provide more timely assessment of the effectiveness of therapy. A goal of this project is to determine if this objective imaging technology can provide information that is equivalent to or better than subjective visual field testing, which though time-consuming and poorly reliable, is the current gold standard for long-term monitoring and management of glaucoma.",Functional and Structural Optical Coherence Tomography for Glaucoma,9952373,R01EY023285,"['Abbreviations', 'Affect', 'Angiography', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Biomedical Engineering', 'Blindness', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Clinical', 'Clinical Research', 'Complex', 'Computer software', 'Consumption', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Effectiveness', 'Evaluation', 'Eye', 'Eyedrops', 'Functional disorder', 'Future', 'Geography', 'Glaucoma', 'Glossary', 'Goals', 'Gold', 'Grant', 'Image', 'Imaging technology', 'Individual', 'Knowledge', 'Lasers', 'Location', 'Longitudinal observational study', 'Machine Learning', 'Measurement', 'Measures', 'Medical', 'Methods', 'Monitor', 'Natural regeneration', 'Nerve Fibers', 'Noise', 'Operative Surgical Procedures', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Pathway Analysis', 'Patients', 'Performance', 'Perfusion', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Postoperative Period', 'Research', 'Research Project Grants', 'Resolution', 'Retina', 'Retinal macula', 'Role', 'Safety', 'Sampling', 'Scanning', 'Sensitivity and Specificity', 'Severities', 'Shunt Device', 'Signal Transduction', 'Source', 'Speed', 'Staging', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Trabeculectomy', 'Variant', 'Vision', 'Visit', 'Visual Fields', 'Work', 'analytical tool', 'artificial neural network', 'base', 'bulk motion', 'cell injury', 'clinical practice', 'cost', 'density', 'diagnostic accuracy', 'fiber cell', 'field study', 'follow-up', 'ganglion cell', 'glaucoma surgery', 'high resolution imaging', 'high risk', 'imaging modality', 'improved', 'innovation', 'insight', 'macula', 'neural network', 'neuroprotection', 'new technology', 'novel', 'prototype', 'quantitative imaging', 'relating to nervous system', 'retina circulation', 'screening', 'tool', 'treatment effect', 'vascular factor', 'visual performance']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,564228,0.05451976952800462
"Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1 Project Summary Glaucoma is a leading cause of vision morbidity and blindness worldwide. Early disease detection and sensitive monitoring of progression are crucial to allow timely treatment for preservation of vision. The introduction of ocular imaging technologies significantly improves these capabilities, but in clinical practice there are still substantial challenges at certain stages of the disease severity spectrum, specifically in the early stage and in advanced disease. These difficulties are due to a variety of causes that change over the course of the disease, including large between-subject variability, inherent measurement variability, image quality, varying dynamic ranges of measurements, minimal measurable level of tissues, etc. In this proposal, we build on our long-standing contribution to ocular imaging and propose novel and sensitive means to detect glaucoma and its progression that are optimized to the various stages of disease severity. We will use information gathered from visual fields (functional information) and a leading ocular imaging technology – optical coherence tomography (OCT; structural information) to map the capability of detecting changes across the entire disease severity spectrum to identify optimal parameters for each stage of the disease. Both commonly used parameters provided by the technologies and newly developed parameters with good diagnostic potential will be analyzed. We will use state-of-the-art automated computerized machine learning methods, namely the deep learning approach, to identify structural features embedded within OCT images that are associated with glaucoma and its progression without any a priori assumptions. This will provide novel insight into structural information, and has shown very encouraging preliminary results. We will also utilize a new imaging technology, the visible light OCT, to generate retinal images with outstanding resolution to extract information about the oxygen saturation of the tissue. This will provide in-vivo, real time, and noninvasive insight into tissue functionality. Taken together, this program will advance the use of structural and functional information with a substantial impact on the clinical management of subjects with glaucoma Project Narrative This research proposal is focusing on the development and refinement of innovative analytical methods and cutting-edge technologies that will substantially improve detection of glaucoma and its progression monitoring in order to prevent blindness.",Novel Glaucoma Diagnostics for Structure and Function  - Renewal - 1,10019553,R01EY013178,"['3-Dimensional', 'Blindness', 'Characteristics', 'Clinical', 'Clinical Management', 'Clinical Research', 'Complex', 'Data', 'Detection', 'Development', 'Diagnostic', 'Discrimination', 'Disease', 'Disease Progression', 'Early Diagnosis', 'Evaluation', 'Eye', 'Floor', 'Future', 'Glaucoma', 'Health', 'Human', 'Image', 'Imaging technology', 'Inner Plexiform Layer', 'Knowledge', 'Laboratories', 'Lead', 'Light', 'Maps', 'Measurable', 'Measurement', 'Measures', 'Metabolic', 'Methodology', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Optic Disk', 'Optical Coherence Tomography', 'Outcome', 'Oxygen Consumption', 'Oxygen saturation measurement', 'Pathology', 'Research Proposals', 'Resolution', 'Retina', 'Retinal Diseases', 'Scanning', 'Severities', 'Severity of illness', 'Signal Transduction', 'Source', 'Structure', 'Structure-Activity Relationship', 'System', 'Techniques', 'Technology', 'Thick', 'Time', 'Tissue Extracts', 'Tissues', 'Translating', 'Visible Radiation', 'Vision', 'Visual Fields', 'Width', 'advanced disease', 'analytical method', 'base', 'clinical practice', 'cohort', 'computerized', 'deep learning', 'density', 'ganglion cell', 'improved', 'in vivo', 'innovation', 'innovative technologies', 'insight', 'instrument', 'invention', 'knowledge base', 'longitudinal dataset', 'machine learning method', 'macula', 'mathematical methods', 'new technology', 'novel', 'novel strategies', 'ocular imaging', 'preservation', 'prevent', 'programs', 'research study', 'retinal imaging', 'retinal nerve fiber layer', 'tissue oxygenation', 'tool']",NEI,NEW YORK UNIVERSITY SCHOOL OF MEDICINE,R01,2020,687519,0.007166499458797702
"Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers Abstract COVID-19 has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. Accordingly, the goal of this COVID Supplement, which builds on and expands the work being conducted by the parent grant, is to develop a COVID map tool that provides fully accessible, non-visual access to maps. This tool will allow visually impaired persons to explore maps and preview routes from the comfort of their home, allowing them to plan their travel along safer, less congested routes using crowdedness data. In addition, the tool will present county-by-county COVID incidence data in a fully accessible form, which will inform their travel plans over greater distances. Thus, this project will give visually impaired persons the tools and confidence to undertake safer, more independent travel. Health Relevance The COVID-19 pandemic has an especially severe impact on people with significant vision impairments or blindness. The need for social distancing and reduced touching of one’s surroundings has made traveling as a blind or visually impaired person much riskier and more difficult than before the pandemic. As a result, people with visual impairments may limit their essential travel such as trips to the doctor’s office, the pharmacy and grocery shopping and walks for exercise or leisure. These travel limitations may have adverse impacts on their physical and mental health. The proposed research would result in a new software tool that could greatly increase the confidence of the approximately 10 million Americans with significant vision impairments or blindness to undertake safe, independent travel.",Leveraging Maps and Computer Vision to Support Indoor Navigation for Blind Travelers,10220178,R01EY029033,"['American', 'Blindness', 'COVID-19', 'COVID-19 pandemic', 'Cellular Phone', 'Color', 'Communities', 'Computer Vision Systems', 'Computers', 'County', 'Crowding', 'Data', 'Destinations', 'Development', 'Ensure', 'Evaluation', 'Exercise', 'Goals', 'Health', 'Home environment', 'Incidence', 'Internet', 'Knowledge', 'Leisures', 'Maps', 'Mental Health', 'Pharmacy facility', 'Process', 'Publications', 'Research', 'Route', 'Running', 'Social Distance', 'Software Tools', 'System', 'Tablets', 'Tactile', 'Target Populations', 'Touch sensation', 'Travel', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Walking', 'Work', 'blind', 'braille', 'coronavirus disease', 'design', 'outreach', 'pandemic disease', 'parent grant', 'physical conditioning', 'software development', 'symposium', 'tool', 'way finding']",NEI,SMITH-KETTLEWELL EYE RESEARCH INSTITUTE,R01,2020,406525,0.06758851007160262
"Validation of a lab-free low-cost screening test for prevention of cervical cancer: automated visual evaluation PROJECT SUMMARY/ABSTRACT Artificial intelligence (AI) has the potential to revolutionize medicine by improving productivity, reducing human error, and assisting with diagnosis and treatment. Image classification algorithms can be used to develop automated visual evaluation (AVE): a potential game-changer for cervical cancer prevention in low- and middle-income countries (LMICs). AVE technology reads digital photographs of a cervix to provide diagnosis and treatment recommendations in seconds. AVE is a true point of care test, low cost and does not require a laboratory. AVE could be used either for stand-alone primary screening, or to triage HPV-positive women. We will compare AVE to common screening methods in LMICs: visual inspection with acetic acid (VIA) and conventional cytology. Enhanced Visual Assessment (EVA) System by MobileODT is a cloud-connected mobile colposcope on a smartphone platform. It is FDA cleared and used in 42 countries. MobileODT is uniquely poised to integrate AVE into the EVA System. Our aim is to validate and commercialize AVE on the EVA platform. Phase I aims will adapt AVE to run on the EVA system using an optimal neural network architecture, running either directly on the phone or as a cloud- based service. Phase II is a prospective clinical trial of 10,000 patients recruited at ministry of health sites in El Salvador. All screen-positive patients, and 10% of negative patients, will undergo colposcopy with biopsy. Sensitivity of AVE as a primary screening test will be compared to cytology and to VIA. In HPV-positive women, AVE will be compared to VIA as a triage test. PROJECT NARRATIVE The proposal involves developing and testing a cervical cancer screening test: automated visual evaluation (AVE) based on an image classification algorithm that runs on smartphone-based colposcope. Included are both technical development to integrate AVE to a mobile phone application (Phase I), and a prospective validation on a screening population of 10,000 women in El Salvador (Phase II). AVE will be compared to standard tests (conventional cytology and visual inspection with acetic acid: VIA) for primary screening, and against VIA triage in an HPV+ population.",Validation of a lab-free low-cost screening test for prevention of cervical cancer: automated visual evaluation,10008280,R44CA247137,"['Acetic Acids', 'Address', 'Algorithm Design', 'Algorithms', 'Architecture', 'Area', 'Artificial Intelligence', 'Bedside Testings', 'Biopsy', 'Car Phone', 'Cellular Phone', 'Cervical', 'Cervical Cancer Screening', 'Cervical Intraepithelial Neoplasia', 'Cervix Uteri', 'Clinical', 'Clinical Trials', 'Colposcopes', 'Colposcopy', 'Country', 'Cytology', 'Data', 'Decision Making', 'Detection', 'Development', 'Diagnosis', 'Documentation', 'El Salvador', 'Guidelines', 'HPV-High Risk', 'Health', 'Histology', 'Histopathology', 'Human Papillomavirus', 'Image', 'Internet', 'Laboratories', 'Medicine', 'Methods', 'Oncogenic', 'Patient Recruitments', 'Patients', 'Pattern', 'Performance', 'Phase', 'Population', 'Predictive Value', 'Prevention', 'Productivity', 'Provider', 'ROC Curve', 'Receiver Operating Characteristics', 'Recommendation', 'Resources', 'Running', 'Sampling', 'Screening procedure', 'Services', 'Site', 'Speed', 'System', 'Technology', 'Telephone', 'Testing', 'Triage', 'Validation', 'Visual', 'Woman', 'World Health Organization', 'automated visual evaluation', 'base', 'cervical cancer prevention', 'classification algorithm', 'cloud based', 'cost', 'data quality', 'deep learning algorithm', 'digital', 'human error', 'improved', 'innovation', 'low and middle-income countries', 'mobile application', 'neural network', 'neural network architecture', 'overtreatment', 'phase II trial', 'primary endpoint', 'product development', 'programs', 'prospective', 'quality assurance', 'screening', 'screening program', 'secondary analysis', 'secondary endpoint', 'tool']",NCI,"MOBILEODT, INC.",R44,2020,297844,0.0011524017120003226
"Objective Quantification of Neural Damage for Screening, Diagnosis and Monitoring of Glaucoma with Fundus Photographs PROJECT SUMMARY Glaucoma is a progressive optic neuropathy and the leading cause of irreversible blindness in the world. As the disease remains largely asymptomatic until late stages, there is a pressing need to develop affordable approaches for screening before visual impairment occurs. Although sophisticated imaging technologies such as Spectral domain-optical coherence tomography (SDOCT) can provide highly reproducible and accurate quantitative assessment of glaucomatous damage, their application in widespread screening or non-specialized settings is unfeasible, given the high cost and operator requirements. Fundus photography is a low-cost alternative that has been used successfully in teleophthalmology programs. However, subjective human grading of fundus photos for glaucoma is poorly reproducible and highly inaccurate, as gradings tend to largely over- or underestimate damage. We propose a new paradigm for assessing glaucomatous damage by training a deep learning (DL) convolutional neural network to provide quantitative estimates of the amount of neural damage from fundus photographs. In our Machine-to-Machine (M2M) approach, we trained a DL network to analyze fundus photos and predict quantitative measurements of glaucomatous damage provided by SDOCT, such as retinal nerve fiber layer (RNFL) thickness and neuroretinal rim measurements. Our preliminary results showed that the M2M predictions have very high correlation and agreement with the original SDOCT observations. This provides an objective method to quantify neural damage in fundus photos without requiring human graders, which could potentially be used for screening, diagnoses and monitoring in teleophthalmology and non- specialized point-of-care settings. In this proposal, we aim at refining and validating the M2M model in suitable, large datasets from population-based studies, electronic medical records, and clinical trial data. Our central hypothesis is that the M2M approach will be more accurate than subjective human gradings in screening, diagnosing, predicting and detecting longitudinal damage over time. In Aim 1, we will investigate the performance of the M2M model to screen for glaucomatous damage using large datasets from 6 population-based studies: Blue Mountains Eye Study, Los Angeles Latino Eye Study, Tema Eye Survey, Beijing Eye Study, Central India Eye and Medical Study and the Ural Eye and Medical Study, which will provide data on over 25,000 subjects of diverse racial groups. In Aim 2, we will investigate the ability of the M2M model to predict future development of glaucoma in eyes of suspects using the data from the Ocular Hypertension Treatment Study (OHTS). In Aim 3, we will investigate the ability of the M2M model in detecting glaucomatous progression over time using data from the Duke Glaucoma Registry, a large database of longitudinal structure and function data in glaucoma with over 25,000 patients followed over time. If successful, this proposal will lead to a validated, inexpensive, and widely applicable tool for screening, early diagnosis and monitoring of glaucoma, that could be applied under population-based settings and also at non-specialized point-of-care settings. Project Narrative Glaucoma is a leading cause of irreversible visual impairment in the world. This proposal will employ a novel artificial intelligence paradigm for quantifying neural damage on ocular fundus photographs for the purpose of screening, diagnosing and monitoring glaucoma damage. The approach will be validated on large datasets from population-based studies, electronic medical records and clinical trial data.","Objective Quantification of Neural Damage for Screening, Diagnosis and Monitoring of Glaucoma with Fundus Photographs",10047364,R21EY031898,"['Agreement', 'Artificial Intelligence', 'Blindness', 'Clinical Trials', 'Computerized Medical Record', 'Consumption', 'Data', 'Data Set', 'Databases', 'Development', 'Diabetic Retinopathy', 'Diagnosis', 'Diagnostic', 'Disease', 'Early Diagnosis', 'Exhibits', 'Eye', 'Eye diseases', 'Fundus', 'Fundus photography', 'Future', 'Glaucoma', 'Human', 'Imaging technology', 'India', 'Investigation', 'Label', 'Latino', 'Los Angeles', 'Manuals', 'Measurement', 'Medical', 'Methods', 'Modeling', 'Monitor', 'Names', 'Nature', 'Ocular Hypertension', 'Ophthalmology', 'Optical Coherence Tomography', 'Output', 'Patients', 'Performance', 'Population Study', 'Race', 'Reference Standards', 'Registries', 'Reproducibility', 'Risk', 'Science', 'Screening procedure', 'Structure', 'Surveys', 'Testing', 'Thick', 'Time', 'Training', 'Validation', 'Visual impairment', 'algorithm training', 'clinical care', 'convolutional neural network', 'cost', 'cost effective', 'deep learning', 'deep learning algorithm', 'deep neural network', 'flexibility', 'hypertension treatment', 'intelligent algorithm', 'interest', 'large datasets', 'learning network', 'longitudinal database', 'novel', 'optic nerve disorder', 'point of care', 'population based', 'predictive modeling', 'programs', 'racial diversity', 'relating to nervous system', 'retinal nerve fiber layer', 'screening', 'time use', 'tool']",NEI,DUKE UNIVERSITY,R21,2020,241500,0.02937892252453732
"Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings Project Summary: Darwin Babino, PhD, a trained pharmacologist/electrophysiologist, has spent the last ten years working on several disciplines in the vision sciences. His proposal entitled “Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings” presents his overarching goal to improve vision restoration approaches by developing methods to test the potential of these techniques thereby accelerating the development of effective interventions. Dr. Babino and his primary mentor, Dr. Russell Van Gelder, have assembled a strong team of co-mentors at the University of Washington SOM and collaborators to guide him through the proposed training and research. His previous training will be supplemented with goals to help his development as an independent investigator: 1) Study design and practical learning in performing panretinal (MEA) biological experiments; 2) Fundamental and advanced techniques of the proposed optogenetic and stem-cell restoration techniques; 3) Application of advanced machine learning techniques; 4) Develop leadership and professional skills to establish an independent group. The ability to assess the function of panretinal circuitry will foster our understanding of the advantages and weaknesses of different restoration techniques (Aim 1). The work proposed here will improve an existing retinal acuity assessment tool which combines machine learning techniques on novel, high-density multielectrode array recordings of ganglion cell responses in several mouse models. The utility of this system will be demonstrated in assessing visual potential of the mouse retina in three different approaches to vision restoration that are challenging for in vivo assessment (Aim 2). In collaboration with Dr. Deepak A. Lamba at UCSF, we will apply our system to animals which have undergone stem-cell replacement of retinal cells including photoreceptor cells. An optogenetics approach will also be evaluated in collaboration with Dr. John Flannery at UC Berkeley whose group has developed vectors for expressing rhodopsin and cone opsins in ganglion and bipolar cells. Finally, differences between native and restored vison with small molecule photoswitches, light-activated inhibitors of voltage-gated potassium channels, which confer light-dependent firing on treated cells, will be assessed. The resulting advanced electrophysiology application will help elucidate fundamental questions about the functional retina, mechanisms that lead to retinal degeneration and the potential of several therapeutics for the treatment of retinal diseases. Furthermore, this career development award will facilitate Dr. Babino’s development into an independent investigator by priming an R01 grant application. Project Narrative: Project Narrative: The prevalence of vision loss from retinal degeneration numbers in the millions world-wide and is expected to double by the year 2050, and despite the development of several promising approaches to restore vision in the blind, progress in developing these therapies has been hampered by challenges in analysis of these methods in animal models. We describe a novel system that analyzes, by machine learning, retinal ganglion cell output in native, degenerated and therapeutically treated blind retinas which can characterize the visual information content of the ‘reanimated’ blind retina and thereby facilitate the development of these technologies. The system developed through this grant, as well as the career development pursued by the investigator, will be readily applicable to the assessment of potential retinal acuity restoration by current and novel therapeutic approaches.",Assessment of murine retinal acuity ex vivo by machine learning of multielectrode array recordings,9943144,K99EY031333,"['Aftercare', 'Amacrine Cells', 'Animal Model', 'Animals', 'Applications Grants', 'Assessment tool', 'Behavioral Assay', 'Biological', 'Blindness', 'Cells', 'Collaborations', 'Cone', 'Contrast Sensitivity', 'Data', 'Development', 'Discipline', 'Dissection', 'Doctor of Philosophy', 'Ectopic Expression', 'Electrophysiology (science)', 'Electroretinography', 'Evolution', 'Feedback', 'Fostering', 'Ganglia', 'Genetic', 'Goals', 'Grant', 'Human', 'Image', 'In Vitro', 'Individual', 'Intervention', 'K-Series Research Career Programs', 'Knockout Mice', 'Knowledge', 'Lead', 'Leadership', 'Learning', 'Light', 'MW opsin', 'Machine Learning', 'Measurable', 'Measurement', 'Measures', 'Mediating', 'Mentors', 'Methods', 'Movement', 'Mus', 'Opsin', 'Output', 'Photoreceptors', 'Prevalence', 'Protocols documentation', 'Psychophysics', 'Research', 'Research Design', 'Research Personnel', 'Resolution', 'Retina', 'Retinal Cone', 'Retinal Degeneration', 'Retinal Diseases', 'Retinal Ganglion Cells', 'Retinal gene therapy', 'Rhodopsin', 'Rod', 'Rodent', 'Saccades', 'Sampling', 'Scanning', 'Science', 'Scientist', 'Specificity', 'Stimulus', 'Synapsins', 'System', 'Systems Analysis', 'Systems Development', 'Techniques', 'Testing', 'Therapeutic', 'Training', 'Transgenic Organisms', 'Universities', 'Vertebrate Photoreceptors', 'Viral', 'Vision', 'Visual', 'Visual Acuity', 'Visual system structure', 'Voltage-Gated Potassium Channel', 'Washington', 'Wild Type Mouse', 'Work', 'base', 'behavior test', 'blind', 'career development', 'cell type', 'cost effective', 'density', 'effective intervention', 'experimental study', 'ganglion cell', 'improved', 'in vivo', 'induced pluripotent stem cell', 'inhibitor/antagonist', 'interest', 'light intensity', 'mimicry', 'mouse model', 'multi-electrode arrays', 'mutant', 'nonhuman primate', 'novel', 'novel therapeutic intervention', 'optogenetics', 'promoter', 'rapid eye movement', 'response', 'restoration', 'scale up', 'skills', 'small molecule', 'stem cells', 'technology development', 'therapy development', 'tool', 'vector', 'vision science', 'visual information']",NEI,UNIVERSITY OF WASHINGTON,K99,2020,113080,-0.0066356513320318085
"Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure Project Summary Glaucoma is the second leading cause of blindness globally, and is characterized by optic nerve damage that leads to the death of retinal ganglion cells with accompanying visual field (VF) loss. The optic nerve head (ONH) is the site of injury to the optic nerve fibers and plays a central role in glaucoma pathogenesis and diagnosis. Traditionally, glaucoma is diagnosed based on fundus inspection of the ONH, which provides information about the surface contour of the ONH. However, the optic nerve damage occurs in the deeper layers. With the development of optical coherence tomography (OCT) techniques for three-dimensional (3D) retinal imaging, parameters derived from the 3D ONH related structure (e.g., Bruch's membrane opening minimum rim width, peripapillary retinal nerve fiber layer thickness, disc tilt etc.) have been studied to better understand glaucoma pathogenesis, and are used to supplement clinical diagnosis. In addition, studies of the ONH biomechanics have also shown that the strain level at the ONH at any given intraocular pressure level depends on the 3D geometry of the ONH related structure. A high strain level is hypothesized to contribute to retinal ganglion cell injury. Previous research has suggested that the 3D ONH related structure is correlated to glaucoma pathogenesis and critically important to glaucoma diagnosis. However, to date, a systematic study using clinical data to determine the impact of the 3D ONH related structure on glaucoma has not been conducted.  We propose to study the relationship between the 3D ONH related structure and glaucoma with a diverse set of combined techniques including image processing, computational mechanics and machine learning. The specific aims of this project are to: (1) Derive features from the 3D ONH related structure and study their implications on VF loss patterns (K99 Phase). (2) Investigate the impact of the strain field patterns at the ONH on glaucoma (K99 Phase). (3) Study the effect of the 3D ONH related features on OCT diagnostic parameters (R00 Phase). (4) Model central vision loss from the 3D ONH related structural features (R00 Phase). Collectively, these studies will provide new insights and perspectives into the structure-function relationships in glaucoma and establish ocular anatomy specific norms of retinal nerve fiber layer profiles, which will advance our current understanding of glaucoma pathogenesis and improve glaucoma diagnosis. Our research is of high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.  Through the proposed research and training plans, the applicant will build a solid knowledge base in ophthalmology and further improve his expertise in mathematical modeling and data science. This project will provide critical training opportunities to further enhance the applicant's capabilities to become an independent computational vision scientist in ophthalmology. Project Narrative The proposed research will study the relationship between the three-dimensional (3D) optic nerve head (ONH) related structure and glaucoma. Our study will advance the current understanding of glaucoma pathogenesis and improve glaucoma diagnosis by gaining new insights into the structure-function relationships in glaucoma and establishing ocular anatomy specific norms of retinal nerve fiber layer profiles. Our research has high clinical relevance and can be potentially translated into clinical practice for better glaucoma diagnosis, monitoring and treatment.",Relationship between Glaucoma and the Three-Dimensional Optic Nerve Head Related Structure,9857605,K99EY028631,"['3-Dimensional', 'Age', 'Anatomy', 'Biomechanics', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Cessation of life', 'Clinical', 'Clinical Data', 'Computer Simulation', 'Cross-Sectional Studies', 'Data', 'Data Science', 'Development', 'Diagnosis', 'Diagnostic', 'Ear', 'Elements', 'Eye', 'Fundus', 'Gaussian model', 'Geometry', 'Glaucoma', 'Hour', 'Image', 'Individual', 'Injury', 'Lead', 'Linear Models', 'Linear Regressions', 'Location', 'Machine Learning', 'Measurement', 'Mechanics', 'Medical Records', 'Methods', 'Modeling', 'Monitor', 'Morphologic artifacts', 'Multivariate Analysis', 'Nerve Fibers', 'Observational Study', 'Ophthalmology', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Participant', 'Pathogenesis', 'Patients', 'Pattern', 'Phase', 'Physiologic Intraocular Pressure', 'Play', 'Population Study', 'Process', 'Quality of life', 'Research', 'Research Training', 'Resolution', 'Retinal Ganglion Cells', 'Role', 'Scanning', 'Scheme', 'Scientist', 'Severities', 'Site', 'Solid', 'Source', 'Structure', 'Structure-Activity Relationship', 'Surface', 'Techniques', 'Testing', 'Thick', 'Translating', 'Variant', 'Vision', 'Visual Fields', 'Width', 'archetypal analysis', 'base', 'cell injury', 'clinical Diagnosis', 'clinical practice', 'clinically relevant', 'deep neural network', 'demographics', 'fundus imaging', 'image processing', 'improved', 'independent component analysis', 'insight', 'knowledge base', 'machine learning method', 'mathematical model', 'nonlinear regression', 'optic cup', 'retina blood vessel structure', 'retinal imaging', 'retinal nerve fiber layer', 'study population', 'training opportunity', 'unsupervised learning']",NEI,SCHEPENS EYE RESEARCH INSTITUTE,K99,2020,145891,0.052850798256662446
"Predicting Human Olfactory Perception from Molecular Structure PROJECT SUMMARY Modern technology makes it possible to capture a visual scene as a photograph, alter it, send it to another country nearly instantaneously, and store it without concern for degradation. None of this is currently possible in olfaction. Although perfumers and flavorists are adept at mixing odorous molecules to produce a desired perceptual effect, the rules underlying this process are poorly understood at a quantitative level. Current methods for displaying odors to a subject are akin to requiring a Polaroid of every visual stimulus of interest. A more efficient method for probing the olfactory system would be to use a set of 'primary odors'—some limited number of odors from which all other complex odors could be reproduced by appropriate mixtures. Both auditory and visual stimuli have been digitized, and this will eventually be possible in olfaction as well. Predicting odor from chemical structure has been a problem in the field since its inception, but recent advances in machine learning algorithms have made great progress in analogous problems, such as facial recognition. The research proposed here will combine these machine learning techniques with high quality human psychophysics to understand how to predict the smell of a molecule or mixture of odorants, which will ultimately help improve our understanding of disease diagnosis using odors as well as eating-related health and illness. HEALTH RELEVANCE The sense of smell plays a critical role in preferences and aversions for specific foods. The proposed research will combine machine learning techniques with high quality human psychophysics to create a model that can predict the smell of odorous molecules. This model will allow us to describe and control odors, which will increase our understanding of food preference and eating-related health and wellness.",Predicting Human Olfactory Perception from Molecular Structure,9887973,R01DC017757,"['Algorithms', 'Characteristics', 'Chemical Structure', 'Chemicals', 'Code', 'Collection', 'Color', 'Communities', 'Complex', 'Complex Mixtures', 'Country', 'Data', 'Data Set', 'Descriptor', 'Detection', 'Development', 'Eating', 'Enrollment', 'Face', 'Food', 'Food Preferences', 'Frequencies', 'Health', 'Human', 'Ligands', 'Machine Learning', 'Mass Fragmentography', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modernization', 'Molecular Structure', 'Neurosciences', 'Non-linear Models', 'Numerical value', 'Odors', 'Olfactory Pathways', 'Perception', 'Play', 'Process', 'Psychophysics', 'Research', 'Research Personnel', 'Role', 'Smell Perception', 'Stimulus', 'Techniques', 'Technology', 'Training', 'Translating', 'Vision', 'Visual', 'Vocabulary', 'Work', 'auditory stimulus', 'base', 'computer monitor', 'disease diagnosis', 'experience', 'high dimensionality', 'improved', 'in silico', 'interest', 'machine learning algorithm', 'member', 'novel', 'physical property', 'predictive modeling', 'predictive test', 'preference', 'receptor', 'relating to nervous system', 'single molecule', 'visual stimulus']",NIDCD,MONELL CHEMICAL SENSES CENTER,R01,2020,496911,-0.023875805451324585
"SBIR Phase I Topic 402 -  Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring This project aims to develop an interpretable, physician-in-the-loop AI-aided software that accurately delineates glioma boundaries in MRIs, computes volumetric curves, and statistically quantifies the tumor growth in longitudinal studies. The current clinical practice of visually analyzing and manually contouring tumors is subjective, time-consuming, and often inconsistent. The novelty of MRIMath's explainable, trustworthy, and physician-in-the-loop AI system is multi-fold. First, we introduce a multi-scale feature extraction framework using the inception modules in contracting and expanding paths of the U-Net image segmentation neural network architecture. Second, we propose a new loss function based on the modified Dice similarity coefficient. Third, we train and test the AI system using two learning regimes: learning to segment intra-tumoral structures and learning to segment glioma sub-regions. Finally, we produce heat maps to visualize the features extracted by the AI, thus offering physicians a view of AI's attention patterns and activation maps that were triggered during AI's decision-making. An intuitive and interactive User Interface will allow the physician to review contouring results, make adjustments and approve contours, visualize AI's explanations and volumetric measurements, and finally review the results of the statistical analysis. Any modifications made by the physician will be used later to re-train AI. n/a","SBIR Phase I Topic 402 -  Artificial Intelligence-Aided Imaging for Cancer Prevention, Diagnosis, and Monitoring",10269837,5N91020C00049,"['Artificial Intelligence', 'Attention', 'Computer software', 'Consumption', 'Contracts', 'Data', 'Data Sources', 'Decision Making', 'Diagnosis', 'Glioma', 'Human', 'Intuition', 'Learning', 'Longitudinal Studies', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measurement', 'Modality', 'Modification', 'Monitor', 'Pattern', 'Phase', 'Physicians', 'Small Business Innovation Research Grant', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'System', 'Testing', 'Time', 'TimeLine', 'Training', 'base', 'cancer imaging', 'cancer prevention', 'clinical practice', 'design', 'feature extraction', 'imaging Segmentation', 'imaging software', 'imaging system', 'loss of function', 'neural network architecture', 'prototype', 'tumor', 'tumor growth', 'usability']",NCI,"MRIMATH, LLC",N43,2020,400000,-0.0093950036672495
"SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions  The project will investigate prosthetic support for people with visual impairment (PVI) that integrates computer vision-based prosthetics with video-mediated human-in-the-loop prosthetics. Computer vision- based (CV) prosthetics construe the fundamental technical challenge for visual prosthetics as one of parsing and identifying objects across scales, distances, and orientations. Visual prosthetic applications have been central drivers in the development of computer vision technology through the past 50 years. Video-mediated remote sighted assistance (RSA) prosthetics are more recent, enabled by different technologies, and construe the orienting technical challenge for visual prosthetics as one of effective helping interactions. RSA services are commercially available now, and have evoked much excitement in the PVI community. The two approaches, CV and RSA, will be successively integrated through a series of increasingly refined Wizard of Oz simulations, and investigate possible synergies between the two approaches. We will employ a human-centered design approach, identifying a set of key assistive interaction scenarios that represent authentic needs and concerns of PVIs, by leveraging our 6-year relationship working directly with our local chapter of the National Federation of the Blind. RELEVANCE (See Instructions): 23.7 million American adults have vision loss; 1.3 million people in US are legally blind. This project addresses a transformational opportunity to enhance human performance and experience, to diversify workplace participation, and to enhance economic and social well-being. n/a",SCH: INT: Conversations for Vision: Human-Computer Synergies in Prosthetic Interactions ,10020434,R01LM013330,"['Address', 'Adult', 'American', 'Articulation', 'Back', 'Blindness', 'Communities', 'Computer Vision Systems', 'Computers', 'Data Set', 'Development', 'Economics', 'Emotional', 'Female', 'Goals', 'Human', 'Information Sciences', 'Instruction', 'Mediating', 'Modeling', 'Ocular Prosthesis', 'Performance', 'Prosthesis', 'Route', 'Self-Help Devices', 'Series', 'Services', 'Social Well-Being', 'Technology', 'Time', 'Underrepresented Students', 'Vision', 'Visual', 'Visual impairment', 'Work', 'Workplace', 'base', 'blind', 'design', 'experience', 'graduate student', 'human-in-the-loop', 'learning materials', 'legally blind', 'outreach', 'prototype', 'simulation', 'synergism', 'undergraduate student']",NLM,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,R01,2020,229482,0.06449300074445148
"Visible-light OCT angiography, velocimetry, and oximetry for characterizing retinal vascular alterations in glaucoma Project Summary Glaucoma damage to the optic nerve and impairment of vision are progressive and irreversible. Understanding mechanisms of glaucomatous injury will help to develop new approaches for treatments that can be used along with traditional therapies that lower intraocular pressure (IOP). Recent developments in optical coherence tomography (OCT) angiography have brought increased attention to the role of the inner retinal circulation in glaucoma. To improve our understanding of retinal vascular alterations in glaucoma, we can take advantage of recent developments in visible-light OCT (vis-OCT) to characterize simultaneously tissue structure, vessel density, blood flow and oxygenation. The goal of this project is to further advance vis-OCT by attaining capillary-level measurements, test the value of measuring their local alterations as early indicators of glaucoma and glaucomatous progression and use this to evaluate impaired retinal autoregulation from retinal ganglion cell (RGC) loss as a potential cause of increased susceptibility in advanced glaucoma. In Specific Aim 1 we will develop high-speed, high-sensitivity, high-resolution vis-OCT. The speed will be double that of the current system. A more stable supercontinuum laser will be used to improve system sensitivity, and a tighter focus will be used to improve lateral resolution. This will enable complete detection of capillaries that may be vulnerable to vascular dysfunction. Specific Aim 2 will develop quantitative OCT angiography, velocimetry and oximetry in capillaries as well as arteries and veins. Building on the high-resolution, high-contrast scans acquired in Aim 1, we will use machine learning to segment capillary plexuses, and advanced image processing to extract capillary architecture. Aided by this capillary architecture, we will automatically measure blood flow and oxygenation in capillary segments and incorporate them into a real-time platform. Specific Aim 3 will use this system to demonstrate that acute loss of RGCs, produced by optic nerve transection, alters retinal capillary plexus density, oximetry and velocimetry over time and that these changes precede altered oximetry and flow in larger retinal vessels. We will also show that loss of RGCs impairs the autoregulatory response to acute IOP challenge. In Specific Aim 4, we will demonstrate that optic nerve injury in a model of controlled, elevated IOP produces early alterations in capillary velocimetry, oximetry and autoregulation, show that they are more persistent with advanced injury, and demonstrate the pathophysiologic consequences of these observations. Successful development of this new technology will improve methods of early glaucoma diagnosis and detection of progression. Better understanding of retinal vascular factors that lead to increased susceptibility in advanced glaucoma will lead to improved treatments for these highly vulnerable patients. Project Narrative This project will develop advanced technology to image retinal capillaries and measure capillary blood flow and oxygen content. This may provide an early indicator of glaucoma progression and help study a potential cause of increased susceptibility to intraocular pressure in glaucoma patients.","Visible-light OCT angiography, velocimetry, and oximetry for characterizing retinal vascular alterations in glaucoma",9944107,R01EY031394,"['3-Dimensional', 'Abbreviations', 'Acute', 'Address', 'Affect', 'Angiography', 'Architecture', 'Arteries', 'Attention', 'Axon', 'Blindness', 'Blood Circulation', 'Blood Vessels', 'Blood capillaries', 'Blood flow', 'Computer software', 'Defect', 'Detection', 'Development', 'Diagnosis', 'Early identification', 'Event', 'Eye', 'Glaucoma', 'Glossary', 'Goals', 'Homeostasis', 'Impairment', 'Injury', 'Intraocular pressure test', 'Lasers', 'Lateral', 'Lead', 'Lighting', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Noise', 'Optic Nerve', 'Optic Nerve Injuries', 'Optic Nerve Transections', 'Optical Coherence Tomography', 'Oxygen', 'Oxygen saturation measurement', 'Patients', 'Performance', 'Physiologic Intraocular Pressure', 'Predisposition', 'Process', 'Progressive Disease', 'Rattus', 'Resolution', 'Retina', 'Retinal Ganglion Cells', 'Rodent Model', 'Role', 'Scanning', 'Signal Transduction', 'Speed', 'Structure', 'Structure of central vein of the retina', 'System', 'Technology', 'Testing', 'Time', 'Tissues', 'Vascular Diseases', 'Veins', 'Velocimetries', 'Visible Radiation', 'Vision', 'Visual impairment', 'attenuation', 'automated segmentation', 'central retinal artery', 'data acquisition', 'deep learning', 'density', 'image processing', 'improved', 'in vivo', 'innovation', 'insight', 'metabolic rate', 'new technology', 'novel strategies', 'preservation', 'pressure', 'prototype', 'response', 'retina blood vessel structure', 'retina circulation', 'retinal imaging', 'retinal ischemia', 'traditional therapy', 'vascular factor']",NEI,OREGON HEALTH & SCIENCE UNIVERSITY,R01,2020,552591,0.034306962372407074
"Accelerating Community-Driven Medical Innovation with VTK Abstract Thousands of medical researchers around the world use VTK —the Visualization Toolkit— an open-source, freely available software development toolkit providing advanced 3D interactive visualization, image processing and data analysis algorithms. They either use VTK directly in their in-house research applications or indirectly via one of the multitude of medical image analysis and bioinformatics applications that is built using VTK: Osirix, 3D Slicer, BioImageXD, MedINRIA, SCIRun, ParaView, and others. Furthermore, VTK also provides 3D visualizations for clinical applications such as BrainLAB’s VectorVision surgical guidance system and Zimmer’s prosthesis design and evaluation platform. VTK has been downloaded many hundreds of thousands of times since its initial release in 1993. Considering its broad distribution and prevalent use, it can be argued that VTK has had a greater impact on medical research, and patient care, than any other open-source visualization package.  This proposal is in response to the multitude of requests we have been receiving from the VTK medical community. The aims are as follows:  1. Aim 1: Adaptive visualization framework: Produce an integrated framework that supports  visualization applications that balance server-side and client-side processing depending on data size,  analysis requirements, and the user platform (e.g., phone, tablet, or GPU-enabled desktop).  2. Aim 2: Integrated, interactive applications: Extend VTK to support a diversity of programming  paradigms ranging from C++ to JavaScript to Python and associated tools such as Jupyter Notebooks,  integrating with emerging technologies such as deep learning technologies.  3. Aim 3: Advanced rendering, including AR/VR: Target shader-based rendering systems and AR/VR  libraries that achieve high frame rates with minimal latency for ubiquitous applications that combine  low-cost, portable devices such as phones, ultrasound transducers, and other biometric sensors for  visually monitoring, guiding, and delivering advanced healthcare.  4. Aim 4: Infrastructure, Outreach, and Validation: Engage the VTK community and the proposed  External Advisory Board during the creation and assessment of the proposed work and corresponding  modern, digital documentation in the form of videos and interactive web-based content. Project Narrative The Visualization Toolkit (VTK) is an open source, freely available software library for the interactive display and processing of medical images. It is being used in most major medical imaging research applications, e.g., 3D Slicer and Osirix, and in several commercial medical applications, e.g., BrainLAB’s VectorVision surgical guidance system. VTK development began in 1993 and since then an extensive community of users and developers has grown around it. However, the rapid advancement of cloud computing, GPU hardware, deep learning algorithms, and VR/AR systems require corresponding advances in VTK so that the research and products that depend on VTK continue to deliver leading edge healthcare technologies. With the proposed updates, not only will existing applications continue to provide advanced healthcare, but new, innovative medical applications will also be inspired.",Accelerating Community-Driven Medical Innovation with VTK,9910382,R01EB014955,"['3-Dimensional', 'Adopted', 'Algorithmic Analysis', 'Algorithms', 'Bioinformatics', 'Biomechanics', 'Biomedical Technology', 'Biometry', 'Client', 'Cloud Computing', 'Cloud Service', 'Code', 'Communities', 'Computational Geometry', 'Computer software', 'Data', 'Data Analyses', 'Development', 'Devices', 'Documentation', 'Emerging Technologies', 'Ensure', 'Environment', 'Equilibrium', 'Evaluation', 'Explosion', 'Foundations', 'Funding', 'Grant', 'Health Technology', 'Healthcare', 'Hybrids', 'Image Analysis', 'Industry', 'Infrastructure', 'Internet', 'Language', 'Letters', 'Libraries', 'Licensing', 'Medical', 'Medical Imaging', 'Medical Research', 'Methods', 'Modernization', 'Monitor', 'Online Systems', 'Operative Surgical Procedures', 'Patient Care', 'Prevalence', 'Process', 'Prosthesis Design', 'Publications', 'Pythons', 'Research', 'Research Personnel', 'Resources', 'Side', 'Surveys', 'System', 'Tablets', 'Techniques', 'Technology', 'Telephone', 'TensorFlow', 'Testing', 'Time', 'Training', 'Ultrasonic Transducer', 'Update', 'Validation', 'Virtual and Augmented reality', 'Visual', 'Visualization', 'Work', 'base', 'clinical application', 'cloud based', 'computerized data processing', 'cost', 'deep learning', 'deep learning algorithm', 'design', 'digital', 'health care delivery', 'image processing', 'innovation', 'interest', 'learning strategy', 'meetings', 'new technology', 'open source', 'outreach', 'point of care', 'portability', 'processing speed', 'real world application', 'response', 'sensor', 'software development', 'statistics', 'success', 'supercomputer', 'synergism', 'three-dimensional visualization', 'tool', 'trend', 'web services']",NIBIB,"KITWARE, INC.",R01,2020,510157,0.0019318346445335102
"Biomechanical Analysis in Strabismus Surgery We propose 3 interrelated aims to define the biomechanics of the eye rotating (extraocular) muscles (EOMs) & optic nerve (ON) in health & visual disease, understand novel EOM actions, & characterize mechanical effects that may contribute to severe myopia. We aim to improve treatment of strabismus, misalignment of visual directions of the eyes; glaucoma & non-arteritic anterior ischemic optic neuropathy (NA-AION), both common blinding ON diseases; & high axial myopia, an ocular elongation & distortion that has become a worldwide epidemic & major cause of blindness. We propose a novel & critical nexus linking the EOMs, ON, & structure of the eye's scleral wall that we will explore using modern imaging & artificial intelligence techniques. Aim I will clarify the kinematic (motion) properties of the human eye, testing by multipositional magnetic resonance imaging (MRI) of the eyeball & EOMs the hypothesis that translational (linear) movement contributes importantly to ocular alignment. MRI will be performed during horizontal convergence & vertical eye rotation in normal people, & in patients who have common forms of strabismus including convergence insufficiency, eye crossing (esotropia), & outward ocular deviation (exotropia), both before & after corrective EOM surgery. Clarification of ocular translation is necessary to understand normal ocular motility and treat its disorders. Aim II will characterize the mechanical loading on the ON caused by eye movements. We will characterize the mechanical effects of ON tractional loading on the eyeball during horizontal & vertical eye rotations at 2 scales in living people, to test the hypothesis that such ON loading deforms it & adjacent retina & blood vessels as loading translates the eye. We propose that the resulting deformation during eye movements may create repetitive strain injury contributing to glaucoma, NA-AION, & axial myopia. In groups of subjects with the foregoing diseases, & in an equal group of matched healthy subjects, we will study mechanical effects of eye movement within the living eye by imaging its internal micro structure & blood vessels with optical coherence tomography, & outside the eyeball in the eye socket using MRI. Effects of tethering during eye movement will be studied ex vivo by precision 3D optical imaging of fresh human eye bank specimens subjected to mechanical tension on the ON that mimic effects of the eye movements imaged in the living subjects. Aim III will model the biomechanics of ocular kinematics. The constitutive mechanical properties of the non-muscular ocular & eye socket tissues will be described by finite element models (FEMs) using modern engineering methods for computational simulation to predict ocular kinematics, as well as local mechanical strains in the ON & sclera that may cause glaucoma, NA-AION, & the ocular deformities underlying extreme nearsightedness. We will determine if FEMs employing normal tissue properties can simulate normal ocular translation during horizontal & vertical rotations & convergence. By FEM simulation, we will also test the hypothesis that ocular loading by eye movement might contribute to: normal vergence, strabismus, & the effects of strabismus surgery. Relevance. Strabismus is a common clinical disorder that can cause double vision in adults and vision loss in children. Strabismus is often treated by surgical manipulation of the eye muscles, although current knowledge of their structure and function is incomplete. Proposed functional imaging and biomechanical studies of the properties of the eye muscles, eyeball, and optic nerve will improve understanding of the causes and treatment of strabismus, optic nerve diseases, and nearsightedness.",Biomechanical Analysis in Strabismus Surgery,9972266,R01EY008313,"['3-Dimensional', 'Accounting', 'Adult', 'Agreement', 'Algorithms', 'Anatomy', 'Anterior Ischemic Optic Neuropathy', 'Artificial Intelligence', 'Behavior', 'Biological Specimen Banks', 'Biomechanics', 'Blindness', 'Blood Vessels', 'Child', 'Choroid', 'Clinical', 'Complex', 'Computer Simulation', 'Computing Methodologies', 'Connective Tissue', 'Convergence Insufficiency', 'Cumulative Trauma Disorders', 'Deformity', 'Degenerative Myopia', 'Diplopia', 'Disease', 'Duct (organ) structure', 'Elements', 'Engineering', 'Epidemic', 'Equilibrium', 'Esotropia', 'Etiology', 'Exotropia', 'Eye', 'Eye Banks', 'Eye Movements', 'Failure', 'Functional Imaging', 'Gap Junctions', 'Glaucoma', 'Health', 'Human', 'Image', 'Individual', 'Knowledge', 'Lasers', 'Link', 'Magnetic Resonance Imaging', 'Matched Group', 'Measurement', 'Measures', 'Mechanics', 'Modeling', 'Modernization', 'Motion', 'Movement', 'Muscle', 'Muscle Contraction', 'Myopia', 'Normal tissue morphology', 'Ocular orbit', 'Operative Surgical Procedures', 'Ophthalmoscopy', 'Optic Disk', 'Optic Nerve', 'Optical Coherence Tomography', 'Optics', 'Patients', 'Physiologic Intraocular Pressure', 'Play', 'Primary Open Angle Glaucoma', 'Property', 'Retina', 'Role', 'Rotation', 'Scanning', 'Sclera', 'Strabismus', 'Stress', 'Structure', 'Techniques', 'Testing', 'Therapeutic', 'Tissues', 'Traction', 'Translating', 'Translations', 'Validation', 'Variant', 'Visual', 'anatomic imaging', 'biomechanical model', 'cell motility', 'crosslink', 'digital imaging', 'ex vivo imaging', 'human tissue', 'improved', 'in vivo imaging', 'in vivo optical imaging', 'kinematics', 'mechanical load', 'mechanical properties', 'model development', 'models and simulation', 'monocular', 'neglect', 'novel', 'ocular imaging', 'optic nerve disorder', 'optical imaging', 'orbit muscle', 'predictive modeling', 'quantitative imaging', 'retina blood vessel structure', 'simulation']",NEI,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,622245,-0.04103058857213173
"Human Face Representation in Deep Convolutional Neural Networks The human visual system can recognize a familiar face across wide variations of viewpoint, illumination, expression, and appearance. This remarkable computational feat is accomplished by large-­scale networks of neurons. We will test a face space theory of the representations that emerge at the top layer of deep learning convolutional neural networks (DCNNs) as a model of human visual representations of faces. Computer-based face recognition has improved in recent years due to DCNNs and the easy availability of labeled training data (faces and identities) from the web. Inspired by the primate visual system, DCNNs are feed­forward artificial neural networks that can map images of faces into representations that support recognition over widely variable images. Although the calculations executed by the simulated neurons are simple, enormous numbers of computations are used to convert an image into a representation. The end result of this processing is a highly compact representation of a face that retains image detail in an invariant, identity­-specific face code. This code is fundamentally different than any representation of faces considered in vision science. This theory we test combines key components of previous face space models (similarity, learning history) with new features (imaging conditions, personal face history) in a unitary space that represents both identity and facial appearance across variable images. We will test whether this model can account for human recognition of familiar faces, which is highly robust to image variability (pose, illumination, expression). The model will also be applied to understanding long standing difficulties humans (and machines) have with faces of other races. We aim to bridge critical gaps in our knowledge of how DCNNs work, linking psychological, neural, and computational perspectives. A fundamentally new theory of face representation will alter the questions we ask about face representations in all three fields. A new focus on understanding how we (or neural networks) “perceive” a single familiar identity in widely variable images will give rise to a search for representations that gracefully merge the properties of faces with the real-­world image conditions in which they are experienced. This project presents a unique opportunity to study, manipulate, and learn from these representations, and to apply the findings to broader questions about high-­level vision from neural and perceptual perspectives. Human recognition of familiar faces is highly robust to image variability (pose, illumination, expression)—a skill that is likely due to the quality and quantity of experience we have with the faces of people we know well. Deep convolutional neural networks are modeled after the primate visual system and have made impressive gains recently on the problem of robust face recognition. Understanding the visual nature of the face “feature” codes that emerge in these networks can give insight into long-standing questions about how the human visual system can, but does not always, represent a face in a way that generalizes across images that vary widely.",Human Face Representation in Deep Convolutional Neural Networks,9873959,R01EY029692,"['Affect', 'Appearance', 'Categories', 'Code', 'Computers', 'Data', 'Data Set', 'Face', 'Face Processing', 'Familiarity', 'Human', 'Image', 'Individual', 'Internet', 'Knowledge', 'Label', 'Learning', 'Lighting', 'Link', 'Maps', 'Methods', 'Modeling', 'Nature', 'Neural Network Simulation', 'Neurons', 'Performance', 'Persons', 'Primates', 'Property', 'Published Comment', 'Race', 'Recording of previous events', 'Space Models', 'Testing', 'Training', 'Variant', 'Vision', 'Visual', 'Visual system structure', 'Work', 'artificial neural network', 'base', 'convolutional neural network', 'deep learning', 'experience', 'feedforward neural network', 'human model', 'improved', 'insight', 'neural network', 'psychologic', 'relating to nervous system', 'representation theory', 'skills', 'theories', 'vision science']",NEI,UNIVERSITY OF TEXAS DALLAS,R01,2020,385203,-0.006876735032169978
"Natural image processing in the visual cortex Project Summary Signals from the natural environment are processed by neuronal populations in the cortex. Understanding the relationship between those signals and cortical activity is central to understanding normal cortical function and how it is impaired in psychiatric and neurodevelopmental disorders. Substantial progress has been made in elucidating cortical processing of simple, parametric stimuli, and computational technology is improving descriptions of neural responses to naturalistic stimuli. However, how cortical populations encode the complex, natural inputs received during every day perceptual experience is largely unknown. This project aims to elucidate how natural visual inputs are represented by neuronal populations in primary visual cortex (V1). Progress to date has been limited primarily by two factors. First, during natural vision, the inputs to V1 neurons are always embedded in a spatial and temporal context, but how V1 integrates this contextual information in natural visual inputs is poorly understood. Second, prior work focused almost exclusively on single-neuron firing rate, but to understand cortical representations one must consider the structure of population activity— the substantial trial-to-trial variability that is shared among neurons and evolves dynamically—as this structure influences population information and perception. The central hypothesis of this project is that cortical response structure is modulated by visual context to approximate an optimal representation of natural visual inputs. To test the hypothesis, this project combines machine learning to quantify the statistical properties of natural visual inputs, with a theory of how cortical populations should encode those images to achieve an optimal representation, to arrive at concrete, falsifiable predictions for V1 response structure. The predictions will be tested with measurements of population activity in V1 of awake monkeys viewing natural images and movies. Specific Aim 1 will determine whether modulation of V1 response structure by spatial context in static images is consistent with optimal encoding of those images, and will compare the predictive power of the proposed model to alternative models. Specific Aim 2 addresses V1 encoding of dynamic natural inputs, and will test whether modulation of V1 activity by temporal context is tuned to the temporal structure of natural sensory signals, as required for optimality. As both spatial and temporal are present simultaneously during natural vision, Specific Aim 3 will determine visual input statistics in free-viewing animals, and test space-time interactions in V1 activity evoked by those inputs. This project will provide the first test of a unified functional theory of contextual modulation in V1 encoding of natural visual inputs, and shed light on key aspects of natural vision that have been neglected to date. Project Narrative This project aims to determine how neurons in the visual cortex represent the inputs encountered during perceptual experience in the natural environment, through correct integration of visual information across space and time. In individuals with neurodevelopmental and psychiatric disorders, integration is often miscalibrated leading to perceptual impairments. Our study will advance knowledge of the relationship between natural sensory inputs and cortical activity, which is central to understanding normal cortical function and how it is impaired in patient populations.",Natural image processing in the visual cortex,10018026,R01EY030578,"['Address', 'Animal Testing', 'Area', 'Complex', 'Dependence', 'Development', 'Environment', 'Experimental Designs', 'Goals', 'Image', 'Impairment', 'Individual', 'Knowledge', 'Light', 'Location', 'Macaca', 'Machine Learning', 'Measurement', 'Measures', 'Mental disorders', 'Modeling', 'Monkeys', 'Motion', 'Neurodevelopmental Disorder', 'Neurons', 'Perception', 'Population', 'Process', 'Property', 'Publications', 'Recording of previous events', 'Sampling', 'Sensory', 'Signal Transduction', 'Stimulus', 'Structure', 'Technology', 'Testing', 'Time', 'V1 neuron', 'Vision', 'Visual', 'Visual Cortex', 'Work', 'area striata', 'awake', 'base', 'computer framework', 'experience', 'experimental study', 'image processing', 'improved', 'model development', 'movie', 'neglect', 'patient population', 'relating to nervous system', 'response', 'sensory input', 'spatiotemporal', 'statistics', 'theories', 'vision science', 'visual information']",NEI,ALBERT EINSTEIN COLLEGE OF MEDICINE,R01,2020,417500,0.0024316805501231166
"Towards a Compositional Generative Model of Human Vision Understanding object recognition has long been a central problem in vision science, because of its applied utility and computational difficulty. Progress has been slow, because of an inability to process complex natural images, where the largest challenges arise. Recently, advances in Deep Convolutional Neural Networks (DCNNs) spurred unprecedented success in natural image recognition. The general goal of this proposal is to leverage this success to test computational theories of human object recognition in natural images. However, DCNNs still markedly underperform humans when challenged with high levels of ambiguity, occlusion, and articulation. We hypothesize that humans' superior performance arises from the use of knowledge about how images and objects are structured. Preliminary evidence for this claim comes from the success of hybrid models, that combine DCNNS for identifying features and parts in images, with explicit knowledge of object and image structure. These computations occur within a hierarchy, which includes both top-down and bottom- up processing. The specific goal of the work proposed here is to strongly test whether these computational strategies, structured, hierarchical representations and bidirectional processing, are used to recognize objects in natural images. Human bodies are composed of hierarchically organized configurable parts, making them an ideal test domain. We examine the complete recognition process, from parts, to pairs of parts, to whole bodies, each in its own aim. Each aim also tests important sub-hypotheses about when and how the computational strategies are used. Aim 1 examines recognition of individual body parts, testing whether it is dependent on parsing images into more basic features and relationships, for example edges and materials. Aim 2 examines pairs of parts, testing the importance of knowledge of body connectedness relationships. Aim 3 examines perception of entire bodies, testing whether knowledge of global body structure guides bidirectional processing. In each aim, we first develop nested computer vision models that either do or do not make use of structural knowledge, to test whether it aids recognition. We then test whether human performance can be accounted for by the availability of that structural knowledge. We next measure neural activity with functional MRI to identify where and how it is used in cortex. Finally, we integrate these results to produce even stronger tests, using the nested models to predict human performance and confusion matrices as well as fMRI activity levels and confusion matrices. Altogether, this work will strongly test key theoretical accounts of object recognition in the most important domain, perception of natural images. The work, based on extensive preliminary data, measures and models the entire body recognition system. The models developed and tested here should surpass the state-of-the-art, and be useful for many real-world recognition tasks. The proposal will also lay the groundwork for future studies of recognition impaired by disease. This research uses computational, behavioral, and brain imaging methods to investigate how the visual system represents and processes information about human bodies. The studies will reveal how and when people can accurately recognize objects in natural images, how the brain supports this function, and how loss of information, similar to that that accompanies visual disease, may affect the ability to interpret everyday scenes.",Towards a Compositional Generative Model of Human Vision,10018020,R01EY029700,"['Affect', 'Area', 'Articulation', 'Behavioral', 'Body Image', 'Body part', 'Brain', 'Brain imaging', 'Complex', 'Computer Vision Systems', 'Confusion', 'Cues', 'Data', 'Development', 'Disease', 'Elbow', 'Feedback', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Human', 'Human body', 'Hybrids', 'Image', 'Impairment', 'Individual', 'Knowledge', 'Link', 'Measures', 'Modeling', 'Perception', 'Performance', 'Predictive Value', 'Process', 'Psychophysics', 'Published Comment', 'Research', 'Structure', 'System', 'Testing', 'Training', 'Vision', 'Visual', 'Visual system structure', 'Work', 'Wrist', 'base', 'convolutional neural network', 'crowdsourcing', 'human model', 'imaging modality', 'improved', 'object recognition', 'relating to nervous system', 'spatial relationship', 'success', 'theories', 'vision science']",NEI,UNIVERSITY OF MINNESOTA,R01,2020,332870,-0.018619043212244106
"Structural and functional tests of ganglion cell damage in glaucoma This project will use a combination of structural and functional measurements to test the hypothesis that early- stage damage in human glaucoma occurs first in the inner plexiform layer (IPL) of the retina – especially its OFF sub-lamina – as suggested by murine glaucoma models. In the first Aim, we will use a novel visible-light optical coherence tomograph (VIS OCT) to study structural changes in the retina of glaucoma patients. The newly developed VIS OCT has sufficient image contrast and resolution to segment the IPL boundaries and to define sub-lamination in volumetric OCT data, something not currently possible with existing near-infrared OCT instruments. We will make comparative measurements within the IPL and between the IPL, the ganglion cell layer (GCL) and the retinal nerve fiber layer (RNFL). Because data from mouse models of glaucoma suggests that early damage occurs preferentially within the OFF sub-lamina of the IPL, we will make separate VIS OCT measurements biased for the OFF- and ON-sublaminae of the IPL and use machine learning approaches to determine whether a similar damage process can be demonstrated in human. To test whether OFF-pathway function is preferentially lost in glaucoma, we will use a novel Steady-State Visual Evoked Potential (SSVEP) paradigm that employs sawtooth increments and decrements to bias the measurement to ON vs OFF pathways, respectively, a paradigm our data suggests discriminates glaucoma from control patients. The second Aim will optimize this SSVEP measurement for testing localized areas of the visual field. The third Aim will make comparative measurements of visual-field, VIS OCT and SSVEP loss patterns in a large sample of glaucoma patients and in age- and sex-matched controls. Thickness and interface reflectivity amplitude maps derived from VIS OCT imaging of the RNFL, GCL and IPL including sublaminae will be correlated topographically with visual field defects to assess the relative sensitivity of our structural biomarkers at and near visual field locations with demonstrable losses on conventional (Humphrey) perimetry. Similarly, SSVEP responses from different locations in the visual field will be correlated topographically with visual field loss patterns and to VIS OCT losses, with special emphasis on correlating structural damage in OFF vs ON sub-laminae of the IPL with the functional correlates derived from regional decremental and incremental SSVEPs. Separately and in combination, our structural and functional measurements are designed to provide strong tests of the biological hypothesis that the OFF pathway is preferentially damaged in human glaucoma, and to reveal new biomarkers for the disease. Improving visual outcomes in glaucoma will require a better understanding of the earliest sites and processes of damage and methods to measure them quickly and accurately in patients. This project will address both needs through a combination of novel Optical Coherence Tomography and electrophysiological measurements. The new imaging and electrophysiological tests that will be developed here, either separately or together, could eventually replace conventional visual field testing which is time-consuming and unreliable.",Structural and functional tests of ganglion cell damage in glaucoma,9913546,R01EY030361,"['Address', 'Affect', 'Age', 'Animal Model', 'Area', 'Atrophic', 'Biological Assay', 'Biological Markers', 'Biological Testing', 'Clinical', 'Complex', 'Consumption', 'Data', 'Disease', 'Early Diagnosis', 'Early treatment', 'Economic Burden', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Frequencies', 'Ganglion Cell Layer', 'Glaucoma', 'Goals', 'Gold', 'Human', 'Image', 'Inner Plexiform Layer', 'Location', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Modification', 'Mus', 'Noise', 'Optical Coherence Tomography', 'Optics', 'Outcome', 'Pathway interactions', 'Patients', 'Pattern', 'Perimetry', 'Process', 'Property', 'Resolution', 'Retina', 'Retinal Ganglion Cells', 'Rodent Model', 'Sampling', 'Scotoma', 'Severities', 'Signal Transduction', 'Site', 'Specificity', 'Speed', 'Structural defect', 'Structure', 'Synapses', 'Techniques', 'Testing', 'Thick', 'Time', 'Visible Radiation', 'Vision', 'Visual', 'Visual Fields', 'Visual evoked cortical potential', 'base', 'cell injury', 'comparative', 'contrast imaging', 'design', 'extrastriate visual cortex', 'field study', 'ganglion cell', 'improved', 'instrument', 'mouse model', 'novel', 'optic nerve disorder', 'response', 'retinal imaging', 'retinal nerve fiber layer', 'sex']",NEI,STANFORD UNIVERSITY,R01,2020,521102,0.0592899317691764
"Cortical computations underlying binocular motion integration PROJECT SUMMARY / ABSTRACT Neuroscience is highly specialized—even visual submodalities such as motion, depth, form and color processing are often studied in isolation. One disadvantage of this isolation is that results from each subfield are not brought together to constrain common underlying neural circuitry. Yet, to understand the cortical computations that support vision, it is important to unify our fragmentary models that capture isolated insights across visual submodalities so that all relevant experimental and theoretical efforts can benefit from the most powerful and robust models that can be achieved. This proposal aims to take the first concrete step in that direction by unifying models of direction selectivity, binocular disparity selectivity and 3D motion selectivity (also known as motion-in-depth) to reveal circuits and understand computations from V1 to area MT. Motion in 3D inherently bridges visual submodalities, necessitating the integration of motion and binocular processing, and we are motivated by two recent paradigm-breaking physiological studies that have shown that area MT has a robust representation of 3D motion. In Aim 1, we will create the first unified model and understanding of the relationship between pattern and 3D motion in MT. In Aim 2, we will construct the first unified model of motion and disparity processing in MT. In Aim 3, we will develop a large-scale biologically plausible model of these selectivities that represents realistic response distributions across an MT population. Having a population output that is complete enough to represent widely-used visual stimuli will amplify our ability to link to population read-out theories and to link to results from psychophysical studies of visual perception. Key elements of our approach are (1) an iterative loop between modeling and electrophysiological experiments; (2) building a set of shared models, stimuli, data and analysis tools in a cloud-based system that unifies efforts across labs, creating opportunities for deep collaboration between labs that specialize in relevant submodalities, and encouraging all interested scientists to contribute and benefit; (3) using model-driven experiments to answer open, inter-related questions that involve motion and binocular processing, including motion opponency, spatial integration, binocular integration and the timely problem of how 3D motion is represented in area MT; (4) unifying insights from filter-based models and conceptual, i.e., non-image- computable, models to generate the first large-scale spiking hierarchical circuits that predict and explain how correlated signals and noise are transformed across multiple cortical stages to carry out essential visual computations; and (5) carrying out novel simultaneous recordings across visual areas. This research also has potential long-term benefits in medicine and technology. It will build fundamental knowledge about functional cortical circuitry that someday may be useful for interpreting dysfunctions of the cortex or for helping biomedical engineers construct devices to interface to the brain. Insights gained from the visual cortex may also help to advance computer vision technology. NARRATIVE The processing of visual motion and depth information is essential for a wide variety of important human abilities, including navigating through the world, avoiding collisions, catching and grabbing objects and interpreting complex scenes. To understand how neurons in the visual cortex transform and represent the information that underlies these abilities, we aim to initiate the development of a more complete, biologically constrained and openly available computer model of motion and depth processing that will be used to guide, and to interpret and incorporate results from, primate visual neurophysiological and psychophysical experiments. Gaining an understanding of the normal function of cortical neural circuitry is an important step in building the fundamental knowledge that someday may help to improve the ability to assess dysfunctions of the cortex and may help bioengineers create devices that interface to cortical circuitry to treat disorders and overcome disabilities.",Cortical computations underlying binocular motion integration,9969438,R01EY027023,"['3-Dimensional', 'Affect', 'Architecture', 'Biological', 'Biomedical Engineering', 'Brain', 'Collaborations', 'Color', 'Complex', 'Computer Models', 'Computer Vision Systems', 'Cues', 'Data', 'Data Analyses', 'Development', 'Devices', 'Disadvantaged', 'Discrimination', 'Disease', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Foundations', 'Frequencies', 'Functional disorder', 'Human', 'Joints', 'Knowledge', 'Link', 'Literature', 'Medicine', 'Modeling', 'Motion', 'Neurons', 'Neurosciences', 'Noise', 'Output', 'Pathway interactions', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Primates', 'Production', 'Psychophysics', 'Reproducibility', 'Research', 'Role', 'Scientist', 'Signal Transduction', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Vision Disparity', 'Visual', 'Visual Cortex', 'Visual Motion', 'Visual Perception', 'area MT', 'base', 'cloud based', 'color processing', 'disability', 'experimental study', 'extrastriate visual cortex', 'fitness', 'improved', 'in vivo', 'insight', 'interest', 'neural circuit', 'neurophysiology', 'novel', 'predictive modeling', 'relating to nervous system', 'response', 'spatial integration', 'spatiotemporal', 'theories', 'tool', 'visual neuroscience', 'visual process', 'visual processing', 'visual stimulus']",NEI,UNIVERSITY OF WASHINGTON,R01,2020,399500,0.013035873771238502
"Device to control circadian-effective light in Alzheimer's disease environments Project Summary This proposed project will develop and field-test a device that accurately monitors and controls the circadian stimulus (CS) for Alzheimer disease (AD) and Alzheimer-disease-related dementia (ADRD) patients in nursing homes. Human biology has evolved to have two distinct optical systems: the visual system, by which we see and process images, and the circadian system, which regulates our biological clock and associated biological systems. These two systems have significantly different spectral and temporal responses to optical input. Specifically, circadian stimulation peaks at 460 nm and responds after several minutes of optical activation, while the visual system peaks at 555 nm and responds nearly instantaneously to inputs. All lighting systems are designed and installed in buildings with consideration only given to the photopic (visual) system and all light meters used to characterize lighting buildings are calibrated to measure photopic light, not CS. While a broad and growing body of research has documented the impacts of the circadian system on human health, including regulating sleep and improving cognition in AD/ADRD patients, research on the CS experienced by AD/ADRD patients is extremely limited. Researchers at the Lighting Research Center at Rensselaer Polytechnic Institute developed the Daysimeter, a calibrated light meter that measures circadian light and circadian stimulus. In Phase I of this project, researchers modified an existing workstation-based lighting control system they previously developed for the visual system to include Daysimeter technology, allowing this control system to record CS measurements. The accuracy of these CS measurements was confirmed in the laboratory and field-testing of 20 of devices is currently ongoing in AD/ADRD nursing homes. In this Phase II application, researchers propose adding control features to this device so that lighting can be controlled to optimize CS dosages in AD/ADRD patient environments. Machine learning-based lighting control algorithms will be driven by continuous light level and spectrum measurements as well as periodic (e.g., daily) patient health data. Data from these devices would be wirelessly transmitted to researchers via an Internet gateway and associated cloud-based data management systems. These data would be of immediate value for gaining a better understanding of AD/ADRD patients' CS exposure and could ultimately result in new lighting systems and/or building codes that consider both our visual and circadian systems. Following the development phase, 30 CS-enabled lighting control systems will be field tested over a 22-week test period. Researchers aim to commercialize this CS-enabled lighting control system shortly after the completion of this field test and the Phase II project specifically targeting AD/ADRD nursing home applications. Project Narrative A growing body of research has demonstrated how light impacts human circadian systems and how these impacts can affect sleep, alertness, cognition and agitation in people with Alzheimer's disease (AD) and Alzheimer's-disease-related dementia (ADRD). Still, significant knowledge gaps exist in determining how much circadian stimulation is typically provided to AD/ADRD patients and there are no commercial products designed to control lighting in AD/ADRD environments in ways that promote circadian-related health. This project aims to fill in these gaps by developing and testing a device specifically designed to measure and control the circadian stimulation experienced by AD/ADRD patients in nursing homes.",Device to control circadian-effective light in Alzheimer's disease environments,10018621,R44AG060857,"['Affect', 'Agitation', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Alzheimer&apos', 's disease related dementia', 'Back', 'Behavior', 'Biological Clocks', 'Building Codes', 'Characteristics', 'Clinical Trials', 'Cognition', 'Data', 'Database Management Systems', 'Development', 'Device or Instrument Development', 'Devices', 'Dose', 'Effectiveness', 'Elderly', 'Environment', 'Feeds', 'Health', 'Hour', 'Human', 'Human Biology', 'Image', 'Institutes', 'Internet', 'Intervention', 'Knowledge', 'Laboratories', 'Light', 'Lighting', 'Machine Learning', 'Measurement', 'Measures', 'Monitor', 'Moods', 'Nursing Homes', 'Optics', 'Patients', 'Pattern', 'Performance', 'Periodicity', 'Phase', 'Phototherapy', 'Planet Earth', 'Population', 'Process', 'Reporting', 'Research', 'Research Personnel', 'Retina', 'Rotation', 'Running', 'Sleep', 'Stimulus', 'System', 'Technology', 'Testing', 'Time', 'Vision', 'Visual', 'Visual system structure', 'Wakefulness', 'Wireless Technology', 'Work', 'active control', 'alertness', 'appropriate dose', 'awake', 'base', 'biological systems', 'circadian', 'circadian pacemaker', 'cloud based', 'commercialization', 'design', 'dosage', 'effectiveness testing', 'experience', 'falls', 'field study', 'health data', 'improved', 'interest', 'meter', 'next generation', 'novel', 'prototype', 'residence', 'response', 'success', 'therapy design']",NIA,"ERIK PAGE AND ASSOCIATES, INC.",R44,2020,1232387,-0.015580797755010917
"Multi-modal Health Information Technology Innovations for Precision Management of Glaucoma PROJECT SUMMARY/ABSTRACT Glaucoma is the world's leading cause of irreversible blindness and will affect >110 million people by 2040. Early detection and treatment are critical, as symptoms typically do not present until the disease is advanced. A data-driven precision medicine approach is needed to better identify individuals who are at greatest risk of developing the disease and who are at greatest risk of progressing quickly to vision loss. While there has been considerable progress in eye imaging and testing to improve glaucoma monitoring, precision management of glaucoma is incomplete without accounting for patients' co-existing systemic conditions, concurrent systemic medications and treatments, and adherence with prescribed glaucoma treatment. Understanding how systemic conditions, and specifically vascular conditions such as hypertension, impact glaucoma presents growing public health importance given the increasing co-morbidities facing aging populations. Preliminary studies have demonstrated the predictive value of systemic data, even without ophthalmic endpoints. Similarly, measuring medication adherence is important for guiding patient counseling and engagement and avoiding downstream interventions such as surgeries, which carry high cost and morbidity. These factors are important for providing a more comprehensive perspective of glaucoma management and for improving patient outcomes, yet they are relatively understudied. I propose applying multi-modal advancements in health information technology (IT) to address these gaps and achieve the following specific aims: (1) Develop machine learning-based predictive models classifying patients at risk for glaucoma progression using systemic electronic health record (EHR) data from a diverse nationwide patient cohort; (2) evaluate how integrating blood pressure (BP) data from novel smartwatch-based home BP monitors enhance predictive models for risk stratification in glaucoma, and (3) measure glaucoma medication adherence using innovative flexible electronic sensors to validate their use for future interventions aimed at improving adherence and clinical outcomes in glaucoma. These studies would leverage state- of-the-art methods in big-data predictive modeling as well as cutting-edge advancements in sensor technologies. This multi-faceted approach will build a foundation for a health IT framework geared toward improving risk stratification and generating novel therapeutic targets for glaucoma patients. PROJECT NARRATIVE A precision medicine approach is critical for early detection and treatment of glaucoma, an insidious chronic eye disease that can lead to blindness and severely decreased quality of life. The relationship between systemic conditions and treatments with glaucoma progression, as well as methods for monitoring and promoting glaucoma medication adherence, represent areas of glaucoma management that are not well-understood and are thus critical opportunities for technology-driven interventions. This study proposes the development and application of multi- modal health information technology innovations – such as machine learning-based predictive modeling, massive electronic datasets, wearable devices, and flexible sensor electronics – to enhance understanding of glaucoma pathophysiology and identify new potential therapeutic strategies.",Multi-modal Health Information Technology Innovations for Precision Management of Glaucoma,10018290,DP5OD029610,"['Accounting', 'Address', 'Adherence', 'Affect', 'African American', 'Aging', 'All of Us Research Program', 'Area', 'Award', 'Big Data', 'Blindness', 'Blood Pressure', 'Blood Vessels', 'Chronic', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Counseling', 'Data', 'Data Science', 'Data Set', 'Department chair', 'Development', 'Devices', 'Disease', 'Disease Management', 'Disease Progression', 'Early Diagnosis', 'Early treatment', 'Electronic Health Record', 'Electronics', 'Ensure', 'Exhibits', 'Eye', 'Eye diseases', 'Eyedrops', 'Fellowship', 'Foundations', 'Functional disorder', 'Future', 'Glaucoma', 'Home Blood Pressure Monitoring', 'Home environment', 'Human Resources', 'Hypertension', 'Image', 'Individual', 'Informatics', 'Institutes', 'Institution', 'Intervention', 'Investigation', 'Latino', 'Lead', 'Leadership', 'Machine Learning', 'Measurement', 'Measures', 'Mentors', 'Methods', 'Modeling', 'Monitor', 'Morbidity - disease rate', 'Nerve Degeneration', 'Operative Surgical Procedures', 'Ophthalmologist', 'Ophthalmology', 'Optic Nerve', 'Outcome', 'Participant', 'Patient Care', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Pharmaceutical Preparations', 'Physical activity', 'Pilot Projects', 'Population', 'Predictive Analytics', 'Predictive Value', 'Public Health', 'Public Health Informatics', 'Quality of life', 'Research', 'Resources', 'Risk', 'Risk stratification', 'Role', 'Sleep', 'Symptoms', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Track and Field', 'Training', 'United States National Institutes of Health', 'Vision', 'Visual Fields', 'Work', 'base', 'biomedical informatics', 'blood pressure regulation', 'circadian regulation', 'clinical phenotype', 'clinical practice', 'cohort', 'comorbidity', 'cost', 'data integration', 'early onset', 'electronic data', 'experience', 'faculty community', 'flexibility', 'health information technology', 'improved', 'innovation', 'medication compliance', 'multidisciplinary', 'multimodality', 'new therapeutic target', 'novel', 'novel therapeutic intervention', 'patient engagement', 'personalized management', 'precision medicine', 'predictive modeling', 'professor', 'programs', 'racial minority', 'sensor', 'sensor technology', 'smart watch', 'treatment adherence', 'wearable device']",OD,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",DP5,2020,393957,0.03150231040380178
"The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets Project Summary/Abstract The ultimate goal of the HIVE MC-IU effort is to develop a common coordinate framework (CCF) for the healthy human body that supports the cataloguing, exploration, and download of different types of tissue and individual cell data. The CCF will use different visual interfaces in order to exploit human and machine intelligence to improve data exploration and communication. The proposed effort combines decades of expertise in data and network visualization, scientific visualization, biology, and biomedical data standards. The goal is to develop a highly accurate and extensible multidimensional spatial basemap of the human body with associated data overlays. This basemap will be designed for online exploration as an atlas of tissue maps composed of diverse cell types, developed in close collaboration with the HIVE MC-NYGC team. To implement this functionality, we will develop methods to map and connect metadata, pixel/voxel data, and extracted vector data, allowing users to “navigate” across multiple levels (whole body, organ, tissue, cells). MC-IU will work in close collaboration with the HIVE Infrastructure and Engagement Component (IEC) and tools components (TCs) to connect and integrate further computational, analytical, visualization, and biometric resources driven by spatial context. Project Narrative This project will create a high-resolution, functional mapping of voxel, vector, and meta datasets in support of integration, interoperability, and visualization of biomedical HuBMAP data and models. We will create an extensible common coordinate framework (CCF) to facilitate the integration of diverse image-based data at spatial scales ranging from the molecular to the anatomical. This project will work in close coordination with the HuBMAP consortium to help drive an ecosystem of useful resources for understanding and leveraging high-resolution human image data and to compile a human body atlas.","The Human Body Atlas: High-Resolution, Functional Mapping of Voxel, Vector and Meta Datasets",10148333,OT2OD026671,"['Anatomy', 'Artificial Intelligence', 'Atlases', 'Biology', 'Biometry', 'Cataloging', 'Catalogs', 'Cells', 'Collaborations', 'Communication', 'Data', 'Data Set', 'Ecosystem', 'Goals', 'Human', 'Human BioMolecular Atlas Program', 'Human body', 'Image', 'Individual', 'Infrastructure', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Organ', 'Resolution', 'Resources', 'Tissues', 'Visual', 'Visualization', 'Work', 'base', 'cell type', 'data exploration', 'data standards', 'design', 'human imaging', 'improved', 'interoperability', 'tool', 'vector']",OD,INDIANA UNIVERSITY BLOOMINGTON,OT2,2020,1000000,-0.008976123133144524
"Disposable, medication-integrated tactile and motion sensor for glaucoma therapy compliance improvement ABSTRACT Glaucoma is the leading cause of irreversible blindness in the world. The number of people with primary open angle glaucoma is expected to exceed 100 million by 2040. The first-line therapy for glaucoma treatment is patient-administered hypotensive medication delivered topically via an eye drop. However, studies have consistently shown that patients rarely adhere to dosing recommendations. According to the Wilmer Glaucoma Center of Excellence, nearly 50% of individuals discontinue eye drops within six months of their first prescription. Of those that persist beyond six months, only 37% continue the therapy after three years, and only 10% of those prescribed glaucoma drops are persistent (without gaps) over the first year. Failure to adhere to dosing recommendations dramatically limits the effectiveness of eye drops in lowering intraocular pressure and delaying or preventing the progression of glaucoma. This well-known problem has prompted the development of several technological solutions for tracking adherence. Retinal Care's proposed solution is fundamentally different. The proposed device is a simple and inexpensive combination of sensors designed to provide feedback to patients, providers, payers, and care coordinators regarding the way a patient interacts with their medication. Tactile and motion data is acquired through a custom force-sensitive resistor (FSR) and a standard three-axis accelerometer. Combined, these two data sources provide significant insight into drop adherence, application mechanics, and patient behavior. More importantly, this data can inform decisions on how and when to intervene with a patient to maximize the likelihood of continued long-term drop adherence and vision preservation. The primary innovations are in the single-use design (enabled by low-cost custom FSR fabrication and the unique combination of low-cost electrical components), the novel mining of motion and tactile information for patient adherence and behavioral data, and the integration with an existing care coordination framework. Retinal Care approaches health care more holistically than traditional companies; recognizing the behavioral and psychosocial factors that are a root cause of blindness from eye diseases like glaucoma. While Retinal Care employs cutting-edge technology such as deep learning and advanced medical devices, we work actively toward implementing technology in a way that translates directly to improved quality of life for our patients by managing the entire care path. The technology proposed here, while novel in design, is not intended primarily as a tracking tool, though it will accomplish that sub-goal with greater effectiveness and at less cost than any existing product. Instead, it is designed explicitly to reduce blindness by providing the crucial data needed to tailor our interaction and intervention strategies to the individual patient; to maximize patient engagement and adherence to glaucoma care through a combination of education, incentivization, tailored communications, and personal interaction with our trained care coordinators. The goal of this project is to demonstrate feasibility (cost and functionality) of the simple device that will enable Retinal Care to effectively reduce blindness and visual impairment from glaucoma. PROJECT NARRATIVE Glaucoma is the leading cause of irreversible blindness globally and in the US, where it affects more than 3 million Americans and costs the US economy $2.86 billion every year in direct costs and productivity losses. Topical medication administered through eye drops is a common and effective first-line therapy for reducing intraocular pressure and controlling glaucoma, but patient adherence to dosing recommendations is a consistent and pervasive issue that contributes significantly to vision loss. Retinal Care Inc. intends to develop and deploy a suite of simple, disposable, bottle-integrated sensors that will continuously collect real-time adherence and behavioral data to inform the design of patient-specific intervention and incentivization strategies to improve adherence and reduce blindness from glaucoma.","Disposable, medication-integrated tactile and motion sensor for glaucoma therapy compliance improvement",10010396,R41EY031632,"['Accelerometer', 'Adherence', 'Adoption', 'Affect', 'Age related macular degeneration', 'American', 'Antihypertensive Agents', 'Behavior', 'Behavioral', 'Blindness', 'Businesses', 'Caring', 'Cells', 'Cellular Phone', 'Clinical', 'Clinical Trials', 'Coin', 'Communication', 'Consumption', 'Coupled', 'Custom', 'Data', 'Data Sources', 'Development', 'Device or Instrument Development', 'Devices', 'Diabetic Retinopathy', 'Direct Costs', 'Dose', 'Drops', 'Education', 'Educational Intervention', 'Effectiveness', 'Electrical Engineering', 'Electronics', 'Engineering', 'Eye', 'Eye diseases', 'Eyedrops', 'Failure', 'Feedback', 'Film', 'Generations', 'Glaucoma', 'Goals', 'Hand', 'Health', 'Healthcare', 'Human', 'Incentives', 'Individual', 'Intervention', 'Kinetics', 'Label', 'Liquid substance', 'Measurement', 'Mechanics', 'Medical', 'Medical Device', 'Memory', 'Methods', 'Mining', 'Mobile Health Application', 'Modeling', 'Motion', 'Nylons', 'Ophthalmology', 'Patients', 'Performance', 'Peripheral', 'Pharmaceutical Preparations', 'Physiologic Intraocular Pressure', 'Plant Roots', 'Polyurethanes', 'Primary Open Angle Glaucoma', 'Provider', 'Psychosocial Factor', 'Quality of life', 'Radio', 'Recommendation', 'Regimen', 'Resistance', 'Retina', 'Tactile', 'Technology', 'Testing', 'Textiles', 'Thinness', 'Time', 'Topical application', 'Touch sensation', 'Training', 'Translating', 'United States', 'Vision', 'Visual Fields', 'Visual impairment', 'Work', 'advanced analytics', 'analog', 'base', 'care coordination', 'compliance behavior', 'cost', 'data integration', 'deep learning', 'design', 'improved', 'improved outcome', 'incentive strategies', 'individual patient', 'innovation', 'insight', 'interoperability', 'iterative design', 'kinematics', 'motion sensor', 'novel', 'patient engagement', 'performance tests', 'predictive modeling', 'preference', 'preservation', 'prevent', 'productivity loss', 'prototype', 'recruit', 'screening', 'sensor', 'tool', 'treatment adherence', 'usability', 'wearable device']",NEI,"RETINAL CARE, INC.",R41,2020,219896,0.031553881097026566
"Genetic Modulators of Glaucoma Glaucoma is the leading cause of irreversible blindness in the world. While elevated intraocular pressure (IOP) is a major risk factor, damage and death of retinal ganglion cells (RGCs) underlies visual field loss. However, a thorough understanding of this disease is a major challenge because its genetic basis is heterogeneous and it represents a family of age-related disorders resulting from intersecting gene-regulated pathophysiologic networks. We propose to continue to use the BXD (C57BL/6 x DBA/2J) family of recombinant inbred (RI) lines of mice as a genetic reference panel (GRP) and to combine our work with human genome wide association studies (GWAS), to uncover and clarify the genetic heterogeneity that underlies optic nerve (ON) damage. We have had recent success using this combined approach in the regulation of intraocular pressure (IOP). We are very well positioned to take the next step and apply this approach to define cellular targets of RGC damage and death. We propose to uncover phenotypic diversities of glaucoma-related ON damage and uncover common underlying mechanisms that are shared with IOP modulation. Our long-term research goal is to identify disease mechanisms and develop neuroprotective therapies to preserve retinal health in patients at risk for glaucoma. Our overall objective is to identify novel gene products and related mechanisms that lead to glaucomatous endophenotypes using multi-dimensional genetic analyses, cross-species comparisons (mouse, rat and human) and validation using novel murine glaucoma models. Our central hypothesis is that molecular processes leading to glaucoma associated-endophenotypes, such as elevated IOP and ON damage, are shared across species, and that species comparisons can uncover common underlying mechanisms, and efficient testing of targeted glaucoma therapeutics. In the current investigation, we perform a systematic analysis of ON damage, and an additional species—rat. We will mine the extensive databases of IOP and ON damage that we are generating for more than 70 BXD strains across five age cohorts with the goal of defining new models of glaucoma. An overall strength of this proposal is the combination of cutting-edge systems genetics methods, species comparisons of glaucoma phenotypes, and a strong interdisciplinary team that includes investigators with extensive experience in systems genetics, glaucoma, GWAS in human and rats, and advanced computational methods. To test our hypothesis, we will perform the following thress studies: 1) Identify the candidate gene on chromosome 12 that modulates ON damage; 2) Determine if modulation of IOP and/or ON damage is shared across rodent species; and 3) Identify novel spontaneous glaucoma models through a comprehensive analysis of our enlarged BXD GRP of 100 or more BXD strains. The outcomes of these studies will define novel genes and molecular networks that underlie glaucoma-associated phenotypes and also provide unique glaucoma models for future analysis. These results are expected to fundamentally advance the field of glaucoma disease mechanisms and enable targeted therapeutic development. Glaucoma is the leading cause of irreversible blindness in the world and a thorough understanding of this disease is a major challenge because its genetic basis is heterogeneous, and it likely represents a family of disorders resulting from intersecting gene-regulated pathophysiologic pathways. Our goals are to: identify candidate gene(s) that modulate optic nerve damage; determine if regulation of intraocular pressure and/or optic nerve damage are shared across species; and identify novel spontaneous glaucoma models. These outcomes will fundamentally advance the field of glaucoma disease mechanisms and enable targeted therapeutic development.",Genetic Modulators of Glaucoma,9857597,R01EY021200,"['Age', 'Axon', 'Blindness', 'Candidate Disease Gene', 'Cell Death', 'Cellular Assay', 'Cessation of life', 'Chromosome 12', 'Clinical', 'Computing Methodologies', 'Data', 'Databases', 'Disease', 'Family', 'Future', 'Generations', 'Genes', 'Genetic', 'Genetic Heterogeneity', 'Genomics', 'Glaucoma', 'Goals', 'Health', 'Human', 'Human Genome', 'Inbred Strains Rats', 'Inbreeding', 'Investigation', 'Laboratories', 'Lead', 'Methods', 'Modeling', 'Molecular', 'Mus', 'Ocular Hypertension', 'Optic Nerve', 'Outcome', 'Outcome Study', 'Pathway interactions', 'Patients', 'Phenotype', 'Physiologic Intraocular Pressure', 'Population', 'Positioning Attribute', 'Process', 'Publications', 'Quantitative Trait Loci', 'Rattus', 'Recombinants', 'Regulation', 'Research', 'Research Personnel', 'Retina', 'Retinal Ganglion Cells', 'Risk', 'Risk Factors', 'Rodent', 'System', 'Testing', 'Therapeutic', 'United States National Institutes of Health', 'Validation', 'Visual Fields', 'Work', 'age related', 'cell injury', 'cellular targeting', 'clinical subtypes', 'cohort', 'deep neural network', 'density', 'endophenotype', 'experience', 'gene product', 'genetic analysis', 'genome wide association study', 'human data', 'human model', 'lead candidate', 'novel', 'novel therapeutics', 'preservation', 'success', 'targeted treatment', 'therapeutic development', 'treatment strategy']",NEI,UNIVERSITY OF TENNESSEE HEALTH SCI CTR,R01,2020,378431,0.015056607471508601
"Gaze-contingent computer screen magnification control for people with low vision ! Project Summary This application describes proposed research with the goal of facilitating use of a computer screen magnifier by people with low vision. Screen magnification is a well-established, popular technology for access of onscreen content. Its main shortcoming is that it requires the user to continuously control, with the mouse or trackpad, the location of the focus of magnification, in order to ensure that the magnified content of interest is within the screen viewport. This tedious process may be time-consuming and ineffective. For example, the simple task of reading the news on a web site requires continuous horizontal scrolling, which affects the experience of using this otherwise very beneficial technology, and may discourage its use, especially by those with poor manual coordination.  We propose to develop a software system that enables hands-free control of a screen magnifier. This system will rely on the user’s eye gaze (measured by a regular IR-based tracker, or from analysis of the images in a camera embedded in the screen) to update the location of the focus of magnification as desired. This research is inspired by preliminary work, which showed promising results with two simple gaze-based control algorithms, tested on three individuals with low vision.  This project will be a collaboration between the Department of Computer Science and Engineering at UC Santa Cruz (PI: Manduchi, Co-I: Prado) and the School of Optometry at UC Berkeley (PI: Chung). Dr. Legge from the Department of Psychology at U. Minnesota will participate as a consultant. Two human subjects studies are planned. In Study 1 with 80 low vision subjects from four different categories of visual impairment, we will investigate the failure rate of a commercial gaze tracker (Aim 1), and will record mouse tracks, gaze tracks, and images from the subjects while performing a number of tasks using two modalities of screen magnification (Aim 2). In Study 2, with the same number of subjects, we will repeat the Study 1 experiment, but using a gaze-based controller trained from the data collected in Study 1, and individually tunable for best performance (Aim 3). In addition, we will experiment with an appearance-based gaze tracker that uses images from the screen camera, thereby removing the need for specialized gaze tracking hardware, as well as with a computer tablet form factor (Aim 4). We expect that reading speed and error rate using our gaze-based controller will be no worse than using mouse-based control. If successful, this study will show that the convenience of hands-free control offered by the proposed system comes at no additional cost in terms of individual performance at the considered tasks. ! ! Project Narrative People with low vision often use screen magnification software to read on a computer screen. Since a magnifier expands the screen content beyond the physical size of the screen (the “viewport”), it is necessary to move the content using the mouse so that the portion of interest falls within the viewport. This project will facilitate use of a screen magnifier by means of a new software system that relies on the user’s own gaze to control scrolling when reading with magnification. !",Gaze-contingent computer screen magnification control for people with low vision,10053172,R01EY030952,"['Affect', 'Age', 'Algorithms', 'Appearance', 'Apple', 'Behavior Control', 'Benchmarking', 'Blindness', 'Categories', 'Collaborations', 'Communication', 'Complex', 'Computer Vision Systems', 'Computer software', 'Computers', 'Consumption', 'Correlation Studies', 'Data', 'Data Set', 'Desktop Video', 'Engineering', 'Ensure', 'Eye', 'Face', 'Failure', 'Funding', 'Glass', 'Goals', 'Hand', 'Image', 'Individual', 'Learning', 'Location', 'Magic', 'Manuals', 'Measures', 'Minnesota', 'Modality', 'Mus', 'Operating System', 'Optometry', 'Performance', 'Peripheral', 'Process', 'Psychological reinforcement', 'Psychology', 'Reader', 'Reading', 'Research', 'Resort', 'Role', 'Schools', 'Science', 'Speech', 'Speed', 'Structure', 'Study Subject', 'System', 'Tablet Computer', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Translating', 'Update', 'Vision', 'Visual', 'Visual impairment', 'Work', 'algorithm development', 'algorithm training', 'base', 'computer science', 'control trial', 'cost', 'data acquisition', 'design', 'experience', 'experimental study', 'falls', 'gaze', 'human subject', 'interest', 'motor control', 'news', 'recurrent neural network', 'sample fixation', 'software systems', 'tool', 'web page', 'web site']",NEI,UNIVERSITY OF CALIFORNIA SANTA CRUZ,R01,2020,350753,0.02025651721507522
"Environmental Localization Mapping and Guidance for Visual Prosthesis Users Project Summary About 1.3 million Americans aged 40 and older are legally blind, a majority because of diseases with onset later in life, such as glaucoma and age-related macular degeneration. Second Sight has developed the world's first FDA approved retinal implant, Argus II, intended to restore some functional vision for people suffering from retinitis pigmentosa (RP). In this era of smart devices, generic navigation technology, such as GPS mapping apps for smartphones, can provide directions to help guide a blind user from point A to point B. However, these navigational aids do little to enable blind users to form an egocentric understanding of the surroundings, are not suited to navigation indoors, and do nothing to assist in avoiding obstacles to mobility. The Argus II, on the other hand, provides blind users with a limited visual representation of their surroundings that improves users' ability to orient themselves and traverse obstacles, yet lacks features for high-level navigation and semantic interpretation of the surroundings. The proposed research aims to address these limitations of the Argus II through a synergy of state-of-the-art stimultaneous localization and mapping (SLAM) and object recognition technologies. For the past three years, JHU/APL has collaborated with Second Sight to develop similar advanced vision-based capabilities for the Argus II, including capabilities for object recognition and obstacle detection by stereo vision. This proposal is driven by the hypothesis that navigation for users of retinal prosthetics can be greatly improved by incorporating SLAM and object recognition technology conveying environmental information via a retinal prosthesis and auditory feedback. SLAM enables the visual prosthesis system to construct a map of the user's environment and locate the user within that map. The system then provides object location and navigational cues via appropriate sensory modalities enabling the user to mentally form an egocentric map of the environment. We propose to develop and test a visual prosthesis system which 1) constructs a map of unfamiliar environments and localizes the user using SLAM technology 2) automatically identifies navigationally-relevant objects and landmarks using object recognition and 3) provides sensory feedback for navigation, obstacle avoidance, and object/landmark identification. Project Narrative The proposed system, when realized, will use advanced simultaneous localization and mapping, and object recognition techniques, to enable visual prosthesis users with unprecedented abilities to autonomously navigate and identify objects/landmarks in unfamiliar environments.",Environmental Localization Mapping and Guidance for Visual Prosthesis Users,10019559,R01EY029741,"['3-Dimensional', 'Address', 'Age related macular degeneration', 'Algorithms', 'American', 'Competence', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Dependence', 'Detection', 'Development', 'Devices', 'Disease', 'Effectiveness', 'Environment', 'Evaluation', 'FDA approved', 'Feedback', 'Glaucoma', 'Goals', 'Image', 'Implant', 'Late-Onset Disorder', 'Lead', 'Learning', 'Life', 'Location', 'Maps', 'Medical Device', 'Modality', 'Motion', 'Ocular Prosthesis', 'Patients', 'Performance', 'Psyche structure', 'Research', 'Retinitis Pigmentosa', 'Running', 'Semantics', 'Sensory', 'Societies', 'System', 'Systems Integration', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Update', 'Vision', 'Visual', 'Volition', 'aged', 'auditory feedback', 'base', 'behavior test', 'blind', 'cognitive load', 'falls', 'human subject', 'improved', 'innovation', 'legally blind', 'navigation aid', 'object recognition', 'portability', 'prosthesis wearer', 'prototype', 'research and development', 'retina implantation', 'retinal prosthesis', 'sensory feedback', 'smartphone Application', 'synergism', 'visual feedback', 'visual information']",NEI,JOHNS HOPKINS UNIVERSITY,R01,2020,662134,0.04829393478010805
"Vision in Natural Tasks Summary/Abstract  In the context of natural behavior, humans make continuous sequences of sensory-motor decisions to satisfy current behavioral goals, and vision must provide the information needed to achieve those goals. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks in natural locomotion, or the requisite information, and the proposal attempts to specify these.  in the context of natural gait, the patterns of optic flow are unexpectedly complex, raising questions about its role. The patterns of motion on the retina during locomotion depend critically on both eye and body motion, and these in turn depend on behavioral goals. Our first Aim is therefore to comprehensively describe the statistics of retinal motion patterns in a variety of terrains and task contexts. We will measure binocular eye and body movements while walking in outdoor terrains of varying roughness, crossing a busy intersection, and making coffee. These contexts will induce different gaze patterns. We will provide a comprehensive description of the motion stimulus in natural locomotion and help separate out self-motion signals from externally generated motion. These data will allow a more precise specification of the response patterns in cortical motion sensitive areas. Because of the complexity of natural motion patterns, we will re-examine the influence of optic flow on walking direction in a virtual reality environment and test alternative explanations for the role of flow.  A central task in walking is foot placement, and we will focus on identifying the image properties that make a good foothold. Stereo, structure from motion, and spatial image structure are all likely contenders. We directly investigate the role of stereo in foothold selection by examining gait patterns in stereo-deficient subjects in terrains with varying degrees of roughness. Using a different strategy, we will attempt to predict gaze locations and footholds in rough terrain using convolution neural nets (CNN’s) to identify potential search templates for footholds in rough terrain. We will describe fixation patterns from crosswalk and sidewalk navigation and attempt to make inferences about their purpose, and use Modular Inverse Reinforcement Learning (MIRL) to predict direction decisions and decompose the behavior into sub-tasks.  The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists The work will be strengthened by the investigation of stereo- deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available. Project Narrative  The central goal of this work is to understand vision in its natural context. This is very important information in order to devise suitable vision aids and rehabilitation strategies for individuals with visual impairments, and it is becoming increasingly accessible because of developments in technology for monitoring eye and body movements. The proposed work examines gaze and walking decisions in locomotion in outdoor environments, taking advantage of our novel system for measuring combined eye and body movements in these contexts. Currently we have only limited understanding of the constituent tasks and requisite information in natural locomotion, and the proposal attempts to specify these. The collection of integrated gaze, body kinematics, and scene images in a range of natural environments is innovative, as little comparable data exists. The work will be strengthened by the investigation of stereo-deficient subjects for whom there is almost no integrated eye and body data. Since much of the work in robotics has no visual input at all this should help in development of visual guidance for robots and also help better define the necessary information for individuals with impaired vision. The data set will be made publicly available.",Vision in Natural Tasks,10004035,R01EY005729,"['Affect', 'Area', 'Behavior', 'Behavioral', 'Binocular Vision', 'Cells', 'Characteristics', 'Coffee', 'Collection', 'Complex', 'Cues', 'Data', 'Data Set', 'Development', 'Distant', 'Environment', 'Eye', 'Eye Movements', 'Gait', 'Goals', 'Grant', 'Head', 'Human', 'Image', 'Individual', 'Investigation', 'Knowledge', 'Learning', 'Link', 'Location', 'Locomotion', 'Machine Learning', 'Measures', 'Monitor', 'Motion', 'Motor', 'Movement', 'Pattern', 'Psychological reinforcement', 'Retina', 'Rewards', 'Robot', 'Robotics', 'Role', 'Sampling', 'Seminal', 'Sensory', 'Signal Transduction', 'Speed', 'Stimulus', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'To specify', 'Uncertainty', 'Vision', 'Visit', 'Visual', 'Visual Fields', 'Visual impairment', 'Walkers', 'Walking', 'Work', 'base', 'convolutional neural network', 'cost', 'experimental study', 'foot', 'gaze', 'imaging properties', 'innovation', 'kinematics', 'novel', 'optic flow', 'rehabilitation strategy', 'response', 'sample fixation', 'statistics', 'virtual reality environment', 'vision aid', 'vision development', 'vision rehabilitation', 'visual information']",NEI,"UNIVERSITY OF TEXAS, AUSTIN",R01,2020,381743,-0.06225153009517022
"Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex Project Summary The use of stimuli with increasingly naturalistic properties has become critical to advance our understanding of vision. Many studies demonstrate that simple artificial stimuli (e.g. sinusoidal gratings and white noise) fail to engage nonlinearities that profoundly alter responses in the retina, lateral geniculate nucleus (LGN), and primary visual cortex (V1). A recent and striking example comes from the use of naturalistic ‘flow’ stimuli, which engage robust responses in V1 that are not predicted from responses to gratings. This gap in understanding motivates the development of a stimulus ensemble and analysis framework that produces a quantitative understanding of visual processing to increasingly naturalistic stimuli and the nonlinearities that they engage. Our objective is to understand how flow stimuli are processed from retina through visual cortex. To meet this goal, we will make neural population recordings in retina (Aims 1 & 3), LGN (Aims 1 & 3) and V1 (Aim 3) using matched experimental conditions and a unified theoretical/modeling framework to map the transformations that occur across these stages of visual processing. Our central hypothesis is that V1 transforms a discrete and heavily light-level-de- pendent retinal representation of natural stimuli into a continuous (uniform) representation that is relatively in- variant to changes in the mean luminance. This invariance places a strong constraint on the class of nonlineari- ties that transform retinal responses to those observed in LGN and V1. We test this hypothesis in three aims: (1) determine early visual processing (retina & LGN) of naturalistic flow stimuli; (2) develop an encoding manifold to capture the population activity at each processing stage and transforms from one stage to the next; (3) test the ability of the manifold description to predict the impact of light adaptation on processing flow stimuli from retina to V1. Aim 1 will yield a matched experimental dataset to an interesting and novel class of ecologically-relevant stimuli. Aim 2 will yield a quantitative framework by which to understand the transformations that occur between retina, LGN, and V1. Aim 3 will provide a platform for globally perturbing the output of the retina by switching from photopic to mesopic and scotopic conditions, and thereby compare predictions of our model to measured changes in LGN and V1 activity. The primary significance of this research is that it will provide a computationally and experimentally unified framework for understanding the transformations that occur in the processing of stim- uli across multiple stages of visual processing. The major innovations are (1) presenting visual stimuli for retinal recordings that are matched to eye movements and pupil dynamics in alert animals; (2) creating a novel analysis framework that captures the responses of neurons at all three levels and the inter-level transformations to in- creasingly complex stimuli; (3) utilizing light adaptation as a method of perturbing retinal output to test our model and the stability (invariance) of LGN and V1 responses to adapting retinal signals. The expected outcome is a data-driven model of the processing from retina to LGN and V1 that generalizes from starlight to sunlight. Project Narrative Restoring vision to the blind likely requires understanding how retinal signals are communicated to the brain and how these signals are transformed in the thalamocortical pathway. This project aims to acquire an understanding of these transformations in the context of complex and more naturalistic visual stimuli.",Elucidating novel features of visual processing and physiological connectivity from retina to primary visual cortex,10050840,R01EY031059,"['Affect', 'Animals', 'Brain', 'Collaborations', 'Complex', 'Cone', 'Data', 'Data Set', 'Development', 'Environment', 'Eye Movements', 'Future', 'Goals', 'Lateral Geniculate Body', 'Light', 'Light Adaptations', 'Machine Learning', 'Maps', 'Mathematics', 'Measurement', 'Measures', 'Mediating', 'Methods', 'Modeling', 'Movement', 'Mus', 'Neurons', 'Noise', 'Optics', 'Outcome', 'Output', 'Pathway interactions', 'Physiological', 'Population', 'Process', 'Property', 'Pupil', 'Research', 'Retina', 'Retinal Ganglion Cells', 'Rod', 'Signal Transduction', 'Stimulus', 'Structure', 'Sunlight', 'Techniques', 'Testing', 'Theoretical model', 'Variant', 'Vision', 'Visual Cortex', 'Visual system structure', 'Work', 'area striata', 'base', 'blind', 'cell type', 'computational neuroscience', 'experimental study', 'in vivo', 'innovation', 'luminance', 'multi-electrode arrays', 'novel', 'predictive modeling', 'receptive field', 'relating to nervous system', 'response', 'retinal neuron', 'stimulus processing', 'visual processing', 'visual stimulus']",NEI,DUKE UNIVERSITY,R01,2020,528815,0.014153385751454854
"GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN PROJECT SUMMARY & ABSTRACT  Human locomotion through natural environments requires the coordination of all levels of the sensorimotor hierarchy, from the cortical areas involved in processing of visual information and high level planning to the subcortical and spinal structures involved in the regulation of the gait and posture. However, despite the complex neural bases of human locomotion, the output is highly regular and well organized around the basic physical dynamics and biomechanics that define the stability and energetic costs of moving a bipedal body through space. There is a rich and growing body of literature describing detailed knowledge each of the individual components of human locomotion, including neural mechanisms, muscular neuromechanics, and biomechanics. However, very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion through a complex and dynamic world. This lack of integrative research not only restricts the breadth of impact of research from these individual disciplines, but also limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to to fill this critical gap in our knowledge about human locomotion, it is necessary to develop an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world. In service of this general goal, this proposal outlines research projects aimed at specific unanswered questions about locomotion over different terrains. This proposal comprises three specific research and training aims on the visual control of locomotion over rough terrain. Aim 1 focuses on the behavioral task itself, Aim 2 investigates the sensory stimulus experienced during real-world locomotion, and Aim 3 examines the motor integration of visually specified goals into the ongoing gait cycle. Aim 1 investigates effects of changing environmental uncertainty and task demands on gaze allocation strategies during locomotion over real-world rough terrain. Aim 2 analyzes and models the visual stimulus experienced during locomotion over real-world rough terrain. Aim 3 determines how visually specified target footholds and targets are integrated into the ongoing preferred steady-state gait. Together these aims will significantly advance our understanding of how humans use vision to control their movement through the natural world, which greatly increase our ability to develop clinical diagnosis and treatment for loss of locomotor function. PROJECT NARRATIVE  Very little research exists on the way that visual input is used to dynamically control locomotion, and the overall control structure of the integrated neural and mechanical system during natural locomotion. This lack of integrative research limits our ability to develop adequate treatment plans for loss of locomotor ability deriving from systems-level factors such as aging, stroke, and Parkinson’s disease. In order to fill this critical gap in our knowledge about human locomotion, this proposal develops an integrated research program that examines the interactions between the visual, neural, and mechanical bases of human movement through the world.",GAZE AND THE VISUAL CONTROL OF FOOT PLACEMENT WHEN WALKING OVER ROUGH TERRAIN,10019556,R00EY028229,"['3-Dimensional', 'Aging', 'Algorithms', 'Area', 'Attention', 'Behavior', 'Behavioral', 'Biomechanics', 'Clinical Treatment', 'Cognitive', 'Complex', 'Computer Vision Systems', 'Development', 'Discipline', 'Environment', 'Eye', 'Gait', 'Goals', 'Human', 'Image', 'Individual', 'Knowledge', 'Link', 'Literature', 'Locomotion', 'Measures', 'Mechanics', 'Mentors', 'Modeling', 'Motion', 'Motor', 'Movement', 'Muscle', 'Musculoskeletal', 'Nature', 'Neuromechanics', 'Output', 'Parkinson Disease', 'Pattern', 'Phase', 'Photic Stimulation', 'Positioning Attribute', 'Postdoctoral Fellow', 'Posture', 'Protocols documentation', 'Regulation', 'Research', 'Research Activity', 'Research Project Grants', 'Research Training', 'Services', 'Signal Transduction', 'Specific qualifier value', 'Spinal', 'Stroke', 'Structure', 'System', 'Training', 'Training Programs', 'Uncertainty', 'Vision', 'Visual', 'Visual Fields', 'Walking', 'Wireless Technology', 'area MST', 'area MT', 'base', 'clinical Diagnosis', 'cost', 'design', 'environmental change', 'experience', 'experimental study', 'foot', 'gaze', 'insight', 'instrument', 'kinematics', 'multidisciplinary', 'neuromechanism', 'neuromuscular', 'novel', 'optic flow', 'programs', 'relating to nervous system', 'response', 'sensory stimulus', 'skills', 'statistics', 'treadmill', 'treatment planning', 'visual control', 'visual information', 'visual processing', 'visual stimulus', 'visual-motor integration']",NEI,NORTHEASTERN UNIVERSITY,R00,2020,243713,-0.001188020647229395
"Development of a visual-to-tactile conversion system for automating tactile graphic generation process PROJECT SUMMARY/ABSTRACT There are an estimated 23.7 million people who are blind or visually-impaired (BVI) in the U.S. and 285 million globally. Of this population, 30% do not travel independently outside of their home, only ~11% have a bachelor’s degree, and more than 70% are unemployed. The goal of this SBIR effort is to develop a novel system, which performs principled down-sampling and translation of visual information from digital documents into tactile equivalents. Timely access to information is one of the biggest challenges for BVI people. While access to textual information has largely been solved via screen reading software (e.g., JAWS or VoiceOver), very little progress has been made in making graphical information accessible. Although few assistive technology (AT) devices aim provide non-visual graphical access, they suffer from several shortcomings including high cost, limited portability, lack of multi-purpose, and inability to present information in a real-time context. Importantly, a common underlying problem across all extant approaches is that they require intensive human effort for producing or authoring tactile (and/or multimodal) graphics, which leads to high production costs and significant delays in the time between when the accessible materials are needed, and when they are actually delivered, adversely impacting BVI individuals in K-12 schools, colleges, and workplace settings. To address this long-standing problem, UNAR Labs aims to develop a novel system, which will automatically down-sample and translate visual graphical information into an intuitive tactile equivalent that can be used in tactile embossers. Building upon eight years of empirical research, this Phase I SBIR effort will prove the technical feasibility and functional viability of a prototype system for automating visual-to-tactile graphic conversion process and using the output in embossers. Two specific aims will guide this Phase I project: (1) to develop a prototype of an automated system for performing visual-to-tactile conversion without human intervention, and (2) to assess the technical feasibility and functional utility of the system through a rigorous human study. Success in this effort will provide a robust automated system for tactile graphic generation and promote empowerment of millions of BVI individuals by supporting increased educational attainment, proliferation of vocational opportunities, and enhancing overall quality of life for BVI people. PROJECT NARRATIVE Lack of equitable and timely access to information among persons who are blind or visually impaired (BVI) is key to realizing an inclusive world for all as it alleviates a known impediment that is hugely detrimental to their success in activities affecting quality of life and socio-economic status. The proposed innovation presents a first- of-its kind on-demand visual-to-tactile translation system, which will fully automate the tactile graphic generation process using bio-inspired sensory substitution rules and will instantly deliver the translated information for use in tactile embossers. Successful completion of this project will significantly reduce tactile graphic production costs and preparation time, and will promote empowerment of millions of BVI individuals by supporting increased educational attainment, vocational opportunities, and overall better quality of life.",Development of a visual-to-tactile conversion system for automating tactile graphic generation process,10008494,R43EY031628,"['Access to Information', 'Address', 'Adoption', 'Affect', 'Bachelor&apos', 's Degree', 'Benchmarking', 'Braille Display', 'Characteristics', 'Cognitive', 'Computer software', 'Computers', 'Data', 'Development', 'Devices', 'Elements', 'Empirical Research', 'Evaluation', 'Floor', 'Generations', 'Goals', 'Graph', 'Home environment', 'Human', 'Individual', 'Information Retrieval', 'Intervention', 'Intuition', 'Maine', 'Maps', 'Nature', 'Output', 'Performance', 'Persons', 'Phase', 'Plant Roots', 'Population', 'Preparation', 'Process', 'Production', 'Productivity', 'Psychophysics', 'Quality of life', 'Readability', 'Reading', 'Research', 'Route', 'Sampling', 'Schools', 'Self-Help Devices', 'Sensory', 'Small Business Innovation Research Grant', 'Socioeconomic Status', 'Software Framework', 'Support System', 'System', 'Tactile', 'Text', 'Time', 'Touch sensation', 'Translating', 'Translations', 'Unemployment', 'Universities', 'Visual', 'Visual impairment', 'Visually Impaired Persons', 'Work', 'Workplace', 'base', 'blind', 'braille', 'college', 'cost', 'data modeling', 'deep learning', 'digital', 'empowerment', 'human study', 'innovation', 'multimodality', 'multisensory', 'novel', 'operation', 'portability', 'prototype', 'success', 'touchscreen', 'usability', 'visual information', 'visual learning']",NEI,"UNAR LABS, LLC",R43,2020,300000,0.010756753034098931
"Access to parietal action representations after stroke lesions in visual cortex PROJECT SUMMARY  The ability to recognize and use objects according to their function (e.g., fork, hammer, pencil) requires integration of visual, semantic and action knowledge across occipital, temporal and parietal areas. Left parietal regions support critical aspects of object-directed action, such as grasping and object manipulation. This research activity uses a combination of fMRI and behavioral measures in patients with ischemic strokes to early and extrastriate visual areas to test the following hypotheses: Aim 1: There is a visual pathway to the parietal grasp region (aIPS) that bypasses processing in primary visual cortex. Aim 2: Left ventral extrastriate cortex is necessary to access manipulation information for visually presented objects. Aim 3A: Ballistic grasping actions to objects in the hemianopic field are influenced by volumetric properties (size, orientation) of targets. Aim 3B: Left ventral extrastriate lesions impair object function (e.g., `scissors used to cut') and disrupt access to manipulation knowledge from visual input. The research leverages strengths of fMRI (whole brain correlational measure) and neuropsychology (causal inference) to test new hypotheses about vision and action.  `Tools' (i.e., small manipulable objects) are an excellent domain in which to address broader questions about the integration of sensory, motor and cognitive processing. This is because tool recognition and tool use require the integration of distinct sensory, motor and cognitive representations, and the neural substrates of tool processing are well described. The research program emphasizes fresh perspectives on longstanding ideas about the dorsal and ventral visual pathways, by a) undertaking the first systematic investigation of the types of information about objects that are extracted by visual pathways that bypass primary visual cortex, and by b) studying how some parietal areas depend on inputs from the ventral stream in order to access the correct action for a given object. The research activity innovates by testing hypotheses about how lesions at different stages in the cortical visual hierarchy affect downstream processing in parietal cortex, combining neural and behavioral measures to study brain damaged patients (generating causal evidence), and by combining univariate and multivariate measures to `read out' the information content of brain regions (parietal cortex) that are anatomically remote from a lesion. The research advances understanding of how lesions in one brain region disrupt computations in other parts of the brain that depend on the damaged region for their inputs, a phenomenon (`dynamic diaschisis') that applies to brain injury generally. Advancing understanding of these basic issues using causal data has broad implications for understanding how the brain selects the correct action for the correct object, and more generally for theories of conceptual organization and causal reasoning. Understanding how the brain accesses actions from visual input has implications for related fields, such as robotics, neuroprosthetics, and evidenced based approaches for rehabilitating function after brain injury. Project Narrative Functional MRI and behavioral testing are used to test hypotheses about how strokes affecting occipital and temporal cortex disrupt access to action representations in parietal cortex. This research will advance understanding of how the brain processes visual information in support of everyday actions.",Access to parietal action representations after stroke lesions in visual cortex,9868307,R01EY028535,"['Address', 'Affect', 'Alexia', 'Anatomy', 'Area', 'Ballistics', 'Behavioral Assay', 'Brain', 'Brain Injuries', 'Brain region', 'Bypass', 'Cognitive', 'Data', 'Dorsal', 'Eating', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grain', 'Imaging Device', 'Impairment', 'Investigation', 'Ischemic Stroke', 'Knowledge', 'Left', 'Lesion', 'Literature', 'Location', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Motor', 'Neural Pathways', 'Neuropsychology', 'Occipital lobe', 'Parahippocampal Gyrus', 'Parietal', 'Parietal Lobe', 'Participant', 'Pathway interactions', 'Patients', 'Population', 'Process', 'Property', 'Prosopagnosia', 'Reading', 'Research', 'Research Activity', 'Robotics', 'Role', 'Semantics', 'Sensory', 'Stimulus', 'Stream', 'Stroke', 'Structure of supramarginal gyrus', 'Temporal Lobe', 'Testing', 'Vision', 'Visual', 'Visual Cortex', 'Visual Fields', 'Visual Pathways', 'Visual system structure', 'Work', 'area striata', 'base', 'behavior measurement', 'behavior test', 'blind', 'cognitive development', 'evidence base', 'extrastriate', 'extrastriate visual cortex', 'fovea centralis', 'grasp', 'information processing', 'innovation', 'lens', 'multisensory', 'neuroprosthesis', 'post stroke', 'programs', 'relating to nervous system', 'sensory integration', 'theories', 'tool', 'visual information', 'visual motor', 'visual object processing', 'visual process', 'visual processing']",NEI,CARNEGIE-MELLON UNIVERSITY,R01,2020,423007,0.0017978131651840562
"Studying crowding as a window into object recognition and development and health of visual cortex ABSTRACT  Our long-term goal is to understand how the human brain recognizes objects. This 3-year project will characterize the computational kernel (computation that is applied independently to many parts of the image data) that is isolated by crowding experiments. We present the discovery that recognition of simple objects is performed by recognition units implementing the same computation at every eccentricity. These units are dense in the fovea and thus hard to isolate there, but they are sparse in the periphery, and easily isolated. Our fMRI & psychophysics pilot data show that each of these units, at every eccentricity, has a circular receptive field with a radius of 2.6±1.5 mm (mean±SD) in human cortical area hV4. Because of cortical magnification, that 2.6 mm corresponds to a tiny 0.05 deg in the fovea, but grows linearly with eccentricity, to a comfortable 3 deg at 10 deg eccentricity. We test this idea by pursuing its implications physiologically (Aim 1), clinically (Aim 2), and psychophysically and computationally (Aim 3).  Aim 1. Better noninvasive measures for the health and development of visual cortex are needed. Conservation of crowding distance (in mm) in a particular cortical area (hV4) would validate crowding distance as a quick, noninvasive measure of that area's condition. Aim 2. Huge public interventions seek to help dyslexic children read faster and identify amblyopic children sooner. It would be valuable to know whether crowding contributes to reading problems and provides a basis for effective screening for dyslexia and amblyopia, as it can be measured before children learn to read. Aim 3. Documenting conservation of efficiency gives evidence that the same universal computation recognizes objects at every eccentricity. We are testing the first computational model of object recognition that accounts for many human characteristics of simple-object recognition. The new work extends to effect of receptive field size and learning. Project Narrative (relevance to public health) This proposal is a collaboration between a psychophysicist, expert on human object recognition, a computer scientist, expert on machine learning for object recognition by computers, and a brain imager, expert on brain mapping, to discover to what extent computer models of object recognition and the brain can account for key properties of human performance. Our first aim tracks the development of crowding in normal and amblyopic children, in collaboration with experts in optometry, reading, and development. Advances in this area could shed light on the problems of people with impaired object recognition, including amblyopia and dyslexia, with a potential for development of early pre-literate screening tests for amblyopia and risk of dyslexia.",Studying crowding as a window into object recognition and development and health of visual cortex,9884770,R01EY027964,"['Address', 'Adult', 'Affect', 'Age', 'Amblyopia', 'Area', 'Atlases', 'Biological', 'Brain', 'Brain Mapping', 'Bypass', 'Child', 'Clinical', 'Collaborations', 'Complex', 'Computer Models', 'Computers', 'Crowding', 'Data', 'Development', 'Disease', 'Dyslexia', 'Face', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Human', 'Human Characteristics', 'Image', 'Immunity', 'Impairment', 'Intervention', 'Italy', 'Joints', 'Learning', 'Letters', 'Light', 'Location', 'Machine Learning', 'Magic', 'Measurement', 'Measures', 'Mediating', 'Modeling', 'Neurosciences', 'Noise', 'Nose', 'Occipital lobe', 'Optometry', 'Participant', 'Perception', 'Performance', 'Peripheral', 'Physiological', 'Population Heterogeneity', 'Postdoctoral Fellow', 'Property', 'Psychophysics', 'Public Health', 'Radial', 'Reading', 'Rest', 'Risk', 'Rome', 'Scientist', 'Speed', 'Stereotyping', 'Stimulus Deprivation-Induced Amblyopia', 'Surface', 'Testing', 'Vision', 'Visual Cortex', 'Visual Fields', 'Work', 'assault', 'cerebral atrophy', 'clinical application', 'convolutional neural network', 'crowdsourcing', 'experimental study', 'extrastriate visual cortex', 'fovea centralis', 'imager', 'literate', 'object recognition', 'physiologic model', 'receptive field', 'relating to nervous system', 'research clinical testing', 'sample fixation', 'screening', 'vision development']",NEI,NEW YORK UNIVERSITY,R01,2020,388626,0.02044151489920811
"Auditory brain-computer interface for communication Project Summary  A fundamental end-goal of brain-computer interfaces (BCI) is to enable communication in individuals with severe motor paralysis. BCIs decode the neural signals and accomplish the intended goal via an effector, such as a computer cursor or a robotic limb. The BCI user relies on the realtime feedback of the effector's performance to modulate their neural strategy to control the external device. To date, this feedback is predominantly visual. However patients with the most severe paralysis resulting from amyotrophic lateral sclerosis (ALS), some forms of stroke and traumatic brain injuries can have severe visual impairments including oculomotor fatigue, nystagmus and ophthalmoparesis - that make the reliable use of a visual-based BCI impossible. This puts a premium on developing novel solutions that can leverage sensory modalities that are intact. In this research, I will develop and test the feasibility of an auditory-based interface to establish BCI control in motor-impaired patients with severe neurological insults  In Aim 1, I propose to implement a novel paradigm using auditory cues in lieu of visual signals, and test its feasibility in controlling an effector (ie, computer cursor) to perform a cued target-acquisition task in healthy participants. This will validate the range of parameter values of the four tested auditory input signals: 1) frequency, 2) amplitude, 3) spatial azimuth and 4) spatial elevation. This approach is distinct from most binary class auditory BCI solutions, since it relies on both the natural ability of humans to localize sounds, and the ability to associate new tones to a virtual space, thus allowing a truly multi-class auditory approach. In Aim 2, I propose to implement the auditory interface into the realtime xPC used for visual presentation in clinical trial participants with intracortical BCIs, and test their performance on the cued target-acquisition task. Although much success has been demonstrated in this task using visual feedback, this auditory approach will permit BCI use by people with visual impairments further compounding their paralysis. Finally in Aim 3, I will test the feasibility of BrainGate BCI users to utilize an auditory BCI speller to perform a copy-typing task and free- typing task.  The accomplishment of the goals of this research will be a critical step towards enabling severely paralyzed individuals with visual impairments to re-establish communication independently, continuously and reliably. Project Narrative Brain-computer interfaces enable motor-impaired individuals to communicate using an effector such as a neural cursor or a robotic arm. The successful completion of the proposed project will develop a unique technology that enables a real-time auditory-reliant BCI for communication in severely paralyzed individuals resulting from stroke, amyotrophic lateral sclerosis and severe brain injuries. Study results will advance our knowledge of developing neurotechnologies that leverage non-visual sensory modalities, as well as provide much insight into the cortical neural activities that underpin motor intention and movement.",Auditory brain-computer interface for communication,9851289,F32MH118709,"['Adult', 'Algorithms', 'Amyotrophic Lateral Sclerosis', 'Auditory', 'Auditory system', 'Base of the Brain', 'Brain Injuries', 'Brain Stem Infarctions', 'Clinical Trials', 'Communication', 'Computers', 'Cues', 'Data', 'Development', 'Devices', 'Eye Movements', 'Family Caregiver', 'Fatigue', 'Feedback', 'Frequencies', 'Functional disorder', 'Future Generations', 'Goals', 'Hearing Tests', 'Human', 'Impairment', 'Individual', 'Infrastructure', 'Institution', 'Intention', 'Joystick', 'Knowledge', 'Laboratories', 'Learning', 'Limb structure', 'Machine Learning', 'Manuals', 'Measures', 'Modality', 'Motor', 'Movement', 'Mus', 'Neurologic', 'Ophthalmopareses', 'Paralysed', 'Participant', 'Pathologic Nystagmus', 'Pathway interactions', 'Patients', 'Performance', 'Positioning Attribute', 'Psyche structure', 'Quadriplegia', 'Quality of life', 'Research', 'Resources', 'Robotics', 'Sensory', 'Signal Transduction', 'Social Interaction', 'Sound Localization', 'Speech', 'Spinal cord injury', 'Stroke', 'System', 'Task Performances', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Training', 'Traumatic Brain Injury', 'Universities', 'User-Computer Interface', 'Vision', 'Visual', 'Visual Fields', 'Visual impairment', 'Workload', 'Writing', 'arm', 'auditory feedback', 'base', 'brain computer interface', 'clinical trial participant', 'engineering design', 'improved', 'insight', 'motor impairment', 'neurotechnology', 'neurotransmission', 'novel', 'oculomotor', 'relating to nervous system', 'speech synthesis', 'spelling', 'success', 'usability', 'virtual', 'visual feedback', 'way finding']",NIMH,BROWN UNIVERSITY,F32,2020,74810,-0.011044407716129414
"Distinguishing normal aging from age-related macular degeneration at the level of single cells int eh living human eye Age-related macular degeneration (AMD) is the leading cause of blindness in the elderly in the developed world; no cure exists and prevalence is rising rapidly. Because only primates have a macula and since no model of AMD exists in non-human primates, the disease course can only be elucidated through in-depth study of humans. Blindness in AMD is caused by progressive and irreversible death of rod and cone photoreceptors secondary to degeneration of the retinal pigment epithelium (RPE) that is essential for their health and function. Clinical imaging and histology have informed us greatly about the later stages of disease but fundamental knowledge to understand how AMD diverges from normal aging at onset is lacking. With advanced adaptive optics ophthalmoscopy (AOO) imaging methods, combined with clinical imaging and visual function testing, we will characterize healthy human retinal aging in cross-sectional study, by defining the in vivo RPE-photoreceptor cellular organization and microscopic autofluorescence variation with age and wavelength. This will produce the largest quantitative in vivo normative dataset of AOO cell-based metrics to date and we will use this data to generate new quantitative analysis tools needed to evaluate emerging therapies designed to prevent or slow vision loss in AMD (Aim 1). In a case-control study, we will then compare normal photoreceptor topography and RPE cell morphometry to clinically defined early AMD to quantitatively define the earliest cellular changes in AMD that can be detected in vivo. This work will identify the cellular alterations and phenotypes that differentiate normal aging from early AMD to facilitate early onset detection. These results will be contextualized by comparison to tissue-level alterations seen with aging and early AMD in clinical imaging, specifically choriocapillaris decline and drusen (Aim 2). The results of this study will result in a paradigm shift from the use of clinical diagnosis and classification systems for AMD that rely solely on tissue- level biomarkers or traditional funduscopic clinical signs to those that rely on rigorous quantitative in vivo cell- based metrics. Together, this knowledge and these tools will lay the foundation needed to develop and evaluate new preventative therapies that are needed to limit or prevent vision loss in AMD. Project Narrative Age-related macular degeneration is the leading cause of blindness in the elderly in the US and is a significant public health issue that is projected to worsen due to the rapidly aging population. Here we aim to understand how retinal cells change in normal aging and how these normal age-related changes differ from the changes that lead to age-related macular degeneration. This project will allow us to detect age-related macular degeneration earlier and will produce new tools to monitor retinal cells that will facilitate the development and testing of preventative therapies to slow or prevent vision loss in age-related macular degeneration.",Distinguishing normal aging from age-related macular degeneration at the level of single cells int eh living human eye,9973645,R01EY030517,"['Age', 'Age related macular degeneration', 'Aging', 'Area', 'Atrophic', 'Biological Markers', 'Blindness', 'Bruch&apos', 's basal membrane structure', 'Case-Control Studies', 'Cells', 'Cessation of life', 'Choroid', 'Classification', 'Clinical', 'Clinical Research', 'Complex', 'Conflict (Psychology)', 'Cross-Sectional Studies', 'Cytoplasmic Granules', 'Data', 'Data Set', 'Detection', 'Development', 'Diagnostic Procedure', 'Disease', 'Drusen', 'Elderly', 'Evaluation', 'Eye', 'Foundations', 'Genetic', 'Goals', 'Health', 'Histology', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Individual', 'Knowledge', 'Lead', 'Lipofuscin', 'Machine Learning', 'Maps', 'Melanins', 'Methods', 'Microscopic', 'Modeling', 'Monitor', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Optics', 'Perfusion', 'Phenotype', 'Photoreceptors', 'Prevalence', 'Preventive therapy', 'Preventive treatment', 'Primate Diseases', 'Primates', 'Public Health', 'Retina', 'Retinal Cone', 'Retinal Degeneration', 'Retinal Photoreceptors', 'Risk', 'Secondary to', 'Spatial Distribution', 'Structure', 'Structure of retinal pigment epithelium', 'System', 'Techniques', 'Technology', 'Testing', 'Therapy Evaluation', 'Time', 'Tissues', 'Variant', 'Vertebrate Photoreceptors', 'Vision', 'Work', 'adaptive optics', 'age related', 'aging population', 'base', 'clinical Diagnosis', 'clinical decision-making', 'clinical imaging', 'cohort', 'early onset', 'fluorophore', 'healthy aging', 'imaging modality', 'imaging platform', 'improved', 'in vivo', 'macula', 'morphometry', 'multimodality', 'neurovascular unit', 'nonhuman primate', 'normal aging', 'prevent', 'restorative treatment', 'retinal imaging', 'retinal rods', 'therapy design', 'tool']",NEI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2020,527040,-0.013292608862833753

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Creating an artificial intelligence therapy-to-data feedback loop for child developmental healthcare Project Summary There is a sharp and increasing imbalance between the number of children with autism in need of care and the availability of specialists certified to treat the disorder in its multi-faceted manifestations. The autism community faces a dual clinical challenge: how to direct scarce specialist resources to service the diverse array of phenomes and how to monitor and validate best practices in treatment. Clinicians must now look to solutions that scale in a decentralized fashion, placing data capture, remote monitoring, and therapy increasingly into the hands of families. Using artificial intelligence (AI) and large amounts of labeled human emotion computer vision data, we have developed a solution for automatic facial expression recognition that runs on Google Glasses and Android smartphones to deliver real time social cues to individuals with autism in the child’s natural environment. We hypothesize that this informatic system can provide real-time therapy in a way that scales to meet the demand of the growing population of autism families, including underserved minorities, while growing data that can be used to measure progress over time and in the development of novel AI. Our first aim will focus on the development of a deep learning model that enables dynamic emotion recognition in the real world, and on domain adaptation procedures that enable minimal manual labeling to personalize the model for optimal accuracy on the individuals with whom the child will interact most regularly at home. Our second aim will focus on the human computer interface, namely the design of the user experience with the Android application that controls the sessions run on the Google Glass wearable. We will work our clinical colleagues and with groups of autism families to develop and enhance a set of games and activity modes that create social engagements ideal for emotion therapy, including an emotion capture and a charades game. The third aim will test our central hypothesis that the Glass system can create a therapy-to-data feedback loop that delivers clinical care while growing data for measurement and model development. We will work with up to 200 children ages 4-8 who have recent autism diagnoses and do not have access to standard behavioral therapy. We will build a community of autism families through crowdsourcing techniques, befitting the mobile paradigm embodied by our work, and through close collaboration with behavioral therapy providers, the autism outreach organization Autism Speaks, and the digital healthcare company, Cognoa. The families will work with us on design and refinement of our “Superpower Glass” system for fit, engagement, and function of use for both therapy and data capture. Importantly, we will send units home with families to use the device for at least 3 twenty-minute sessions per week for a minimum of 6 weeks. This remote period will generate a massive database to quantify overall social learning, emotion comprehension, eye contact, and sustained social acuity. In all, our work program will show that mobile wearable AI can bring the social learning process out of the clinic and into the real world for faster and more adaptive intervention. Project Narrative There is a sharp and growing imbalance between the number of clinical care providers available and the number of children with an autism diagnosis that leave most children without therapy until after critical periods in development have passed. We intend to address this problem through creation of a machine learning-enabled wearable that brings effective care to the home and empowers both parents, patients, and clinicians with mobile solutions that personalize care delivery to dramatically improve children’s outcomes. The Superpower Glass system, which delivers social cues to children during real-time interactions and provides several engagement modes for families, is a promising solution that enables greater access to care for families across the US, and potentially, across the globe.!",Creating an artificial intelligence therapy-to-data feedback loop for child developmental healthcare,9716320,R01LM013083,"['Address', 'Affect', 'Age', 'Android', 'Artificial Intelligence', 'Award', 'Awareness', 'Behavior Therapy', 'Car Phone', 'Caregivers', 'Caring', 'Cellular Phone', 'Child', 'Child Support', 'Classification', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Comprehension', 'Computer Vision Systems', 'Computer software', 'Control Groups', 'Cues', 'Data', 'Databases', 'Decentralization', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Emotions', 'Environment', 'Eye', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Family', 'Feedback', 'Future', 'Glass', 'Goals', 'Hand', 'Health Services Accessibility', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Informatics', 'Intervention', 'Label', 'Learning', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Parents', 'Participant', 'Patients', 'Persons', 'Population', 'Procedures', 'Process', 'Provider', 'Resources', 'Running', 'Secure', 'Self-Direction', 'Services', 'Severities', 'Social Interaction', 'Socialization', 'Specialist', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'User-Computer Interface', 'Variant', 'Waiting Lists', 'Work', 'adaptive intervention', 'applied behavior analysis', 'autism spectrum disorder', 'autistic children', 'base', 'care delivery', 'care providers', 'clinical care', 'combat', 'critical period', 'crowdsourcing', 'deep learning', 'design', 'digital', 'experience', 'improved', 'mobile application', 'mobile computing', 'model development', 'novel', 'outreach', 'personalized care', 'phenome', 'practical application', 'programs', 'prototype', 'skills', 'smartphone Application', 'social', 'social engagement', 'social learning', 'standard of care', 'tool', 'underserved minority', 'user-friendly']",NLM,STANFORD UNIVERSITY,R01,2019,677664,0.019868676670375198
"QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development   Children with autism struggle to recognize facial expressions, make eye contact, and engage in social interactions. Many of these children can have dramatic recoveries, particularly if social skills are taught from an early age. However, in today's healthcare system the delivery of the behavioral intervention is bottlenecked by a sharp and increasing imbalance in the number of behavioral therapists and the number of children in need of care. As such, there is an urgent need to develop mobilized methods of care delivery. We have developed an artificial intelligence tool for automatic facial expression recognition that runs on Google Glass through an Android app and delivers instantaneous social cues to individuals with autism in their natural environment, providing therapy that today is given only by clinicians in non-scalable person-to- person sessions. The system leverages Glass's outward facing camera to read a person's facial expressions and passes facial landmarks to an Android native app for immediate machine learning-based emotion classification. The system then gives the child wearer real-time social cues and records social responses. We believe that the system's ability to provide continuous behavioral therapy outside of clinical settings will enable dramatically faster gains in social acuity that will, within a limited and self-directed period of use, permit the child to engage in social settings on his/her own. This proposal outlines three main aims needed to test, refine and optimize the tools and a series of validation experiments needed to bring our system from prototype to a viable clinical tool that every family can use regularly from home for precision healthcare. We have developed a combined software-hardware solution built on top of Google Glass that enables real lime expression recognition and field-of-view eye tracking to capture social interaction data and give guiding social cues to an individual with Autism. This project will test and optimize the potential of this tool to provide continuous and naturalized behavioral therapy to children with autism from their homes.",QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development  ,9749146,R01EB025025,"['Address', 'Advertising', 'Affective', 'Age', 'Android', 'Artificial Intelligence', 'Augmented Reality', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Benchmarking', 'Big Data', 'Biomedical Engineering', 'Caregivers', 'Caring', 'Child', 'Child Development', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Cues', 'Data', 'Databases', 'Emotions', 'Environment', 'Eye', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Family', 'Family member', 'Feedback', 'Future', 'Glass', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Home environment', 'Human', 'Image', 'Imagery', 'Individual', 'Internet', 'Intervention', 'Label', 'Learning', 'Lighting', 'Limes', 'Machine Learning', 'Methods', 'Modeling', 'Outcome', 'Parents', 'Participant', 'Persons', 'Precision Health', 'Procedures', 'Records', 'Recovery', 'Running', 'Self-Direction', 'Series', 'Social Interaction', 'Social Marketing', 'System', 'Testing', 'Therapeutic', 'Time', 'Training', 'Update', 'Validation', 'Variant', 'Work', 'adaptive learning', 'autism spectrum disorder', 'autistic children', 'base', 'care delivery', 'deep learning', 'design', 'empowered', 'experimental study', 'human-in-the-loop', 'improved', 'mobile computing', 'novel', 'personalized health care', 'prototype', 'response', 'smartphone Application', 'social', 'social engagement', 'social learning', 'social skills', 'tool']",NIBIB,STANFORD UNIVERSITY,R01,2019,333071,0.045716272127997414
"Thought disorder and social cognition in clinical risk states for schizophrenia Project Summary  In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decade, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a wide range of cognitive processes to try to identify core deficits of schizophrenia evident before psychosis onset. Subthreshold thought disorder and impaired emotion recognition have emerged as profound deficits that predate, rather than follow, psychosis onset and thus may be indicators of schizophrenia liability, consistent with studies in other risk cohorts, including genetic high risk. Further, subthreshold thought disorder and emotion recognition deficit are significantly correlated, suggesting shared neural substrates in temporoparietal regions.  This study aims to identify the neural mechanisms that underlie subthreshold thought disorder and emotion recognition deficit in 125 CHR individuals followed prospectively for psychosis outcome. CHR cohorts are enriched with early cases of schizophrenia, as 20-25% develop schizophrenia and related psychotic disorders within 1-2 years. CHR cohorts may be optimal for studying core characteristics of illness as they otherwise have low-level symptoms, less illness chronicity and minimum exposure to antipsychotics. 25 individuals with schizophrenia and 50 healthy volunteers are included for comparison.  Subthreshold thought disorder and emotion recognition deficits will be studied across behavioral, physiological and circuit levels. For thought disorder, we will use automated speech analysis approaches developed in collaboration with IBM to identify constituent impairments in semantics and syntax, and a listening task that elicits reliable activation in language circuits. Our automated machine-learning approach to speech analysis, informed by artificial intelligence, derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to what they read or hear. Emotion recognition will be measured using standard tasks, naturalistic tasks with dynamic face stimuli and parametric face morph tasks that discriminate between perception and appraisal; task-related BOLD activity will be used to identify relevant circuits. Associations with basic sensory impairment will be tested, including novel auditory mismatch negativity paradigms. Resting state functional connectivity (RSFC) methods will be used for circuit-level analysis of language production and emotion recognition across stages of illness, to determine unique and shared substrates of these constructs in early schizophrenia. If successful, this proposal will identify neural targets for remediation of cognitive impairments. Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9693300,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Chronic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Cognitive remediation', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'EEG-based imaging', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Production', 'Prospective Studies', 'Psychotic Disorders', 'Public Health', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Severities', 'Social Functioning', 'Speech', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'auditory processing', 'automated analysis', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'predictive modeling', 'prevent', 'prognostic tool', 'prognostic value', 'prospective', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual processing', 'young adult']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2019,676659,0.061599684698335426
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9619075,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deaf', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'machine learning algorithm', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2019,318386,0.06958067002054058
"Affective science and smoking cessation: Real time real world assessment Tobacco use plays a causal role in almost 20 different types of cancer, and although smoking cessation is a cornerstone of cancer risk reduction, the vast majority of smoking quit attempts fail. Numerous conceptual models, as well as a large body of empirical evidence, underscore that affect is a potent determinant of smoking lapse. Unfortunately, very little is known about how the constellation and temporal dynamics of distinct emotions and other factors play out in real time in the real world to influence lapse risk. This lack of knowledge severely hampers both our conceptual models and our ability to optimally intervene. Thus, the overarching objectives of this research are to create a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. The proposed research directly addresses several objectives from the PAR including the influence of distinct emotions and their time course on cancer risk behaviors, whether the role of distinct emotions is altered by the presence of other emotions (e.g., “blended” emotional states), and how the influence of affective experience is modified by context. The proposed longitudinal cohort study among 300 smokers attempting to quit is guided by a conceptual framework grounded in affective science and conceptual models of self-regulation and addiction. Participants will be followed from 1 week prior to their quit date through 6 months post-quit date. They will be assessed from 1 week pre-quit date through 2 weeks post-quit date using AutoSense, geographic positioning system (GPS), and ecological momentary assessment (EMA). AutoSense, GPS, and EMA collect real time data in natural environments, communicate wirelessly with each other, and data are processed in real time on a smartphone. AutoSense detects specific behavioral and physiologic “signatures” of smoking (the primary outcome) and self regulatory capacity (an intermediate outcome; assessed using high frequency heart rate variability) in real time. GPS real time spatial tracking will be linked with spatially and temporally relevant characteristics of the environment using geographic information system (GIS) data. EMAs assess self-reported emotions, cognition, and context. Analyses utilize advanced dynamic risk prediction models and machine learning approaches to model the dynamics of real time, real world associations among distinct emotions, SRC, and lapse. Tobacco use is the leading preventable cause of death and disability in the U.S. The proposed study will yield a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop more effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. This knowledge can be utilized to reduce the public health burden of tobacco use.",Affective science and smoking cessation: Real time real world assessment,9627951,R01CA224537,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anger', 'Behavioral', 'Cause of Death', 'Cellular Phone', 'Characteristics', 'Cognition', 'Complement', 'Complex', 'Data', 'Depressed mood', 'Ecological momentary assessment', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Foundations', 'Frequencies', 'Geographic Information Systems', 'Geography', 'Home environment', 'Informal Social Control', 'Intervention', 'Knowledge', 'Link', 'Longitudinal cohort study', 'Machine Learning', 'Measurement', 'Mediating', 'Mediator of activation protein', 'Modeling', 'Outcome', 'Participant', 'Patient Self-Report', 'Physiological', 'Play', 'Positioning Attribute', 'Process', 'Public Health', 'Real-Time Systems', 'Reporting', 'Research', 'Risk', 'Risk Behaviors', 'Risk Reduction', 'Role', 'Science', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Stress', 'System', 'Testing', 'Time', 'Tobacco', 'Tobacco use', 'Volition', 'Wireless Technology', 'adaptive intervention', 'addiction', 'base', 'cancer prevention', 'cancer risk', 'cancer type', 'disability', 'effective intervention', 'experience', 'heart rate variability', 'knowledge base', 'mobile computing', 'negative affect', 'primary outcome', 'risk prediction model', 'role model', 'smoking cessation', 'success']",NCI,UNIVERSITY OF UTAH,R01,2019,600514,0.04673414117981004
"Predicting complicated grief from grief processing PROJECT SUMMARY Most people grieving the loss of a loved one will experience a period of intense pain and focusing on the loss lasting around 6 months, which is known as acute grief. Complicated grief (CG) occurs when the experiences of acute grief extend well past 6-months post-loss. Thoughts and feelings about the loss (i.e. grief processing) occurring during acute grief may play a role in healthy grieving and protect against CG development. Identification of the cognitive and emotional mechanisms of grief processing that contribute to healthy grief resolution would advance knowledge of the goals of grieving and assist the development of interventions for complicated grief. Two core components of grief processing are top-down regulation and balanced loss confrontation. Top-down pursue related emotional representations and recruit proportion regulation is the ability to suppress processing of intrusive emotional information to a stated goal. Top-down regulation may facilitate healthy grieving by allowing reprieve from intense loss thinking. Balanced loss confrontation refers to the processing of the loss in a way that protects against overload. Confrontation with the l oss may assist in the process of reforming one's mental of the deceased. This tudy will test extrinsic and intrinsic measures of top-down regulation balanced loss confrontation during acute grieving as predictors of CG development a year later We will a sample at high-risk for CG, the suicide-bereaved, in order to maximize the likeliness that a significant of the sample develops CG. The s . findings produced by this study may advance the knowledge of how CG develops, assist in the identification of people at high-risk for developing CG and potentially form the basis for targeted interventions.  The following K23 presents a research and training program that will support the applicant on the path of becoming an independent investigator of the role of grief processing in the development of complicated grief. The research mentorship, coursework, hands-on experience, seminars and classes ingrained in this training and plan will propel the applicant to independence in the domains of1) Clinical Research, 2) Psychometric Assessment of Grief Processing, 3) Machine Learning analysis of fMRI, 4) Biostatistics, 5) Scientific Independence. team independent and The combination of the environment, t raining plan, research strategy and mentorship will not only provide the candidate with a spectrum of new methods and skills that will establish him as an research scientist, but will also produce a body of knowledge that will clarify the specific cognitive emotional grief processes that contribute to the development of CG. PROJECT NARRATIVE Complicated grief describes an inability to adjust to the loss of a loved one over the course of the first year following the death. This study will identify cognitive, emotional and neural processes occurring in the early grieving period (3 to 5-months post-loss) that predict or protect against the development of complicated grief a year later in suicide bereaved subjects, a sample at high-risk for developing complicated grief. These findings may advance understanding of the process of grief, facilitate early identification of high-risk grievers and potentially form the basis for targeted treatment of complicated grief.",Predicting complicated grief from grief processing,9686793,K23MH114021,"['Acute', 'Age', 'Attention', 'Biometry', 'Cessation of life', 'Clinical', 'Clinical Research', 'Cognitive', 'Data', 'Depressed mood', 'Development', 'Down-Regulation', 'Early identification', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Family member', 'Feeling', 'Functional Magnetic Resonance Imaging', 'Gender', 'Goals', 'Grief reaction', 'Guilt', 'High Prevalence', 'Individual', 'Instruction', 'Intervention', 'Interview', 'Knowledge', 'Machine Learning', 'Measures', 'Mental Depression', 'Mentorship', 'Methods', 'Modeling', 'Pain', 'Pathogenesis', 'Pattern', 'Play', 'Process', 'Psyche structure', 'Psychometrics', 'Questionnaires', 'Rain', 'Reaction Time', 'Recording of previous events', 'Regulation', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Role', 'Sampling', 'Scientist', 'Severities', 'Shame', 'Stimulus', 'Suicide', 'Techniques', 'Testing', 'Thinking', 'Time', 'Training', 'Training Programs', 'Trauma', 'Unconscious State', 'Validation', 'attentional bias', 'base', 'experience', 'high risk', 'indexing', 'intense pain', 'loved ones', 'neural patterning', 'recruit', 'relating to nervous system', 'response', 'sex', 'skills', 'sustained attention', 'targeted treatment', 'therapy development']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,K23,2019,199800,0.05377112980576799
"Improving measurement of emotional granularity to investigate affective mechanisms of cardiovascular disease and metabolic syndrome PROJECT SUMMARY/ABSTRACT Cardiovascular disease (CVD) and metabolic syndrome are a leading cause of disability and death worldwide. Detrimental shifts in the resting (tonic) contributions of the autonomic nervous system (ANS) to visceral functions throughout the body, a form of compromised allostasis, have been observed with both emotional dysregulation and disordered mood and may be a common, core vulnerability for CVD and metabolic syndrome. A crucial but understudied psychological vulnerability to compromised allostasis is low emotional granularity, or the inability to experience emotion with precision and detail (e.g., the inability to distinguish anger vs. frustration, or even anger vs. sadness). A critical barrier to ameliorating low granularity, and therefore reducing susceptibility to CVD and metabolic syndrome, has been the lack of a theoretical framework linking emotional granularity to physiological regulation, as well as tools for effectively measuring and improving granularity. Theoretical advances in affective science posit that the use of more precise emotion concepts is associated with a peripheral physiological system better able to respond to environmental perturbations (e.g., stressors). If low granularity results from impoverished emotion concepts, then the brain is less able to predict and categorize viscerosensory changes that arise from regulation of the body’s internal milieu. Increased parasympathetic tone at rest permits more efficient regulation by promoting recovery and energy conservation, and substantial evidence suggests that lowered risk profiles for CVD are associated with increased high frequency heart rate variability (HF HRV). The proposed research will use experience sampling and ambulatory monitoring data to map variability in emotional granularity in everyday life and examine its consequences for peripheral physiology, with a focus on resting HF HRV. In Aim 1, machine learning will be used to assess whether individuals lower in emotional granularity have less efficient allostasis, as reflected by both lower resting parasympathetic activity and fewer distinct patterns of ANS activity. In Aim 2, graph theory will be used to develop metrics for measuring temporal and contextual dynamics of emotional granularity, which provide meaningful variance necessary to describe patterns of subjective experience and physiological activity. Exploratory Aim 3 will assess whether in-lab emotion concept training can be used to improve granularity of emotion concepts, with the ultimate goal of developing longer-term training aimed at increasing physiological specificity and resting HF HRV. By integrating modeling from engineering and computer science to better capture idiographic variation in emotion, the proposed research offers an innovative approach for understanding how a psychological vulnerability could increase the risk for cardiovascular illness. The outcomes of this work will allow for the development of psychological interventions that can decrease risk for physical illness by increasing efficient allostasis and encouraging adaptive coping mechanisms. PROJECT NARRATIVE A wide range of emotion-related risk factors for cardiovascular disease and metabolic syndrome have been identified, yet little is known about how the underlying mechanisms that link emotion to physiological regulation of the body can be leveraged to reduce susceptibility. The proposed work addresses this critical barrier by examining how one psychological vulnerability is linked to parasympathetic activation, and can be more effectively measured in patterns of autonomic nervous system activity and subjective experience. By further identifying possible interventions that target this vulnerability, this work seeks to decrease risk for physical illness by increasing efficient allostasis and encouraging adaptive coping mechanisms.",Improving measurement of emotional granularity to investigate affective mechanisms of cardiovascular disease and metabolic syndrome,9769513,F31HL140943,"['Address', 'Affective', 'Alcohol consumption', 'Alexithymias', 'Ambulatory Monitoring', 'Anger', 'Autonomic nervous system', 'Behavioral', 'Binge Eating', 'Brain', 'Cardiovascular Diseases', 'Cessation of life', 'Common Core', 'Data', 'Depressed mood', 'Development', 'Emotional', 'Emotional disorder', 'Emotions', 'Engineering', 'Fellowship', 'Frequencies', 'Frustration', 'Future', 'Goals', 'Hypertension', 'Hypertriglyceridemia', 'Immune system', 'Individual', 'Individual Differences', 'Intervention', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Metabolic', 'Metabolic syndrome', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'Neurosecretory Systems', 'Obesity', 'Outcome', 'Pattern', 'Peripheral', 'Physiological', 'Physiology', 'Population', 'Predisposition', 'Recovery', 'Regulation', 'Reporting', 'Research', 'Rest', 'Risk', 'Risk Factors', 'Sampling', 'Science', 'Specificity', 'System', 'Testing', 'Time', 'Training', 'Variant', 'Visceral', 'Work', 'alcohol risk', 'allostasis', 'cardiovascular risk factor', 'career', 'computer science', 'contextual factors', 'coping', 'coping mechanism', 'disability', 'emotion dysregulation', 'emotion regulation', 'emotional experience', 'experience', 'graph theory', 'heart rate variability', 'improved', 'indexing', 'innovation', 'neuromechanism', 'novel', 'physical conditioning', 'protective factors', 'psychologic', 'stressor', 'theories', 'tool']",NHLBI,NORTHEASTERN UNIVERSITY,F31,2019,38616,0.0993208963044602
"Computational and brain predictors of emotion cue integration The purpose of this project is to develop computational and brain-based models of emotion cue integration: people’s inferences about others’ emotions based on dynamic, multimodal cues. Observers often decide how targets feel based on cues such as facial expressions, prosody, and language. Such inferences scaffold healthy social interaction, and abnormal inference both marks and exacerbates social deficits in numerous psychiatric disorders. Psychologists and neuroscientists have studied emotion inference for decades, but the vast majority of this work employs simplified social cues, such as vignettes or static images of faces. By contrast, “real world” emotion cues are complex, dynamic, and multimodal. Cue integration—inference based on naturalistic emotion information—likely differs from simpler inference at cognitive and neural levels, but this phenomenon remains poorly understood. This means that scientists lack a clear model of how observers adaptively process complex emotion cues, and how that processing goes awry in mental illness. Especially lacking are mechanistic models that can describe the computations and brain processes involved in cue integration with sufficient precision to predict inference in new cases, observers, and samples. This project will merge tools from social psychology, computer science, and neuroscience to generate a novel and rigorous model of emotion cue integration. We have demonstrated that in the face of complex emotion cues, observers dynamically “weight” cues from each modality (e.g., visual, linguistic) over time, a process that (i) tracks shifts in brain activity and connectivity; and (ii) can be captured using Bayesian models. Here, we will expand this work in several ways. First, we will develop precise computational tools to isolate features of emotion cues—such as facial movements, prosody, and linguistic sentiment—that track observers’ use of each cue modality during integration. Second, we will develop multi-region “signatures” of brain activity and connectivity that track emotion inference in each modality. We will use these signatures in conjunction with machine learning to predict unimodal emotion inference and cue integration in new observers and samples, based on brain data alone. Third, we will explore the context-dependence of naturalistic emotion inference by testing whether reinforcement learning can bias observers’ cue integration and accompanying brain signatures. Finally, we will model computational and neural abnormalities associated with cue integration in patients with Major Depressive Disorder and Bipolar Disorder. At the level of basic science, these data will generate a fundamentally new—and more naturalistic—approach to the neuroscience of emotion inference. The computational and brain metrics we produce will also be made publically available to facilitate the open and cumulative study of emotion inference across labs. At a translational level, we will provide a mechanistic, rich account of abnormal emotion inference in mood disorders, paving the way for computational and brain markers that can be used to assess social dysfunction and treatment efficacy in these and other mental illnesses. PROJECT NARRATIVE The proposed research will use methods from social psychology, cognitive neuroscience, and computer science to (i) precisely model people’s inferences about others’ emotions based on complex, dynamic cues, (ii) generate multi-region, brain-based predictors of these inferences, and (iii) characterize abnormalities in inference among individuals with mood disorders. Several psychiatric and neurodevelopmental disorders are characterized by difficulties understanding others’ emotions, which in turn worsen social functioning in patients. In addition to providing new insights about social processes—a core target within the NIMH’s research domain criteria (RDoC) framework—the proposed research will offer powerful, novel computational and neural targets through which to assess and treat difficulties in emotion inference, and to eventually reduce the social burden faced by people with mental illness on a broad scale.",Computational and brain predictors of emotion cue integration,9652845,R01MH112560,"['Affect', 'Agreement', 'Base of the Brain', 'Basic Science', 'Bayesian Modeling', 'Bipolar Disorder', 'Brain', 'Brain region', 'Classification', 'Cognitive', 'Complex', 'Computer Simulation', 'Cues', 'Data', 'Dependence', 'Emotional', 'Emotions', 'Event', 'Exhibits', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Future', 'Image', 'Individual', 'Language', 'Lateral', 'Learning', 'Life', 'Linguistics', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Moods', 'Movement', 'National Institute of Mental Health', 'Neurodevelopmental Disorder', 'Neurosciences', 'Observer Variation', 'Participant', 'Patients', 'Pattern', 'Perception', 'Process', 'Psychological reinforcement', 'Psychologist', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Running', 'Sampling', 'Scanning', 'Scientist', 'Sensory', 'Social Functioning', 'Social Interaction', 'Social Processes', 'Social Psychology', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Visual', 'Weight', 'Work', 'affective computing', 'base', 'brain abnormalities', 'cognitive neuroscience', 'computer science', 'computerized tools', 'executive function', 'insight', 'language comprehension', 'multimodality', 'neuroimaging', 'novel', 'recruit', 'relating to nervous system', 'response', 'scaffold', 'social', 'social deficits', 'tool']",NIMH,STANFORD UNIVERSITY,R01,2019,474391,0.13157813104076466
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9690817,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'multidimensional data', 'multimodal data', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2019,758607,0.06348538773156562
"CRCNS: Optimization of closed-loop control of gamma oscillations Throughout the brain, specialized systems carry out different but complementary functions, sometimes  independently but often in cooperation. However, we do not understand how their activity is dynamically  coordinated, and dysregulation of this is associated with many mental health conditions. Neuronal oscillations, which are detectable in local field potentials (LFPs) at various frequencies, are a promising  target for this coordination. Gamma oscillations (40-100 Hz) in particular have been singled out since they enhance stimulus responses, facilitate interactions between brain regions, and are expressed ubiquitously across cortical and subcortical regions. Indeed, gamma oscillations occur in the basolateral  nucleus of the amygdala (BL), an important regulator of emotional behaviors. BL gamma oscillations are  enhanced during periods of heightened vigilance during a foraging task, following emotionally salient experiences, and upon presentation of socially-relevant stimuli. The variety of circumstances that engage  it make it a promising target for interventions affecting emotional behaviors in general. However, technical challenges abound because gamma manifests as brief intermittent oscillatory bursts, layered atop numerous ongoing activities in other frequency bands. This precludes manipulating gamma exclusively with traditional pharmacological, optogenetic, or chemogenetic approaches, since these have substantial  effects on ongoing non-gamma activities, and are delivered irrespective of whether gamma bursts are present or absent. To overcome this, a closed-loop algorithm was developed that monitors the LFP in  real-time for gamma oscillations and delivers precisely timed optogenetic stimulation capable of enhancing or suppressing gamma strength on a cycle-by-cycle basis. While this improves upon the status  quo,, further refinement is needed. Aim 1 of this proposal seeks to clarify how the gamma modulation technique operates via biophysically detailed modeling of the local circuits in the BL that generate gamma, the effects of optogenetic stimulation, and the closed-loop algorithm. Aim 2 designs better signal  processing routines for detecting and parameterizing gamma in real-time. Aim 3 develops an approach to  create customized biophysical models that reproduce the properties of gamma observed in individual  subjects, which when combined with the results of Aims 1 and 2 should allow for optimized control over gamma oscillations in individual subjects. RELEVANCE (See instructions):  Gamma oscillations occur in the basolateral amygdala, a brain region implicated in emotional regulation.  By developing improved methods to manipulate these oscillations, we hope to better understand their  function and improve our ability to control emotional states and behaviors. n/a",CRCNS: Optimization of closed-loop control of gamma oscillations,9914633,R01MH122023,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Behavior', 'Behavioral', 'Biophysics', 'Brain', 'Brain region', 'Cell Nucleus', 'Cells', 'Collaborations', 'Communication', 'Custom', 'Detection', 'Emotional', 'Frequencies', 'Genetic', 'Goals', 'Implant', 'Individual', 'Individual Differences', 'Instruction', 'Interneurons', 'Intervention', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Neurons', 'Performance', 'Pharmacology', 'Phase', 'Physiological', 'Property', 'Response to stimulus physiology', 'Rodent Model', 'Route', 'Scheme', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Twin Multiple Birth', 'biophysical model', 'cognitive function', 'design', 'emotion regulation', 'emotional behavior', 'experience', 'experimental study', 'improved', 'in vivo', 'individual variation', 'insight', 'neurophysiology', 'novel', 'optogenetics', 'predictive modeling', 'response', 'signal processing', 'social', 'vigilance']",NIMH,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2019,297755,0.05257777699072336
"Mechanisms of Dynamic Neural Coupling during Face-to-Face Expressions of Emotion PROJECT SUMMARY Little is known about the neural mechanisms that regulate natural dynamic cues during human social and emotional interactions, although these mechanisms are impaired in many psychiatric and neurological disorders. Although it is widely understood that social signals such as facial expressions carry salient, but implicit, emotional and social cues, these “real-time” pathways have not been investigated with dual-brain neuroimaging techniques. This unmet need is largely due to technological limitations that prevent neuroimaging of two or more individuals during natural interactive situations. We overcome this technical “roadblock” with recent advances in an emerging human brain imaging technology, functional near-infrared spectroscopy (fNIRS). This non-invasive technique detects active neural tissue based on hemodynamic signals measured by variations in the absorption spectra associated with oxyhemoglobin and de-oxyhemoglobin. Because detectors and emitters are surface mounted on the head, absent a high magnetic field, they are relatively insensitive to head movement and thus successfully applied to dyadic experiments. The focus of this proposal is to gain a comprehensive understanding of the mechanisms that underlie dynamic cross-brain neural coupling during real interpersonal interactions. Cross-brain neural coupling is defined as the correlation between the temporal patterns of the signals of two brains. It has been proposed that these matched patterns represent shared neural processes including dynamic exchanges of information. However, the basic assumptions of shared information and temporal resonance patterns between specific brain-to-brain regions has not been tested. We pioneer tests of these hypothesis using eye-to-eye contact as a metric of shared information and predict that dynamic neural coupling between the two brains will increase with increasing numbers of eye-to-eye contact events. Mimicry of facial expressions is also a metric of emotional contagion as well as shared information between brains. We further test the hypothesis that neural coupling will increase with the level of mimicry also by virtue of the shared information Confirmation of the hypothesis that neural coupling represents shared information between the two brains would provide a singular advance for understanding mechanisms for dynamic interactions. Both approaches include variations of emotive expressions to test the additional hypothesis that content as well as shared information might influence dynamic coupling mechanisms. Findings from these studies are expected to open a new direction for the study of live and dynamic interactions between individuals, and provide foundational components to a general framework for models of face-to-face interactions. A long-term goal is to understand the neural underpinnings of affective disorders as they present in clinically-relevant and real-world situations. Project Narrative How do two brains interact? We pioneer a novel neuroimaging technology, near infrared spectroscopy, to investigate basic mechanisms of neural coupling between two dynamically interacting individuals. Using these new methods we test the fundamental hypothesis that neural coupling (temporal synchrony between two brains) represents shared information across the two brains.",Mechanisms of Dynamic Neural Coupling during Face-to-Face Expressions of Emotion,9711359,R01MH119430,"['Affective', 'Arousal', 'Brain', 'Brain imaging', 'Brain region', 'Classification', 'Code', 'Communication', 'Coupled', 'Coupling', 'Cues', 'Detection', 'Development', 'Elements', 'Emotional', 'Event', 'Eye', 'Eyebrow structure', 'Face', 'Face Processing', 'Facial Expression', 'Feeling', 'Foundations', 'Goals', 'Head', 'Head Movements', 'Human', 'Imaging technology', 'Impairment', 'Individual', 'International', 'Interpersonal Relations', 'Machine Learning', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Mood Disorders', 'Near-Infrared Spectroscopy', 'Negative Valence', 'Oral cavity', 'Oxyhemoglobin', 'Participant', 'Pathway interactions', 'Pattern', 'Positioning Attribute', 'Positive Valence', 'Process', 'Shapes', 'Signal Transduction', 'Smiling', 'Social Interaction', 'Stimulus', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Variant', 'absorption', 'analog', 'base', 'clinically relevant', 'contagion', 'detector', 'dyadic interaction', 'emotional reaction', 'experimental study', 'hemodynamics', 'magnetic field', 'mimicry', 'nervous system disorder', 'neural correlate', 'neuroimaging', 'neuromechanism', 'novel', 'prevent', 'relating to nervous system', 'response', 'showing emotion', 'social']",NIMH,YALE UNIVERSITY,R01,2019,405576,0.04929243783582008
"Dynamics of Large-Scale Networks During Emotional and Social Processing The goal of this application is to develop a research program that, as stated in the call entitled The Neural Mechanisms of Multi-Dimensional Emotional and Social Representation (RFA-MH- 17-300), incorporates innovative approaches designed to move the fields of affective and social neuroscience beyond single region-based, modular, and static models of brain function and behavior. The RFA calls for research that is multi-dimensional, that is, that investigates the role of (among others) complex contexts, as well as distributed and/or dynamic processes that unfold over time. The objective of the present application is to jointly investigate emotional and social processes in a richly multi-dimensional manner. Aim 1: Network organization and evolution during emotional and social processing. The objective of this aim is to uncover how large-scale brain networks are organized and evolve temporally during emotional and social processing. Networks will include brain regions that robustly respond to the tasks proposed, and regions from well characterized networks, including the salience, executive control, and task-negative networks. Aim 2: Naturalistic processing during emotional and social processing. The objective of this aim is to understand naturalistic processing of emotional and social information. Although standard experimental designs afford great control over experimental conditions, they lack ecological validity and restrict the experiments that can be studied. We propose to investigate continuous (“naturalistic”) processing during movie watching involving emotional and social content. Continuous processing will be investigated via intersubject correlation analysis, which measures the extent to which signals are correlated across participants. Aim 3: Development of network organization/evolution and naturalistic processing. The objective of this aim is to investigate multi-dimensional emotional and social processes from a developmental perspective. Most developmental research in emotion has focused on observing amygdala responses and those of a few other brain regions during face perception. We focus on a question largely neglected in prior research, specifically sustained threat processing and the involvement of the bed nucleus of the stria terminalis. In the context of social processing, this aim examines the development of intersubject synchrony. Across the emotional and social domains, we propose to study middle childhood (8-9 y), early adolescence (12-13 y), and young adulthood (18-19 y). The goal of this application is to investigate the neural mechanisms of multi-dimensional emotional and social representation. Functional MRI experiments are proposed to investigate the following general questions: organization and evolution of brain networks; naturalistic processing during movie watching; and development.",Dynamics of Large-Scale Networks During Emotional and Social Processing,9664686,R01MH112517,"['Adolescence', 'Adopted', 'Affective', 'Amygdaloid structure', 'Anxiety', 'Behavior', 'Brain', 'Brain region', 'Complex', 'Data', 'Development', 'Developmental Process', 'Disease', 'Emotional', 'Emotions', 'Evolution', 'Experimental Designs', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Individual Differences', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Participant', 'Pathway Analysis', 'Persons', 'Process', 'Property', 'Research', 'Risk', 'Role', 'Series', 'Signal Transduction', 'Social Development', 'Social Environment', 'Social Interaction', 'Social Processes', 'Stimulus', 'Structure', 'Structure of terminal stria nuclei of preoptic region', 'Testing', 'Time', 'Work', 'affective neuroscience', 'age group', 'autism spectrum disorder', 'base', 'design', 'early adolescence', 'executive function', 'experimental study', 'face perception', 'graph theory', 'improved', 'innovation', 'middle childhood', 'movie', 'neglect', 'neurobiological mechanism', 'neuromechanism', 'novel', 'peer', 'positive mood', 'programs', 'relating to nervous system', 'response', 'social', 'social neuroscience', 'young adult']",NIMH,"UNIV OF MARYLAND, COLLEGE PARK",R01,2019,674876,0.10500034482042042
"Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease PROJECT ABSTRACT/SUMMARY The prevalence of psychiatric disorders has reached nearly epidemic proportions. Rates of common affective diseases (unipolar depression, anxiety and stress disorders) are high across the lifespan and these diseases place a tremendous social and economic burden on the individual and society. Clear evidence indicates that most affective disorders emerge at the intersection of pre-existing vulnerability and significant, highly stressful, life-events. However, current models of emotion-related risk do not adequately account for this confluence of biological, historical, and situational factors. In this investigation, we build upon our prior work demonstrating broad associations between flexible emotion processing and psychological health and adjustment, and in-flexible emotion and psychological risk and affective disease. Specifically, we will recruit 400 adults in hospital following a potentially traumatic event (e.g., accident, violence, fire, etc.) in order to model the influence of early emotion processing on trajectories of adjustment. We focus our investigation on the super-ordinate construct of Emotion flexibility (EF) which encompasses the ability to generate or up-regulate emotions, as well as to shift or down-regulate emotions according to needs and/or environmental demands. EF is well-suited to inform models of emotion-related risk and adjustment as it characterizes an optimal balance of two biologically-based, constituent dimensions: “bottom-up” threat-related processing and “top-down” cognitive control increasingly recognized as central to all emotion processing. We propose rigorous methods to assess EF and related processing in-vivo in lab and via experience sampling. Moreover, we will follow participants to 18 months post event so as to effectively model the association between emotion processing and trajectories of adjustment, while also considering established influences such as physical health status, psychiatric history, childhood maltreatment, daily stress/hassles, and social support. In particular, we will incorporate recent developments in advanced statistical modelling to better characterize the complex and interactive influence of historical and contemporary factors on moment-level emotion processing, EF and adjustment. Broadly, this project is in line with the most recent NIMH strategic plan and will contribute to more complex models of the most common affective diseases, including facilitating the charting of illness trajectories to help determine when, where, and how to intervene. Moreover, this research will directly examine how variation in key systems can influence emotion-processing and adjustment to aversive life events, fitting complex influences more directly into models of risk for the most common and burdensome affective diseases. PUBLIC HEALTH RELEVANCE/NARRATIVE Emotion-related psychiatric disorders, including depression and anxiety, affect a considerable portion of adults in this country and rank as many of the most burdensome diseases worldwide. In this investigation, we will follow an at-risk sample of adults in order to better understand how one key pathway, relating to how individuals process emotion, influences risk for emotion-related diseases over time. In addition, we test the role by which certain other factors, both contemporary and historical (physical health, life stress, social support, psychiatric treatment history, or childhood experiences) may increase or decrease risk via this particular pathway.",Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease,9675331,R01MH113622,"['Accidents', 'Adult', 'Affect', 'Affective', 'Anxiety', 'Anxiety Disorders', 'Behavioral', 'Biological', 'Categories', 'Child Abuse and Neglect', 'Childhood', 'Clinical', 'Clinical Sciences', 'Complex', 'Country', 'Derivation procedure', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Economic Burden', 'Emotions', 'Epidemic', 'Equilibrium', 'Event', 'Fire - disasters', 'Health', 'Health Status', 'Heritability', 'Hospitals', 'Individual', 'Inherited', 'Investigation', 'Life', 'Life Stress', 'Longevity', 'Machine Learning', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Participant', 'Pathway interactions', 'Patients', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Prevalence', 'Process', 'Psychiatric therapeutic procedure', 'Psychological adjustment', 'Qualifying', 'Recording of previous events', 'Regulation', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Adjustment', 'Risk Factors', 'Role', 'Sampling', 'Shapes', 'Sleep disturbances', 'Social support', 'Societies', 'Statistical Models', 'Strategic Planning', 'Stress', 'Symptoms', 'System', 'Testing', 'Time', 'Unipolar Depression', 'Variant', 'Violence', 'Work', 'base', 'childhood adversity', 'clinical diagnostics', 'clinically relevant', 'cognitive control', 'emotion regulation', 'experience', 'flexibility', 'improved', 'in vivo', 'indexing', 'laboratory experience', 'learning strategy', 'longitudinal design', 'negative mood', 'network models', 'physical conditioning', 'post-traumatic stress', 'prospective', 'psychologic', 'public health relevance', 'recruit', 'response', 'social', 'stress disorder', 'tool', 'traumatic event']",NIMH,KENT STATE UNIVERSITY,R01,2019,569073,0.1142494328799957
"Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues. People signal their internal emotional state by a range of cues including facial expressions, non-verbal vocalizations and the tone and content of speech. Much work to date on emotional signaling has focused on a small number of emotions and has relied on stimulus sets with limited numbers of exemplars and poor representation of individuals of different ages and ethnicities. The computer vision literature has long recognized the need to be able to compare models, for example of face recognition, across complex, diverse, and naturalistic stimulus sets. Here, we argue that a parallel approach needs to be applied if we are to achieve a robust and generalizable understanding of how the human brain represents emotion and identity related information derived from facial and vocal cues. Three multi-session functional magnetic resonance imaging (fMRI) experiments are proposed. In the first, participants will view a large corpus (~2000) of faces of individuals varying in age, gender and ethnicity and showing a wide range of emotional expressions. In the second, participants will be presented with a large (~1000) and equally diverse set of emotional vocalizations. The third experiment will make use of an even more complex and naturalistic stimulus set comprising ~1000 video clips of individuals expressing emotions through facial expression, non-verbal vocalizations and emotion-laden speech. This set will be broken into three parts, with as closely matched content as possible. These will be presented to participants in audio only, visual only and audio-visual (bimodal) conditions, with the stimuli allocated to each condition balanced across participants. For each experiment, data collection will comprise separate Model Estimation and Model Validation periods with the majority of stimuli presented at Validation being distinct from those presented at Estimation. A range of models will be fit to the fMRI data acquired during Estimation runs and tested and compared using data acquired during Validation runs. These models contain sets of features that describe the emotional content of the stimuli presented in terms of either dimensional or categorical models of emotion. By comparing the fit of these models we can determine which models capture most variance in voxel response profiles and examine how this varies across brain regions. Across the three experiments, we also seek to establish whether there are regions where voxels show common coding of emotional state regardless of whether information is carried by facial or vocal cues or a combination of both. Further, by contrasting models with and without terms for characteristics such as age, gender and ethnicity we can also investigate the (in)dependence of the representation of emotion and identity-related features. The proposed research has the potential to greatly advance our understanding of how cues to others' emotional state are represented in the human brain and the extent to which this is influenced by the characteristics of the person we are interacting with. In the medium term, we plan to extend this to also model listener/ viewer characteristics (both demographics and predisposition to anxiety or depression) in a hope to advance our understanding of how biases in the interpretation of emotional signals can arise. Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial  and vocal cues. Individuals with psychiatric conditions ranging from Social Anxiety and Depression to Autism Spectrum Disorder and Schizophrenia struggle to correctly interpret others' emotional states. To understand how alterations in brain function can give rise to the misperception of emotion cues, we first need better normative models of how information about others' emotional state derived from facial and vocal cues is represented within the human brain. Hence, here we use large, diverse and naturalistic stimulus sets encompassing facial expressions, emotional vocalizations and multi-modal videos to investigate the extent to which consistent representation of emotional state is observed across stimulus modalities, and to explore the organizational principles that can best explain voxel response profiles or tuning patterns to facial and vocal emotion cues.",Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues,9731295,R01MH112541,"['Address', 'Age', 'Anxiety', 'Arousal', 'Auditory', 'Brain', 'Brain region', 'Categories', 'Characteristics', 'Clip', 'Code', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Data Collection', 'Data Set', 'Decision Making', 'Dependence', 'Dimensions', 'Emotional', 'Emotions', 'Ethnic Origin', 'Expressed Emotion', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Gender', 'Human', 'Image', 'Individual', 'Label', 'Literature', 'Mental Depression', 'Mental disorders', 'Modality', 'Modeling', 'Participant', 'Pattern', 'Persons', 'Predisposition', 'Property', 'Research', 'Running', 'Schizophrenia', 'Signal Transduction', 'Source', 'Speech', 'Stimulus', 'Structure', 'Techniques', 'Testing', 'Validation', 'Visual', 'Weight', 'Work', 'autism spectrum disorder', 'blood oxygen level dependent', 'data acquisition', 'data modeling', 'demographics', 'experimental study', 'imaging study', 'information model', 'multimodality', 'neural model', 'novel', 'relating to nervous system', 'response', 'showing emotion', 'social anxiety', 'theories', 'trait', 'visual information', 'visual neuroscience', 'vocalization']",NIMH,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2019,125000,0.17660734439641768
"Neurophysiology underlying neural representations of value A range of behavioral, physiological, and cognitive responses (e.g. approach and avoidance, autonomic reactivity, and subjective feelings) reflects a subject's emotional state. The cognitive regulation of emotion refers to the capacity to regulate these emotional responses in a flexible manner according to a cognitive operation. Deficits in the cognitive regulation of emotional processes characterize many psychiatric disorders. In everyday life, however, particular sensory stimuli and/or actions can elicit different emotional responses depending upon the situation or context. Contexts often rely on a cognitive understanding of one's current situation in the absence of explicit cues. These types of contexts may be referred to as “abstract” contexts. This grant studies a type of abstract context where the context is determined by a task set. A task set is the set of stimulus- response-outcome mappings (or rules) that dictate correct performance for trials within a particular block. Previous research demonstrates the capacity of primates to learn these abstract contexts, and neural representations of abstract contexts exist in the amygdala and two areas in the prefrontal cortex (PFC), the anterior cingulate and orbitofrontal cortices (ACC and OFC). This grant seeks to understand the mechanisms that underlie the formation and maintenance of these representations of contexts. In contrast to supervised learning driven by error signals, we hypothesize that the occurrence of temporally associated trial types triggers unsupervised learning, presumably through a Hebbian mechanism involving activity-dependent plasticity. This learning could underlie formation of representations of abstract contexts defined by task sets, which will be explored with electrophysiological recordings in Aim 1. The creation of a representation of a task set requires combining information about the current trial with information about the trials that have occurred recently. Brain structures that provide memory traces of recent events and/or that combine information over time could create representations of a task set prior to the emergence of the representations observed in amygdala, OFC, and ACC. Our next experiments therefore target the hippocampus and dorsolateral PFC (DLPFC), which are implicated in memory processes, working memory, and executive functions. We will compare and contrast the encoding of task sets in hippocampus, DLPFC, OFC, and ACC during and after learning about task sets (Aim 2). Finally, we will use causal methods to determine if PFC input to the amygdala and the hippocampus acts to maintain these context representations, which could be a vital mechanism for the cognitive regulation of emotion (Aim 3). Overall, these experiments promise to illuminate neurophysiological mechanisms critical for normal adaptive emotional health. ! The aim of this proposal is to understand brain mechanisms responsible for the cognitive regulation of emotion. Since many psychiatric disorders, like anxiety and mood disorders as well as schizophrenia, autism, and addiction, involve cognitive dysfunction mediated by neural circuits in the brain areas under study, this project promises to provide new insights about neural network function critical for developing new treatments.",Neurophysiology underlying neural representations of value,9618883,R01MH082017,"['Amygdaloid structure', 'Anterior', 'Anxiety Disorders', 'Area', 'Behavioral', 'Brain', 'Cognitive', 'Cues', 'Development', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event', 'Feeling', 'Grant', 'Health', 'Hippocampus (Brain)', 'Impaired cognition', 'Knowledge', 'Lead', 'Learning', 'Life', 'Maintenance', 'Mediating', 'Memory', 'Mental disorders', 'Methods', 'Monkeys', 'Mood Disorders', 'Outcome', 'Perception', 'Performance', 'Physiological', 'Play', 'Prefrontal Cortex', 'Primates', 'Process', 'Psychological reinforcement', 'Research', 'Response to stimulus physiology', 'Reversal Learning', 'Rewards', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Signal Transduction', 'Stimulus', 'Structure', 'Task Performances', 'Testing', 'Time', 'Update', 'Work', 'addiction', 'autism spectrum disorder', 'cognitive control', 'cognitive process', 'cognitive reappraisal', 'emotion regulation', 'emotional experience', 'executive function', 'expectation', 'experimental study', 'flexibility', 'insight', 'memory process', 'neural circuit', 'neural network', 'neurophysiology', 'operation', 'optogenetics', 'prevent', 'relating to nervous system', 'response', 'sensory stimulus', 'statistics', 'supervised learning', 'unsupervised learning']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2019,720901,0.05840088934701091
"QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development   Children with autism struggle to recognize facial expressions, make eye contact, and engage in social interactions. Many of these children can have dramatic recoveries, particularly if social skills are taught from an early age. However, in today's healthcare system the delivery of the behavioral intervention is bottlenecked by a sharp and increasing imbalance in the number of behavioral therapists and the number of children in need of care. As such, there is an urgent need to develop mobilized methods of care delivery. We have developed an artificial intelligence tool for automatic facial expression recognition that runs on Google Glass through an Android app and delivers instantaneous social cues to individuals with autism in their natural environment, providing therapy that today is given only by clinicians in non-scalable person-to- person sessions. The system leverages Glass's outward facing camera to read a person's facial expressions and passes facial landmarks to an Android native app for immediate machine learning-based emotion classification. The system then gives the child wearer real-time social cues and records social responses. We believe that the system's ability to provide continuous behavioral therapy outside of clinical settings will enable dramatically faster gains in social acuity that will, within a limited and self-directed period of use, permit the child to engage in social settings on his/her own. This proposal outlines three main aims needed to test, refine and optimize the tools and a series of validation experiments needed to bring our system from prototype to a viable clinical tool that every family can use regularly from home for precision healthcare. RELEVANCE {See instructions): We have developed a combined software-hardware solution built on top of Google Glass that enables real lime expression recognition and field-of-view eye tracking to capture social interaction data and give guiding social cues to an individual with Autism. This project will test and optimize the potential of this tool to provide continuous and naturalized behavioral therapy to children with autism from their homes. n/a",QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development  ,9523415,R01EB025025,"['Address', 'Advertising', 'Affective', 'Age', 'Android', 'Artificial Intelligence', 'Augmented Reality', 'Autistic Disorder', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Benchmarking', 'Big Data', 'Biomedical Engineering', 'Caregivers', 'Caring', 'Child', 'Child Development', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Cues', 'Data', 'Databases', 'Diagnosis', 'Emotions', 'Environment', 'Eye', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Family', 'Family member', 'Feedback', 'Future', 'Glass', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Home environment', 'Human', 'Image', 'Imagery', 'Incidence', 'Individual', 'Instruction', 'Internet', 'Intervention', 'Label', 'Learning', 'Lighting', 'Limes', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Motivation', 'Outcome', 'Parents', 'Participant', 'Persons', 'Precision Health', 'Procedures', 'Records', 'Recovery', 'Running', 'Self-Direction', 'Series', 'Social Interaction', 'Social Marketing', 'Structure', 'System', 'Testing', 'Therapeutic', 'Time', 'Training', 'Update', 'Validation', 'Variant', 'Work', 'adaptive learning', 'applied behavior analysis', 'base', 'care delivery', 'deep learning', 'design', 'empowered', 'experimental study', 'human-in-the-loop', 'improved', 'mobile computing', 'novel', 'prototype', 'response', 'smartphone Application', 'social', 'social engagement', 'social learning', 'social skills', 'standard care', 'tool']",NIBIB,STANFORD UNIVERSITY,R01,2018,384860,0.045908541834206826
"Thought disorder and social cognition in clinical risk states for schizophrenia Project Summary  In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decade, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a wide range of cognitive processes to try to identify core deficits of schizophrenia evident before psychosis onset. Subthreshold thought disorder and impaired emotion recognition have emerged as profound deficits that predate, rather than follow, psychosis onset and thus may be indicators of schizophrenia liability, consistent with studies in other risk cohorts, including genetic high risk. Further, subthreshold thought disorder and emotion recognition deficit are significantly correlated, suggesting shared neural substrates in temporoparietal regions.  This study aims to identify the neural mechanisms that underlie subthreshold thought disorder and emotion recognition deficit in 125 CHR individuals followed prospectively for psychosis outcome. CHR cohorts are enriched with early cases of schizophrenia, as 20-25% develop schizophrenia and related psychotic disorders within 1-2 years. CHR cohorts may be optimal for studying core characteristics of illness as they otherwise have low-level symptoms, less illness chronicity and minimum exposure to antipsychotics. 25 individuals with schizophrenia and 50 healthy volunteers are included for comparison.  Subthreshold thought disorder and emotion recognition deficits will be studied across behavioral, physiological and circuit levels. For thought disorder, we will use automated speech analysis approaches developed in collaboration with IBM to identify constituent impairments in semantics and syntax, and a listening task that elicits reliable activation in language circuits. Our automated machine-learning approach to speech analysis, informed by artificial intelligence, derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to what they read or hear. Emotion recognition will be measured using standard tasks, naturalistic tasks with dynamic face stimuli and parametric face morph tasks that discriminate between perception and appraisal; task-related BOLD activity will be used to identify relevant circuits. Associations with basic sensory impairment will be tested, including novel auditory mismatch negativity paradigms. Resting state functional connectivity (RSFC) methods will be used for circuit-level analysis of language production and emotion recognition across stages of illness, to determine unique and shared substrates of these constructs in early schizophrenia. If successful, this proposal will identify neural targets for remediation of cognitive impairments. Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9481191,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Chronic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Cognitive remediation', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'EEG-based imaging', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Production', 'Prospective Studies', 'Psychotic Disorders', 'Public Health', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Severities', 'Social Functioning', 'Speech', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'auditory processing', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'predictive modeling', 'prevent', 'prognostic tool', 'prognostic value', 'prospective', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual processing', 'young adult']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2018,616434,0.061599684698335426
"Craniofacial Microsmia: Facial Expression from Ages 1 to 3 Years Project Summary Significance. Craniofacial microsomia (CFM) impairs facial muscle movement, speech, and hearing, and compromises socio-emotional development. Children with CFM have elevated levels of internalizing behavior (shy, withdrawn) and reduced social competence and peer acceptance. Unknown at present are the mechanisms through which CFM and these social-emotional outcomes become linked. Facial asymmetries and cranial neuropathies associated with CFM likely play an important role in impairing socio-emotional outcomes. Asymmetries of the facial skeleton, soft tissue, and cranial nerve have both intra- and interpersonal effects. Intra-personally, they impact function (unilateral hearing loss, malocclusion, facial expressiveness) and form (noticeable craniofacial malformations), which can impair social signaling and responsiveness. Because asymmetry is negatively correlated with attractiveness, there may be non-specific social effects as well. Many surgical treatments for CFM are designed to restore facial symmetry in static pose (e.g., neutral expression). Less is known about restoring or even measuring spontaneous facial expressiveness. From a developmental perspective, one of the most important consequences of limitations in facial muscle movement is its potentially negative impact on affective communication. In a longitudinal design, we propose to test the hypothesis that deficits in facial expressiveness and structural and functional asymmetry increase risk for internalizing and externalizing problems. If supported, the findings would inform our understanding of socio-emotional development in children with CFM and contribute to clinical evaluation and treatment. Innovation. This is the first effort to 1) use automated, objective measurement of facial expressiveness of communicative behavior and functional asymmetry of children with CFM; 2) model change with development in these parameters and their relation to internalizing and externalizing problems; and 3) use machine learning to investigate the relation among dynamics of expressiveness and asymmetry in relation to CBCL. Approach. Children with and without CFM will be video-recorded at 1 and 3 years with an examiner. Age 1 is an interactive context intended to elicit positive and negative emotion. Age 3 is an interactive context to assess expressive speech and attention. Expressiveness and structural and functional asymmetry are assessed using automatic, objective computer-vision based measurement. Analyses include complementary approaches: statistical (regression and ANOVA) hypothesis testing and machine learning (convolutional neural networks). Relevance Using objective, automatic computer-vision-based measurements, we propose to test the hypothesis that deficits in facial expressiveness and structural and functional asymmetry among children with CFM increases their risk for internalizing and externalizing problems. If supported, clinical assessments of expressiveness and functional asymmetry could effectively target children for specialized interventions and be used to evaluate surgical interventions. Because the proposed procedures are cost effective, they could be applied in a wide range of settings to benefit children with craniofacial disorders and have applicability to other conditions and age groups in which facial expression is compromised (e.g., Mobius Syndrome, Bell's Palsy, injury/burns, and stroke). Project Narrative Craniofacial microsomia (CFM) is among the most common craniofacial anomalies. CFM impairs facial muscle movement, speech, and hearing, and compromises socio-emotional development. Using computer-vision based face analysis, statistical analysis (regression and ANOVA), and machine learning, we will investigate the relation among expressiveness, structural and functional asymmetry, and behavior problems in children with CFM.",Craniofacial Microsmia: Facial Expression from Ages 1 to 3 Years,9525940,R03DE026513,"['1 year old', '3 year old', 'Affective', 'Age', 'Analysis of Variance', 'Articulation', 'Attention', 'Behavior', 'Bell Palsy', 'Biological Neural Networks', 'Burn injury', 'Child', 'Child Behavior Checklist', 'Clinical Treatment', 'Clinical assessments', 'Communication', 'Computer Vision Systems', 'Cranial Nerves', 'Cranial nerve diseases', 'Craniofacial Abnormalities', 'Development', 'Emotional', 'Emotions', 'Face', 'Facial Expression', 'Facial Muscles', 'Facial asymmetry', 'Funding', 'Hearing', 'Impairment', 'Intervention', 'Linear Regressions', 'Link', 'Machine Learning', 'Malocclusion', 'Measurement', 'Measures', 'Mobius Syndrome', 'Modeling', 'Movement', 'National Institute of Dental and Craniofacial Research', 'Operative Surgical Procedures', 'Outcome', 'Participant', 'Play', 'Problem behavior', 'Regression Analysis', 'Risk', 'Role', 'Sampling', 'Signal Transduction', 'Skeleton', 'Speech', 'Statistical Data Interpretation', 'Stroke', 'Testing', 'Time', 'Training', 'Unilateral Hearing Loss', 'Validation', 'Video Recording', 'age group', 'base', 'behavioral outcome', 'cost effective', 'craniofacial', 'craniofacial disorder', 'craniofacial microsomia', 'design', 'innovation', 'longitudinal analysis', 'longitudinal design', 'malformation', 'outcome prediction', 'parent grant', 'peer', 'procedure cost', 'research clinical testing', 'social', 'social skills', 'soft tissue']",NIDCR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R03,2018,154777,0.085343060618587
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9402599,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2018,318898,0.06958067002054058
"Predicting complicated grief from grief processing PROJECT SUMMARY Most people grieving the loss of a loved one will experience a period of intense pain and focusing on the loss lasting around 6 months, which is known as acute grief. Complicated grief (CG) occurs when the experiences of acute grief extend well past 6-months post-loss. Thoughts and feelings about the loss (i.e. grief processing) occurring during acute grief may play a role in healthy grieving and protect against CG development. Identification of the cognitive and emotional mechanisms of grief processing that contribute to healthy grief resolution would advance knowledge of the goals of grieving and assist the development of interventions for complicated grief. Two core components of grief processing are top-down regulation and balanced loss confrontation. Top-down pursue related emotional representations and recruit proportion regulation is the ability to suppress processing of intrusive emotional information to a stated goal. Top-down regulation may facilitate healthy grieving by allowing reprieve from intense loss thinking. Balanced loss confrontation refers to the processing of the loss in a way that protects against overload. Confrontation with the l oss may assist in the process of reforming one's mental of the deceased. This tudy will test extrinsic and intrinsic measures of top-down regulation balanced loss confrontation during acute grieving as predictors of CG development a year later We will a sample at high-risk for CG, the suicide-bereaved, in order to maximize the likeliness that a significant of the sample develops CG. The s . findings produced by this study may advance the knowledge of how CG develops, assist in the identification of people at high-risk for developing CG and potentially form the basis for targeted interventions.  The following K23 presents a research and training program that will support the applicant on the path of becoming an independent investigator of the role of grief processing in the development of complicated grief. The research mentorship, coursework, hands-on experience, seminars and classes ingrained in this training and plan will propel the applicant to independence in the domains of1) Clinical Research, 2) Psychometric Assessment of Grief Processing, 3) Machine Learning analysis of fMRI, 4) Biostatistics, 5) Scientific Independence. team independent and The combination of the environment, t raining plan, research strategy and mentorship will not only provide the candidate with a spectrum of new methods and skills that will establish him as an research scientist, but will also produce a body of knowledge that will clarify the specific cognitive emotional grief processes that contribute to the development of CG. PROJECT NARRATIVE Complicated grief describes an inability to adjust to the loss of a loved one over the course of the first year following the death. This study will identify cognitive, emotional and neural processes occurring in the early grieving period (3 to 5-months post-loss) that predict or protect against the development of complicated grief a year later in suicide bereaved subjects, a sample at high-risk for developing complicated grief. These findings may advance understanding of the process of grief, facilitate early identification of high-risk grievers and potentially form the basis for targeted treatment of complicated grief.",Predicting complicated grief from grief processing,9527376,K23MH114021,"['Acute', 'Age', 'Attention', 'Biometry', 'Cessation of life', 'Clinical', 'Clinical Research', 'Cognitive', 'Data', 'Depressed mood', 'Development', 'Down-Regulation', 'Early identification', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Family member', 'Feeling', 'Functional Magnetic Resonance Imaging', 'Gender', 'Goals', 'Grief reaction', 'Guilt', 'High Prevalence', 'Individual', 'Instruction', 'Intervention', 'Interview', 'Knowledge', 'Machine Learning', 'Measures', 'Mental Depression', 'Mentorship', 'Methods', 'Modeling', 'Pain', 'Pathogenesis', 'Pattern', 'Play', 'Process', 'Psyche structure', 'Psychometrics', 'Questionnaires', 'Rain', 'Reaction Time', 'Recording of previous events', 'Regulation', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Role', 'Sampling', 'Scientist', 'Severities', 'Shame', 'Stimulus', 'Suicide', 'Techniques', 'Testing', 'Thinking', 'Time', 'Training', 'Training Programs', 'Trauma', 'Unconscious State', 'Validation', 'attentional bias', 'base', 'experience', 'high risk', 'indexing', 'intense pain', 'loved ones', 'neural patterning', 'recruit', 'relating to nervous system', 'response', 'sex', 'skills', 'sustained attention', 'targeted treatment', 'therapy development']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,K23,2018,199797,0.05377112980576799
"Affective science and smoking cessation: Real time real world assessment Tobacco use plays a causal role in almost 20 different types of cancer, and although smoking cessation is a cornerstone of cancer risk reduction, the vast majority of smoking quit attempts fail. Numerous conceptual models, as well as a large body of empirical evidence, underscore that affect is a potent determinant of smoking lapse. Unfortunately, very little is known about how the constellation and temporal dynamics of distinct emotions and other factors play out in real time in the real world to influence lapse risk. This lack of knowledge severely hampers both our conceptual models and our ability to optimally intervene. Thus, the overarching objectives of this research are to create a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. The proposed research directly addresses several objectives from the PAR including the influence of distinct emotions and their time course on cancer risk behaviors, whether the role of distinct emotions is altered by the presence of other emotions (e.g., “blended” emotional states), and how the influence of affective experience is modified by context. The proposed longitudinal cohort study among 300 smokers attempting to quit is guided by a conceptual framework grounded in affective science and conceptual models of self-regulation and addiction. Participants will be followed from 1 week prior to their quit date through 6 months post-quit date. They will be assessed from 1 week pre-quit date through 2 weeks post-quit date using AutoSense, geographic positioning system (GPS), and ecological momentary assessment (EMA). AutoSense, GPS, and EMA collect real time data in natural environments, communicate wirelessly with each other, and data are processed in real time on a smartphone. AutoSense detects specific behavioral and physiologic “signatures” of smoking (the primary outcome) and self regulatory capacity (an intermediate outcome; assessed using high frequency heart rate variability) in real time. GPS real time spatial tracking will be linked with spatially and temporally relevant characteristics of the environment using geographic information system (GIS) data. EMAs assess self-reported emotions, cognition, and context. Analyses utilize advanced dynamic risk prediction models and machine learning approaches to model the dynamics of real time, real world associations among distinct emotions, SRC, and lapse. Tobacco use is the leading preventable cause of death and disability in the U.S. The proposed study will yield a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop more effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. This knowledge can be utilized to reduce the public health burden of tobacco use.",Affective science and smoking cessation: Real time real world assessment,9471172,R01CA224537,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anger', 'Behavioral', 'Cause of Death', 'Cellular Phone', 'Characteristics', 'Cognition', 'Complement', 'Complex', 'Data', 'Depressed mood', 'Ecological momentary assessment', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Foundations', 'Frequencies', 'Geographic Information Systems', 'Geography', 'Home environment', 'Informal Social Control', 'Intervention', 'Knowledge', 'Link', 'Longitudinal cohort study', 'Machine Learning', 'Measurement', 'Mediating', 'Mediator of activation protein', 'Modeling', 'Outcome', 'Participant', 'Patient Self-Report', 'Physiological', 'Play', 'Positioning Attribute', 'Process', 'Public Health', 'Real-Time Systems', 'Reporting', 'Research', 'Risk', 'Risk Behaviors', 'Risk Reduction', 'Role', 'Science', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Stress', 'System', 'Testing', 'Time', 'Tobacco', 'Tobacco use', 'Volition', 'Wireless Technology', 'addiction', 'base', 'cancer prevention', 'cancer risk', 'cancer type', 'disability', 'effective intervention', 'experience', 'heart rate variability', 'knowledge base', 'mobile computing', 'negative affect', 'predictive modeling', 'primary outcome', 'role model', 'smoking cessation', 'success']",NCI,UNIVERSITY OF UTAH,R01,2018,682804,0.04673414117981004
"Development of a novel neurotechnology to promote emotion recognition in autism DESCRIPTION (provided by applicant): Difficulties in facial emotion recognition (FER) are thought to cause or exacerbate social disability in people with autism spectrum disorder (ASD) by preventing 1) accurate detection of social/emotional information conveyed through the face, particularly the eye-region, and 2) the deployment of emotionally appropriate responses. Although the neural systems thought to underlie FER deficits in ASD are increasingly appreciated, their plasticity remains speculative. The goal of this project is to develop an assistive technology to promote facial emotion recognition in ASD [R21]. We propose that FER can be rehabilitated using a brain-computer interface (BCI) device [R33]. To develop an FER assistant, we plan to first [R21] determine whether it is possible to develop a multi-voxel classifier that is temporally predictive of successful emotion recognition during functional magnetic resonance imaging (fMRI). An adaptive, real-time fMRI (rt-fMRI) paradigm will interpret the output of a subject's brain to assess whether a computer-generated actor's emotion is recognized. If not, the expressed facial emotion will be increased in intensity until the computer determines that the subject has recognized the emotion. After tuning this supervised learning algorithm produced by a support vector machine (SVM), we then transform the massively multidimensional classifier to low-dimensionality space, which can be replicated by a single- or dual-EEG sensor placed on the scalp. The proof of principle is that the multivariate classifier can be forward transformed into frequency (EEG) space. The EEG sensor can be comfortably worn outside of the scanner (BCI device), and can be wirelessly linked to a portable tablet (iPad). We will then demonstrate the feasibility of an ambulatory BCI 'FER assistant' [R33] in a between-group, randomized design (genuine neurofeedback vs placebo neurofeedback). The FER assistant is a virtual reality- based iPad application that uses the EEG sensor data to assist users with emotion recognition by manipulating the avatar's emotion intensity until it is recognized by the user, who will receive points the earlier the emotion is recognized. The purpose of this randomized controlled trial (RCT) is to assess feasibility including acceptability of the intervention, recruitment and randomization procedures, intervention implementation, blinded assessment procedures, and participant retention within the context of an RCT in preparation for a well- powered efficacy trial. This study's products include demonstration of the neural processes that underlie FER deficits and evidence of their plasticity, and an easily exportable, minimal-cost computer-based intervention. There has been little treatment research for this under-studied population, and social deficits may post unique challenges to people with ASD during late adolescence and early adulthood, as they face multiple life transitions and developmental tasks requiring social competence (e.g., securing employment). Ultimately, we plan to evaluate the efficacy of this emergent intervention in an adequately powered randomized clinical trial. PUBLIC HEALTH RELEVANCE: The social disability that characterizes Autism Spectrum Disorder (ASD) pervades other areas of adaptive behavior, is predictive of secondary mental health problems, and adversely affects long-term outcome. Although ASD is a chronic condition, there has been little research on interventions for adults with ASD to target social disability. We propose to first establish the neural plasticity of specific brain mechanisms underlying difficultis with facial emotion recognition, a core deficit believed to be pivotal in the behavioral expression of ASD-social disability, and subsequently develop a novel, computer-based intervention using real-time feedback to ameliorate emotion recognition deficits.",Development of a novel neurotechnology to promote emotion recognition in autism,9442821,R33MH100268,"['Adaptive Behaviors', 'Adolescence', 'Adolescent', 'Adult', 'Affect', 'Algorithms', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Markers', 'Blinded', 'Brain', 'Chronic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Computer Simulation', 'Computers', 'Correlation Studies', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Dimensions', 'Electroencephalography', 'Emotional', 'Emotions', 'Employment', 'Eye', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Mental Health', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Neuronal Plasticity', 'Outcome', 'Output', 'Participant', 'Pattern', 'Placebos', 'Population', 'Preparation', 'Procedures', 'Process', 'Randomized', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Reporting', 'Research', 'Sampling', 'Scalp structure', 'Secure', 'Self-Help Devices', 'Signal Transduction', 'Social Environment', 'Supervision', 'Symptoms', 'System', 'Tablets', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Wireless Technology', 'Work', 'active method', 'autism spectrum disorder', 'base', 'brain computer interface', 'computer generated', 'cost', 'design', 'disability', 'efficacy trial', 'emerging adult', 'innovation', 'neurofeedback', 'neurotechnology', 'novel', 'participant retention', 'portability', 'prevent', 'programs', 'public health relevance', 'recruit', 'relating to nervous system', 'response', 'satisfaction', 'sensor', 'skills', 'social', 'social skills', 'study population', 'tool', 'virtual reality']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R33,2018,237842,0.051295233889622094
"Improving measurement of emotional granularity to investigate affective mechanisms of cardiovascular disease and metabolic syndrome PROJECT SUMMARY/ABSTRACT Cardiovascular disease (CVD) and metabolic syndrome are a leading cause of disability and death worldwide. Detrimental shifts in the resting (tonic) contributions of the autonomic nervous system (ANS) to visceral functions throughout the body, a form of compromised allostasis, have been observed with both emotional dysregulation and disordered mood and may be a common, core vulnerability for CVD and metabolic syndrome. A crucial but understudied psychological vulnerability to compromised allostasis is low emotional granularity, or the inability to experience emotion with precision and detail (e.g., the inability to distinguish anger vs. frustration, or even anger vs. sadness). A critical barrier to ameliorating low granularity, and therefore reducing susceptibility to CVD and metabolic syndrome, has been the lack of a theoretical framework linking emotional granularity to physiological regulation, as well as tools for effectively measuring and improving granularity. Theoretical advances in affective science posit that the use of more precise emotion concepts is associated with a peripheral physiological system better able to respond to environmental perturbations (e.g., stressors). If low granularity results from impoverished emotion concepts, then the brain is less able to predict and categorize viscerosensory changes that arise from regulation of the body’s internal milieu. Increased parasympathetic tone at rest permits more efficient regulation by promoting recovery and energy conservation, and substantial evidence suggests that lowered risk profiles for CVD are associated with increased high frequency heart rate variability (HF HRV). The proposed research will use experience sampling and ambulatory monitoring data to map variability in emotional granularity in everyday life and examine its consequences for peripheral physiology, with a focus on resting HF HRV. In Aim 1, machine learning will be used to assess whether individuals lower in emotional granularity have less efficient allostasis, as reflected by both lower resting parasympathetic activity and fewer distinct patterns of ANS activity. In Aim 2, graph theory will be used to develop metrics for measuring temporal and contextual dynamics of emotional granularity, which provide meaningful variance necessary to describe patterns of subjective experience and physiological activity. Exploratory Aim 3 will assess whether in-lab emotion concept training can be used to improve granularity of emotion concepts, with the ultimate goal of developing longer-term training aimed at increasing physiological specificity and resting HF HRV. By integrating modeling from engineering and computer science to better capture idiographic variation in emotion, the proposed research offers an innovative approach for understanding how a psychological vulnerability could increase the risk for cardiovascular illness. The outcomes of this work will allow for the development of psychological interventions that can decrease risk for physical illness by increasing efficient allostasis and encouraging adaptive coping mechanisms. PROJECT NARRATIVE A wide range of emotion-related risk factors for cardiovascular disease and metabolic syndrome have been identified, yet little is known about how the underlying mechanisms that link emotion to physiological regulation of the body can be leveraged to reduce susceptibility. The proposed work addresses this critical barrier by examining how one psychological vulnerability is linked to parasympathetic activation, and can be more effectively measured in patterns of autonomic nervous system activity and subjective experience. By further identifying possible interventions that target this vulnerability, this work seeks to decrease risk for physical illness by increasing efficient allostasis and encouraging adaptive coping mechanisms.",Improving measurement of emotional granularity to investigate affective mechanisms of cardiovascular disease and metabolic syndrome,9470193,F31HL140943,"['Address', 'Affective', 'Alcohol consumption', 'Alexithymias', 'Ambulatory Monitoring', 'Anger', 'Autonomic nervous system', 'Behavioral', 'Binge Eating', 'Brain', 'Cardiovascular Diseases', 'Cessation of life', 'Common Core', 'Data', 'Depressed mood', 'Development', 'Emotional', 'Emotional disorder', 'Emotions', 'Engineering', 'Fellowship', 'Frequencies', 'Frustration', 'Future', 'Goals', 'Hypertension', 'Hypertriglyceridemia', 'Immune system', 'Individual', 'Individual Differences', 'Intervention', 'Knowledge', 'Life', 'Link', 'Machine Learning', 'Maps', 'Measurement', 'Measures', 'Metabolic', 'Metabolic syndrome', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'Neurosecretory Systems', 'Obesity', 'Outcome', 'Pattern', 'Peripheral', 'Physiological', 'Physiology', 'Population', 'Predisposition', 'Recovery', 'Regulation', 'Reporting', 'Research', 'Rest', 'Risk', 'Risk Factors', 'Sampling', 'Science', 'Specificity', 'System', 'Testing', 'Time', 'Training', 'Variant', 'Visceral', 'Work', 'alcohol risk', 'allostasis', 'cardiovascular risk factor', 'career', 'computer science', 'contextual factors', 'coping', 'coping mechanism', 'disability', 'emotion dysregulation', 'emotion regulation', 'emotional experience', 'experience', 'graph theory', 'heart rate variability', 'improved', 'indexing', 'innovation', 'neuromechanism', 'novel', 'physical conditioning', 'protective factors', 'psychologic', 'stressor', 'theories', 'tool']",NHLBI,NORTHEASTERN UNIVERSITY,F31,2018,38124,0.0993208963044602
"Emotion Processing, Cognition and Functioning in Comorbid Psychosis Spectrum and Cocaine Use Disorders NIDA's strategic planning emphasizes the need to increase the real-world relevance of research by prioritizing “research that incorporates real-world complexities including common comorbidities” that “interact to affect symptom profiles, illness trajectory, and treatment outcomes”.  The overall goal of the current application is to characterize the impact of the common comorbidity between psychosis spectrum disorders (PSD, including schizophrenia, schizoaffective disorder and bipolar disorder) and cocaine use disorders (CUD) on cognition, emotion processing and functional outcomes. Clinical and sociodemographic correlates of comorbid PSD+CUD, and pathways to functional outcomes in comorbid PSD+CUD will also be explored.  To do this, three existing datasets will be used, with comprehensive diagnostic, clinical, cognitive and functional data, including patients with comorbid PSD and CUD (PSD+CUD), and 3 comparison groups: patients with PSD without CUD, patients with CUD without PSD, and healthy controls without psychiatric disorder. This unique design, including all 3 relevant control groups, will allow to elucidate the combined effect of comorbid PSD+CUD, compared to the effect of PSD or CUD alone, on cognition and emotion processing, and test two alternate models: an additive model and a self- medication model. The present study will focus on 3 specific aims: 1) To characterize sociodemographic and clinical correlates of comorbid PSD+CUD; 2) To characterize cognitive and emotion processing impairments in comorbid PSD+CUD; 3) To characterize functional impairments in comorbid PSD+CUD, and to identify predictors and pathways to social functioning. ANOVAs and Chi- square tests will be used to compare the groups on sociodemographic features, clinical features, cognitive and emotion processing performance and functional outcomes. Linear and logistic regressions, structural equation modeling and machine learning will be used to identify predictors and model pathways to functional outcomes, taking into account possible mediators.  Data from this study will increase our understanding of the impact of comorbid PSD+CUD on cognition, emotion processing and pathways to functional outcomes. We will also characterize the sociodemographic and clinical correlates of comorbid PSD+CUD. This will allow to identify those at risk for comorbid PSD+CUD, and to tailor personalized prevention and intervention approaches, targeting specific factors identified in the models. For example, cognitive and emotion processing impairments are modifiable through cognitive/emotion processing remediation strategies. The overall goal of the current application is to examine the impact of the common comorbidity of psychosis spectrum disorders (PSD) and cocaine use disorders (CUD) on cognition, emotion processing and social functioning. We will also characterize the clinical and sociodemographic correlates of PSD+CUD and explore predictors and pathways to functional outcomes. Understanding the clinical and sociodemographic correlates of comorbid PSD+CUD, and the factors that underlie functional outcomes, will allow to identify those at risk and tailor personalized prevention and intervention approaches.","Emotion Processing, Cognition and Functioning in Comorbid Psychosis Spectrum and Cocaine Use Disorders",9599895,R03DA045094,"['Address', 'Affect', 'Alcohol or Other Drugs use', 'Attention deficit hyperactivity disorder', 'Behavioral Sciences', 'Bipolar Disorder', 'Cannabis', 'Chi-Square Tests', 'Clinical', 'Cocaine', 'Cognition', 'Cognitive', 'Comorbidity', 'Control Groups', 'Cross-Sectional Studies', 'Data', 'Data Set', 'Diagnostic', 'Disease', 'Drug abuse', 'Drug usage', 'Early identification', 'Education', 'Emotions', 'Equation', 'Etiology', 'Frequencies', 'Goals', 'Hospitalization', 'Impairment', 'Intervention', 'Light', 'Linear Regressions', 'Logistic Regressions', 'Machine Learning', 'Mediating', 'Mediator of activation protein', 'Mental disorders', 'Modeling', 'National Institute of Drug Abuse', 'Nicotine', 'Outcome', 'Pathway interactions', 'Patient risk', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Preventive Intervention', 'Psychotic Disorders', 'Regression Analysis', 'Research', 'Research Personnel', 'Risk', 'Schizoaffective Disorders', 'Schizophrenia', 'Self Medication', 'Severities', 'Social Functioning', 'Strategic Planning', 'Stress', 'Substance Use Disorder', 'Symptoms', 'Testing', 'Treatment outcome', 'base', 'cocaine use', 'comparison group', 'design', 'early onset', 'functional disability', 'functional outcomes', 'indexing', 'individual patient', 'individualized prevention', 'interest', 'low socioeconomic status', 'male', 'personalized intervention', 'predictive modeling', 'remediation', 'research study', 'sex']",NIDA,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R03,2018,127125,0.028635332459773847
"Computational and brain predictors of emotion cue integration The purpose of this project is to develop computational and brain-based models of emotion cue integration: people’s inferences about others’ emotions based on dynamic, multimodal cues. Observers often decide how targets feel based on cues such as facial expressions, prosody, and language. Such inferences scaffold healthy social interaction, and abnormal inference both marks and exacerbates social deficits in numerous psychiatric disorders. Psychologists and neuroscientists have studied emotion inference for decades, but the vast majority of this work employs simplified social cues, such as vignettes or static images of faces. By contrast, “real world” emotion cues are complex, dynamic, and multimodal. Cue integration—inference based on naturalistic emotion information—likely differs from simpler inference at cognitive and neural levels, but this phenomenon remains poorly understood. This means that scientists lack a clear model of how observers adaptively process complex emotion cues, and how that processing goes awry in mental illness. Especially lacking are mechanistic models that can describe the computations and brain processes involved in cue integration with sufficient precision to predict inference in new cases, observers, and samples. This project will merge tools from social psychology, computer science, and neuroscience to generate a novel and rigorous model of emotion cue integration. We have demonstrated that in the face of complex emotion cues, observers dynamically “weight” cues from each modality (e.g., visual, linguistic) over time, a process that (i) tracks shifts in brain activity and connectivity; and (ii) can be captured using Bayesian models. Here, we will expand this work in several ways. First, we will develop precise computational tools to isolate features of emotion cues—such as facial movements, prosody, and linguistic sentiment—that track observers’ use of each cue modality during integration. Second, we will develop multi-region “signatures” of brain activity and connectivity that track emotion inference in each modality. We will use these signatures in conjunction with machine learning to predict unimodal emotion inference and cue integration in new observers and samples, based on brain data alone. Third, we will explore the context-dependence of naturalistic emotion inference by testing whether reinforcement learning can bias observers’ cue integration and accompanying brain signatures. Finally, we will model computational and neural abnormalities associated with cue integration in patients with Major Depressive Disorder and Bipolar Disorder. At the level of basic science, these data will generate a fundamentally new—and more naturalistic—approach to the neuroscience of emotion inference. The computational and brain metrics we produce will also be made publically available to facilitate the open and cumulative study of emotion inference across labs. At a translational level, we will provide a mechanistic, rich account of abnormal emotion inference in mood disorders, paving the way for computational and brain markers that can be used to assess social dysfunction and treatment efficacy in these and other mental illnesses. PROJECT NARRATIVE The proposed research will use methods from social psychology, cognitive neuroscience, and computer science to (i) precisely model people’s inferences about others’ emotions based on complex, dynamic cues, (ii) generate multi-region, brain-based predictors of these inferences, and (iii) characterize abnormalities in inference among individuals with mood disorders. Several psychiatric and neurodevelopmental disorders are characterized by difficulties understanding others’ emotions, which in turn worsen social functioning in patients. In addition to providing new insights about social processes—a core target within the NIMH’s research domain criteria (RDoC) framework—the proposed research will offer powerful, novel computational and neural targets through which to assess and treat difficulties in emotion inference, and to eventually reduce the social burden faced by people with mental illness on a broad scale.",Computational and brain predictors of emotion cue integration,9487048,R01MH112560,"['Affect', 'Affective', 'Agreement', 'Base of the Brain', 'Basic Science', 'Bayesian Modeling', 'Bipolar Disorder', 'Brain', 'Brain region', 'Classification', 'Cognitive', 'Complex', 'Computer Simulation', 'Cues', 'Data', 'Dependence', 'Emotional', 'Emotions', 'Event', 'Exhibits', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Image', 'Individual', 'Language', 'Lateral', 'Learning', 'Life', 'Linguistics', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Moods', 'Movement', 'National Institute of Mental Health', 'Neurodevelopmental Disorder', 'Neurosciences', 'Observer Variation', 'Participant', 'Patients', 'Pattern', 'Perception', 'Process', 'Psychological reinforcement', 'Psychologist', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Running', 'Sampling', 'Scanning', 'Scientist', 'Sensory', 'Social Functioning', 'Social Interaction', 'Social Psychology', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Visual', 'Weight', 'Work', 'base', 'brain abnormalities', 'cognitive neuroscience', 'computer science', 'computerized tools', 'executive function', 'insight', 'language comprehension', 'multimodality', 'neuroimaging', 'novel', 'recruit', 'relating to nervous system', 'response', 'scaffold', 'social', 'tool']",NIMH,STANFORD UNIVERSITY,R01,2018,474391,0.13157813104076466
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9530685,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'high dimensionality', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2018,771712,0.06348538773156562
"Dynamics of Large-Scale Networks During Emotional and Social Processing The goal of this application is to develop a research program that, as stated in the call entitled The Neural Mechanisms of Multi-Dimensional Emotional and Social Representation (RFA-MH- 17-300), incorporates innovative approaches designed to move the fields of affective and social neuroscience beyond single region-based, modular, and static models of brain function and behavior. The RFA calls for research that is multi-dimensional, that is, that investigates the role of (among others) complex contexts, as well as distributed and/or dynamic processes that unfold over time. The objective of the present application is to jointly investigate emotional and social processes in a richly multi-dimensional manner. Aim 1: Network organization and evolution during emotional and social processing. The objective of this aim is to uncover how large-scale brain networks are organized and evolve temporally during emotional and social processing. Networks will include brain regions that robustly respond to the tasks proposed, and regions from well characterized networks, including the salience, executive control, and task-negative networks. Aim 2: Naturalistic processing during emotional and social processing. The objective of this aim is to understand naturalistic processing of emotional and social information. Although standard experimental designs afford great control over experimental conditions, they lack ecological validity and restrict the experiments that can be studied. We propose to investigate continuous (“naturalistic”) processing during movie watching involving emotional and social content. Continuous processing will be investigated via intersubject correlation analysis, which measures the extent to which signals are correlated across participants. Aim 3: Development of network organization/evolution and naturalistic processing. The objective of this aim is to investigate multi-dimensional emotional and social processes from a developmental perspective. Most developmental research in emotion has focused on observing amygdala responses and those of a few other brain regions during face perception. We focus on a question largely neglected in prior research, specifically sustained threat processing and the involvement of the bed nucleus of the stria terminalis. In the context of social processing, this aim examines the development of intersubject synchrony. Across the emotional and social domains, we propose to study middle childhood (8-9 y), early adolescence (12-13 y), and young adulthood (18-19 y). The goal of this application is to investigate the neural mechanisms of multi-dimensional emotional and social representation. Functional MRI experiments are proposed to investigate the following general questions: organization and evolution of brain networks; naturalistic processing during movie watching; and development.",Dynamics of Large-Scale Networks During Emotional and Social Processing,9484330,R01MH112517,"['Adolescence', 'Adopted', 'Affective', 'Amygdaloid structure', 'Anxiety', 'Autistic Disorder', 'Behavior', 'Brain', 'Brain region', 'Complex', 'Data', 'Development', 'Developmental Process', 'Disease', 'Emotional', 'Emotions', 'Evolution', 'Experimental Designs', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Individual Differences', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Participant', 'Pathway Analysis', 'Persons', 'Process', 'Property', 'Research', 'Risk', 'Role', 'Series', 'Signal Transduction', 'Social Development', 'Social Environment', 'Social Interaction', 'Stimulus', 'Structure', 'Structure of terminal stria nuclei of preoptic region', 'Testing', 'Time', 'Work', 'affective neuroscience', 'age group', 'base', 'design', 'early adolescence', 'executive function', 'experimental study', 'face perception', 'graph theory', 'improved', 'innovation', 'middle childhood', 'movie', 'neglect', 'neurobiological mechanism', 'neuromechanism', 'novel', 'peer', 'positive mood', 'programs', 'relating to nervous system', 'response', 'social', 'social neuroscience', 'young adult']",NIMH,"UNIV OF MARYLAND, COLLEGE PARK",R01,2018,674022,0.10500034482042042
"Prefrontal-Amygdala Interactions in Social Learning PROJECT SUMMARY This R01 renewal application proposes functional neuroimaging studies with human subjects to elucidate the role of the prefrontal cortex and amygdala in the processing of facial identities and expressions that predict critical social outcomes. Presentations of facial expressions of emotion in neuroimaging studies have proven particularly robust stimuli for activating amygdala and prefrontal regions involved in processing biologically- relevant social cues. Here we propose to further develop our novel structural and functional neuroimaging methods to better understand how the amygdala interacts with reciprocally connected prefrontal areas when such expressions are encountered. Specifically, following up on our previous findings that the structural integrity of an amygdala-prefrontal pathway predicts individual differences in reported anxiety – we replicated this effect in > 250 subjects and observed an exciting sex difference; this effect is compellingly stronger in females than in males. Here we propose to follow up on this effect with higher resolution DTI methods and to extend it to functional resting state data to see if the same sex difference is observed functionally. In addition, we propose a new mathematical model where we believe we can disentangle the effects of valence from arousal in brain imaging data, a confound the field continues to struggle with. Finally, we propose the development of a new facial expression stimulus set where we record the psychological status of the models posing for the expressions so we can determine any interaction this might have with the psychological status of the our subjects of study. The field can then usefully compare these data to complementary developmental research (i.e., with children and adolescents) and will be amenable  to direct translation to clinical populations (e.g., anxiety and depression). The experiments proposed here will increase our understanding of the brain mechanisms involved in  this learning. A number of experiments highlight an important difference between men and women that  we discovered during our last grant and follow up on here. Specifically, brain connections with the  prefrontal cortex in females explain how anxious they report being but we do not see this effect in  males. With this information, we can then better understand what goes wrong in the brains of  individuals with major depression and anxiety disorders.",Prefrontal-Amygdala Interactions in Social Learning,9561940,R56MH080716,"['Adolescent', 'Affect', 'Affective', 'Amygdaloid structure', 'Anatomy', 'Anxiety', 'Anxiety Disorders', 'Area', 'Arousal', 'Attention', 'Award', 'Behavioral', 'Biological', 'Brain', 'Brain imaging', 'Child', 'Clinical', 'Cognitive', 'Collaborations', 'Complex', 'Cues', 'Data', 'Development', 'Emotional', 'Emotional disorder', 'Face', 'Facial Expression', 'Failure', 'Female', 'Friends', 'Grant', 'Human', 'Image', 'Individual', 'Individual Differences', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measurement', 'Measures', 'Mental Depression', 'Methods', 'Modeling', 'Outcome', 'Participant', 'Pathway interactions', 'Population', 'Prefrontal Cortex', 'Publishing', 'Regulation', 'Reporting', 'Research', 'Resolution', 'Rest', 'Role', 'Same-sex', 'Sampling', 'Sex Characteristics', 'Signal Transduction', 'Social Behavior', 'Stimulus', 'Structure', 'Study Subject', 'Supervision', 'System', 'Techniques', 'Translations', 'Woman', 'Work', 'anxious', 'base', 'behavioral outcome', 'behavioral response', 'experimental study', 'follow-up', 'human subject', 'indexing', 'innovation', 'male', 'mathematical model', 'men', 'neuroimaging', 'novel', 'psychologic', 'relating to nervous system', 'response', 'showing emotion', 'social', 'social learning', 'social situation', 'two-dimensional', 'white matter']",NIMH,DARTMOUTH COLLEGE,R56,2018,405000,0.045777171845548616
"Neurophysiology of auditory emotion recognition in the human brain Summary/Abstract:  Auditory emotion recognition (AER) is the process of extracting emotional content out of the acoustic features of speech, and is disrupted in many neuropsychiatric disorders including autism and schizophrenia. Both the encoding of this information in the auditory cortex as well as the knowledge of which brain areas participate in the circuit that underlies AER are not known. With a unique access to invasive intracranial neurophysiology in awake, behaving humans, we propose to study the neural basis of AER in patients undergoing invasive electrode monitoring as part of staged epilepsy surgery protocols. These subjects will perform an AER task while brain responses are recorded in an effort to decipher the neural processes that identify emotion and while specific areas of their brain are stimulated in order to causally link focal activation of AER circuit components to perception.  In Aim 1, we will identify the neuronal mechanisms underlying emotion recognition using a task that breaks down AER into three sequential steps: encoding, integration and judgment. First, we will identify the encoding of emotionally relevant acoustic cues by examining how sensory features such as pitch, intensity and modulation are encoded by neuronal responses in the auditory cortex and other brain areas involved in emotional processing such as the amygdala, insula and prefrontal cortex. Second, we will examine how these acoustic cues are integrated into an emergent emotional percept by measuring activity in putative areas as well as the interaction among these areas after relevant sensory input while subjects consider possible responses. Finally, we will examine the subjects’ response to the AER task and correlate the judgment of emotion with the extent to which it can be predicted by neuronal activity in different brain regions of interest. While most analyses will focus on intraindividual data, differences in brain activity as it relates to AER will be compared in males versus females in order to examine biological differences in AER between sexes. In aim 2, we will determine the effect of neural stimulation on AER. We will apply focal single pulse stimulation to brain areas at those critical time windows identified in aim 1 in order to bias emotional decisions. This type of stimulation results in very focal and temporally restricted disruption of function and can be used to determine a brain area’s causal role in perception.  We seek to build a detailed neurobiological model of the AER. This would be a precursor to designing individualized therapeutic approaches to treat deficits in auditory emotional processing as well as identifying putative targets to treat affective disorders in general. Narrative: Identifying emotion in speech is a critical brain process required for social interaction and is impaired in many neuropsychiatric diseases. We will study how the brain codes the emotional content of speech and how stimulating specific brain areas can potentially improve this process. !",Neurophysiology of auditory emotion recognition in the human brain,9520422,R21MH114166,"['Acoustics', 'Affect', 'Amygdaloid structure', 'Animal Model', 'Area', 'Attention', 'Auditory', 'Auditory area', 'Autistic Disorder', 'Behavioral', 'Biological', 'Brain', 'Brain region', 'Clinical Protocols', 'Code', 'Cognitive', 'Communication', 'Comprehension', 'Cues', 'Data', 'Disease', 'Electric Stimulation', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Emotional', 'Emotions', 'Engineering', 'Epilepsy', 'Evaluation', 'Female', 'Functional Magnetic Resonance Imaging', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Judgment', 'Knowledge', 'Link', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Mental Depression', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Neurobiology', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Pattern', 'Perception', 'Performance', 'Personal Communication', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Prefrontal Cortex', 'Process', 'Protocols documentation', 'Psychiatrist', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Role', 'Scalp structure', 'Schizophrenia', 'Sensory', 'Social Interaction', 'Speech', 'Speech Pathologist', 'Speed', 'Stimulus', 'Stream', 'Structure', 'Superior temporal gyrus', 'Testing', 'Therapeutic', 'Time', 'Transcranial magnetic stimulation', 'Woman', 'Work', 'autism spectrum disorder', 'awake', 'behavioral study', 'computer studies', 'design', 'experimental study', 'human subject', 'improved', 'innovation', 'interest', 'learning strategy', 'male', 'men', 'neural model', 'neural stimulation', 'neuroimaging', 'neurophysiology', 'neuropsychiatric disorder', 'novel', 'physical property', 'relating to nervous system', 'response', 'sensory input', 'sensory system', 'sex', 'social', 'social skills', 'spatiotemporal', 'temporal measurement']",NIMH,FEINSTEIN INSTITUTE FOR MEDICAL RESEARCH,R21,2018,202112,0.08524525916615908
"Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease PROJECT ABSTRACT/SUMMARY The prevalence of psychiatric disorders has reached nearly epidemic proportions. Rates of common affective diseases (unipolar depression, anxiety and stress disorders) are high across the lifespan and these diseases place a tremendous social and economic burden on the individual and society. Clear evidence indicates that most affective disorders emerge at the intersection of pre-existing vulnerability and significant, highly stressful, life-events. However, current models of emotion-related risk do not adequately account for this confluence of biological, historical, and situational factors. In this investigation, we build upon our prior work demonstrating broad associations between flexible emotion processing and psychological health and adjustment, and in-flexible emotion and psychological risk and affective disease. Specifically, we will recruit 400 adults in hospital following a potentially traumatic event (e.g., accident, violence, fire, etc.) in order to model the influence of early emotion processing on trajectories of adjustment. We focus our investigation on the super-ordinate construct of Emotion flexibility (EF) which encompasses the ability to generate or up-regulate emotions, as well as to shift or down-regulate emotions according to needs and/or environmental demands. EF is well-suited to inform models of emotion-related risk and adjustment as it characterizes an optimal balance of two biologically-based, constituent dimensions: “bottom-up” threat-related processing and “top-down” cognitive control increasingly recognized as central to all emotion processing. We propose rigorous methods to assess EF and related processing in-vivo in lab and via experience sampling. Moreover, we will follow participants to 18 months post event so as to effectively model the association between emotion processing and trajectories of adjustment, while also considering established influences such as physical health status, psychiatric history, childhood maltreatment, daily stress/hassles, and social support. In particular, we will incorporate recent developments in advanced statistical modelling to better characterize the complex and interactive influence of historical and contemporary factors on moment-level emotion processing, EF and adjustment. Broadly, this project is in line with the most recent NIMH strategic plan and will contribute to more complex models of the most common affective diseases, including facilitating the charting of illness trajectories to help determine when, where, and how to intervene. Moreover, this research will directly examine how variation in key systems can influence emotion-processing and adjustment to aversive life events, fitting complex influences more directly into models of risk for the most common and burdensome affective diseases. PUBLIC HEALTH RELEVANCE/NARRATIVE Emotion-related psychiatric disorders, including depression and anxiety, affect a considerable portion of adults in this country and rank as many of the most burdensome diseases worldwide. In this investigation, we will follow an at-risk sample of adults in order to better understand how one key pathway, relating to how individuals process emotion, influences risk for emotion-related diseases over time. In addition, we test the role by which certain other factors, both contemporary and historical (physical health, life stress, social support, psychiatric treatment history, or childhood experiences) may increase or decrease risk via this particular pathway.",Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease,9523855,R01MH113622,"['Accidents', 'Adult', 'Affect', 'Affective', 'Anxiety', 'Anxiety Disorders', 'Behavioral', 'Biological', 'Categories', 'Child Abuse and Neglect', 'Childhood', 'Clinical', 'Clinical Sciences', 'Complex', 'Country', 'Derivation procedure', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Economic Burden', 'Emotions', 'Epidemic', 'Equilibrium', 'Event', 'Fire - disasters', 'Health', 'Health Status', 'Heritability', 'Hospitals', 'Individual', 'Inherited', 'Investigation', 'Life', 'Life Stress', 'Longevity', 'Machine Learning', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Participant', 'Pathway interactions', 'Patient Recruitments', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Prevalence', 'Process', 'Psychiatric therapeutic procedure', 'Psychological adjustment', 'Qualifying', 'Recording of previous events', 'Regulation', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Adjustment', 'Risk Factors', 'Role', 'Sampling', 'Shapes', 'Sleep', 'Social support', 'Societies', 'Statistical Models', 'Strategic Planning', 'Stress', 'Symptoms', 'System', 'Testing', 'Time', 'Unipolar Depression', 'Variant', 'Violence', 'Work', 'base', 'clinical diagnostics', 'clinically relevant', 'cognitive control', 'emotion regulation', 'experience', 'flexibility', 'improved', 'in vivo', 'indexing', 'laboratory experience', 'learning strategy', 'longitudinal design', 'negative mood', 'network models', 'physical conditioning', 'post-traumatic stress', 'prospective', 'psychologic', 'public health relevance', 'recruit', 'response', 'social', 'stress disorder', 'tool', 'traumatic event']",NIMH,KENT STATE UNIVERSITY,R01,2018,594230,0.1142494328799957
"Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues. People signal their internal emotional state by a range of cues including facial expressions, non-verbal vocalizations and the tone and content of speech. Much work to date on emotional signaling has focused on a small number of emotions and has relied on stimulus sets with limited numbers of exemplars and poor representation of individuals of different ages and ethnicities. The computer vision literature has long recognized the need to be able to compare models, for example of face recognition, across complex, diverse, and naturalistic stimulus sets. Here, we argue that a parallel approach needs to be applied if we are to achieve a robust and generalizable understanding of how the human brain represents emotion and identity related information derived from facial and vocal cues. Three multi-session functional magnetic resonance imaging (fMRI) experiments are proposed. In the first, participants will view a large corpus (~2000) of faces of individuals varying in age, gender and ethnicity and showing a wide range of emotional expressions. In the second, participants will be presented with a large (~1000) and equally diverse set of emotional vocalizations. The third experiment will make use of an even more complex and naturalistic stimulus set comprising ~1000 video clips of individuals expressing emotions through facial expression, non-verbal vocalizations and emotion-laden speech. This set will be broken into three parts, with as closely matched content as possible. These will be presented to participants in audio only, visual only and audio-visual (bimodal) conditions, with the stimuli allocated to each condition balanced across participants. For each experiment, data collection will comprise separate Model Estimation and Model Validation periods with the majority of stimuli presented at Validation being distinct from those presented at Estimation. A range of models will be fit to the fMRI data acquired during Estimation runs and tested and compared using data acquired during Validation runs. These models contain sets of features that describe the emotional content of the stimuli presented in terms of either dimensional or categorical models of emotion. By comparing the fit of these models we can determine which models capture most variance in voxel response profiles and examine how this varies across brain regions. Across the three experiments, we also seek to establish whether there are regions where voxels show common coding of emotional state regardless of whether information is carried by facial or vocal cues or a combination of both. Further, by contrasting models with and without terms for characteristics such as age, gender and ethnicity we can also investigate the (in)dependence of the representation of emotion and identity-related features. The proposed research has the potential to greatly advance our understanding of how cues to others' emotional state are represented in the human brain and the extent to which this is influenced by the characteristics of the person we are interacting with. In the medium term, we plan to extend this to also model listener/ viewer characteristics (both demographics and predisposition to anxiety or depression) in a hope to advance our understanding of how biases in the interpretation of emotional signals can arise. Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial  and vocal cues. Individuals with psychiatric conditions ranging from Social Anxiety and Depression to Autism Spectrum Disorder and Schizophrenia struggle to correctly interpret others' emotional states. To understand how alterations in brain function can give rise to the misperception of emotion cues, we first need better normative models of how information about others' emotional state derived from facial and vocal cues is represented within the human brain. Hence, here we use large, diverse and naturalistic stimulus sets encompassing facial expressions, emotional vocalizations and multi-modal videos to investigate the extent to which consistent representation of emotional state is observed across stimulus modalities, and to explore the organizational principles that can best explain voxel response profiles or tuning patterns to facial and vocal emotion cues.",Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues,9545868,R01MH112541,"['Address', 'Age', 'Anxiety', 'Arousal', 'Auditory', 'Brain', 'Brain region', 'Categories', 'Characteristics', 'Clip', 'Code', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Data Collection', 'Data Set', 'Decision Making', 'Dependence', 'Dimensions', 'Emotional', 'Emotions', 'Ethnic Origin', 'Expressed Emotion', 'Face', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Gender', 'Human', 'Image', 'Individual', 'Label', 'Literature', 'Mental Depression', 'Mental disorders', 'Modality', 'Modeling', 'Participant', 'Pattern', 'Persons', 'Predisposition', 'Property', 'Research', 'Running', 'Schizophrenia', 'Signal Transduction', 'Source', 'Speech', 'Stimulus', 'Structure', 'Techniques', 'Testing', 'Validation', 'Visual', 'Weight', 'Work', 'autism spectrum disorder', 'blood oxygen level dependent', 'data acquisition', 'data modeling', 'demographics', 'experimental study', 'imaging study', 'information model', 'neural model', 'novel', 'relating to nervous system', 'response', 'showing emotion', 'social anxiety', 'theories', 'trait', 'visual information', 'visual neuroscience', 'vocalization']",NIMH,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2018,125000,0.17660734439641768
"Neurophysiology underlying neural representations of value A range of behavioral, physiological, and cognitive responses (e.g. approach and avoidance, autonomic reactivity, and subjective feelings) reflects a subject's emotional state. The cognitive regulation of emotion refers to the capacity to regulate these emotional responses in a flexible manner according to a cognitive operation. Deficits in the cognitive regulation of emotional processes characterize many psychiatric disorders. In everyday life, however, particular sensory stimuli and/or actions can elicit different emotional responses depending upon the situation or context. Contexts often rely on a cognitive understanding of one's current situation in the absence of explicit cues. These types of contexts may be referred to as “abstract” contexts. This grant studies a type of abstract context where the context is determined by a task set. A task set is the set of stimulus- response-outcome mappings (or rules) that dictate correct performance for trials within a particular block. Previous research demonstrates the capacity of primates to learn these abstract contexts, and neural representations of abstract contexts exist in the amygdala and two areas in the prefrontal cortex (PFC), the anterior cingulate and orbitofrontal cortices (ACC and OFC). This grant seeks to understand the mechanisms that underlie the formation and maintenance of these representations of contexts. In contrast to supervised learning driven by error signals, we hypothesize that the occurrence of temporally associated trial types triggers unsupervised learning, presumably through a Hebbian mechanism involving activity-dependent plasticity. This learning could underlie formation of representations of abstract contexts defined by task sets, which will be explored with electrophysiological recordings in Aim 1. The creation of a representation of a task set requires combining information about the current trial with information about the trials that have occurred recently. Brain structures that provide memory traces of recent events and/or that combine information over time could create representations of a task set prior to the emergence of the representations observed in amygdala, OFC, and ACC. Our next experiments therefore target the hippocampus and dorsolateral PFC (DLPFC), which are implicated in memory processes, working memory, and executive functions. We will compare and contrast the encoding of task sets in hippocampus, DLPFC, OFC, and ACC during and after learning about task sets (Aim 2). Finally, we will use causal methods to determine if PFC input to the amygdala and the hippocampus acts to maintain these context representations, which could be a vital mechanism for the cognitive regulation of emotion (Aim 3). Overall, these experiments promise to illuminate neurophysiological mechanisms critical for normal adaptive emotional health. ! The aim of this proposal is to understand brain mechanisms responsible for the cognitive regulation of emotion. Since many psychiatric disorders, like anxiety and mood disorders as well as schizophrenia, autism, and addiction, involve cognitive dysfunction mediated by neural circuits in the brain areas under study, this project promises to provide new insights about neural network function critical for developing new treatments.",Neurophysiology underlying neural representations of value,9238947,R01MH082017,"['Amygdaloid structure', 'Anterior', 'Anxiety Disorders', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Neural Networks', 'Brain', 'Cognitive', 'Cues', 'Development', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event', 'Feeling', 'Grant', 'Health', 'Hippocampus (Brain)', 'Impaired cognition', 'Knowledge', 'Lead', 'Learning', 'Life', 'Maintenance', 'Mediating', 'Memory', 'Mental disorders', 'Methods', 'Monkeys', 'Mood Disorders', 'Outcome', 'Perception', 'Performance', 'Physiological', 'Play', 'Prefrontal Cortex', 'Primates', 'Process', 'Psychological reinforcement', 'Research', 'Response to stimulus physiology', 'Reversal Learning', 'Rewards', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Signal Transduction', 'Stimulus', 'Structure', 'Supervision', 'Task Performances', 'Testing', 'Time', 'Update', 'Work', 'addiction', 'cognitive control', 'cognitive process', 'cognitive reappraisal', 'emotion regulation', 'emotional experience', 'executive function', 'expectation', 'experimental study', 'flexibility', 'insight', 'memory process', 'neural circuit', 'neurophysiology', 'operation', 'optogenetics', 'prevent', 'relating to nervous system', 'response', 'sensory stimulus', 'statistics', 'unsupervised learning']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2018,777587,0.05840088934701091
"QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development   Children with autism struggle to recognize facial expressions, make eye contact, and engage in social interactions. Many of these children can have dramatic recoveries, particularly if social skills are taught from an early age. However, in today's healthcare system the delivery of the behavioral intervention is bottlenecked by a sharp and increasing imbalance in the number of behavioral therapists and the number of children in need of care. As such, there is an urgent need to develop mobilized methods of care delivery. We have developed an artificial intelligence tool for automatic facial expression recognition that runs on Google Glass through an Android app and delivers instantaneous social cues to individuals with autism in their natural environment, providing therapy that today is given only by clinicians in non-scalable person-to- person sessions. The system leverages Glass's outward facing camera to read a person's facial expressions and passes facial landmarks to an Android native app for immediate machine learning-based emotion classification. The system then gives the child wearer real-time social cues and records social responses. We believe that the system's ability to provide continuous behavioral therapy outside of clinical settings will enable dramatically faster gains in social acuity that will, within a limited and self-directed period of use, permit the child to engage in social settings on his/her own. This proposal outlines three main aims needed to test, refine and optimize the tools and a series of validation experiments needed to bring our system from prototype to a viable clinical tool that every family can use regularly from home for precision healthcare. RELEVANCE {See instructions): We have developed a combined software-hardware solution built on top of Google Glass that enables real lime expression recognition and field-of-view eye tracking to capture social interaction data and give guiding social cues to an individual with Autism. This project will test and optimize the potential of this tool to provide continuous and naturalized behavioral therapy to children with autism from their homes. n/a",QuBBD: Wearable artificial intelligence for bid data-driven healthcare in child development  ,9394169,R01EB025025,"['Address', 'Advertising', 'Affective', 'Age', 'Android', 'Artificial Intelligence', 'Augmented Reality', 'Autistic Disorder', 'Awareness', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Benchmarking', 'Big Data', 'Biomedical Engineering', 'Caregivers', 'Caring', 'Child', 'Child Development', 'Classification', 'Clinical', 'Complex', 'Computer software', 'Cues', 'Data', 'Databases', 'Diagnosis', 'Emotions', 'Environment', 'Eye', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Family', 'Family member', 'Feedback', 'Future', 'Glass', 'Goals', 'Health', 'Healthcare', 'Healthcare Systems', 'Home environment', 'Human', 'Image', 'Imagery', 'Incidence', 'Individual', 'Instruction', 'Internet', 'Intervention', 'Label', 'Learning', 'Lighting', 'Limes', 'Machine Learning', 'Medical', 'Methods', 'Modeling', 'Motivation', 'Outcome', 'Parents', 'Participant', 'Persons', 'Procedures', 'Records', 'Recovery', 'Running', 'Self-Direction', 'Series', 'Social Interaction', 'Social Marketing', 'Structure', 'System', 'Testing', 'Therapeutic', 'Time', 'Training', 'Update', 'Validation', 'Variant', 'Work', 'adaptive learning', 'applied behavior analysis', 'base', 'care delivery', 'design', 'empowered', 'experimental study', 'human-in-the-loop', 'improved', 'mobile computing', 'novel', 'prototype', 'response', 'social', 'social engagement', 'social learning', 'social skills', 'standard care', 'tool']",NIBIB,STANFORD UNIVERSITY,R01,2017,373870,0.045908541834206826
"Thought disorder and social cognition in clinical risk states for schizophrenia Project Summary  In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decade, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a wide range of cognitive processes to try to identify core deficits of schizophrenia evident before psychosis onset. Subthreshold thought disorder and impaired emotion recognition have emerged as profound deficits that predate, rather than follow, psychosis onset and thus may be indicators of schizophrenia liability, consistent with studies in other risk cohorts, including genetic high risk. Further, subthreshold thought disorder and emotion recognition deficit are significantly correlated, suggesting shared neural substrates in temporoparietal regions.  This study aims to identify the neural mechanisms that underlie subthreshold thought disorder and emotion recognition deficit in 125 CHR individuals followed prospectively for psychosis outcome. CHR cohorts are enriched with early cases of schizophrenia, as 20-25% develop schizophrenia and related psychotic disorders within 1-2 years. CHR cohorts may be optimal for studying core characteristics of illness as they otherwise have low-level symptoms, less illness chronicity and minimum exposure to antipsychotics. 25 individuals with schizophrenia and 50 healthy volunteers are included for comparison.  Subthreshold thought disorder and emotion recognition deficits will be studied across behavioral, physiological and circuit levels. For thought disorder, we will use automated speech analysis approaches developed in collaboration with IBM to identify constituent impairments in semantics and syntax, and a listening task that elicits reliable activation in language circuits. Our automated machine-learning approach to speech analysis, informed by artificial intelligence, derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to what they read or hear. Emotion recognition will be measured using standard tasks, naturalistic tasks with dynamic face stimuli and parametric face morph tasks that discriminate between perception and appraisal; task-related BOLD activity will be used to identify relevant circuits. Associations with basic sensory impairment will be tested, including novel auditory mismatch negativity paradigms. Resting state functional connectivity (RSFC) methods will be used for circuit-level analysis of language production and emotion recognition across stages of illness, to determine unique and shared substrates of these constructs in early schizophrenia. If successful, this proposal will identify neural targets for remediation of cognitive impairments. Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9331744,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Chronic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Cognitive remediation', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'EEG-based imaging', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Production', 'Prospective Studies', 'Psychotic Disorders', 'Public Health', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Severities', 'Social Functioning', 'Speech', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'auditory processing', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'predictive modeling', 'prevent', 'prognostic tool', 'prognostic value', 'prospective', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual processing', 'young adult']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2017,193734,0.061599684698335426
"Craniofacial Microsmia: Facial Expression from Ages 1 to 3 Years Project Summary Significance. Craniofacial microsomia (CFM) impairs facial muscle movement, speech, and hearing, and compromises socio-emotional development. Children with CFM have elevated levels of internalizing behavior (shy, withdrawn) and reduced social competence and peer acceptance. Unknown at present are the mechanisms through which CFM and these social-emotional outcomes become linked. Facial asymmetries and cranial neuropathies associated with CFM likely play an important role in impairing socio-emotional outcomes. Asymmetries of the facial skeleton, soft tissue, and cranial nerve have both intra- and interpersonal effects. Intra-personally, they impact function (unilateral hearing loss, malocclusion, facial expressiveness) and form (noticeable craniofacial malformations), which can impair social signaling and responsiveness. Because asymmetry is negatively correlated with attractiveness, there may be non-specific social effects as well. Many surgical treatments for CFM are designed to restore facial symmetry in static pose (e.g., neutral expression). Less is known about restoring or even measuring spontaneous facial expressiveness. From a developmental perspective, one of the most important consequences of limitations in facial muscle movement is its potentially negative impact on affective communication. In a longitudinal design, we propose to test the hypothesis that deficits in facial expressiveness and structural and functional asymmetry increase risk for internalizing and externalizing problems. If supported, the findings would inform our understanding of socio-emotional development in children with CFM and contribute to clinical evaluation and treatment. Innovation. This is the first effort to 1) use automated, objective measurement of facial expressiveness of communicative behavior and functional asymmetry of children with CFM; 2) model change with development in these parameters and their relation to internalizing and externalizing problems; and 3) use machine learning to investigate the relation among dynamics of expressiveness and asymmetry in relation to CBCL. Approach. Children with and without CFM will be video-recorded at 1 and 3 years with an examiner. Age 1 is an interactive context intended to elicit positive and negative emotion. Age 3 is an interactive context to assess expressive speech and attention. Expressiveness and structural and functional asymmetry are assessed using automatic, objective computer-vision based measurement. Analyses include complementary approaches: statistical (regression and ANOVA) hypothesis testing and machine learning (convolutional neural networks). Relevance Using objective, automatic computer-vision-based measurements, we propose to test the hypothesis that deficits in facial expressiveness and structural and functional asymmetry among children with CFM increases their risk for internalizing and externalizing problems. If supported, clinical assessments of expressiveness and functional asymmetry could effectively target children for specialized interventions and be used to evaluate surgical interventions. Because the proposed procedures are cost effective, they could be applied in a wide range of settings to benefit children with craniofacial disorders and have applicability to other conditions and age groups in which facial expression is compromised (e.g., Mobius Syndrome, Bell's Palsy, injury/burns, and stroke). Project Narrative Craniofacial microsomia (CFM) is among the most common craniofacial anomalies. CFM impairs facial muscle movement, speech, and hearing, and compromises socio-emotional development. Using computer-vision based face analysis, statistical analysis (regression and ANOVA), and machine learning, we will investigate the relation among expressiveness, structural and functional asymmetry, and behavior problems in children with CFM.",Craniofacial Microsmia: Facial Expression from Ages 1 to 3 Years,9387016,R03DE026513,"['1 year old', '3 year old', 'Affective', 'Age', 'Articulation', 'Attention', 'Behavior', 'Bell Palsy', 'Biological Neural Networks', 'Burn injury', 'Child', 'Child Behavior Checklist', 'Clinical Treatment', 'Clinical assessments', 'Communication', 'Computer Vision Systems', 'Cranial Nerves', 'Cranial nerve diseases', 'Craniofacial Abnormalities', 'Development', 'Disease', 'Emotional', 'Emotions', 'Face', 'Facial Expression', 'Facial Muscles', 'Facial asymmetry', 'Funding', 'Hearing', 'Impairment', 'Intervention', 'Linear Regressions', 'Link', 'Machine Learning', 'Malocclusion', 'Measurement', 'Measures', 'Mobius Syndrome', 'Modeling', 'Movement', 'National Institute of Dental and Craniofacial Research', 'Operative Surgical Procedures', 'Outcome', 'Participant', 'Play', 'Problem behavior', 'Procedures', 'Risk', 'Role', 'Sampling', 'Signal Transduction', 'Skeleton', 'Speech', 'Statistical Data Interpretation', 'Stroke', 'Testing', 'Time', 'Training', 'Unilateral Hearing Loss', 'Validation', 'Video Recording', 'age group', 'base', 'behavioral outcome', 'cost effective', 'craniofacial', 'craniofacial microsomia', 'design', 'innovation', 'longitudinal analysis', 'longitudinal design', 'malformation', 'outcome prediction', 'parent grant', 'peer', 'research clinical testing', 'social', 'social skills', 'soft tissue']",NIDCR,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R03,2017,168654,0.085343060618587
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9199411,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Controlled Study', 'Databases', 'Detection', 'Devices', 'Dimensions', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2017,319382,0.06958067002054058
"Development of a novel neurotechnology to promote emotion recognition in autism DESCRIPTION (provided by applicant): Difficulties in facial emotion recognition (FER) are thought to cause or exacerbate social disability in people with autism spectrum disorder (ASD) by preventing 1) accurate detection of social/emotional information conveyed through the face, particularly the eye-region, and 2) the deployment of emotionally appropriate responses. Although the neural systems thought to underlie FER deficits in ASD are increasingly appreciated, their plasticity remains speculative. The goal of this project is to develop an assistive technology to promote facial emotion recognition in ASD [R21]. We propose that FER can be rehabilitated using a brain-computer interface (BCI) device [R33]. To develop an FER assistant, we plan to first [R21] determine whether it is possible to develop a multi-voxel classifier that is temporally predictive of successful emotion recognition during functional magnetic resonance imaging (fMRI). An adaptive, real-time fMRI (rt-fMRI) paradigm will interpret the output of a subject's brain to assess whether a computer-generated actor's emotion is recognized. If not, the expressed facial emotion will be increased in intensity until the computer determines that the subject has recognized the emotion. After tuning this supervised learning algorithm produced by a support vector machine (SVM), we then transform the massively multidimensional classifier to low-dimensionality space, which can be replicated by a single- or dual-EEG sensor placed on the scalp. The proof of principle is that the multivariate classifier can be forward transformed into frequency (EEG) space. The EEG sensor can be comfortably worn outside of the scanner (BCI device), and can be wirelessly linked to a portable tablet (iPad). We will then demonstrate the feasibility of an ambulatory BCI 'FER assistant' [R33] in a between-group, randomized design (genuine neurofeedback vs placebo neurofeedback). The FER assistant is a virtual reality- based iPad application that uses the EEG sensor data to assist users with emotion recognition by manipulating the avatar's emotion intensity until it is recognized by the user, who will receive points the earlier the emotion is recognized. The purpose of this randomized controlled trial (RCT) is to assess feasibility including acceptability of the intervention, recruitment and randomization procedures, intervention implementation, blinded assessment procedures, and participant retention within the context of an RCT in preparation for a well- powered efficacy trial. This study's products include demonstration of the neural processes that underlie FER deficits and evidence of their plasticity, and an easily exportable, minimal-cost computer-based intervention. There has been little treatment research for this under-studied population, and social deficits may post unique challenges to people with ASD during late adolescence and early adulthood, as they face multiple life transitions and developmental tasks requiring social competence (e.g., securing employment). Ultimately, we plan to evaluate the efficacy of this emergent intervention in an adequately powered randomized clinical trial. PUBLIC HEALTH RELEVANCE: The social disability that characterizes Autism Spectrum Disorder (ASD) pervades other areas of adaptive behavior, is predictive of secondary mental health problems, and adversely affects long-term outcome. Although ASD is a chronic condition, there has been little research on interventions for adults with ASD to target social disability. We propose to first establish the neural plasticity of specific brain mechanisms underlying difficultis with facial emotion recognition, a core deficit believed to be pivotal in the behavioral expression of ASD-social disability, and subsequently develop a novel, computer-based intervention using real-time feedback to ameliorate emotion recognition deficits.",Development of a novel neurotechnology to promote emotion recognition in autism,9232220,R33MH100268,"['Adaptive Behaviors', 'Adolescence', 'Adolescent', 'Adult', 'Affect', 'Algorithms', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Markers', 'Blinded', 'Brain', 'Chronic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Computer Simulation', 'Computers', 'Correlation Studies', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Dimensions', 'Electroencephalography', 'Emotional', 'Emotions', 'Employment', 'Eye', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Mental Health', 'Methodology', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Neuronal Plasticity', 'Outcome', 'Output', 'Participant', 'Pattern', 'Placebos', 'Population', 'Preparation', 'Procedures', 'Process', 'Randomized', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Recruitment Activity', 'Reporting', 'Research', 'Sampling', 'Scalp structure', 'Secure', 'Self-Help Devices', 'Signal Transduction', 'Social Environment', 'Supervision', 'Symptoms', 'System', 'Tablets', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Wireless Technology', 'Work', 'active method', 'autism spectrum disorder', 'base', 'brain computer interface', 'computer generated', 'control trial', 'cost', 'design', 'disability', 'efficacy trial', 'emerging adult', 'innovation', 'neurofeedback', 'neurotechnology', 'novel', 'portability', 'prevent', 'programs', 'public health relevance', 'relating to nervous system', 'response', 'satisfaction', 'sensor', 'skills', 'social', 'social skills', 'study population', 'tool', 'virtual reality']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R33,2017,259157,0.051295233889622094
"Computational and brain predictors of emotion cue integration The purpose of this project is to develop computational and brain-based models of emotion cue integration: people’s inferences about others’ emotions based on dynamic, multimodal cues. Observers often decide how targets feel based on cues such as facial expressions, prosody, and language. Such inferences scaffold healthy social interaction, and abnormal inference both marks and exacerbates social deficits in numerous psychiatric disorders. Psychologists and neuroscientists have studied emotion inference for decades, but the vast majority of this work employs simplified social cues, such as vignettes or static images of faces. By contrast, “real world” emotion cues are complex, dynamic, and multimodal. Cue integration—inference based on naturalistic emotion information—likely differs from simpler inference at cognitive and neural levels, but this phenomenon remains poorly understood. This means that scientists lack a clear model of how observers adaptively process complex emotion cues, and how that processing goes awry in mental illness. Especially lacking are mechanistic models that can describe the computations and brain processes involved in cue integration with sufficient precision to predict inference in new cases, observers, and samples. This project will merge tools from social psychology, computer science, and neuroscience to generate a novel and rigorous model of emotion cue integration. We have demonstrated that in the face of complex emotion cues, observers dynamically “weight” cues from each modality (e.g., visual, linguistic) over time, a process that (i) tracks shifts in brain activity and connectivity; and (ii) can be captured using Bayesian models. Here, we will expand this work in several ways. First, we will develop precise computational tools to isolate features of emotion cues—such as facial movements, prosody, and linguistic sentiment—that track observers’ use of each cue modality during integration. Second, we will develop multi-region “signatures” of brain activity and connectivity that track emotion inference in each modality. We will use these signatures in conjunction with machine learning to predict unimodal emotion inference and cue integration in new observers and samples, based on brain data alone. Third, we will explore the context-dependence of naturalistic emotion inference by testing whether reinforcement learning can bias observers’ cue integration and accompanying brain signatures. Finally, we will model computational and neural abnormalities associated with cue integration in patients with Major Depressive Disorder and Bipolar Disorder. At the level of basic science, these data will generate a fundamentally new—and more naturalistic—approach to the neuroscience of emotion inference. The computational and brain metrics we produce will also be made publically available to facilitate the open and cumulative study of emotion inference across labs. At a translational level, we will provide a mechanistic, rich account of abnormal emotion inference in mood disorders, paving the way for computational and brain markers that can be used to assess social dysfunction and treatment efficacy in these and other mental illnesses. PROJECT NARRATIVE The proposed research will use methods from social psychology, cognitive neuroscience, and computer science to (i) precisely model people’s inferences about others’ emotions based on complex, dynamic cues, (ii) generate multi-region, brain-based predictors of these inferences, and (iii) characterize abnormalities in inference among individuals with mood disorders. Several psychiatric and neurodevelopmental disorders are characterized by difficulties understanding others’ emotions, which in turn worsen social functioning in patients. In addition to providing new insights about social processes—a core target within the NIMH’s research domain criteria (RDoC) framework—the proposed research will offer powerful, novel computational and neural targets through which to assess and treat difficulties in emotion inference, and to eventually reduce the social burden faced by people with mental illness on a broad scale.",Computational and brain predictors of emotion cue integration,9286515,R01MH112560,"['Affect', 'Affective', 'Agreement', 'Base of the Brain', 'Basic Science', 'Bayesian Modeling', 'Bipolar Disorder', 'Brain', 'Brain region', 'Classification', 'Cognitive', 'Complex', 'Computer Simulation', 'Cues', 'Data', 'Dependence', 'Emotional', 'Emotions', 'Event', 'Exhibits', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Image', 'Individual', 'Language', 'Lateral', 'Learning', 'Life', 'Linguistics', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Moods', 'Movement', 'National Institute of Mental Health', 'Neurodevelopmental Disorder', 'Neurosciences', 'Observer Variation', 'Participant', 'Patients', 'Pattern', 'Perception', 'Process', 'Psychological reinforcement', 'Psychologist', 'Recruitment Activity', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Running', 'Sampling', 'Scanning', 'Scientist', 'Sensory', 'Social Functioning', 'Social Interaction', 'Social Psychology', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Visual', 'Weight', 'Work', 'base', 'brain abnormalities', 'cognitive neuroscience', 'computer science', 'computerized tools', 'executive function', 'insight', 'language comprehension', 'multimodality', 'neuroimaging', 'novel', 'relating to nervous system', 'response', 'scaffold', 'social', 'tool']",NIMH,STANFORD UNIVERSITY,R01,2017,505201,0.13157813104076466
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9314855,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modality', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Recruitment Activity', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data management', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'executive function', 'experience', 'follow-up', 'hedonic', 'high dimensionality', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'learning strategy', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2017,784664,0.06348538773156562
"Thought disorder and social cognition in clinical risk states for schizophrenia No abstract available Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9563758,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Chronic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Cognitive remediation', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'EEG-based imaging', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Production', 'Prospective Studies', 'Psychotic Disorders', 'Public Health', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Severities', 'Social Functioning', 'Speech', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'auditory processing', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'predictive modeling', 'prevent', 'prognostic tool', 'prognostic value', 'prospective', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual processing', 'young adult']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,380712,0.02277107712649141
"Dynamics of Large-Scale Networks During Emotional and Social Processing The goal of this application is to develop a research program that, as stated in the call entitled The Neural Mechanisms of Multi-Dimensional Emotional and Social Representation (RFA-MH- 17-300), incorporates innovative approaches designed to move the fields of affective and social neuroscience beyond single region-based, modular, and static models of brain function and behavior. The RFA calls for research that is multi-dimensional, that is, that investigates the role of (among others) complex contexts, as well as distributed and/or dynamic processes that unfold over time. The objective of the present application is to jointly investigate emotional and social processes in a richly multi-dimensional manner. Aim 1: Network organization and evolution during emotional and social processing. The objective of this aim is to uncover how large-scale brain networks are organized and evolve temporally during emotional and social processing. Networks will include brain regions that robustly respond to the tasks proposed, and regions from well characterized networks, including the salience, executive control, and task-negative networks. Aim 2: Naturalistic processing during emotional and social processing. The objective of this aim is to understand naturalistic processing of emotional and social information. Although standard experimental designs afford great control over experimental conditions, they lack ecological validity and restrict the experiments that can be studied. We propose to investigate continuous (“naturalistic”) processing during movie watching involving emotional and social content. Continuous processing will be investigated via intersubject correlation analysis, which measures the extent to which signals are correlated across participants. Aim 3: Development of network organization/evolution and naturalistic processing. The objective of this aim is to investigate multi-dimensional emotional and social processes from a developmental perspective. Most developmental research in emotion has focused on observing amygdala responses and those of a few other brain regions during face perception. We focus on a question largely neglected in prior research, specifically sustained threat processing and the involvement of the bed nucleus of the stria terminalis. In the context of social processing, this aim examines the development of intersubject synchrony. Across the emotional and social domains, we propose to study middle childhood (8-9 y), early adolescence (12-13 y), and young adulthood (18-19 y). The goal of this application is to investigate the neural mechanisms of multi-dimensional emotional and social representation. Functional MRI experiments are proposed to investigate the following general questions: organization and evolution of brain networks; naturalistic processing during movie watching; and development.",Dynamics of Large-Scale Networks During Emotional and Social Processing,9283803,R01MH112517,"['Adolescence', 'Adopted', 'Affective', 'Amygdaloid structure', 'Anxiety', 'Autistic Disorder', 'Behavior', 'Brain', 'Brain region', 'Complex', 'Data', 'Development', 'Developmental Process', 'Disease', 'Emotional', 'Emotions', 'Evolution', 'Experimental Designs', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Individual Differences', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Participant', 'Pathway Analysis', 'Persons', 'Process', 'Property', 'Research', 'Risk', 'Role', 'Series', 'Signal Transduction', 'Social Development', 'Social Environment', 'Social Interaction', 'Stimulus', 'Structure', 'Structure of terminal stria nuclei of preoptic region', 'Testing', 'Time', 'Work', 'affective neuroscience', 'age group', 'base', 'design', 'early adolescence', 'executive function', 'experimental study', 'face perception', 'graph theory', 'improved', 'innovation', 'middle childhood', 'movie', 'neglect', 'neurobiological mechanism', 'neuromechanism', 'novel', 'peer', 'positive mood', 'programs', 'relating to nervous system', 'response', 'social', 'social neuroscience', 'young adult']",NIMH,"UNIV OF MARYLAND, COLLEGE PARK",R01,2017,675505,0.10500034482042042
"Prefrontal-Amygdala Interactions in Social Learning PROJECT SUMMARY This R01 renewal application proposes functional neuroimaging studies with human subjects to elucidate the role of the prefrontal cortex and amygdala in the processing of facial identities and expressions that predict critical social outcomes. Presentations of facial expressions of emotion in neuroimaging studies have proven particularly robust stimuli for activating amygdala and prefrontal regions involved in processing biologically- relevant social cues. Here we propose to further develop our novel structural and functional neuroimaging methods to better understand how the amygdala interacts with reciprocally connected prefrontal areas when such expressions are encountered. Specifically, following up on our previous findings that the structural integrity of an amygdala-prefrontal pathway predicts individual differences in reported anxiety – we replicated this effect in > 250 subjects and observed an exciting sex difference; this effect is compellingly stronger in females than in males. Here we propose to follow up on this effect with higher resolution DTI methods and to extend it to functional resting state data to see if the same sex difference is observed functionally. In addition, we propose a new mathematical model where we believe we can disentangle the effects of valence from arousal in brain imaging data, a confound the field continues to struggle with. Finally, we propose the development of a new facial expression stimulus set where we record the psychological status of the models posing for the expressions so we can determine any interaction this might have with the psychological status of the our subjects of study. The field can then usefully compare these data to complementary developmental research (i.e., with children and adolescents) and will be amenable  to direct translation to clinical populations (e.g., anxiety and depression). The experiments proposed here will increase our understanding of the brain mechanisms involved in  this learning. A number of experiments highlight an important difference between men and women that  we discovered during our last grant and follow up on here. Specifically, brain connections with the  prefrontal cortex in females explain how anxious they report being but we do not see this effect in  males. With this information, we can then better understand what goes wrong in the brains of  individuals with major depression and anxiety disorders.",Prefrontal-Amygdala Interactions in Social Learning,9499980,R56MH080716,"['Adolescent', 'Affect', 'Affective', 'Amygdaloid structure', 'Anatomy', 'Anxiety', 'Anxiety Disorders', 'Area', 'Arousal', 'Attention', 'Award', 'Behavioral', 'Biological', 'Brain', 'Brain imaging', 'Child', 'Clinical', 'Cognitive', 'Collaborations', 'Complex', 'Cues', 'Data', 'Development', 'Emotional', 'Emotional disorder', 'Face', 'Facial Expression', 'Failure', 'Female', 'Friends', 'Grant', 'Human', 'Image', 'Individual', 'Individual Differences', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Measurement', 'Measures', 'Mental Depression', 'Methods', 'Modeling', 'Outcome', 'Participant', 'Pathway interactions', 'Population', 'Prefrontal Cortex', 'Publishing', 'Regulation', 'Reporting', 'Research', 'Resolution', 'Rest', 'Role', 'Same-sex', 'Sampling', 'Sex Characteristics', 'Signal Transduction', 'Social Behavior', 'Stimulus', 'Structure', 'Study Subject', 'Supervision', 'System', 'Techniques', 'Translations', 'Woman', 'Work', 'anxious', 'base', 'behavioral outcome', 'behavioral response', 'experimental study', 'follow-up', 'human subject', 'indexing', 'innovation', 'male', 'mathematical model', 'men', 'neuroimaging', 'novel', 'psychologic', 'relating to nervous system', 'response', 'showing emotion', 'social', 'social learning', 'social situation', 'two-dimensional', 'white matter']",NIMH,DARTMOUTH COLLEGE,R56,2017,405000,0.045777171845548616
"Neurophysiology of auditory emotion recognition in the human brain Summary/Abstract:  Auditory emotion recognition (AER) is the process of extracting emotional content out of the acoustic features of speech, and is disrupted in many neuropsychiatric disorders including autism and schizophrenia. Both the encoding of this information in the auditory cortex as well as the knowledge of which brain areas participate in the circuit that underlies AER are not known. With a unique access to invasive intracranial neurophysiology in awake, behaving humans, we propose to study the neural basis of AER in patients undergoing invasive electrode monitoring as part of staged epilepsy surgery protocols. These subjects will perform an AER task while brain responses are recorded in an effort to decipher the neural processes that identify emotion and while specific areas of their brain are stimulated in order to causally link focal activation of AER circuit components to perception.  In Aim 1, we will identify the neuronal mechanisms underlying emotion recognition using a task that breaks down AER into three sequential steps: encoding, integration and judgment. First, we will identify the encoding of emotionally relevant acoustic cues by examining how sensory features such as pitch, intensity and modulation are encoded by neuronal responses in the auditory cortex and other brain areas involved in emotional processing such as the amygdala, insula and prefrontal cortex. Second, we will examine how these acoustic cues are integrated into an emergent emotional percept by measuring activity in putative areas as well as the interaction among these areas after relevant sensory input while subjects consider possible responses. Finally, we will examine the subjects’ response to the AER task and correlate the judgment of emotion with the extent to which it can be predicted by neuronal activity in different brain regions of interest. While most analyses will focus on intraindividual data, differences in brain activity as it relates to AER will be compared in males versus females in order to examine biological differences in AER between sexes. In aim 2, we will determine the effect of neural stimulation on AER. We will apply focal single pulse stimulation to brain areas at those critical time windows identified in aim 1 in order to bias emotional decisions. This type of stimulation results in very focal and temporally restricted disruption of function and can be used to determine a brain area’s causal role in perception.  We seek to build a detailed neurobiological model of the AER. This would be a precursor to designing individualized therapeutic approaches to treat deficits in auditory emotional processing as well as identifying putative targets to treat affective disorders in general. Narrative: Identifying emotion in speech is a critical brain process required for social interaction and is impaired in many neuropsychiatric diseases. We will study how the brain codes the emotional content of speech and how stimulating specific brain areas can potentially improve this process. !",Neurophysiology of auditory emotion recognition in the human brain,9373513,R21MH114166,"['Acoustics', 'Affect', 'Amygdaloid structure', 'Animal Model', 'Area', 'Attention', 'Auditory', 'Auditory area', 'Autistic Disorder', 'Behavioral', 'Biological', 'Brain', 'Brain region', 'Clinical Protocols', 'Code', 'Cognitive', 'Communication', 'Comprehension', 'Cues', 'Data', 'Disease', 'Electric Stimulation', 'Electrodes', 'Electrophysiology (science)', 'Elements', 'Emotional', 'Emotions', 'Engineering', 'Epilepsy', 'Evaluation', 'Female', 'Functional Magnetic Resonance Imaging', 'Human', 'Impairment', 'Individual', 'Insula of Reil', 'Judgment', 'Knowledge', 'Link', 'Machine Learning', 'Magnetoencephalography', 'Measures', 'Mental Depression', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Neurobiology', 'Neurons', 'Operative Surgical Procedures', 'Patients', 'Pattern', 'Perception', 'Performance', 'Personal Communication', 'Physiologic pulse', 'Physiological', 'Physiology', 'Play', 'Population', 'Prefrontal Cortex', 'Process', 'Protocols documentation', 'Psychiatrist', 'Psychologist', 'Public Health', 'Research', 'Resolution', 'Role', 'Scalp structure', 'Schizophrenia', 'Sensory', 'Social Interaction', 'Speech', 'Speech Pathologist', 'Speed', 'Stimulus', 'Stream', 'Structure', 'Superior temporal gyrus', 'Testing', 'Therapeutic', 'Time', 'Transcranial magnetic stimulation', 'Woman', 'Work', 'autism spectrum disorder', 'awake', 'behavioral study', 'computer studies', 'design', 'experimental study', 'human subject', 'improved', 'innovation', 'interest', 'learning strategy', 'male', 'men', 'neural model', 'neural stimulation', 'neuroimaging', 'neurophysiology', 'neuropsychiatric disorder', 'novel', 'physical property', 'relating to nervous system', 'response', 'sensory input', 'sensory system', 'sex', 'social', 'social skills', 'spatiotemporal', 'temporal measurement']",NIMH,FEINSTEIN INSTITUTE FOR MEDICAL RESEARCH,R21,2017,253863,0.08524525916615908
"Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues. People signal their internal emotional state by a range of cues including facial expressions, non-verbal vocalizations and the tone and content of speech. Much work to date on emotional signaling has focused on a small number of emotions and has relied on stimulus sets with limited numbers of exemplars and poor representation of individuals of different ages and ethnicities. The computer vision literature has long recognized the need to be able to compare models, for example of face recognition, across complex, diverse, and naturalistic stimulus sets. Here, we argue that a parallel approach needs to be applied if we are to achieve a robust and generalizable understanding of how the human brain represents emotion and identity related information derived from facial and vocal cues. Three multi-session functional magnetic resonance imaging (fMRI) experiments are proposed. In the first, participants will view a large corpus (~2000) of faces of individuals varying in age, gender and ethnicity and showing a wide range of emotional expressions. In the second, participants will be presented with a large (~1000) and equally diverse set of emotional vocalizations. The third experiment will make use of an even more complex and naturalistic stimulus set comprising ~1000 video clips of individuals expressing emotions through facial expression, non-verbal vocalizations and emotion-laden speech. This set will be broken into three parts, with as closely matched content as possible. These will be presented to participants in audio only, visual only and audio-visual (bimodal) conditions, with the stimuli allocated to each condition balanced across participants. For each experiment, data collection will comprise separate Model Estimation and Model Validation periods with the majority of stimuli presented at Validation being distinct from those presented at Estimation. A range of models will be fit to the fMRI data acquired during Estimation runs and tested and compared using data acquired during Validation runs. These models contain sets of features that describe the emotional content of the stimuli presented in terms of either dimensional or categorical models of emotion. By comparing the fit of these models we can determine which models capture most variance in voxel response profiles and examine how this varies across brain regions. Across the three experiments, we also seek to establish whether there are regions where voxels show common coding of emotional state regardless of whether information is carried by facial or vocal cues or a combination of both. Further, by contrasting models with and without terms for characteristics such as age, gender and ethnicity we can also investigate the (in)dependence of the representation of emotion and identity-related features. The proposed research has the potential to greatly advance our understanding of how cues to others' emotional state are represented in the human brain and the extent to which this is influenced by the characteristics of the person we are interacting with. In the medium term, we plan to extend this to also model listener/ viewer characteristics (both demographics and predisposition to anxiety or depression) in a hope to advance our understanding of how biases in the interpretation of emotional signals can arise. Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial  and vocal cues. Individuals with psychiatric conditions ranging from Social Anxiety and Depression to Autism Spectrum Disorder and Schizophrenia struggle to correctly interpret others' emotional states. To understand how alterations in brain function can give rise to the misperception of emotion cues, we first need better normative models of how information about others' emotional state derived from facial and vocal cues is represented within the human brain. Hence, here we use large, diverse and naturalistic stimulus sets encompassing facial expressions, emotional vocalizations and multi-modal videos to investigate the extent to which consistent representation of emotional state is observed across stimulus modalities, and to explore the organizational principles that can best explain voxel response profiles or tuning patterns to facial and vocal emotion cues.",Multi-feature modeling of the neural representation of emotion- and identity-related information derived from facial and vocal cues,9285649,R01MH112541,"['Address', 'Age', 'Anxiety', 'Arousal', 'Auditory', 'Brain', 'Brain region', 'Categories', 'Characteristics', 'Clip', 'Code', 'Complex', 'Computer Vision Systems', 'Cues', 'Data', 'Data Collection', 'Data Set', 'Decision Making', 'Dependence', 'Dimensions', 'Emotional', 'Emotions', 'Ethnic Origin', 'Expressed Emotion', 'Face', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Gender', 'Human', 'Image', 'Individual', 'Label', 'Literature', 'Mental Depression', 'Mental disorders', 'Modality', 'Modeling', 'Participant', 'Pattern', 'Persons', 'Predisposition', 'Property', 'Research', 'Running', 'Schizophrenia', 'Signal Transduction', 'Source', 'Speech', 'Stimulus', 'Structure', 'Techniques', 'Testing', 'Validation', 'Visual', 'Weight', 'Work', 'autism spectrum disorder', 'blood oxygen level dependent', 'data acquisition', 'data modeling', 'demographics', 'experimental study', 'imaging study', 'information model', 'neural model', 'novel', 'relating to nervous system', 'response', 'showing emotion', 'social anxiety', 'theories', 'trait', 'visual information', 'visual neuroscience', 'vocalization']",NIMH,UNIVERSITY OF CALIFORNIA BERKELEY,R01,2017,125000,0.17660734439641768
"An Olfactory Method for Controlling Cigarette Craving DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving,9379081,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Neurotics', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'behavioral pharmacology', 'biobehavior', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'experimental study', 'hedonic', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'multimodality', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'psychologic', 'public health relevance', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2017,54468,0.06274245663647815
"An Olfactory Method for Controlling Cigarette Craving DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving,9256440,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Neurotics', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'behavioral pharmacology', 'biobehavior', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'experimental study', 'hedonic', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'multimodality', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'psychologic', 'public health relevance', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2017,307746,0.06274245663647815
"Thought disorder and social cognition in clinical risk states for schizophrenia Project Summary  In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decade, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a wide range of cognitive processes to try to identify core deficits of schizophrenia evident before psychosis onset. Subthreshold thought disorder and impaired emotion recognition have emerged as profound deficits that predate, rather than follow, psychosis onset and thus may be indicators of schizophrenia liability, consistent with studies in other risk cohorts, including genetic high risk. Further, subthreshold thought disorder and emotion recognition deficit are significantly correlated, suggesting shared neural substrates in temporoparietal regions.  This study aims to identify the neural mechanisms that underlie subthreshold thought disorder and emotion recognition deficit in 125 CHR individuals followed prospectively for psychosis outcome. CHR cohorts are enriched with early cases of schizophrenia, as 20-25% develop schizophrenia and related psychotic disorders within 1-2 years. CHR cohorts may be optimal for studying core characteristics of illness as they otherwise have low-level symptoms, less illness chronicity and minimum exposure to antipsychotics. 25 individuals with schizophrenia and 50 healthy volunteers are included for comparison.  Subthreshold thought disorder and emotion recognition deficits will be studied across behavioral, physiological and circuit levels. For thought disorder, we will use automated speech analysis approaches developed in collaboration with IBM to identify constituent impairments in semantics and syntax, and a listening task that elicits reliable activation in language circuits. Our automated machine-learning approach to speech analysis, informed by artificial intelligence, derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to what they read or hear. Emotion recognition will be measured using standard tasks, naturalistic tasks with dynamic face stimuli and parametric face morph tasks that discriminate between perception and appraisal; task-related BOLD activity will be used to identify relevant circuits. Associations with basic sensory impairment will be tested, including novel auditory mismatch negativity paradigms. Resting state functional connectivity (RSFC) methods will be used for circuit-level analysis of language production and emotion recognition across stages of illness, to determine unique and shared substrates of these constructs in early schizophrenia. If successful, this proposal will identify neural targets for remediation of cognitive impairments. Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9176279,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'Electroencephalography', 'Electrophysiology (science)', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Modeling', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Process', 'Production', 'Psychotic Disorders', 'Public Health', 'Reading', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Sensory Process', 'Severities', 'Social Functioning', 'Speech', 'Staging', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'prevent', 'prognostic tool', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual process', 'visual processing', 'young adult']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,R01,2016,574983,0.061599684698335426
"Automated Facial Expression Analysis for Research and Clinical Use DESCRIPTION (provided by applicant): Facial expression has been a focus of emotion research for over a hundred years. In recent decades observations of facial expressions have yielded critical and dramatic insights about the etiology of psychopathology, and have proven capable of predicting treatment outcomes (see Ekman & Rosenberg, 2005). Despite these original striking findings, there has been surprisingly little follow-up work. The primary reason fr the lack of sustained research is that the most reliable manual systems for measuring facial expression often require considerable training and are labor intensive. Automated measurement using computer vision and machine learning seeks to address the need for valid, efficient, and reproducible measurement. Recent systems have shown promise in fairly small studies using posed behavior or structured contexts with confederates, or trained interviewers, or pre-trained (person-specific) face models. For automated coding to be applied in real-world settings, a large data base with ample variability in pose, head motion, skin color, gender, partial occlusion, and expression intensity is needed. We have developed a unique database that meets this need and the algorithms necessary to enable robust automated coding. The database consists of 720 participants in three-person groups engaged in a group formation task. In a preliminary study, we demonstrated that our algorithms can successfully code two key facial signals associated with human emotion in this relatively unconstrained context (Cohn & Sayette, 2010). To achieve efficient, accurate, and valid measurement of facial expression usable in research and clinical settings, we aim to 1) train and validate classifiers to achieve reliable facial expression detectin across this unprecedentedly large, diverse data set; 2) extend the previous person-specific methods to person-independent (generic) facial feature detection, tracking, and alignment; and 3) make these tools available for research and clinical use. The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.",Automated Facial Expression Analysis for Research and Clinical Use,9029353,R01MH096951,"['Address', 'Algorithms', 'Anxiety', 'Behavior', 'Behavioral Sciences', 'Benchmarking', 'Bipolar Disorder', 'Clinical', 'Code', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Emotions', 'Employee Strikes', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Gender', 'Generic Drugs', 'Head', 'Hour', 'Human', 'Imagery', 'Instruction', 'Interview', 'Interviewer', 'Label', 'Laboratories', 'Learning', 'Lighting', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Movement', 'Nature', 'Pain', 'Participant', 'Personal Computers', 'Persons', 'Process', 'Psychopathology', 'Reporting', 'Research', 'Research Personnel', 'Running', 'Schizophrenia', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'clinical practice', 'follow-up', 'insight', 'meetings', 'nervous system disorder', 'positive emotional state', 'skin color', 'suicidal risk', 'tool']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2016,452541,0.14306841992987776
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ﻿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand.         PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.            ",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9054574,R01DC014498,"['Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Controlled Study', 'Databases', 'Detection', 'Devices', 'Educational process of instructing', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Hearing Impaired Persons', 'Human', 'Image', 'Individual', 'Intervention', 'Joints', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deafness', 'design', 'experience', 'face perception', 'innovation', 'instructor', 'interest', 'prevent', 'public health relevance', 'reconstruction', 'research study', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2016,331310,0.06958067002054058
"Development of a novel neurotechnology to promote emotion recognition in autism DESCRIPTION (provided by applicant): Difficulties in facial emotion recognition (FER) are thought to cause or exacerbate social disability in people with autism spectrum disorder (ASD) by preventing 1) accurate detection of social/emotional information conveyed through the face, particularly the eye-region, and 2) the deployment of emotionally appropriate responses. Although the neural systems thought to underlie FER deficits in ASD are increasingly appreciated, their plasticity remains speculative. The goal of this project is to develop an assistive technology to promote facial emotion recognition in ASD [R21]. We propose that FER can be rehabilitated using a brain-computer interface (BCI) device [R33]. To develop an FER assistant, we plan to first [R21] determine whether it is possible to develop a multi-voxel classifier that is temporally predictive of successful emotion recognition during functional magnetic resonance imaging (fMRI). An adaptive, real-time fMRI (rt-fMRI) paradigm will interpret the output of a subject's brain to assess whether a computer-generated actor's emotion is recognized. If not, the expressed facial emotion will be increased in intensity until the computer determines that the subject has recognized the emotion. After tuning this supervised learning algorithm produced by a support vector machine (SVM), we then transform the massively multidimensional classifier to low-dimensionality space, which can be replicated by a single- or dual-EEG sensor placed on the scalp. The proof of principle is that the multivariate classifier can be forward transformed into frequency (EEG) space. The EEG sensor can be comfortably worn outside of the scanner (BCI device), and can be wirelessly linked to a portable tablet (iPad). We will then demonstrate the feasibility of an ambulatory BCI 'FER assistant' [R33] in a between-group, randomized design (genuine neurofeedback vs placebo neurofeedback). The FER assistant is a virtual reality- based iPad application that uses the EEG sensor data to assist users with emotion recognition by manipulating the avatar's emotion intensity until it is recognized by the user, who will receive points the earlier the emotion is recognized. The purpose of this randomized controlled trial (RCT) is to assess feasibility including acceptability of the intervention, recruitment and randomization procedures, intervention implementation, blinded assessment procedures, and participant retention within the context of an RCT in preparation for a well- powered efficacy trial. This study's products include demonstration of the neural processes that underlie FER deficits and evidence of their plasticity, and an easily exportable, minimal-cost computer-based intervention. There has been little treatment research for this under-studied population, and social deficits may post unique challenges to people with ASD during late adolescence and early adulthood, as they face multiple life transitions and developmental tasks requiring social competence (e.g., securing employment). Ultimately, we plan to evaluate the efficacy of this emergent intervention in an adequately powered randomized clinical trial. PUBLIC HEALTH RELEVANCE: The social disability that characterizes Autism Spectrum Disorder (ASD) pervades other areas of adaptive behavior, is predictive of secondary mental health problems, and adversely affects long-term outcome. Although ASD is a chronic condition, there has been little research on interventions for adults with ASD to target social disability. We propose to first establish the neural plasticity of specific brain mechanisms underlying difficultis with facial emotion recognition, a core deficit believed to be pivotal in the behavioral expression of ASD-social disability, and subsequently develop a novel, computer-based intervention using real-time feedback to ameliorate emotion recognition deficits.",Development of a novel neurotechnology to promote emotion recognition in autism,9131476,R33MH100268,"['Adaptive Behaviors', 'Adolescence', 'Adolescent', 'Adult', 'Affect', 'Algorithms', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Markers', 'Blinded', 'Brain', 'Chronic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Computer Simulation', 'Computers', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Emotional', 'Emotions', 'Employment', 'Eye', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Neuronal Plasticity', 'Outcome', 'Output', 'Participant', 'Pattern', 'Placebos', 'Population', 'Preparation', 'Procedures', 'Process', 'Randomized', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Reporting', 'Research', 'Sampling', 'Scalp structure', 'Secure', 'Self-Help Devices', 'Signal Transduction', 'Social Environment', 'Symptoms', 'System', 'Tablets', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'active method', 'autism spectrum disorder', 'base', 'brain computer interface', 'computer generated', 'control trial', 'cost', 'design', 'disability', 'efficacy trial', 'emerging adult', 'innovation', 'neurofeedback', 'neurotechnology', 'novel', 'prevent', 'programs', 'relating to nervous system', 'response', 'satisfaction', 'sensor', 'skills', 'social', 'social skills', 'study population', 'tool', 'virtual reality']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R33,2016,346148,0.051295233889622094
"Automated assessment using facial coding Abstract  Transition-age youth (TAY) from ages 18-26 are especially vulnerable to substance use, misuse, and substance use disorder (SUD). This developmental period, termed the “age of instability"" and ""emerging adulthood"" includes tasks such as leaving home, entering college; identifying vocational goals; working for the first time; body and sexuality changes; coalescing with a peer group; and for some, aging out of foster care or state custody, which end at age 18. The emerging- adulthood period is prime for experimentation with substances and the development of SUD, and associated problems such as binge drinking, driving under the influence, accidents, fighting and violence, HIV, gang involvement, suicide and self-harm, and vulnerability to date-rape and other sexual assault. A major challenge for TAY is the capacity to regulate their emotions, given the hormone changes and emotional intensity of this stage of life. Indeed, substance use is often described as a short-term way to regulate emotion and there is a long-standing literature documenting the association between emotion regulation (ER) problems and SUD. The importance of ER is also grounded in the developmental literature in which ER is identified as a core skill that is critical to the successful transition to adulthood. The initial component of ER, accurate identification of emotion, is itself a major challenge, especially for people with SUD as they are often not aware of their feelings or confused about them due to the nature of the disorder as well as mental health disorders that often co-occur with SUD.  In this phase 1 proposal we describe the development of a mobile app that uses exciting new technology (automated facial coding) to help TAY better recognize their emotions in relation to SUD variables. We describe the use of front-end focus groups to provide pre-development input on the app; we detail the app features and user experience; and define the app’s technical specifications. Phase 1 also includes a pilot feasibility study of the prototype product among TAY with SUD. They will have four weeks to use the app in their natural environment, such as home, and we will conduct pre- and post-evaluation using validated instruments to characterize the sample, quantity satisfaction and feasibility, evaluate emotion- and substance-related variables, as well as explore the app metrics and its technical performance. If positive results are found this would set us up well for phase 2, which could expand the app in some really interesting directions as well as further study it in a randomized controlled trial. The app product we envision would have major public health and clinical impact. It could help improve TAY’s ability to become more aware of their emotions and understand them in relation to SUD via an exciting emotion-focused technology innovation. If the product is successful it could also be expanded to other populations. Our stellar team includes experts in SUD, TAY, clinical innovations, technology, and app development. Narrative We propose to help young adults (ages 18-26) who have substance abuse problems. Our goal is to help them learn more about their emotions in relation to their substance abuse as a way to ultimately help them reduce their substance use. We plan to use a sophisticated technology related to emotion recognition, and to use that to help support the young adults in learning more about managing their emotions. Our work will include the development of a mobile app related to these goals.",Automated assessment using facial coding,9199791,R43DA042640,"['Accidents', 'Adult', 'Age', 'Aging', 'Alcohol or Other Drugs use', 'Algorithms', 'Anger', 'Automation', 'Awareness', 'Cellular Phone', 'Classification', 'Clinical', 'Code', 'Computer Vision Systems', 'Computer software', 'Computers', 'Country', 'Data', 'Development', 'Devices', 'Disease', 'Emotional', 'Emotions', 'Environment', 'Evaluation', 'Face', 'Facial Expression', 'Feasibility Studies', 'Feedback', 'Feeling', 'Focus Groups', 'Forcible intercourse', 'Goals', 'Guns', 'HIV', 'Home environment', 'Hormones', 'Human', 'Learning', 'Left', 'Life', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Mental Depression', 'Mental disorders', 'Methods', 'Military Personnel', 'Nature', 'Overdose', 'Participant', 'Patient Self-Report', 'Peer Group', 'Performance', 'Persons', 'Pharmaceutical Preparations', 'Phase', 'Population', 'Psychiatric Diagnosis', 'Public Health', 'Randomized Controlled Trials', 'Reaction', 'Recovery', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Sales', 'Sampling', 'Self-Injurious Behavior', 'Sexuality', 'Smiling', 'Staging', 'Stress', 'Substance Use Disorder', 'Substance abuse problem', 'Suicide', 'System', 'Tablets', 'Technology', 'Thinking', 'Time', 'Touch sensation', 'Violence', 'Work', 'Youth', 'abstracting', 'base', 'binge drinking', 'cloud based', 'college', 'craving', 'driving under influence', 'emerging adulthood', 'emotion regulation', 'evidence base', 'experience', 'fighting', 'foster care', 'gang', 'improved', 'information display', 'innovation', 'instrument', 'interest', 'killings', 'lens', 'literacy', 'mobile application', 'new technology', 'prototype', 'response', 'satisfaction', 'sexual assault', 'skills', 'visual stimulus', 'young adult']",NIDA,"TREATMENT INNOVATIONS, LLC",R43,2016,225000,-0.02703095417628464
"Resting State and Task-Evoked Neural Bases of Rumination and Affective Dysfunction ﻿    DESCRIPTION (provided by applicant): [Depression is a common disorder with a major public health impact.] Recent research has linked [depression with abnormal function] in the default-mode network (DMN), a network of regions that is typically active at rest, and deactivates during cognitive tasks. [Depression has been linked with increased DMN connectivity] during resting-state fMRI, and a relative failure to suppress DMN when individuals view negative images. In addition, depressed individuals demonstrate enhanced and prolonged amygdala responses to negative images. [This has led to the theory that DMN abnormalities might interfere with recruitment of brain regions to regulate amygdala in depressed individuals.] [These neural markers in depressed individuals have also been related to brooding rumination, a type of maladaptive coping involving repetitive negative thoughts.] This work proposes to test this model using cross-modal comparison of fMRI data in individuals during a resting-state scan and while participants are viewing negative images. Two main hypotheses will be tested: 1) resting-state DMN [function] will predict decreased top-down regulation of emotion during viewing of negative images, and 2) increased resting-state DMN [function], and decreased top-down emotion regulation during negative-image-viewing, will be predicted by increased trait [brooding]. Assessment of top-down emotion regulation during negative-image-viewing will be multimodal and will focus on the time period 6-12s after image offset, capturing ""recovery"" from the negative image, while controlling for initial 6 s of activity in response to the image (""reactivity""). Measures will include amygdala activity, amygdala-prefrontal connectivity, and electromyography of the corrugator supercilii, a muscle that is a reliable indicator of the experience of negative emotion. [In addition, multivariate pattern analysis (MVPA) will be used to classify negative vs. neutral images as an alternative measure of neural responses.] DMN [function] will be assessed in two ways. First, functional connectivity between two major nodes of the DMN (medial prefrontal cortex and posterior cingulate cortex) will be calculated using correlation-based methods. Second, [a metric of DMN dominance over a cognitive control network will be calculated following methods used previously by other researchers. In order to assess how resting-state DMN function predicts top-down emotion regulation, these DMN metrics will be regressed onto all of the above metrics of emotion regulation. In order to assess how these measures correspond to trait brooding, brooding will be assessed using a standardized measure (the Ruminative Responses Scale; RRS) [and also regressed onto the outcome measures above]. This work will elucidate the relationship of DMN activity, [neural correlates of] emotion regulation and [brooding] with implications for risk and treatment of affective disorders.         PUBLIC HEALTH RELEVANCE: Depression is a common and devastating disorder with significant morbidity and mortality, but its cognitive and neural bases remain incompletely understood. The proposed study will assess a neurocognitive model of [depression] by modeling fMRI BOLD activity while participants are resting and also while they regulate emotion during a task, allowing us to make cross-modal comparisons of neural activity. [We will additionally assess individual differences in trait brooding, an important risk factor for depression.] Using these data, we will be able to explicate critical hypotheses about the neural basis of depression and its associations with emotional style and emotion regulation, which in turn will afford us further insights relevant to understanding mechanisms of risk and treatment for affective psychopathology.                ",Resting State and Task-Evoked Neural Bases of Rumination and Affective Dysfunction,9132618,F30MH106191,"['Affect', 'Affective', 'Amygdaloid structure', 'Behavior', 'Brain region', 'Classification', 'Clinical', 'Cognitive', 'Communities', 'Data', 'Depressed mood', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Down-Regulation', 'Electromyography', 'Emotional', 'Emotions', 'Failure', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Hyperactive behavior', 'Image', 'Individual', 'Individual Differences', 'Link', 'Machine Learning', 'Maintenance', 'Major Depressive Disorder', 'Maps', 'Measures', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'Mood Disorders', 'Morbidity - disease rate', 'Multivariate Analysis', 'Muscle', 'Neurocognitive', 'Nucleic Acid Regulatory Sequences', 'Outcome', 'Outcome Measure', 'Participant', 'Pattern', 'Prefrontal Cortex', 'Psychopathology', 'Psychophysiology', 'Public Health', 'Recovery', 'Regulation', 'Research', 'Research Design', 'Research Personnel', 'Rest', 'Risk', 'Risk Factors', 'Role', 'Sampling', 'Scanning', 'Signal Transduction', 'Techniques', 'Testing', 'Thinking', 'Time', 'Validation', 'Work', 'base', 'cingulate cortex', 'cognitive control', 'cognitive task', 'coping', 'depression model', 'emotion regulation', 'emotional adjustment', 'emotional experience', 'experience', 'insight', 'interest', 'mortality', 'neural correlate', 'neuromechanism', 'prognostic significance', 'public health relevance', 'relating to nervous system', 'response', 'standardize measure', 'success', 'symptomatology', 'theories', 'trait']",NIMH,UNIVERSITY OF WISCONSIN-MADISON,F30,2016,48332,0.05155257841932038
"Modeling the Dynamics of Early Communication and Development DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts. PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,9124921,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Health', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Untranslated RNA', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'dynamic system', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2016,504474,0.05297480669690097
"The Development and Neural Bases of Emotion Processing DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness. Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.",The Development and Neural Bases of Emotion Processing,9033949,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Social Behavior', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'anxious behavior', 'base', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'signal processing', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2016,871361,0.12275310936264823
"An Olfactory Method for Controlling Cigarette Craving DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving,9038335,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Computers', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Health', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Staging', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'research study', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2016,308090,0.06274245663647815
"An Olfactory Method for Controlling Cigarette Craving DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving,9240944,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Computers', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Health', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Staging', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'research study', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2016,54468,0.06274245663647815
"Face De-Identification for Research and Clinical Use DESCRIPTION (provided by applicant): This application addresses NIH's call to promote data sharing and patient privacy. A major obstacle to sharing of recorded video has been the need to protect participants' identity. Similarly, concern about stigma is a reason that many people in need of mental health services (e.g., in the military) fail to do so. We propose a system to de-identify patients and research participants in video. Face de-identification transfers facial expression automatically from source face images, which are confidential, to target face images, which are not. The system safeguards face anonymity while preserving the facial expression of the original source video. The target video then can communicate the emotion, communicative intent, pain, and neurological or physiological status of the source person without displaying the source person's face. Face de-identification would enable video archive sharing among researchers and clinicians without compromising privacy or confidentiality. Moreover, a version of this system could potentially be used to preserve privacy and anonymity in internet-based interviews. Innovation. The project has four innovations. The approach (1) Removes identity information while retaining facial dynamics; thus preserving the information value of the face to communicate emotion, pain, and related states. (2) Accommodates subtle and spontaneous facial actions, rather than imitating only some predefined molar expressions (e.g., happy or sad). (3) Requires no training steps by target persons. (4) And requires no hand annotation of video. The system is entirely automatic. Approach. The software will take as input a video with the face of a subject (source) and automatically generate or output a video with the face de-identified. The project will use new machine learning and computer vision algorithms for transferring subtle facial expression from a source subject (original video) to a target subject, using only one frontal image of the target subject. A major novelty of the approach is to make the process completely automatic. The algorithm will be validated using commercially available software for face recognition and custom software for facial expression analysis. PUBLIC HEALTH RELEVANCE: Sharing of video recordings for research and clinical uses would significantly contribute to scientific discovery and patient diagnosis, treatment, and evaluation. We will develop and validate a fully automatic system that preserves facial expression in video while fully protecting face identity.",Face De-Identification for Research and Clinical Use,8908047,R21MH099487,"['Address', 'Age', 'Algorithms', 'Archives', 'Behavioral Sciences', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Vision Systems', 'Computer software', 'Confidentiality', 'Custom', 'Data Set', 'Detection', 'Diagnosis', 'Education', 'Emotions', 'Evaluation', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Goals', 'Gold', 'Hand', 'Health', 'Human', 'Image', 'Informed Consent', 'Intention', 'Internet', 'Interview', 'Judgment', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health Services', 'Methods', 'Military Personnel', 'Modeling', 'Neurologic', 'Output', 'Pain', 'Participant', 'Patients', 'Perception', 'Personal Computers', 'Persons', 'Physiological', 'Privacy', 'Process', 'Research', 'Research Personnel', 'Running', 'Source', 'Step training', 'System', 'Testing', 'Training', 'Twin Multiple Birth', 'Video Recording', 'base', 'clinical practice', 'data sharing', 'graphical user interface', 'innovation', 'middle age', 'patient privacy', 'sex', 'social stigma', 'targeted imaging', 'user-friendly', 'young adult']",NIMH,CARNEGIE-MELLON UNIVERSITY,R21,2015,193512,0.06039374988311494
"Automated Facial Expression Analysis for Research and Clinical Use DESCRIPTION (provided by applicant): Facial expression has been a focus of emotion research for over a hundred years. In recent decades observations of facial expressions have yielded critical and dramatic insights about the etiology of psychopathology, and have proven capable of predicting treatment outcomes (see Ekman & Rosenberg, 2005). Despite these original striking findings, there has been surprisingly little follow-up work. The primary reason fr the lack of sustained research is that the most reliable manual systems for measuring facial expression often require considerable training and are labor intensive. Automated measurement using computer vision and machine learning seeks to address the need for valid, efficient, and reproducible measurement. Recent systems have shown promise in fairly small studies using posed behavior or structured contexts with confederates, or trained interviewers, or pre-trained (person-specific) face models. For automated coding to be applied in real-world settings, a large data base with ample variability in pose, head motion, skin color, gender, partial occlusion, and expression intensity is needed. We have developed a unique database that meets this need and the algorithms necessary to enable robust automated coding. The database consists of 720 participants in three-person groups engaged in a group formation task. In a preliminary study, we demonstrated that our algorithms can successfully code two key facial signals associated with human emotion in this relatively unconstrained context (Cohn & Sayette, 2010). To achieve efficient, accurate, and valid measurement of facial expression usable in research and clinical settings, we aim to 1) train and validate classifiers to achieve reliable facial expression detectin across this unprecedentedly large, diverse data set; 2) extend the previous person-specific methods to person-independent (generic) facial feature detection, tracking, and alignment; and 3) make these tools available for research and clinical use. The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.",Automated Facial Expression Analysis for Research and Clinical Use,8816133,R01MH096951,"['Address', 'Algorithms', 'Anxiety', 'Behavior', 'Behavioral Sciences', 'Benchmarking', 'Bipolar Disorder', 'Clinical', 'Code', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Emotions', 'Employee Strikes', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Gender', 'Generic Drugs', 'Head', 'Hour', 'Human', 'Imagery', 'Instruction', 'Interview', 'Interviewer', 'Label', 'Laboratories', 'Learning', 'Lighting', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Movement', 'Nature', 'Pain', 'Participant', 'Personal Computers', 'Persons', 'Process', 'Psychopathology', 'Reporting', 'Research', 'Research Personnel', 'Running', 'Schizophrenia', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'clinical practice', 'follow-up', 'insight', 'meetings', 'nervous system disorder', 'positive emotional state', 'skin color', 'suicidal risk', 'tool']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,452541,0.14306841992987776
"Development of a novel neurotechnology to promote emotion recognition in autism DESCRIPTION (provided by applicant): Difficulties in facial emotion recognition (FER) are thought to cause or exacerbate social disability in people with autism spectrum disorder (ASD) by preventing 1) accurate detection of social/emotional information conveyed through the face, particularly the eye-region, and 2) the deployment of emotionally appropriate responses. Although the neural systems thought to underlie FER deficits in ASD are increasingly appreciated, their plasticity remains speculative. The goal of this project is to develop an assistive technology to promote facial emotion recognition in ASD [R21]. We propose that FER can be rehabilitated using a brain-computer interface (BCI) device [R33]. To develop an FER assistant, we plan to first [R21] determine whether it is possible to develop a multi-voxel classifier that is temporally predictive of successful emotion recognition during functional magnetic resonance imaging (fMRI). An adaptive, real-time fMRI (rt-fMRI) paradigm will interpret the output of a subject's brain to assess whether a computer-generated actor's emotion is recognized. If not, the expressed facial emotion will be increased in intensity until the computer determines that the subject has recognized the emotion. After tuning this supervised learning algorithm produced by a support vector machine (SVM), we then transform the massively multidimensional classifier to low-dimensionality space, which can be replicated by a single- or dual-EEG sensor placed on the scalp. The proof of principle is that the multivariate classifier can be forward transformed into frequency (EEG) space. The EEG sensor can be comfortably worn outside of the scanner (BCI device), and can be wirelessly linked to a portable tablet (iPad). We will then demonstrate the feasibility of an ambulatory BCI 'FER assistant' [R33] in a between-group, randomized design (genuine neurofeedback vs placebo neurofeedback). The FER assistant is a virtual reality- based iPad application that uses the EEG sensor data to assist users with emotion recognition by manipulating the avatar's emotion intensity until it is recognized by the user, who will receive points the earlier the emotion is recognized. The purpose of this randomized controlled trial (RCT) is to assess feasibility including acceptability of the intervention, recruitment and randomization procedures, intervention implementation, blinded assessment procedures, and participant retention within the context of an RCT in preparation for a well- powered efficacy trial. This study's products include demonstration of the neural processes that underlie FER deficits and evidence of their plasticity, and an easily exportable, minimal-cost computer-based intervention. There has been little treatment research for this under-studied population, and social deficits may post unique challenges to people with ASD during late adolescence and early adulthood, as they face multiple life transitions and developmental tasks requiring social competence (e.g., securing employment). Ultimately, we plan to evaluate the efficacy of this emergent intervention in an adequately powered randomized clinical trial. PUBLIC HEALTH RELEVANCE: The social disability that characterizes Autism Spectrum Disorder (ASD) pervades other areas of adaptive behavior, is predictive of secondary mental health problems, and adversely affects long-term outcome. Although ASD is a chronic condition, there has been little research on interventions for adults with ASD to target social disability. We propose to first establish the neural plasticity of specific brain mechanisms underlying difficultis with facial emotion recognition, a core deficit believed to be pivotal in the behavioral expression of ASD-social disability, and subsequently develop a novel, computer-based intervention using real-time feedback to ameliorate emotion recognition deficits.",Development of a novel neurotechnology to promote emotion recognition in autism,8821669,R21MH100268,"['Adaptive Behaviors', 'Adolescence', 'Adolescent', 'Adult', 'Affect', 'Algorithms', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Markers', 'Blinded', 'Brain', 'Chronic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Computer Simulation', 'Computers', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Emotional', 'Emotions', 'Employment', 'Eye', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Health', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Neuronal Plasticity', 'Outcome', 'Output', 'Participant', 'Pattern', 'Placebos', 'Population', 'Population Study', 'Preparation', 'Procedures', 'Process', 'Randomized', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Reporting', 'Research', 'Sampling', 'Scalp structure', 'Secure', 'Self-Help Devices', 'Signal Transduction', 'Social Environment', 'Symptoms', 'System', 'Tablets', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'active method', 'autism spectrum disorder', 'base', 'brain computer interface', 'computer generated', 'control trial', 'cost', 'design', 'disability', 'efficacy trial', 'emerging adult', 'innovation', 'neurofeedback', 'neurotechnology', 'novel', 'prevent', 'programs', 'relating to nervous system', 'response', 'satisfaction', 'sensor', 'skills', 'social', 'social skills', 'tool', 'virtual', 'virtual reality']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R21,2015,225262,0.051295233889622094
"Resting State and Task-Evoked Neural Bases of Rumination and Affective Dysfunction ﻿    DESCRIPTION (provided by applicant): [Depression is a common disorder with a major public health impact.] Recent research has linked [depression with abnormal function] in the default-mode network (DMN), a network of regions that is typically active at rest, and deactivates during cognitive tasks. [Depression has been linked with increased DMN connectivity] during resting-state fMRI, and a relative failure to suppress DMN when individuals view negative images. In addition, depressed individuals demonstrate enhanced and prolonged amygdala responses to negative images. [This has led to the theory that DMN abnormalities might interfere with recruitment of brain regions to regulate amygdala in depressed individuals.] [These neural markers in depressed individuals have also been related to brooding rumination, a type of maladaptive coping involving repetitive negative thoughts.] This work proposes to test this model using cross-modal comparison of fMRI data in individuals during a resting-state scan and while participants are viewing negative images. Two main hypotheses will be tested: 1) resting-state DMN [function] will predict decreased top-down regulation of emotion during viewing of negative images, and 2) increased resting-state DMN [function], and decreased top-down emotion regulation during negative-image-viewing, will be predicted by increased trait [brooding]. Assessment of top-down emotion regulation during negative-image-viewing will be multimodal and will focus on the time period 6-12s after image offset, capturing ""recovery"" from the negative image, while controlling for initial 6 s of activity in response to the image (""reactivity""). Measures will include amygdala activity, amygdala-prefrontal connectivity, and electromyography of the corrugator supercilii, a muscle that is a reliable indicator of the experience of negative emotion. [In addition, multivariate pattern analysis (MVPA) will be used to classify negative vs. neutral images as an alternative measure of neural responses.] DMN [function] will be assessed in two ways. First, functional connectivity between two major nodes of the DMN (medial prefrontal cortex and posterior cingulate cortex) will be calculated using correlation-based methods. Second, [a metric of DMN dominance over a cognitive control network will be calculated following methods used previously by other researchers. In order to assess how resting-state DMN function predicts top-down emotion regulation, these DMN metrics will be regressed onto all of the above metrics of emotion regulation. In order to assess how these measures correspond to trait brooding, brooding will be assessed using a standardized measure (the Ruminative Responses Scale; RRS) [and also regressed onto the outcome measures above]. This work will elucidate the relationship of DMN activity, [neural correlates of] emotion regulation and [brooding] with implications for risk and treatment of affective disorders.         PUBLIC HEALTH RELEVANCE: Depression is a common and devastating disorder with significant morbidity and mortality, but its cognitive and neural bases remain incompletely understood. The proposed study will assess a neurocognitive model of [depression] by modeling fMRI BOLD activity while participants are resting and also while they regulate emotion during a task, allowing us to make cross-modal comparisons of neural activity. [We will additionally assess individual differences in trait brooding, an important risk factor for depression.] Using these data, we will be able to explicate critical hypotheses about the neural basis of depression and its associations with emotional style and emotion regulation, which in turn will afford us further insights relevant to understanding mechanisms of risk and treatment for affective psychopathology.                ",Resting State and Task-Evoked Neural Bases of Rumination and Affective Dysfunction,8978161,F30MH106191,"['Affect', 'Affective', 'Amygdaloid structure', 'Behavior', 'Brain region', 'Classification', 'Clinical', 'Cognitive', 'Communities', 'Data', 'Depressed mood', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Down-Regulation', 'Electromyography', 'Emotional', 'Emotions', 'Failure', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Hyperactive behavior', 'Image', 'Individual', 'Individual Differences', 'Link', 'Machine Learning', 'Maintenance', 'Major Depressive Disorder', 'Maps', 'Measures', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'Mood Disorders', 'Morbidity - disease rate', 'Multivariate Analysis', 'Muscle', 'Neurocognitive', 'Nucleic Acid Regulatory Sequences', 'Outcome', 'Outcome Measure', 'Participant', 'Pattern', 'Prefrontal Cortex', 'Psychopathology', 'Psychophysiology', 'Public Health', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Design', 'Research Personnel', 'Rest', 'Risk', 'Risk Factors', 'Role', 'Sampling', 'Scanning', 'Signal Transduction', 'Techniques', 'Testing', 'Thinking', 'Time', 'Validation', 'Work', 'base', 'cingulate cortex', 'cognitive control', 'cognitive task', 'coping', 'emotion regulation', 'emotional adjustment', 'emotional experience', 'experience', 'insight', 'interest', 'mortality', 'neural correlate', 'neuromechanism', 'prognostic', 'public health relevance', 'relating to nervous system', 'response', 'standardize measure', 'success', 'theories', 'trait']",NIMH,UNIVERSITY OF WISCONSIN-MADISON,F30,2015,33015,0.05155257841932038
"Modeling the Dynamics of Early Communication and Development DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts. PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,8897409,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Health', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Untranslated RNA', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2015,521457,0.05297480669690097
"The Development and neural Bases of Emotion Processing DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness. Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.",The Development and neural Bases of Emotion Processing,8937324,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Relative (related person)', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Social Behavior', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'anxious behavior', 'base', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'signal processing', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2015,112804,0.12275310936264823
"The Development and Neural Bases of Emotion Processing DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness. Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.",The Development and Neural Bases of Emotion Processing,8816129,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Relative (related person)', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Social Behavior', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'anxious behavior', 'base', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'signal processing', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2015,764840,0.12275310936264823
"Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders DESCRIPTION (provided by applicant): An inability to convey emotion by vocal modulation (prosody) is a major aspect of social cognitive dysfunction for patients afflicted with a variety of neuropsychiatric illnesses such as schizophrenia and autism. The neural mechanisms of these deficits, and how they develop from childhood to adulthood have received remarkably little research attention. This training proposal seeks to prepare the candidate for independent research that addresses both of these issues. In terms of the neural mechanisms of prosody, there is some evidence that prosodic dysfunction is associated with a disruption of interactions between temporal and frontal cortex. The temporal component of this circuit appears to parse the acoustic signal for emotional distinctions, which are subsequently evaluated for emotional meaning by the frontal component. It is likely that this circuit mediates prosodic dysfunction in schizophrenia, which is associated with basic audio-sensory deficits and abnormal temporo-frontal interactions. The research proposed here will employ multimodal neuroimaging approaches to produce the first comprehensive description of this temporo-frontal circuit, in both healthy adults and patients with schizophrenia, by integrating three state-of-the-art neuroimaging techniques. First, magnetoencephalography (MEG) in conjunction with magnetic resonance functional connectivity analysis (fMRI) will be used to determine the time course of processing by the circuit. Then, by applying Diffusion Imaging, these functional estimates will be related to neurostructural estimates of auditory pathway integrity. We expect to demonstrate that neurostructural integrity deficits underpin abnormal temporo-frontal interactions during prosodic processing in schizophrenia. In addition to training the candidate in multimodal neuroimaging, the proposed activities will provide him a background in the developmental psychology of language and auditory neurobiology, as well as a framework to develop and adapt prosodic tasks for children and adolescents. The candidate is an Instructor in Psychiatry at the University of Pennsylvania and has an established interest in social communication. Drs. Timothy Roberts and Ruben Gur will mentor him. Dr. Roberts is an expert in MEG and DTI techniques, with a research focus on the development of auditory and language processing in healthy children and patients with autism. Dr. Gur is an expert in neuropsychological assessment and neuroimaging, whose research concerns emotional and cognitive dysfunction in schizophrenia. These mentors will assure that the experience resulting from the proposed training plan will enable the candidate to establish an independent research program that can make significant contributions in the developmental neuroscience of social communication, and the understanding of how dysfunction in such communication emerges in neuropsychiatric disorders like schizophrenia and autism. Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.",Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders,8847802,K01MH094689,"['Acoustics', 'Address', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Amygdaloid structure', 'Attention', 'Auditory', 'Autistic Disorder', 'Behavioral', 'Brain', 'Child', 'Childhood', 'Cognitive deficits', 'Communication', 'Comprehension', 'Cues', 'Data', 'Development', 'Diagnostic', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Dorsal', 'Emotional', 'Emotions', 'Encapsulated', 'Evaluation', 'Functional Imaging', 'Functional disorder', 'Goals', 'Gur', 'Impaired cognition', 'Impairment', 'Independent Living', 'Inferior frontal gyrus', 'Language', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Marshal', 'Measures', 'Mediating', 'Mentors', 'Modeling', 'Multimodal Imaging', 'Nature', 'Neurobiology', 'Neurosciences', 'Pathology', 'Patients', 'Pattern', 'Pennsylvania', 'Phase', 'Process', 'Psychiatry', 'Public Health', 'Relative (related person)', 'Research', 'Ruthenium Ben', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Severities', 'Signal Transduction', 'Social Functioning', 'Societies', 'Staging', 'Stimulus', 'Structural defect', 'Structure of middle temporal gyrus', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Training Programs', 'Universities', 'Variant', 'Work', 'auditory pathway', 'career', 'developmental disease', 'developmental psychology', 'experience', 'frontal lobe', 'functional outcomes', 'insight', 'instructor', 'interest', 'language processing', 'neural circuit', 'neuroimaging', 'neuromechanism', 'neuropsychiatry', 'neuropsychological', 'pediatric patients', 'prognostic', 'programs', 'relating to nervous system', 'social', 'social cognition', 'social communication', 'tool', 'white matter']",NIMH,UNIVERSITY OF PENNSYLVANIA,K01,2015,168157,0.06338910876386301
"Bayesian Modeling of Mood Effects on Decision-Making in Amphetamine Dependence     DESCRIPTION (provided by applicant): Amphetamine dependence (AD) is an important public health problem, which has been linked to persistent executive and affective processing impairments. Specifically, amphetamine dependent individuals (ADI) demonstrate poor decision-making, particularly with respect to keeping track of new information and using this knowledge to guide future decisions. In addition, ADI have difficulties in appraising and regulating negative emotion, which has been linked to poor social functioning and higher likelihood of relapse. Given that emotion plays an important role in modulating and guiding decision-making, surprisingly little is known about how such dysfunctions interact in ADI, and how they may contribute to impairments in everyday functioning. Bayesian learning models provide a way to formally represent an individual's beliefs about the environment and the dynamic updating of those beliefs when new observations are made. Such computational framework can help to better delineate the potential impact of emotion on the dynamic representation of individuals' expectations about choice options, on the degree to which this cumulative knowledge is used to predict future outcomes, and on the cognitive strategies individuals use to select actions. The proposed study will use such Bayesian modeling approach, combined with event-related functional magnetic resonance imaging (fMRI), to assess the neurocognitive processes underlying a) potential deficits in the strategic and predictive processes guiding reward-based decision-making in ADI and b) ADI potential overrepresentation of or failure to integrate negative emotion into such decision-making. To do so, 40 ADI and 40 healthy comparison subjects (CS) will perform a gambling task under neutral and sad mood, and while undergoing fMRI. Several decision strategies and the associated Bayesian outcome predictions will be estimated and correlated with neural activity at the time of decision. These measures will be used to assess and quantify differences between ADI and CS 1) in the computational processes underlying reward-based decision (Aim1) and 2) in the computational processes underlying the integration of negative emotion into reward-based decision-making (Aim 2). The outcomes of this study will help to refine neurocognitive models of AD by delineating the computational processes that go awry in ADI and will help identify more precise neurocognitive predictors of executive and affective dysfunction an ADI.         PUBLIC HEALTH RELEVANCE: Amphetamine dependence has been linked to decision-making and emotion processing impairments. Understanding how the disease may impact the computational mechanisms underlying affect infusion into decision-making will provide a mechanistic rationale for developing interventions aimed at improving amphetamine dependent individuals' daily functioning and response to adverse emotions, and will help develop more precise severity markers and predictors of relapse for amphetamine dependence. This in turn could advantageously reduce costs to the healthcare and criminal justice systems.                ",Bayesian Modeling of Mood Effects on Decision-Making in Amphetamine Dependence,8892807,F32DA036959,"['Accounting', 'Affect', 'Affective', 'Amphetamine Dependence', 'Amphetamines', 'Amygdaloid structure', 'Anterior', 'Attenuated', 'Bayesian Modeling', 'Belief', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Criminal Justice', 'Cues', 'Decision Making', 'Disease', 'Emotional', 'Emotions', 'Environment', 'Event', 'Exhibits', 'Failure', 'Financial Support', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Gambling', 'Healthcare', 'Human', 'Impairment', 'Individual', 'Individual Differences', 'Infusion procedures', 'Insula of Reil', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Link', 'Maintenance', 'Measures', 'Mediating', 'Modeling', 'Moods', 'Neurocognitive', 'Outcome', 'Outcome Study', 'Performance', 'Play', 'Prefrontal Cortex', 'Process', 'Public Health', 'Recording of previous events', 'Relapse', 'Relative (related person)', 'Research', 'Rewards', 'Risk', 'Role', 'Serotonergic System', 'Severities', 'Signal Transduction', 'Social Functioning', 'Social support', 'System', 'Time', 'Update', 'base', 'computer framework', 'cost', 'daily functioning', 'data modeling', 'expectation', 'improved', 'information processing', 'neuroimaging', 'neuromechanism', 'public health relevance', 'relating to nervous system', 'response', 'success']",NIDA,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",F32,2015,56042,0.06939304373200887
"An Olfactory Method for Controlling Cigarette Craving - Supplement DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving - Supplement,8988066,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Computers', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Health', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Staging', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'research study', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,54468,0.06140266994003206
"An Olfactory Method for Controlling Cigarette Craving DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving. PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.",An Olfactory Method for Controlling Cigarette Craving,8830440,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cigarette', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Computers', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Health', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Preventable cancer cause', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Staging', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'cancer risk', 'cognitive process', 'coping', 'craving', 'effective therapy', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'research study', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2015,308423,0.06274245663647815
"Face De-Identification for Research and Clinical Use     DESCRIPTION (provided by applicant): This application addresses NIH's call to promote data sharing and patient privacy. A major obstacle to sharing of recorded video has been the need to protect participants' identity. Similarly, concern about stigma is a reason that many people in need of mental health services (e.g., in the military) fail to do so. We propose a system to de-identify patients and research participants in video. Face de-identification transfers facial expression automatically from source face images, which are confidential, to target face images, which are not. The system safeguards face anonymity while preserving the facial expression of the original source video. The target video then can communicate the emotion, communicative intent, pain, and neurological or physiological status of the source person without displaying the source person's face. Face de-identification would enable video archive sharing among researchers and clinicians without compromising privacy or confidentiality. Moreover, a version of this system could potentially be used to preserve privacy and anonymity in internet-based interviews. Innovation. The project has four innovations. The approach (1) Removes identity information while retaining facial dynamics; thus preserving the information value of the face to communicate emotion, pain, and related states. (2) Accommodates subtle and spontaneous facial actions, rather than imitating only some predefined molar expressions (e.g., happy or sad). (3) Requires no training steps by target persons. (4) And requires no hand annotation of video. The system is entirely automatic. Approach. The software will take as input a video with the face of a subject (source) and automatically generate or output a video with the face de-identified. The project will use new machine learning and computer vision algorithms for transferring subtle facial expression from a source subject (original video) to a target subject, using only one frontal image of the target subject. A major novelty of the approach is to make the process completely automatic. The algorithm will be validated using commercially available software for face recognition and custom software for facial expression analysis.         PUBLIC HEALTH RELEVANCE: Sharing of video recordings for research and clinical uses would significantly contribute to scientific discovery and patient diagnosis, treatment, and evaluation. We will develop and validate a fully automatic system that preserves facial expression in video while fully protecting face identity.            ",Face De-Identification for Research and Clinical Use,8772435,R21MH099487,"['Address', 'Age', 'Algorithms', 'Archives', 'Behavioral Sciences', 'Clinical', 'Code', 'Communication', 'Communities', 'Computer Vision Systems', 'Computer software', 'Confidentiality', 'Custom', 'Data Set', 'Detection', 'Diagnosis', 'Education', 'Emotions', 'Evaluation', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Goals', 'Gold', 'Hand', 'Human', 'Image', 'Informed Consent', 'Intention', 'Internet', 'Interview', 'Judgment', 'Machine Learning', 'Maps', 'Measurement', 'Mental Health Services', 'Methods', 'Military Personnel', 'Modeling', 'Neurologic', 'Output', 'Pain', 'Participant', 'Patients', 'Perception', 'Personal Computers', 'Persons', 'Physiological', 'Privacy', 'Process', 'Research', 'Research Personnel', 'Running', 'Source', 'Step training', 'System', 'Testing', 'Training', 'Twin Multiple Birth', 'Video Recording', 'base', 'clinical practice', 'data sharing', 'graphical user interface', 'innovation', 'middle age', 'patient privacy', 'public health relevance', 'sex', 'social stigma', 'user-friendly', 'young adult']",NIMH,CARNEGIE-MELLON UNIVERSITY,R21,2014,194915,0.06039374988311494
"Automated Facial Expression Analysis for Research and Clinical Use     DESCRIPTION (provided by applicant): Facial expression has been a focus of emotion research for over a hundred years. In recent decades observations of facial expressions have yielded critical and dramatic insights about the etiology of psychopathology, and have proven capable of predicting treatment outcomes (see Ekman & Rosenberg, 2005). Despite these original striking findings, there has been surprisingly little follow-up work. The primary reason fr the lack of sustained research is that the most reliable manual systems for measuring facial expression often require considerable training and are labor intensive. Automated measurement using computer vision and machine learning seeks to address the need for valid, efficient, and reproducible measurement. Recent systems have shown promise in fairly small studies using posed behavior or structured contexts with confederates, or trained interviewers, or pre-trained (person-specific) face models. For automated coding to be applied in real-world settings, a large data base with ample variability in pose, head motion, skin color, gender, partial occlusion, and expression intensity is needed. We have developed a unique database that meets this need and the algorithms necessary to enable robust automated coding. The database consists of 720 participants in three-person groups engaged in a group formation task. In a preliminary study, we demonstrated that our algorithms can successfully code two key facial signals associated with human emotion in this relatively unconstrained context (Cohn & Sayette, 2010). To achieve efficient, accurate, and valid measurement of facial expression usable in research and clinical settings, we aim to 1) train and validate classifiers to achieve reliable facial expression detectin across this unprecedentedly large, diverse data set; 2) extend the previous person-specific methods to person-independent (generic) facial feature detection, tracking, and alignment; and 3) make these tools available for research and clinical use.          The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.            ",Automated Facial Expression Analysis for Research and Clinical Use,8633060,R01MH096951,"['Address', 'Algorithms', 'Anxiety', 'Behavior', 'Behavioral Sciences', 'Benchmarking', 'Bipolar Disorder', 'Clinical', 'Code', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Emotions', 'Employee Strikes', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Gender', 'Generic Drugs', 'Head', 'Hour', 'Human', 'Imagery', 'Instruction', 'Interview', 'Interviewer', 'Label', 'Laboratories', 'Learning', 'Lighting', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Movement', 'Nature', 'Pain', 'Participant', 'Personal Computers', 'Persons', 'Process', 'Psychopathology', 'Reporting', 'Research', 'Research Personnel', 'Running', 'Schizophrenia', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'clinical practice', 'follow-up', 'insight', 'meetings', 'nervous system disorder', 'positive emotional state', 'skin color', 'suicidal risk', 'tool']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,452541,0.14306841992987776
"Multivariate Representations of Emotion     DESCRIPTION (provided by applicant): A central goal of affective neuroscience is to understand the brain systems and mechanisms underlying the evaluation, experience, and expression of emotion. For example, a widely studied and hotly debated issue in the field is the manner in which biophysical responses to emotional stimuli can be characterized, whether by distinct categories or alternatively along dimensions of valence and arousal. Advances in the fields of psychology, neuroscience, and computer science have fostered significant progress in identifying brain regions involved in processing emotion generally; however, consistent and specific neural markers for distinct affective states have yet to be found. This proposal uses a cutting-edge approach to this core, unresolved question in the field by harnessing emerging pattern classification techniques that are capable of detecting subtle yet coordinated signals from an array of sources. The overarching goal is to identify multivariate patterns of behavioral and biological responding to specific affective states and determine whether these states are organized according to categorical or dimensional architectures. By combining psychophysiology (Aim 1) and functional magnetic resonance imaging (fMRI) (Aim 2), these studies will examine how humans respond to emotional stimuli that vary in duration, modality, and categorical nature. Study 1 focuses on distinct emotions elicited by instrumental music and movie clips whereas Study 2 focuses on those elicited by facial and vocal affect. Together, the aims will provide an integrative, computationally-rigorous method to identify biomarkers of specific emotions that are typically overlooked by conventional univariate statistical approaches. Applying machine learning algorithms in this innovative way could be fruitful for identifying how emotional representations are altered in affective disorders, with the potential for developing novel therapeutic targets. Moreover, identifying patterns of overlap between specific emotion categories may further aid efforts to understand comorbidity issues in anxiety and mood disorders.         PUBLIC HEALTH RELEVANCE: The proposed research will identify patterns of brain and bodily responses that are representative of unique emotional states, using analytical methods that examine changes in multiple biological signals in concert. Identifying neural and peripheral representations of emotional states and how they relate to self-reports of emotion will provide a better understanding of how emotions are organized and coded in the brain and body. These biomarkers of specific emotions can provide clues as to how emotional processing is altered in mood and anxiety disorders and can lead to novel treatment targets.                    ",Multivariate Representations of Emotion,8685333,R21MH098149,"['Accounting', 'Address', 'Affect', 'Affective', 'Algorithms', 'Anterior', 'Anxiety Disorders', 'Architecture', 'Arousal', 'Auditory', 'Behavioral', 'Biological', 'Biological Markers', 'Brain', 'Brain region', 'Categories', 'Classification', 'Clip', 'Code', 'Comorbidity', 'Complex', 'Data', 'Dimensions', 'Emotional', 'Emotions', 'Evaluation', 'Event', 'Experimental Designs', 'Face', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Human', 'Image', 'Individual', 'Investigation', 'Label', 'Lead', 'Literature', 'Machine Learning', 'Measures', 'Meta-Analysis', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Mood Disorders', 'Music', 'Nature', 'Neurons', 'Neurosciences', 'Participant', 'Patient Self-Report', 'Pattern', 'Peripheral', 'Physiological', 'Process', 'Psychology', 'Psychopathology', 'Psychophysiology', 'Research', 'Sensory', 'Signal Transduction', 'Source', 'Statistical Methods', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visual', 'Weight', 'Work', 'affective neuroscience', 'analytical method', 'base', 'computer science', 'design', 'emotional experience', 'emotional stimulus', 'experience', 'imaging modality', 'innovation', 'insight', 'movie', 'multisensory', 'neural patterning', 'neuroimaging', 'new therapeutic target', 'novel', 'novel strategies', 'public health relevance', 'relating to nervous system', 'response', 'showing emotion', 'theories', 'tool']",NIMH,DUKE UNIVERSITY,R21,2014,235500,0.1436616106356431
"Development of a novel neurotechnology to promote emotion recognition in autism     DESCRIPTION (provided by applicant): Difficulties in facial emotion recognition (FER) are thought to cause or exacerbate social disability in people with autism spectrum disorder (ASD) by preventing 1) accurate detection of social/emotional information conveyed through the face, particularly the eye-region, and 2) the deployment of emotionally appropriate responses. Although the neural systems thought to underlie FER deficits in ASD are increasingly appreciated, their plasticity remains speculative. The goal of this project is to develop an assistive technology to promote facial emotion recognition in ASD [R21]. We propose that FER can be rehabilitated using a brain-computer interface (BCI) device [R33]. To develop an FER assistant, we plan to first [R21] determine whether it is possible to develop a multi-voxel classifier that is temporally predictive of successful emotion recognition during functional magnetic resonance imaging (fMRI). An adaptive, real-time fMRI (rt-fMRI) paradigm will interpret the output of a subject's brain to assess whether a computer-generated actor's emotion is recognized. If not, the expressed facial emotion will be increased in intensity until the computer determines that the subject has recognized the emotion. After tuning this supervised learning algorithm produced by a support vector machine (SVM), we then transform the massively multidimensional classifier to low-dimensionality space, which can be replicated by a single- or dual-EEG sensor placed on the scalp. The proof of principle is that the multivariate classifier can be forward transformed into frequency (EEG) space. The EEG sensor can be comfortably worn outside of the scanner (BCI device), and can be wirelessly linked to a portable tablet (iPad). We will then demonstrate the feasibility of an ambulatory BCI 'FER assistant' [R33] in a between-group, randomized design (genuine neurofeedback vs placebo neurofeedback). The FER assistant is a virtual reality- based iPad application that uses the EEG sensor data to assist users with emotion recognition by manipulating the avatar's emotion intensity until it is recognized by the user, who will receive points the earlier the emotion is recognized. The purpose of this randomized controlled trial (RCT) is to assess feasibility including acceptability of the intervention, recruitment and randomization procedures, intervention implementation, blinded assessment procedures, and participant retention within the context of an RCT in preparation for a well- powered efficacy trial. This study's products include demonstration of the neural processes that underlie FER deficits and evidence of their plasticity, and an easily exportable, minimal-cost computer-based intervention. There has been little treatment research for this under-studied population, and social deficits may post unique challenges to people with ASD during late adolescence and early adulthood, as they face multiple life transitions and developmental tasks requiring social competence (e.g., securing employment). Ultimately, we plan to evaluate the efficacy of this emergent intervention in an adequately powered randomized clinical trial.         PUBLIC HEALTH RELEVANCE: The social disability that characterizes Autism Spectrum Disorder (ASD) pervades other areas of adaptive behavior, is predictive of secondary mental health problems, and adversely affects long-term outcome. Although ASD is a chronic condition, there has been little research on interventions for adults with ASD to target social disability. We propose to first establish the neural plasticity of specific brain mechanisms underlying difficultis with facial emotion recognition, a core deficit believed to be pivotal in the behavioral expression of ASD-social disability, and subsequently develop a novel, computer-based intervention using real-time feedback to ameliorate emotion recognition deficits.                ",Development of a novel neurotechnology to promote emotion recognition in autism,8635153,R21MH100268,"['Adaptive Behaviors', 'Adolescence', 'Adolescent', 'Adult', 'Affect', 'Algorithms', 'Area', 'Autistic Disorder', 'Behavioral', 'Biological Markers', 'Blinded', 'Brain', 'Chronic', 'Clinical', 'Clinical Trials', 'Cognitive', 'Computer Simulation', 'Computers', 'Data', 'Data Analyses', 'Detection', 'Development', 'Devices', 'Electroencephalography', 'Emotional', 'Emotions', 'Employment', 'Eye', 'Face', 'Feedback', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Intervention', 'Investigation', 'Knowledge', 'Learning', 'Life', 'Link', 'Machine Learning', 'Mental Health', 'Modeling', 'Morbidity - disease rate', 'Nature', 'Neuronal Plasticity', 'Outcome', 'Output', 'Participant', 'Pattern', 'Placebos', 'Population', 'Population Study', 'Preparation', 'Procedures', 'Process', 'Randomized', 'Randomized Clinical Trials', 'Randomized Controlled Trials', 'Reporting', 'Research', 'Sampling', 'Scalp structure', 'Secure', 'Self-Help Devices', 'Signal Transduction', 'Social Environment', 'Symptoms', 'System', 'Tablets', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Work', 'active method', 'autism spectrum disorder', 'base', 'brain computer interface', 'computer generated', 'control trial', 'cost', 'design', 'disability', 'efficacy trial', 'emerging adult', 'innovation', 'neurofeedback', 'neurotechnology', 'novel', 'prevent', 'programs', 'public health relevance', 'relating to nervous system', 'response', 'satisfaction', 'sensor', 'skills', 'social', 'social skills', 'tool', 'virtual', 'virtual reality']",NIMH,VIRGINIA POLYTECHNIC INST AND ST UNIV,R21,2014,269650,0.051295233889622094
"Modeling the Dynamics of Early Communication and Development     DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts.         PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.",Modeling the Dynamics of Early Communication and Development,8711519,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Functional RNA', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'public health relevance', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2014,480422,0.05297480669690097
"The Development and Neural Bases of Emotion Processing     DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness.          Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.            ",The Development and Neural Bases of Emotion Processing,8625827,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Relative (related person)', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'base', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'signal processing', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2014,768504,0.12275310936264823
"Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders    DESCRIPTION (provided by applicant): An inability to convey emotion by vocal modulation (prosody) is a major aspect of social cognitive dysfunction for patients afflicted with a variety of neuropsychiatric illnesses such as schizophrenia and autism. The neural mechanisms of these deficits, and how they develop from childhood to adulthood have received remarkably little research attention. This training proposal seeks to prepare the candidate for independent research that addresses both of these issues. In terms of the neural mechanisms of prosody, there is some evidence that prosodic dysfunction is associated with a disruption of interactions between temporal and frontal cortex. The temporal component of this circuit appears to parse the acoustic signal for emotional distinctions, which are subsequently evaluated for emotional meaning by the frontal component. It is likely that this circuit mediates prosodic dysfunction in schizophrenia, which is associated with basic audio-sensory deficits and abnormal temporo-frontal interactions. The research proposed here will employ multimodal neuroimaging approaches to produce the first comprehensive description of this temporo-frontal circuit, in both healthy adults and patients with schizophrenia, by integrating three state-of-the-art neuroimaging techniques. First, magnetoencephalography (MEG) in conjunction with magnetic resonance functional connectivity analysis (fMRI) will be used to determine the time course of processing by the circuit. Then, by applying Diffusion Imaging, these functional estimates will be related to neurostructural estimates of auditory pathway integrity. We expect to demonstrate that neurostructural integrity deficits underpin abnormal temporo-frontal interactions during prosodic processing in schizophrenia. In addition to training the candidate in multimodal neuroimaging, the proposed activities will provide him a background in the developmental psychology of language and auditory neurobiology, as well as a framework to develop and adapt prosodic tasks for children and adolescents. The candidate is an Instructor in Psychiatry at the University of Pennsylvania and has an established interest in social communication. Drs. Timothy Roberts and Ruben Gur will mentor him. Dr. Roberts is an expert in MEG and DTI techniques, with a research focus on the development of auditory and language processing in healthy children and patients with autism. Dr. Gur is an expert in neuropsychological assessment and neuroimaging, whose research concerns emotional and cognitive dysfunction in schizophrenia. These mentors will assure that the experience resulting from the proposed training plan will enable the candidate to establish an independent research program that can make significant contributions in the developmental neuroscience of social communication, and the understanding of how dysfunction in such communication emerges in neuropsychiatric disorders like schizophrenia and autism.        Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.            ",Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders,8651944,K01MH094689,"['Acoustics', 'Address', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Amygdaloid structure', 'Attention', 'Auditory', 'Autistic Disorder', 'Behavioral', 'Brain', 'Child', 'Childhood', 'Cognitive deficits', 'Communication', 'Comprehension', 'Cues', 'Data', 'Development', 'Diagnostic', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Disease', 'Dorsal', 'Emotional', 'Emotions', 'Encapsulated', 'Evaluation', 'Functional Imaging', 'Functional disorder', 'Goals', 'Gur', 'Impaired cognition', 'Impairment', 'Independent Living', 'Inferior frontal gyrus', 'Language', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Marshal', 'Measures', 'Mediating', 'Mentors', 'Modeling', 'Multimodal Imaging', 'Nature', 'Neurobiology', 'Neurosciences', 'Pathology', 'Patients', 'Pattern', 'Pennsylvania', 'Phase', 'Process', 'Psychiatry', 'Public Health', 'Relative (related person)', 'Research', 'Ruthenium Ben', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Severities', 'Signal Transduction', 'Social Functioning', 'Societies', 'Staging', 'Stimulus', 'Structure of middle temporal gyrus', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Training Programs', 'Universities', 'Variant', 'Work', 'auditory pathway', 'career', 'developmental disease', 'developmental psychology', 'experience', 'frontal lobe', 'functional outcomes', 'insight', 'instructor', 'interest', 'language processing', 'neural circuit', 'neuroimaging', 'neuromechanism', 'neuropsychiatry', 'neuropsychological', 'prognostic', 'programs', 'relating to nervous system', 'social', 'social cognition', 'social communication', 'tool', 'white matter']",NIMH,UNIVERSITY OF PENNSYLVANIA,K01,2014,168157,0.06338910876386301
"Bayesian Modeling of Mood Effects on Decision-Making in Amphetamine Dependence     DESCRIPTION (provided by applicant): Amphetamine dependence (AD) is an important public health problem, which has been linked to persistent executive and affective processing impairments. Specifically, amphetamine dependent individuals (ADI) demonstrate poor decision-making, particularly with respect to keeping track of new information and using this knowledge to guide future decisions. In addition, ADI have difficulties in appraising and regulating negative emotion, which has been linked to poor social functioning and higher likelihood of relapse. Given that emotion plays an important role in modulating and guiding decision-making, surprisingly little is known about how such dysfunctions interact in ADI, and how they may contribute to impairments in everyday functioning. Bayesian learning models provide a way to formally represent an individual's beliefs about the environment and the dynamic updating of those beliefs when new observations are made. Such computational framework can help to better delineate the potential impact of emotion on the dynamic representation of individuals' expectations about choice options, on the degree to which this cumulative knowledge is used to predict future outcomes, and on the cognitive strategies individuals use to select actions. The proposed study will use such Bayesian modeling approach, combined with event-related functional magnetic resonance imaging (fMRI), to assess the neurocognitive processes underlying a) potential deficits in the strategic and predictive processes guiding reward-based decision-making in ADI and b) ADI potential overrepresentation of or failure to integrate negative emotion into such decision-making. To do so, 40 ADI and 40 healthy comparison subjects (CS) will perform a gambling task under neutral and sad mood, and while undergoing fMRI. Several decision strategies and the associated Bayesian outcome predictions will be estimated and correlated with neural activity at the time of decision. These measures will be used to assess and quantify differences between ADI and CS 1) in the computational processes underlying reward-based decision (Aim1) and 2) in the computational processes underlying the integration of negative emotion into reward-based decision-making (Aim 2). The outcomes of this study will help to refine neurocognitive models of AD by delineating the computational processes that go awry in ADI and will help identify more precise neurocognitive predictors of executive and affective dysfunction an ADI.         PUBLIC HEALTH RELEVANCE: Amphetamine dependence has been linked to decision-making and emotion processing impairments. Understanding how the disease may impact the computational mechanisms underlying affect infusion into decision-making will provide a mechanistic rationale for developing interventions aimed at improving amphetamine dependent individuals' daily functioning and response to adverse emotions, and will help develop more precise severity markers and predictors of relapse for amphetamine dependence. This in turn could advantageously reduce costs to the healthcare and criminal justice systems.                ",Bayesian Modeling of Mood Effects on Decision-Making in Amphetamine Dependence,8782905,F32DA036959,"['Accounting', 'Affect', 'Affective', 'Amphetamine Dependence', 'Amphetamines', 'Amygdaloid structure', 'Anterior', 'Attenuated', 'Bayesian Modeling', 'Belief', 'Cognitive', 'Computer Simulation', 'Corpus striatum structure', 'Criminal Justice', 'Cues', 'Decision Making', 'Disease', 'Emotional', 'Emotions', 'Environment', 'Event', 'Exhibits', 'Failure', 'Financial Support', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Gambling', 'Healthcare', 'Human', 'Impairment', 'Individual', 'Individual Differences', 'Infusion procedures', 'Insula of Reil', 'Intervention', 'Knowledge', 'Lead', 'Learning', 'Link', 'Maintenance', 'Measures', 'Mediating', 'Modeling', 'Moods', 'Neurocognitive', 'Outcome', 'Outcome Study', 'Performance', 'Play', 'Prefrontal Cortex', 'Process', 'Public Health', 'Recording of previous events', 'Relapse', 'Relative (related person)', 'Reliance', 'Research', 'Rewards', 'Risk', 'Role', 'Severities', 'Signal Transduction', 'Social Functioning', 'Social support', 'System', 'Time', 'Update', 'base', 'computer framework', 'cost', 'daily functioning', 'data modeling', 'expectation', 'improved', 'information processing', 'neuroimaging', 'neuromechanism', 'public health relevance', 'relating to nervous system', 'response', 'success']",NIDA,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",F32,2014,53282,0.06939304373200887
"An Olfactory Method for Controlling Cigarette Craving     DESCRIPTION (provided by applicant):  Cigarette craving is a vital feature of smoking, which is the leading preventable cause of cancer. While smokers generally recognize this danger, during ""hot"" moments of temptation the appeal of smoking a cigarette rises, previously learned coping skills or ""quit-smoking"" messages may be either ignored or abandoned, and often the smoking habit persists. Despite its importance, research has struggled to develop effective treatments for craving relief and new innovative approaches are sorely needed. The proposed project addresses RFA-CA- 12-015: Research Answers to NCI's Provocative Questions (PQA3) by evaluating a novel bio-behavioral approach to help smokers reduce their cigarette cravings. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings. In addition - and pertinent to the RFA - this research tests a range of individual difference factors such as working memory, personality, motivation to quit, and gender, which prior theory and research suggest should moderate the craving-reducing effects of olfactory cues. As a consequence, the project will advance knowledge of why certain individuals may have particular trouble managing their cravings and refraining from smoking.  Abstinent smokers (N=250) with varying motivations to quit will attend a multi-session experiment. Initially participants will sample and rate a serie of olfactory cues on several dimensions, including pleasantness, familiarity, and associated memories. Participants then will be exposed to in vivo smoking cues, which in the context of smoking abstinence, produce robust cigarette cravings. While at peak craving, they will be randomly assigned to sniff an odor that they had previously rated as either being most pleasant (and unrelated to smoking), a tobacco odor, or a neutral odor while urge, mood, and a novel set of craving-related responses derived from basic research in cognition and emotion (including Paul Ekman's Facial Action Coding System) will be assessed. This research also will test key mechanisms of craving relief that relate to existing theories of craving and addiction. In addition the project will monitor the durability of this predicted odor- induced craving-relief within a sinle experimental session and across sessions conducted on different days.  This conceptually-driven research is motivated by neurobiological and behavioral research indicating the unique power of olfaction to trigger emotional memories and to fundamentally alter emotional states such as craving. The proposed project will examine interactions between emotional and cognitive processes that, while craving, may serve to hamper effective coping, and will set the stage for future research testing the impact of olfaction - alone or combined with other agents (e.g., nicotine patches) - on smoking cessation. Irrespective of the outcome, the proposed research using a novel set of measures will provide critical data regarding the interaction of emotional and cognitive processes during craving.         PUBLIC HEALTH RELEVANCE: Although quitting smoking is the most important action a smoker can take to prevent cancer, cessation has proven difficult. Given observed relations between craving and smoking relapse, novel approaches to craving relief are sorely needed. Integrating basic theory and research derived from three disciplines that rarely have been applied to smoking research (olfaction, emotion, and cognition), the proposed project aims to test the effectiveness of specific olfactory cues to reduce cigarette cravings, setting the stage fr future research testing its value as a component of a smoking cessation intervention.            ",An Olfactory Method for Controlling Cigarette Craving,8685449,R01CA184779,"['Abstinence', 'Address', 'Affect', 'Attenuated', 'Automobile Driving', 'Basic Science', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Behavioral Research', 'Cancer Etiology', 'Cigarette', 'Code', 'Cognition', 'Cognitive', 'Computer Vision Systems', 'Computers', 'Coping Skills', 'Cues', 'Data', 'Dimensions', 'Discipline', 'Effectiveness', 'Emotional', 'Emotions', 'Episodic memory', 'Face', 'Facial Expression', 'Familiarity', 'Gender', 'Habits', 'Impulsive Behavior', 'Individual', 'Individual Differences', 'Interdisciplinary Study', 'Intervention', 'Intervention Studies', 'Knowledge', 'Learning', 'Malignant Neoplasms', 'Measures', 'Mediating', 'Mediator of activation protein', 'Memory', 'Methods', 'Modeling', 'Monitor', 'Moods', 'Motivation', 'Neurobiology', 'Neurotic Disorders', 'Nicotine Dependence', 'Odors', 'Outcome', 'Participant', 'Patient Self-Report', 'Personality', 'Persons', 'Pharmacological Treatment', 'Process', 'Public Health', 'Randomized', 'Reporting', 'Research', 'Role', 'Sampling', 'Short-Term Memory', 'Smell Perception', 'Smoke', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Staging', 'Subgroup', 'Testing', 'Tobacco', 'Vision research', 'Withdrawal', 'Woman', 'addiction', 'base', 'cancer risk', 'coping', 'craving', 'effective therapy', 'heuristics', 'high risk', 'in vivo', 'innovation', 'insight', 'interest', 'nicotine patch', 'novel', 'novel strategies', 'olfactory stimulus', 'prevent', 'programs', 'public health relevance', 'research study', 'response', 'smoking cessation', 'smoking relapse', 'theories']",NCI,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2014,301587,0.06274245663647815
"A Study of the Computational Space of Facial Expressions of Emotion  Project Summary Past research has been very successful in defining how facial expressions of emotion are produced, including which muscle movements create the most commonly seen expressions. These facial expressions of emotion are then interpreted by our visual system. Yet, little is known about how these facial expressions are recognized. The overarching goal of this proposal is to define the form and dimensions of the cognitive (computational) space used in this visual recognition. In particular, this proposal will study the following three hypotheses: Although facial expressions are produced by a complex set of muscle movements, expressions are generally easily identified at different spatial and time resolutions. However, it is not know what these limits are. Our first hypothesis (H1) is that recognition of facial expressions of emotion can be achieved at low resolutions and after short exposure times. In Aim 1, we define experiments to determine how many pixels and milliseconds (ms) are needed to successfully identify different emotions. The fact that expressions of emotion can be recognized quickly at low resolution indicates that simple features robust to image manipulation are employed. Our second hypothesis (H2) is that the recognition of facial expressions of emotion is partially accomplished by an analysis of configural features. Configural cues are known to play an important role in other face recognition tasks, but their role in the processing of expressions of emotion is not yet well understood. Aim 2 will identify a number of these configural cues. We will use real images of faces, manipulated versions of these face images, and schematic drawings. It is also known that shape features play a role in facial expressions (e.g., the curvature of the mouth in happiness). In Aim 3, we define a shape-based computational model. Our hypothesis (H3) is that the configural and shape features are defined as deviations from a mean (or norm) face as opposed to being described as a set of independent exemplars (Gnostic neurons). The importance of this computational space is not only to further justify the results of the previous aims, but to make new predictions that can be verified with additional experiments with human subjects.  Project Narrative Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.",A Study of the Computational Space of Facial Expressions of Emotion,8669977,R01EY020834,"['Acute', 'Address', 'Affect', 'Aging-Related Process', 'Anger', 'Arts', 'Autistic Disorder', 'Behavior', 'Child Abuse', 'Classification', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Conscious', 'Cues', 'Depressed mood', 'Dimensions', 'Duchenne muscular dystrophy', 'Emotions', 'Evolution', 'Eye diseases', 'Face', 'Face Processing', 'Facial Expression', 'Facial Muscles', 'Fright', 'Goals', 'Happiness', 'Human', 'Huntington Disease', 'Image', 'Individual', 'Lead', 'Movement', 'Muscle', 'Neurons', 'Oral cavity', 'Perception', 'Play', 'Positioning Attribute', 'Primates', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Role', 'Schizophrenia', 'Shapes', 'Social Interaction', 'System', 'Time', 'To specify', 'Visual', 'Visual impairment', 'Visual system structure', 'base', 'cognitive system', 'computer human interaction', 'computer studies', 'court', 'design', 'human subject', 'millisecond', 'psychologic', 'research study', 'showing emotion', 'visual process', 'visual processing']",NEI,OHIO STATE UNIVERSITY,R01,2014,358680,0.17962122942202915
"Automated Facial Expression Analysis for Research and Clinical Use     DESCRIPTION (provided by applicant): Facial expression has been a focus of emotion research for over a hundred years. In recent decades observations of facial expressions have yielded critical and dramatic insights about the etiology of psychopathology, and have proven capable of predicting treatment outcomes (see Ekman & Rosenberg, 2005). Despite these original striking findings, there has been surprisingly little follow-up work. The primary reason fr the lack of sustained research is that the most reliable manual systems for measuring facial expression often require considerable training and are labor intensive. Automated measurement using computer vision and machine learning seeks to address the need for valid, efficient, and reproducible measurement. Recent systems have shown promise in fairly small studies using posed behavior or structured contexts with confederates, or trained interviewers, or pre-trained (person-specific) face models. For automated coding to be applied in real-world settings, a large data base with ample variability in pose, head motion, skin color, gender, partial occlusion, and expression intensity is needed. We have developed a unique database that meets this need and the algorithms necessary to enable robust automated coding. The database consists of 720 participants in three-person groups engaged in a group formation task. In a preliminary study, we demonstrated that our algorithms can successfully code two key facial signals associated with human emotion in this relatively unconstrained context (Cohn & Sayette, 2010). To achieve efficient, accurate, and valid measurement of facial expression usable in research and clinical settings, we aim to 1) train and validate classifiers to achieve reliable facial expression detectin across this unprecedentedly large, diverse data set; 2) extend the previous person-specific methods to person-independent (generic) facial feature detection, tracking, and alignment; and 3) make these tools available for research and clinical use.          The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.            ",Automated Facial Expression Analysis for Research and Clinical Use,8464280,R01MH096951,"['Address', 'Algorithms', 'Anxiety', 'Behavior', 'Behavioral Sciences', 'Benchmarking', 'Bipolar Disorder', 'Clinical', 'Code', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Emotions', 'Employee Strikes', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Gender', 'Generic Drugs', 'Head', 'Hour', 'Human', 'Imagery', 'Instruction', 'Interview', 'Interviewer', 'Label', 'Laboratories', 'Learning', 'Lighting', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Movement', 'Nature', 'Pain', 'Participant', 'Personal Computers', 'Persons', 'Process', 'Psychopathology', 'Reporting', 'Research', 'Research Personnel', 'Running', 'Schizophrenia', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'clinical practice', 'follow-up', 'insight', 'meetings', 'nervous system disorder', 'positive emotional state', 'skin color', 'suicidal risk', 'tool']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2013,497842,0.14306841992987776
"Multivariate Representations of Emotion     DESCRIPTION (provided by applicant): A central goal of affective neuroscience is to understand the brain systems and mechanisms underlying the evaluation, experience, and expression of emotion. For example, a widely studied and hotly debated issue in the field is the manner in which biophysical responses to emotional stimuli can be characterized, whether by distinct categories or alternatively along dimensions of valence and arousal. Advances in the fields of psychology, neuroscience, and computer science have fostered significant progress in identifying brain regions involved in processing emotion generally; however, consistent and specific neural markers for distinct affective states have yet to be found. This proposal uses a cutting-edge approach to this core, unresolved question in the field by harnessing emerging pattern classification techniques that are capable of detecting subtle yet coordinated signals from an array of sources. The overarching goal is to identify multivariate patterns of behavioral and biological responding to specific affective states and determine whether these states are organized according to categorical or dimensional architectures. By combining psychophysiology (Aim 1) and functional magnetic resonance imaging (fMRI) (Aim 2), these studies will examine how humans respond to emotional stimuli that vary in duration, modality, and categorical nature. Study 1 focuses on distinct emotions elicited by instrumental music and movie clips whereas Study 2 focuses on those elicited by facial and vocal affect. Together, the aims will provide an integrative, computationally-rigorous method to identify biomarkers of specific emotions that are typically overlooked by conventional univariate statistical approaches. Applying machine learning algorithms in this innovative way could be fruitful for identifying how emotional representations are altered in affective disorders, with the potential for developing novel therapeutic targets. Moreover, identifying patterns of overlap between specific emotion categories may further aid efforts to understand comorbidity issues in anxiety and mood disorders.         PUBLIC HEALTH RELEVANCE: The proposed research will identify patterns of brain and bodily responses that are representative of unique emotional states, using analytical methods that examine changes in multiple biological signals in concert. Identifying neural and peripheral representations of emotional states and how they relate to self-reports of emotion will provide a better understanding of how emotions are organized and coded in the brain and body. These biomarkers of specific emotions can provide clues as to how emotional processing is altered in mood and anxiety disorders and can lead to novel treatment targets.                    ",Multivariate Representations of Emotion,8510264,R21MH098149,"['Accounting', 'Address', 'Affect', 'Affective', 'Algorithms', 'Anterior', 'Anxiety Disorders', 'Architecture', 'Arousal', 'Auditory', 'Behavioral', 'Biological', 'Biological Markers', 'Brain', 'Brain region', 'Categories', 'Classification', 'Clip', 'Code', 'Comorbidity', 'Complex', 'Data', 'Dimensions', 'Emotional', 'Emotions', 'Evaluation', 'Event', 'Experimental Designs', 'Face', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Goals', 'Grant', 'Human', 'Image', 'Individual', 'Investigation', 'Label', 'Lead', 'Literature', 'Machine Learning', 'Measures', 'Meta-Analysis', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Mood Disorders', 'Music', 'Nature', 'Neurons', 'Neurosciences', 'Participant', 'Patient Self-Report', 'Pattern', 'Peripheral', 'Physiological', 'Process', 'Psychology', 'Psychopathology', 'Psychophysiology', 'Research', 'Sensory', 'Signal Transduction', 'Source', 'Statistical Methods', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visual', 'Weight', 'Work', 'affective neuroscience', 'analytical method', 'base', 'computer science', 'design', 'emotional experience', 'emotional stimulus', 'experience', 'imaging modality', 'innovation', 'insight', 'movie', 'multisensory', 'neural patterning', 'neuroimaging', 'new therapeutic target', 'novel', 'novel strategies', 'public health relevance', 'relating to nervous system', 'response', 'showing emotion', 'theories', 'tool']",NIMH,DUKE UNIVERSITY,R21,2013,196250,0.1436616106356431
"Modeling the Dynamics of Early Communication and Development     DESCRIPTION (provided by applicant):      Significance. Computational modeling is central to a rigorous understanding of the development of the child's first social relationships. The project will address this challenge by modeling longitudinal change in the dynamics of early social interactions. Modeling will integrate objective (automated) measurements of emotion and attention and common genetic variants relevant to those constructs.     Innovation. Objective measurement of behavior will involve the automated modeling and classification of the physical properties of communicative signals-such as facial expressions and vocalizations. Dynamic models of self-regulation and interactive influence during dyadic interaction will utilize precise measurements of expressive behavior as moderated by genetic markers associated with dopaminergic and serotonergic functioning. The interdisciplinary team includes investigators including from developmental and quantitative psychology, genetics, affective computing, computer vision, and physics who model dynamic interactive processes at a variety of time scales.     Approach. Infant-mother interaction, its perturbation, and its development, will be investigated using the Face-to-Face/Still-Face (FFSF) procedure at 2, 4, 6, and 8 months. Facial modeling, head, and arm/hand modeling will be used to conduct objective measurements of a multimodal suite of interactive behaviors including facial expression, gaze direction, head movement, tickling, and vocalization. Models will be trained and evaluated with respect to expert coding and non-experts' perceptions of emotional valence constructs. Dynamic approaches to time-series modeling will focus on the development of self-regulation and interactive influence. Inverse optimal control modeling will be used to infer infant and mother preferences for particular dyadic states given observed patterns of behavior. The context-dependence of these parameters will be assessed with respect to the perturbation introduced by the still-face (a brief period of investigator-requested adult non-responsivity). Individual differences in infant and mother behavioral parameters will be modeled with respect to genetic indices of infant and mother dopaminergic and serotonergic function. Modeling algorithms, measurement software, and coded recordings will be shared with the scientific community to catalyze progress in the understanding of behavioral systems. These efforts will increase understanding of pathways to healthy cognitive and socio-emotional development, and shed light on the potential for change that will inform early intervention efforts.         PUBLIC HEALTH RELEVANCE:     The modeling of early infant-parent relationships is central to a rigorous quantitative understanding of social development. Objective measurements of communicative behavior and related genetic markers in infant and mother will be used to model the development of self- regulation and interactive influence as they develop longitudinally. Genetically informed modeling of the infant-mother interactive system will produce a rigorous understanding of parameters that describe the diversity of early developmental pathways and potential psychopathological deviations from those pathways.                ",Modeling the Dynamics of Early Communication and Development,8452565,R01GM105004,"['Address', 'Adult', 'Affective', 'Age', 'Algorithms', 'Attention', 'Behavior', 'Behavioral', 'Child Development', 'Classification', 'Code', 'Cognitive', 'Communication', 'Communities', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data Set', 'Dependence', 'Development', 'Early Intervention', 'Elements', 'Emotional', 'Emotions', 'Event', 'Evolution', 'Face', 'Facial Expression', 'Functional RNA', 'Genetic', 'Genetic Markers', 'Hand', 'Head', 'Head Movements', 'Human', 'Individual', 'Individual Differences', 'Infant', 'Informal Social Control', 'Joints', 'Light', 'Manuals', 'Measurement', 'Measures', 'Metaphor', 'Modeling', 'Mothers', 'Motion', 'Neurophysiology - biologic function', 'Parents', 'Pathway interactions', 'Pattern', 'Perception', 'Physics', 'Play', 'Procedures', 'Process', 'Psychology', 'Research Personnel', 'Scientist', 'Series', 'Signal Transduction', 'Social Behavior', 'Social Development', 'Social Interaction', 'System', 'Time', 'Training', 'Work', 'arm', 'behavior measurement', 'dyadic interaction', 'gaze', 'genetic variant', 'indexing', 'infant monitoring', 'innovation', 'insight', 'model development', 'multilevel analysis', 'physical property', 'preference', 'public health relevance', 'response', 'social', 'social skills', 'vocalization']",NIGMS,UNIVERSITY OF MIAMI CORAL GABLES,R01,2013,571955,0.05297480669690097
"The Development and Neural Bases of Emotion Processing     DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness.          Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.            ",The Development and Neural Bases of Emotion Processing,8435408,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Relative (related person)', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'base', 'computerized data processing', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2013,729130,0.12275310936264823
"Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders    DESCRIPTION (provided by applicant): An inability to convey emotion by vocal modulation (prosody) is a major aspect of social cognitive dysfunction for patients afflicted with a variety of neuropsychiatric illnesses such as schizophrenia and autism. The neural mechanisms of these deficits, and how they develop from childhood to adulthood have received remarkably little research attention. This training proposal seeks to prepare the candidate for independent research that addresses both of these issues. In terms of the neural mechanisms of prosody, there is some evidence that prosodic dysfunction is associated with a disruption of interactions between temporal and frontal cortex. The temporal component of this circuit appears to parse the acoustic signal for emotional distinctions, which are subsequently evaluated for emotional meaning by the frontal component. It is likely that this circuit mediates prosodic dysfunction in schizophrenia, which is associated with basic audio-sensory deficits and abnormal temporo-frontal interactions. The research proposed here will employ multimodal neuroimaging approaches to produce the first comprehensive description of this temporo-frontal circuit, in both healthy adults and patients with schizophrenia, by integrating three state-of-the-art neuroimaging techniques. First, magnetoencephalography (MEG) in conjunction with magnetic resonance functional connectivity analysis (fMRI) will be used to determine the time course of processing by the circuit. Then, by applying Diffusion Imaging, these functional estimates will be related to neurostructural estimates of auditory pathway integrity. We expect to demonstrate that neurostructural integrity deficits underpin abnormal temporo-frontal interactions during prosodic processing in schizophrenia. In addition to training the candidate in multimodal neuroimaging, the proposed activities will provide him a background in the developmental psychology of language and auditory neurobiology, as well as a framework to develop and adapt prosodic tasks for children and adolescents. The candidate is an Instructor in Psychiatry at the University of Pennsylvania and has an established interest in social communication. Drs. Timothy Roberts and Ruben Gur will mentor him. Dr. Roberts is an expert in MEG and DTI techniques, with a research focus on the development of auditory and language processing in healthy children and patients with autism. Dr. Gur is an expert in neuropsychological assessment and neuroimaging, whose research concerns emotional and cognitive dysfunction in schizophrenia. These mentors will assure that the experience resulting from the proposed training plan will enable the candidate to establish an independent research program that can make significant contributions in the developmental neuroscience of social communication, and the understanding of how dysfunction in such communication emerges in neuropsychiatric disorders like schizophrenia and autism.        Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.            ",Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders,8468751,K01MH094689,"['Acoustics', 'Address', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Amygdaloid structure', 'Attention', 'Auditory', 'Autistic Disorder', 'Behavioral', 'Brain', 'Child', 'Childhood', 'Cognitive deficits', 'Communication', 'Comprehension', 'Cues', 'Data', 'Development', 'Diagnostic', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Disease', 'Dorsal', 'Emotional', 'Emotions', 'Encapsulated', 'Evaluation', 'Functional Imaging', 'Functional disorder', 'Goals', 'Gur', 'Impaired cognition', 'Impairment', 'Independent Living', 'Inferior frontal gyrus', 'Language', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Marshal', 'Measures', 'Mediating', 'Mentors', 'Modeling', 'Multimodal Imaging', 'Nature', 'Neurobiology', 'Neurosciences', 'Pathology', 'Patients', 'Pattern', 'Pennsylvania', 'Phase', 'Process', 'Psychiatry', 'Public Health', 'Relative (related person)', 'Research', 'Ruthenium Ben', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Severities', 'Signal Transduction', 'Social Functioning', 'Societies', 'Staging', 'Stimulus', 'Structure of middle temporal gyrus', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Training Programs', 'Universities', 'Variant', 'Work', 'auditory pathway', 'career', 'developmental disease', 'developmental psychology', 'experience', 'frontal lobe', 'functional outcomes', 'insight', 'instructor', 'interest', 'language processing', 'neural circuit', 'neuroimaging', 'neuromechanism', 'neuropsychiatry', 'neuropsychological', 'prognostic', 'programs', 'relating to nervous system', 'social', 'social cognition', 'social communication', 'tool', 'white matter']",NIMH,UNIVERSITY OF PENNSYLVANIA,K01,2013,168157,0.06338910876386301
"A Study of the Computational Space of Facial Expressions of Emotion  Project Summary Past research has been very successful in defining how facial expressions of emotion are produced, including which muscle movements create the most commonly seen expressions. These facial expressions of emotion are then interpreted by our visual system. Yet, little is known about how these facial expressions are recognized. The overarching goal of this proposal is to define the form and dimensions of the cognitive (computational) space used in this visual recognition. In particular, this proposal will study the following three hypotheses: Although facial expressions are produced by a complex set of muscle movements, expressions are generally easily identified at different spatial and time resolutions. However, it is not know what these limits are. Our first hypothesis (H1) is that recognition of facial expressions of emotion can be achieved at low resolutions and after short exposure times. In Aim 1, we define experiments to determine how many pixels and milliseconds (ms) are needed to successfully identify different emotions. The fact that expressions of emotion can be recognized quickly at low resolution indicates that simple features robust to image manipulation are employed. Our second hypothesis (H2) is that the recognition of facial expressions of emotion is partially accomplished by an analysis of configural features. Configural cues are known to play an important role in other face recognition tasks, but their role in the processing of expressions of emotion is not yet well understood. Aim 2 will identify a number of these configural cues. We will use real images of faces, manipulated versions of these face images, and schematic drawings. It is also known that shape features play a role in facial expressions (e.g., the curvature of the mouth in happiness). In Aim 3, we define a shape-based computational model. Our hypothesis (H3) is that the configural and shape features are defined as deviations from a mean (or norm) face as opposed to being described as a set of independent exemplars (Gnostic neurons). The importance of this computational space is not only to further justify the results of the previous aims, but to make new predictions that can be verified with additional experiments with human subjects.  Project Narrative Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.",A Study of the Computational Space of Facial Expressions of Emotion,8494053,R01EY020834,"['Acute', 'Address', 'Affect', 'Aging-Related Process', 'Anger', 'Arts', 'Autistic Disorder', 'Behavior', 'Child Abuse', 'Classification', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Conscious', 'Cues', 'Depressed mood', 'Dimensions', 'Duchenne muscular dystrophy', 'Emotions', 'Evolution', 'Eye diseases', 'Face', 'Face Processing', 'Facial Expression', 'Facial Muscles', 'Fright', 'Goals', 'Happiness', 'Human', 'Huntington Disease', 'Image', 'Individual', 'Lead', 'Movement', 'Muscle', 'Neurons', 'Oral cavity', 'Perception', 'Play', 'Positioning Attribute', 'Primates', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Role', 'Schizophrenia', 'Shapes', 'Social Interaction', 'System', 'Time', 'To specify', 'Visual', 'Visual impairment', 'Visual system structure', 'base', 'cognitive system', 'computer human interaction', 'computer studies', 'court', 'design', 'human subject', 'millisecond', 'psychologic', 'research study', 'showing emotion', 'visual process', 'visual processing']",NEI,OHIO STATE UNIVERSITY,R01,2013,347700,0.17962122942202915
"Automated Facial Expression Analysis for Research and Clinical Use     DESCRIPTION (provided by applicant): Facial expression has been a focus of emotion research for over a hundred years. In recent decades observations of facial expressions have yielded critical and dramatic insights about the etiology of psychopathology, and have proven capable of predicting treatment outcomes (see Ekman & Rosenberg, 2005). Despite these original striking findings, there has been surprisingly little follow-up work. The primary reason fr the lack of sustained research is that the most reliable manual systems for measuring facial expression often require considerable training and are labor intensive. Automated measurement using computer vision and machine learning seeks to address the need for valid, efficient, and reproducible measurement. Recent systems have shown promise in fairly small studies using posed behavior or structured contexts with confederates, or trained interviewers, or pre-trained (person-specific) face models. For automated coding to be applied in real-world settings, a large data base with ample variability in pose, head motion, skin color, gender, partial occlusion, and expression intensity is needed. We have developed a unique database that meets this need and the algorithms necessary to enable robust automated coding. The database consists of 720 participants in three-person groups engaged in a group formation task. In a preliminary study, we demonstrated that our algorithms can successfully code two key facial signals associated with human emotion in this relatively unconstrained context (Cohn & Sayette, 2010). To achieve efficient, accurate, and valid measurement of facial expression usable in research and clinical settings, we aim to 1) train and validate classifiers to achieve reliable facial expression detectin across this unprecedentedly large, diverse data set; 2) extend the previous person-specific methods to person-independent (generic) facial feature detection, tracking, and alignment; and 3) make these tools available for research and clinical use.        PUBLIC HEALTH RELEVANCE: The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.              The project has two target application domains. For behavioral science, automated facial expression analysis will provide researchers with powerful tools to examine basic questions in emotion and interpersonal processes, as well as emotion processes underlying diverse forms of psychopathology and neurologic disorder. For clinical use, automated facial expression analysis will help clinicians to assess vulnerability and protective factors and objectively evaluate course of treatment across a wide range of disorders including major depression, bipolar disorder, schizophrenia, anxiety, addiction, suicide risk, and pain.            ",Automated Facial Expression Analysis for Research and Clinical Use,8270831,R01MH096951,"['Address', 'Algorithms', 'Anxiety', 'Behavior', 'Behavioral Sciences', 'Benchmarking', 'Bipolar Disorder', 'Clinical', 'Code', 'Communities', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Emotions', 'Employee Strikes', 'Environment', 'Etiology', 'Face', 'Facial Expression', 'Gender', 'Generic Drugs', 'Head', 'Hour', 'Human', 'Imagery', 'Instruction', 'Interview', 'Interviewer', 'Label', 'Laboratories', 'Learning', 'Lighting', 'Machine Learning', 'Major Depressive Disorder', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Motion', 'Movement', 'Nature', 'Pain', 'Participant', 'Personal Computers', 'Persons', 'Process', 'Psychopathology', 'Reporting', 'Research', 'Research Personnel', 'Running', 'Schizophrenia', 'Signal Transduction', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Treatment outcome', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'clinical practice', 'follow-up', 'insight', 'meetings', 'nervous system disorder', 'positive emotional state', 'skin color', 'suicidal risk', 'tool']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2012,621438,0.12896034256568983
"The Development and Neural Bases of Emotion Processing     DESCRIPTION (provided by applicant): An important function of the brain is to scan incoming sensory information for the presence of biologically relevant features and process and act on this information. For humans, the most salient signals of emotion are often social in nature, such as expressions of fear or anger. The goal of the current competing renewal is to study the nature and neural architecture of emotion processing across the first three years of life. Five-, seven-, and twelve-month-old infants, as well as three-year-old typically developing children will serve as participants across 5 specific aims. Aim 1 seeks to examine the neural and cardiac correlates of the infant's ability to process emotion in both faces and non-face stimuli. Aim 2 examines a similar question, except that autonomic activity (skin conductance and pupil diameter) will be recorded in conjunction with functional Near Infrared Spectroscopy (fNIRS). Aim 3 seeks to elucidate the neural networks involved in emotion processing, and will do so by using state-of-the-art signal processing software to extract theta activity from the ongoing EEG. Aim 4 will focus on individual differences in emotion processing viewed through the lens of genetics; specifically, all infants serving as participants in Aims 1 and 2 will be genotyped, with most attention focused on 5 SNPs, with an additional 5 SNPs serving as a secondary aim. All SNPS have been shown to be relevant to emotion processing in both humans and non-human species. Finally, in Aim 5 we examine whether early biases in emotion processing (i.e., whether infants show greater visual or neural activity to one emotion vs. another; e.g., fear) predict (or are associated with) behavioral inhibition and anxiety. Although the current project focuses on typically developing children, this work has enormous implications for children and adults who suffer from deficits in social-emotional communication. First, this work seeks to explicate the ontogeny of facial emotion processing; an ability that likely provides a foundation upon which higher-level social communication builds. As a result, it may well be the case that errors in this ability that occur early in development can develop into more insidious deficits that occur later i development. Second, the approach adopted in this project is highly innovative, and can easily be extended to various clinical populations, such as toddlers with autism or children diagnosed with depression or bipolar illness.        PUBLIC HEALTH RELEVANCE: Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.              Prior to the onset of language, infants and adults primarily communicate through non-verbal channels. Of particular importance is the infant's ability to decode facial expressions of emotion, an ability that is often compromised in children with autism or who has been maltreated and in adults with schizophrenia and bipolar illness. The goal of the current proposal is to examine the development and neural bases of emotion perception and recognition.            ",The Development and Neural Bases of Emotion Processing,8291570,R01MH078829,"['1 year old', '3 year old', 'AVPR1A gene', 'Adopted', 'Adult', 'Affective', 'Age', 'Age-Months', 'Anger', 'Animal Model', 'Anxiety', 'Architecture', 'Arousal', 'Attention', 'Autistic Disorder', 'Behavior', 'Behavioral inhibition', 'Biological Neural Networks', 'Brain', 'Brain-Derived Neurotrophic Factor', 'Caliber', 'Cardiac', 'Child', 'Child Behavior', 'Clinical', 'Code', 'Communication', 'Computer software', 'DRD4 gene', 'Data', 'Development', 'Diagnosis', 'Electroencephalography', 'Emotional', 'Emotions', 'Experimental Designs', 'Eye', 'Face', 'Facial Expression', 'Foundations', 'Fright', 'Galvanic Skin Response', 'Genes', 'Genetic', 'Genetic Polymorphism', 'Genetic Variation', 'Genotype', 'Goals', 'Growth', 'Heart Rate', 'Human', 'Individual Differences', 'Infant', 'Inferior', 'Life', 'Machine Learning', 'Measures', 'Mental Depression', 'Metabolic', 'Nature', 'Near-Infrared Spectroscopy', 'Participant', 'Perception', 'Population', 'Process', 'Pupil', 'RGS2 gene', 'Relative (related person)', 'Scalp structure', 'Scanning', 'Schizophrenia', 'Sensory', 'Signal Transduction', 'Stimulus', 'Testing', 'Toddler', 'Variant', 'Visual', 'Work', 'base', 'computerized data processing', 'design', 'early childhood', 'experience', 'innovation', 'language onset', 'lens', 'neurodevelopment', 'preference', 'relating to nervous system', 'response', 'showing emotion', 'social', 'social communication', 'stimulus processing', 'tool']",NIMH,BOSTON CHILDREN'S HOSPITAL,R01,2012,781847,0.11134822376871069
"Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders    DESCRIPTION (provided by applicant): An inability to convey emotion by vocal modulation (prosody) is a major aspect of social cognitive dysfunction for patients afflicted with a variety of neuropsychiatric illnesses such as schizophrenia and autism. The neural mechanisms of these deficits, and how they develop from childhood to adulthood have received remarkably little research attention. This training proposal seeks to prepare the candidate for independent research that addresses both of these issues. In terms of the neural mechanisms of prosody, there is some evidence that prosodic dysfunction is associated with a disruption of interactions between temporal and frontal cortex. The temporal component of this circuit appears to parse the acoustic signal for emotional distinctions, which are subsequently evaluated for emotional meaning by the frontal component. It is likely that this circuit mediates prosodic dysfunction in schizophrenia, which is associated with basic audio-sensory deficits and abnormal temporo-frontal interactions. The research proposed here will employ multimodal neuroimaging approaches to produce the first comprehensive description of this temporo-frontal circuit, in both healthy adults and patients with schizophrenia, by integrating three state-of-the-art neuroimaging techniques. First, magnetoencephalography (MEG) in conjunction with magnetic resonance functional connectivity analysis (fMRI) will be used to determine the time course of processing by the circuit. Then, by applying Diffusion Imaging, these functional estimates will be related to neurostructural estimates of auditory pathway integrity. We expect to demonstrate that neurostructural integrity deficits underpin abnormal temporo-frontal interactions during prosodic processing in schizophrenia. In addition to training the candidate in multimodal neuroimaging, the proposed activities will provide him a background in the developmental psychology of language and auditory neurobiology, as well as a framework to develop and adapt prosodic tasks for children and adolescents. The candidate is an Instructor in Psychiatry at the University of Pennsylvania and has an established interest in social communication. Drs. Timothy Roberts and Ruben Gur will mentor him. Dr. Roberts is an expert in MEG and DTI techniques, with a research focus on the development of auditory and language processing in healthy children and patients with autism. Dr. Gur is an expert in neuropsychological assessment and neuroimaging, whose research concerns emotional and cognitive dysfunction in schizophrenia. These mentors will assure that the experience resulting from the proposed training plan will enable the candidate to establish an independent research program that can make significant contributions in the developmental neuroscience of social communication, and the understanding of how dysfunction in such communication emerges in neuropsychiatric disorders like schizophrenia and autism.        Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.            ",Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders,8296567,K01MH094689,"['Acoustics', 'Address', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Amygdaloid structure', 'Attention', 'Auditory', 'Autistic Disorder', 'Behavioral', 'Brain', 'Child', 'Childhood', 'Cognitive deficits', 'Communication', 'Comprehension', 'Cues', 'Data', 'Development', 'Diagnostic', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Disease', 'Dorsal', 'Emotional', 'Emotions', 'Encapsulated', 'Evaluation', 'Functional Imaging', 'Functional disorder', 'Goals', 'Gur', 'Impaired cognition', 'Impairment', 'Independent Living', 'Inferior frontal gyrus', 'Language', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Marshal', 'Measures', 'Mediating', 'Mentors', 'Modeling', 'Multimodal Imaging', 'Nature', 'Neurobiology', 'Neurosciences', 'Pathology', 'Patients', 'Pattern', 'Pennsylvania', 'Phase', 'Process', 'Psychiatry', 'Public Health', 'Relative (related person)', 'Research', 'Ruthenium Ben', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Severities', 'Signal Transduction', 'Social Functioning', 'Societies', 'Staging', 'Stimulus', 'Structure of middle temporal gyrus', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Training Programs', 'Universities', 'Variant', 'Work', 'auditory pathway', 'career', 'developmental disease', 'developmental psychology', 'experience', 'frontal lobe', 'functional outcomes', 'insight', 'instructor', 'interest', 'language processing', 'neural circuit', 'neuroimaging', 'neuromechanism', 'neuropsychiatry', 'neuropsychological', 'prognostic', 'programs', 'relating to nervous system', 'social', 'social cognition', 'social communication', 'tool', 'white matter']",NIMH,UNIVERSITY OF PENNSYLVANIA,K01,2012,168157,0.06338910876386301
"A Study of the Computational Space of Facial Expressions of Emotion  Project Summary Past research has been very successful in defining how facial expressions of emotion are produced, including which muscle movements create the most commonly seen expressions. These facial expressions of emotion are then interpreted by our visual system. Yet, little is known about how these facial expressions are recognized. The overarching goal of this proposal is to define the form and dimensions of the cognitive (computational) space used in this visual recognition. In particular, this proposal will study the following three hypotheses: Although facial expressions are produced by a complex set of muscle movements, expressions are generally easily identified at different spatial and time resolutions. However, it is not know what these limits are. Our first hypothesis (H1) is that recognition of facial expressions of emotion can be achieved at low resolutions and after short exposure times. In Aim 1, we define experiments to determine how many pixels and milliseconds (ms) are needed to successfully identify different emotions. The fact that expressions of emotion can be recognized quickly at low resolution indicates that simple features robust to image manipulation are employed. Our second hypothesis (H2) is that the recognition of facial expressions of emotion is partially accomplished by an analysis of configural features. Configural cues are known to play an important role in other face recognition tasks, but their role in the processing of expressions of emotion is not yet well understood. Aim 2 will identify a number of these configural cues. We will use real images of faces, manipulated versions of these face images, and schematic drawings. It is also known that shape features play a role in facial expressions (e.g., the curvature of the mouth in happiness). In Aim 3, we define a shape-based computational model. Our hypothesis (H3) is that the configural and shape features are defined as deviations from a mean (or norm) face as opposed to being described as a set of independent exemplars (Gnostic neurons). The importance of this computational space is not only to further justify the results of the previous aims, but to make new predictions that can be verified with additional experiments with human subjects.  Project Narrative Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.",A Study of the Computational Space of Facial Expressions of Emotion,8266468,R01EY020834,"['Acute', 'Address', 'Affect', 'Aging-Related Process', 'Anger', 'Arts', 'Autistic Disorder', 'Behavior', 'Child Abuse', 'Classification', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Conscious', 'Cues', 'Depressed mood', 'Dimensions', 'Duchenne muscular dystrophy', 'Emotions', 'Evolution', 'Eye diseases', 'Face', 'Face Processing', 'Facial Expression', 'Facial Muscles', 'Fright', 'Goals', 'Happiness', 'Human', 'Huntington Disease', 'Image', 'Individual', 'Lead', 'Movement', 'Muscle', 'Neurons', 'Oral cavity', 'Perception', 'Play', 'Positioning Attribute', 'Primates', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Role', 'Schizophrenia', 'Shapes', 'Social Interaction', 'System', 'Time', 'To specify', 'Visual', 'Visual impairment', 'Visual system structure', 'base', 'cognitive system', 'computer human interaction', 'computer studies', 'court', 'design', 'human subject', 'millisecond', 'psychologic', 'research study', 'showing emotion', 'visual process', 'visual processing']",NEI,OHIO STATE UNIVERSITY,R01,2012,366000,0.17962122942202915
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,8120220,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2011,161797,0.041874437521812455
"Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders    DESCRIPTION (provided by applicant): An inability to convey emotion by vocal modulation (prosody) is a major aspect of social cognitive dysfunction for patients afflicted with a variety of neuropsychiatric illnesses such as schizophrenia and autism. The neural mechanisms of these deficits, and how they develop from childhood to adulthood have received remarkably little research attention. This training proposal seeks to prepare the candidate for independent research that addresses both of these issues. In terms of the neural mechanisms of prosody, there is some evidence that prosodic dysfunction is associated with a disruption of interactions between temporal and frontal cortex. The temporal component of this circuit appears to parse the acoustic signal for emotional distinctions, which are subsequently evaluated for emotional meaning by the frontal component. It is likely that this circuit mediates prosodic dysfunction in schizophrenia, which is associated with basic audio-sensory deficits and abnormal temporo-frontal interactions. The research proposed here will employ multimodal neuroimaging approaches to produce the first comprehensive description of this temporo-frontal circuit, in both healthy adults and patients with schizophrenia, by integrating three state-of-the-art neuroimaging techniques. First, magnetoencephalography (MEG) in conjunction with magnetic resonance functional connectivity analysis (fMRI) will be used to determine the time course of processing by the circuit. Then, by applying Diffusion Imaging, these functional estimates will be related to neurostructural estimates of auditory pathway integrity. We expect to demonstrate that neurostructural integrity deficits underpin abnormal temporo-frontal interactions during prosodic processing in schizophrenia. In addition to training the candidate in multimodal neuroimaging, the proposed activities will provide him a background in the developmental psychology of language and auditory neurobiology, as well as a framework to develop and adapt prosodic tasks for children and adolescents. The candidate is an Instructor in Psychiatry at the University of Pennsylvania and has an established interest in social communication. Drs. Timothy Roberts and Ruben Gur will mentor him. Dr. Roberts is an expert in MEG and DTI techniques, with a research focus on the development of auditory and language processing in healthy children and patients with autism. Dr. Gur is an expert in neuropsychological assessment and neuroimaging, whose research concerns emotional and cognitive dysfunction in schizophrenia. These mentors will assure that the experience resulting from the proposed training plan will enable the candidate to establish an independent research program that can make significant contributions in the developmental neuroscience of social communication, and the understanding of how dysfunction in such communication emerges in neuropsychiatric disorders like schizophrenia and autism.      PUBLIC HEALTH RELEVANCE: Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.              Impaired social communication is an enduring and debilitating aspect of neuropsychiatric illness. Greater understanding of the neural nature of this dysfunction and how it unfolds during development should provide important etiopathic information that could lead to new treatments. This would benefit public health by helping to facilitate patient integration into society.            ",Multimodal Neuroimaging of Prosody in Schizophrenia and Developmental Disorders,8164940,K01MH094689,"['Acoustics', 'Address', 'Adolescent', 'Adult', 'Affect', 'Affective', 'Amygdaloid structure', 'Attention', 'Auditory', 'Autistic Disorder', 'Behavioral', 'Brain', 'Child', 'Childhood', 'Cognitive deficits', 'Communication', 'Comprehension', 'Cues', 'Data', 'Development', 'Diagnostic', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Diffusion weighted imaging', 'Disease', 'Dorsal', 'Emotional', 'Emotions', 'Encapsulated', 'Evaluation', 'Functional Imaging', 'Functional disorder', 'Goals', 'Gur', 'Impaired cognition', 'Impairment', 'Independent Living', 'Inferior frontal gyrus', 'Language', 'Lead', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Magnetoencephalography', 'Marshal', 'Measures', 'Mediating', 'Mentors', 'Modeling', 'Multimodal Imaging', 'Nature', 'Neurobiology', 'Neurosciences', 'Pathology', 'Patients', 'Pattern', 'Pennsylvania', 'Phase', 'Process', 'Psychiatry', 'Public Health', 'Relative (related person)', 'Research', 'Ruthenium Ben', 'Schizophrenia', 'Sensory', 'Sensory Process', 'Severities', 'Signal Transduction', 'Social Functioning', 'Societies', 'Staging', 'Stimulus', 'Structure of middle temporal gyrus', 'Symptoms', 'Techniques', 'Temporal Lobe', 'Testing', 'Time', 'Training', 'Training Programs', 'Universities', 'Variant', 'Work', 'auditory pathway', 'career', 'developmental disease', 'developmental psychology', 'experience', 'frontal lobe', 'functional outcomes', 'insight', 'instructor', 'interest', 'language processing', 'neural circuit', 'neuroimaging', 'neuromechanism', 'neuropsychiatry', 'neuropsychological', 'prognostic', 'programs', 'relating to nervous system', 'social', 'social cognition', 'social communication', 'tool', 'white matter']",NIMH,UNIVERSITY OF PENNSYLVANIA,K01,2011,168157,0.05716313310081757
"Biological and Information Processing Mechanisms Underlying Autism This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34. n/a",Biological and Information Processing Mechanisms Underlying Autism,8126284,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2011,1821700,0.02148808444688589
"A Study of the Computational Space of Facial Expressions of Emotion    DESCRIPTION (provided by applicant): Past research has been very successful in defining how facial expressions of emotion are produced, including which muscle movements create the most commonly seen expressions. These facial expressions of emotion are then interpreted by our visual system. Yet, little is known about how these facial expressions are recognized. The overarching goal of this proposal is to define the form and dimensions of the cognitive (computational) space used in this visual recognition. In particular, this proposal will study the following three hypotheses: Although facial expressions are produced by a complex set of muscle movements, expressions are generally easily identified at different spatial and time resolutions. However, it is not know what these limits are. Our first hypothesis (H1) is that recognition of facial expressions of emotion can be achieved at low resolutions and after short exposure times. In Aim 1, we define experiments to determine how many pixels and milliseconds (ms) are needed to successfully identify different emotions. The fact that expressions of emotion can be recognized quickly at low resolution indicates that simple features robust to image manipulation are employed. Our second hypothesis (H2) is that the recognition of facial expressions of emotion is partially accomplished by an analysis of configural features. Configural cues are known to play an important role in other face recognition tasks, but their role in the processing of expressions of emotion is not yet well understood. Aim 2 will identify a number of these configural cues. We will use real images of faces, manipulated versions of these face images, and schematic drawings. It is also known that shape features play a role in facial expressions (e.g., the curvature of the mouth in happiness). In Aim 3, we define a shape-based computational model. Our hypothesis (H3) is that the configural and shape features are defined as deviations from a mean (or norm) face as opposed to being described as a set of independent exemplars (Gnostic neurons). The importance of this computational space is not only to further justify the results of the previous aims, but to make new predictions that can be verified with additional experiments with human subjects.      PUBLIC HEALTH RELEVANCE: Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.           Project Narrative Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.",A Study of the Computational Space of Facial Expressions of Emotion,8142075,R01EY020834,"['Acute', 'Address', 'Affect', 'Aging-Related Process', 'Anger', 'Arts', 'Autistic Disorder', 'Behavior', 'Child Abuse', 'Classification', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Conscious', 'Cues', 'Depressed mood', 'Dimensions', 'Duchenne muscular dystrophy', 'Emotions', 'Evolution', 'Eye diseases', 'Face', 'Face Processing', 'Facial Expression', 'Facial Muscles', 'Fright', 'Goals', 'Happiness', 'Human', 'Huntington Disease', 'Image', 'Individual', 'Lead', 'Movement', 'Muscle', 'Neurons', 'Oral cavity', 'Perception', 'Play', 'Positioning Attribute', 'Primates', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Role', 'Schizophrenia', 'Shapes', 'Social Interaction', 'System', 'Time', 'To specify', 'Visual', 'Visual impairment', 'Visual system structure', 'base', 'cognitive system', 'computer human interaction', 'computer studies', 'court', 'design', 'human subject', 'millisecond', 'psychologic', 'public health relevance', 'research study', 'showing emotion', 'visual process', 'visual processing']",NEI,OHIO STATE UNIVERSITY,R01,2011,366000,0.1791460017532975
"USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING    DESCRIPTION (provided by applicant): Understanding mechanisms of action is key to improving psychosocial interventions for cancer and other chronic disease conditions. In cancer, emotional expression has been identified as one possible mediator of the effect of psychosocial intervention on patient-reported outcomes. However, scientific evaluations of psychological mechanisms of adjustment to cancer and other chronic diseases are constrained by limitations associated with self-report measures. Because self-care resources, peer-to-peer networks, and more recent forms of psychosocial intervention are increasingly being delivered online, linguistic and behavioral data can be used to characterize internal coping processes, social interactions, and other manifest behaviors. Few tools are currently available for harnessing text as a potential data source, and signal detection indices of existing tools leave room for considerable improvement in these methodologies (Bantum & Owen, 2009). In the present study, natural language processing and other tools of computational linguistics will be used to develop a machine-learning classifier to identify emotional expression in electronic text data. The aims of the study are: 1) to annotate a large text corpus from cancer survivors using an objective and reliable emotion-coding procedure, 2) to incorporate linguistic and psychological features into a machine-learning classification method and identify which of these features are most strongly associated with codes assigned by trained human raters, and 3) to develop combined psychological and natural language processing (NLP) methods for identifying linguistic markers of emotional coping behaviors. To accomplish these aims, a comprehensive corpus of emotionally-laden cancer communications will be developed from 5 existing linguistic datasets. Five raters will be selected and undergo a rigorous training procedure for coding emotional expression using an emotion-coding system previously developed by the research. Coding will take place using an Internet-based coding interface that will allow the investigators to continuously monitor inter-rater reliability. Simultaneous with the coding process, the investigators will link the electronic text data with key linguistic and psychological features, including Linguistic Inquiry and Word Count (LIWC), Affective Norms for English Words (ANEW), WordNet, part of speech tags, patterns of capitalization and punctuation, emoticons, and textual context. A machine-learning classifier, using tools of natural language processing, will then be applied to the text/feature data and validated against human-rated emotion codes. The long-term objective of this research is to advance a methodology for objectively identifying coping behavior, particularly emotional expression, in order to supplement self-report measures and improve scientific understanding of adjustment to chronic disease, trauma, or other psychological conditions. This work is essential for identifying mechanisms of action in psychosocial interventions for cancer survivors and others and has significance for the fields of medicine, psychology, computational linguistics, and artificial intelligence.      PUBLIC HEALTH RELEVANCE: Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.           Identifying specific emotional, cognitive, and behavioral factors that contribute to adjustment to cancer and other chronic diseases is essential for being able to develop and improve effective interventions to promote health and well-being. To date, the study of these factors as mechanisms of action has been limited to self-report measures that may not correlate well with other more objective indicators. The proposed study will improve our ability to identify mechanisms of action by supplementing self-report measures with objectively identified markers of coping behaviors such as emotional expression in natural language used by individuals living with cancer.",USE OF NATURAL LANGUAGE PROCESSING TO IDENTIFY LINGUISTIC MARKERS OF COPING,7991498,R21CA143642,"['Affective', 'Artificial Intelligence', 'Behavior', 'Behavior Therapy', 'Behavioral', 'Cancer Intervention', 'Cancer Survivor', 'Categories', 'Characteristics', 'Chronic Disease', 'Classification', 'Code', 'Cognitive', 'Communication', 'Coping Behavior', 'Coping Skills', 'Data', 'Data Set', 'Data Sources', 'Decision Making', 'Detection', 'Distress', 'Educational process of instructing', 'Effectiveness of Interventions', 'Electronics', 'Emotional', 'Emotions', 'Goals', 'Health', 'Health behavior', 'Heart Rate', 'Human', 'Hydrocortisone', 'Individual', 'Internet', 'Intervention', 'Intervention Studies', 'Left', 'Life', 'Linguistics', 'Link', 'Machine Learning', 'Malignant Neoplasms', 'Measurement', 'Measures', 'Mediator of activation protein', 'Medicine', 'Methodology', 'Methods', 'Monitor', 'Natural Language Processing', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Predictive Value', 'Problem Solving', 'Procedures', 'Process', 'Psychological adjustment', 'Psychology', 'Publishing', 'Quality of life', 'Recommendation', 'Recording of previous events', 'Recovery', 'Regulation', 'Relative (related person)', 'Research', 'Research Personnel', 'Resources', 'Rest', 'Sampling', 'Scientific Evaluation', 'Screening procedure', 'Self Care', 'Signal Transduction', 'Social Interaction', 'Social support', 'Specificity', 'Speech', 'Survey Methodology', 'Sympathetic Nervous System', 'Symptoms', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Trauma', 'Treatment/Psychosocial Effects', 'Work', 'anticancer research', 'base', 'behavior observation', 'computerized', 'computerized tools', 'coping', 'effective intervention', 'emotional experience', 'experience', 'improved', 'indexing', 'innovation', 'lexical', 'natural language', 'peer', 'programs', 'psychologic', 'psychosocial', 'public health relevance', 'showing emotion', 'skills', 'skills training', 'symptom management', 'tool']",NCI,LOMA LINDA UNIVERSITY,R21,2010,223207,0.041874437521812455
"Sensorimotor learning of facial expressions: A novel intervention for autism    DESCRIPTION (provided by applicant): Sensorimotor learning of facial expressions: A novel intervention for autism Project Summary This application addresses broad Challenge Area (04): Clinical Research, and specific Challenge Topic 04-MH-101 Autism: Addressing the challenge: Novel interventions. Children with Autism Spectrum Disorders (ASD) are impaired in their ability to produce and perceive dynamic facial expressions (Adolphs, Sears, and Piven, 2001). Advanced computer vision technologies can now be leveraged in the investigation of issues such as the facial expression recognition and production deficits common to children with autism spectrum disorder (ASD). Not only can these technologies assist in quantifying these deficits, but they can also be used as part of interventions aimed at reducing deficit severity. In this project, automated facial expression recognition will be employed for the development of training exercises for facial expression production in children with ASD. Previous research by Tanaka and Schultz developed the Let's Face It! intervention tool shown to effectively improve the face processing abilities of children with ASD. Let's Face It! focused on perception but not production. Bartlett and Movellan have developed the Computer Expression Recognition Toolbox (CERT), a computer vision system which measures 37 facial expression dimensions in real-time. This project brings together these two research groups in order to develop a computer assisted intervention system to enhance the social communication skills of children with Autism Spectrum Disorder (ASD). The computer vision tool enables novel interventions that focus on facial expression production. Intervention games will be developed to provide automated feedback on facial expressions produced by children with ASD in an engaging and cost- effective manner. In addition to improving production skills, practice with facial expression production may also improve perception skills by linking motor production with perception. The intervention exercises developed here will include closed-loop sensorimotor expression training in which subjects both see and produce facial expressions, and then view the effects of their facial expressions in the games. This project will also characterize facial expression production of children with ASD in a battery of assessment tasks, including automated facial expression measurement with CERT. Upon completion, this project will contribute to our understanding of facial expression production in children with ASD, provide a new intervention tool for facial expression production, and provide data on the efficacy of the intervention. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning. Sensorimotor learning of facial expressions: A novel Intervention for autism Project Narrative Automated facial expression recognition technology opens new possibilities for clinical research, assessment, and intervention systems. Integration of such technology in clinical research is both timely and crucial. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning.               Sensorimotor learning of facial expressions: A novel  Intervention for autism Project Narrative Automated facial expression recognition technology opens new possibilities for clinical research, assessment, and intervention systems. Integration of such technology in clinical research is both timely and crucial. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning.",Sensorimotor learning of facial expressions: A novel intervention for autism,7940926,RC1MH088633,"['Address', 'Area', 'Attention Deficit Disorder', 'Autistic Disorder', 'Child', 'Clinical Research', 'Computer Assisted', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Dimensions', 'Disease', 'Educational Intervention', 'Empathy', 'Exercise', 'Face', 'Face Processing', 'Facial Expression', 'Facial Expression Perception', 'Facial Expression Recognition', 'Feedback', 'Galvanic Skin Response', 'Goals', 'Grant', 'Heart Rate', 'Intervention', 'Intervention Studies', 'Investigation', 'Learning', 'Link', 'Measurement', 'Measures', 'Modeling', 'Motor', 'Perception', 'Physiological', 'Production', 'Psychophysiology', 'Research', 'Research Personnel', 'Schizophrenia', 'Severities', 'Social Development', 'Social Functioning', 'Statistical Models', 'Stimulus', 'System', 'Technology', 'Time', 'Training', 'Treatment Efficacy', 'autism spectrum disorder', 'base', 'computer design', 'cost', 'cost effective', 'face perception', 'improved', 'intervention program', 'mimicry', 'motor skill learning', 'novel', 'patient population', 'post intervention', 'programs', 'response', 'skills', 'social communication', 'statistics', 'tool']",NIMH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",RC1,2010,494454,-0.006735351945213767
"Biological and Information Processing Mechanisms Underlying Autism This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34. n/a",Biological and Information Processing Mechanisms Underlying Autism,7904203,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2010,1868838,0.02148808444688589
"A Study of the Computational Space of Facial Expressions of Emotion    DESCRIPTION (provided by applicant): Past research has been very successful in defining how facial expressions of emotion are produced, including which muscle movements create the most commonly seen expressions. These facial expressions of emotion are then interpreted by our visual system. Yet, little is known about how these facial expressions are recognized. The overarching goal of this proposal is to define the form and dimensions of the cognitive (computational) space used in this visual recognition. In particular, this proposal will study the following three hypotheses: Although facial expressions are produced by a complex set of muscle movements, expressions are generally easily identified at different spatial and time resolutions. However, it is not know what these limits are. Our first hypothesis (H1) is that recognition of facial expressions of emotion can be achieved at low resolutions and after short exposure times. In Aim 1, we define experiments to determine how many pixels and milliseconds (ms) are needed to successfully identify different emotions. The fact that expressions of emotion can be recognized quickly at low resolution indicates that simple features robust to image manipulation are employed. Our second hypothesis (H2) is that the recognition of facial expressions of emotion is partially accomplished by an analysis of configural features. Configural cues are known to play an important role in other face recognition tasks, but their role in the processing of expressions of emotion is not yet well understood. Aim 2 will identify a number of these configural cues. We will use real images of faces, manipulated versions of these face images, and schematic drawings. It is also known that shape features play a role in facial expressions (e.g., the curvature of the mouth in happiness). In Aim 3, we define a shape-based computational model. Our hypothesis (H3) is that the configural and shape features are defined as deviations from a mean (or norm) face as opposed to being described as a set of independent exemplars (Gnostic neurons). The importance of this computational space is not only to further justify the results of the previous aims, but to make new predictions that can be verified with additional experiments with human subjects.      PUBLIC HEALTH RELEVANCE: Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.           Project Narrative Understanding how facial expressions of emotion are processed by our cognitive system will be important for studies of abnormal face and emotion visual processing in schizophrenia, autism and Huntington's disease. Also, abused children are more acute at recognizing emotions, suggesting a higher degree of expertise to some image features. Identifying which features are used by the cognitive system will help develop protocols for reducing their unwanted effects. Understanding the limits in spatial and time resolution will also be important for studies of low vision (acuity), which are typical problems in several eye diseases and in the normal process of aging.",A Study of the Computational Space of Facial Expressions of Emotion,7946918,R01EY020834,"['Acute', 'Address', 'Affect', 'Aging-Related Process', 'Anger', 'Arts', 'Autistic Disorder', 'Behavior', 'Child Abuse', 'Classification', 'Code', 'Cognition', 'Cognitive', 'Complex', 'Computer Simulation', 'Computer Vision Systems', 'Conscious', 'Cues', 'Depressed mood', 'Dimensions', 'Duchenne muscular dystrophy', 'Emotions', 'Evolution', 'Eye diseases', 'Face', 'Face Processing', 'Facial Expression', 'Facial Muscles', 'Fright', 'Goals', 'Happiness', 'Human', 'Huntington Disease', 'Image', 'Individual', 'Lead', 'Movement', 'Muscle', 'Neurons', 'Oral cavity', 'Perception', 'Play', 'Positioning Attribute', 'Primates', 'Process', 'Protocols documentation', 'Research', 'Resolution', 'Role', 'Schizophrenia', 'Shapes', 'Social Interaction', 'System', 'Time', 'To specify', 'Visual', 'Visual impairment', 'Visual system structure', 'base', 'cognitive system', 'computer human interaction', 'computer studies', 'court', 'design', 'human subject', 'millisecond', 'psychologic', 'research study', 'showing emotion', 'visual process', 'visual processing']",NEI,OHIO STATE UNIVERSITY,R01,2010,285938,0.1791460017532975
"Sensorimotor learning of facial expressions: A novel intervention for autism    DESCRIPTION (provided by applicant): Sensorimotor learning of facial expressions: A novel intervention for autism Project Summary This application addresses broad Challenge Area (04): Clinical Research, and specific Challenge Topic 04-MH-101 Autism: Addressing the challenge: Novel interventions. Children with Autism Spectrum Disorders (ASD) are impaired in their ability to produce and perceive dynamic facial expressions (Adolphs, Sears, and Piven, 2001). Advanced computer vision technologies can now be leveraged in the investigation of issues such as the facial expression recognition and production deficits common to children with autism spectrum disorder (ASD). Not only can these technologies assist in quantifying these deficits, but they can also be used as part of interventions aimed at reducing deficit severity. In this project, automated facial expression recognition will be employed for the development of training exercises for facial expression production in children with ASD. Previous research by Tanaka and Schultz developed the Let's Face It! intervention tool shown to effectively improve the face processing abilities of children with ASD. Let's Face It! focused on perception but not production. Bartlett and Movellan have developed the Computer Expression Recognition Toolbox (CERT), a computer vision system which measures 37 facial expression dimensions in real-time. This project brings together these two research groups in order to develop a computer assisted intervention system to enhance the social communication skills of children with Autism Spectrum Disorder (ASD). The computer vision tool enables novel interventions that focus on facial expression production. Intervention games will be developed to provide automated feedback on facial expressions produced by children with ASD in an engaging and cost- effective manner. In addition to improving production skills, practice with facial expression production may also improve perception skills by linking motor production with perception. The intervention exercises developed here will include closed-loop sensorimotor expression training in which subjects both see and produce facial expressions, and then view the effects of their facial expressions in the games. This project will also characterize facial expression production of children with ASD in a battery of assessment tasks, including automated facial expression measurement with CERT. Upon completion, this project will contribute to our understanding of facial expression production in children with ASD, provide a new intervention tool for facial expression production, and provide data on the efficacy of the intervention. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning. Sensorimotor learning of facial expressions: A novel Intervention for autism Project Narrative Automated facial expression recognition technology opens new possibilities for clinical research, assessment, and intervention systems. Integration of such technology in clinical research is both timely and crucial. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning.               Sensorimotor learning of facial expressions: A novel  Intervention for autism Project Narrative Automated facial expression recognition technology opens new possibilities for clinical research, assessment, and intervention systems. Integration of such technology in clinical research is both timely and crucial. The intervention system proposed here could have tremendous impact on social development in any patient population with atypical facial expression production, including ASD, and attention deficit disorder. The project will also contribute to the understanding of the development and learning of motor skills essential to social functioning.",Sensorimotor learning of facial expressions: A novel intervention for autism,7829637,RC1MH088633,"['Address', 'Area', 'Attention Deficit Disorder', 'Autistic Disorder', 'Child', 'Clinical Research', 'Computer Assisted', 'Computer Vision Systems', 'Computers', 'Data', 'Development', 'Dimensions', 'Disease', 'Educational Intervention', 'Empathy', 'Exercise', 'Face', 'Face Processing', 'Facial Expression', 'Facial Expression Perception', 'Facial Expression Recognition', 'Feedback', 'Galvanic Skin Response', 'Goals', 'Grant', 'Heart Rate', 'Intervention', 'Intervention Studies', 'Investigation', 'Learning', 'Link', 'Measurement', 'Measures', 'Modeling', 'Motor', 'Perception', 'Physiological', 'Production', 'Psychophysiology', 'Research', 'Research Personnel', 'Schizophrenia', 'Severities', 'Social Development', 'Social Functioning', 'Statistical Models', 'Stimulus', 'System', 'Technology', 'Time', 'Training', 'Treatment Efficacy', 'autism spectrum disorder', 'base', 'computer design', 'cost', 'cost effective', 'face perception', 'improved', 'intervention program', 'mimicry', 'motor skill learning', 'novel', 'patient population', 'post intervention', 'programs', 'response', 'skills', 'social communication', 'statistics', 'tool']",NIMH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",RC1,2009,497336,-0.006735351945213767
"Biological and Information Processing Mechanisms Underlying Autism    DESCRIPTION (provided by applicant): This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34.           n/a",Biological and Information Processing Mechanisms Underlying Autism,7669358,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2009,1892417,0.0222167115564145
"Biological and Information Processing Mechanisms Underlying Autism    DESCRIPTION (provided by applicant): This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34.           n/a",Biological and Information Processing Mechanisms Underlying Autism,7933194,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2009,400722,0.0222167115564145
"Modeling the Nonmanuals in American Sign Language    DESCRIPTION (provided by applicant): This revised proposal describes a project to systematically investigate the facial components, combinations of components, and interactions of components that constitute facial expressions (nonmanual markers) in the grammar of American Sign Language (ASL). Some of these components have already been shown to differ in significant ways from those used by the general hearing population. They may carry semantic, prosodic, pragmatic, and syntactic information that may not be provided by the manual signing itself. We will compile an inventory of facial articulations, construct a database of video images of these in isolation and in context, and use these data and innovative computational tools to construct a model of facial behavior in ASL. To successfully accomplish this, we propose an innovative integrated linguistic and computational approach to the study of nonmanuals. Our goal in this project is to construct an initial phonological model of ASL nonmanuals. We have targeted a relevant set of facial features and have identified 4 experiments to obtain appropriate information on each of them. A necessary step in preparation for these experiments is to develop computer vision and pattern recognition algorithms that automatically extract these facial features from a large quantity of videos. These algorithms will be capable of processing data more accurately and efficiently than can be done by hand. Finally, by comparing these results with those obtained from native ASL signers in a series of perceptual studies, we can determine what further modifications are still needed. The study of facial expressions in ASL has very practical applications to several areas affecting the lives of Deaf individuals. The absence of clear information on the facial components makes teaching them to individuals trying to learn ASL, such as parents, deaf children, future teachers and interpreters, a pedagogical nightmare. Another important practical application is the development of systems that automatically recognize ASL. Such a system is not feasible without the ability to handle ASL nonmanuals, which carry grammatical information.         n/a",Modeling the Nonmanuals in American Sign Language,7316099,R01DC005241,"['Affect', 'Algorithms', 'Archives', 'Area', 'Articulators', 'Behavior', 'Child', 'Code', 'Communication', 'Computer Systems Development', 'Computer Vision Systems', 'Computers', 'Data', 'Databases', 'Development', 'Education', 'Educational process of instructing', 'Emotions', 'Equipment and supply inventories', 'Face', 'Facial Expression', 'Future', 'Goals', 'Guidelines', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Image', 'Individual', 'Investigation', 'Joints', 'Learning', 'Life', 'Lighting', 'Linguistics', 'Manuals', 'Modeling', 'Modification', 'Nightmare', 'Parents', 'Pattern Recognition', 'Population', 'Preparation', 'Process', 'Regulation', 'Research Design', 'Semantics', 'Series', 'Sign Language', 'Social Interaction', 'Speech', 'Stimulus', 'Structure', 'System', 'Testing', 'Translations', 'Trees', 'Work', 'computerized data processing', 'computerized tools', 'direct application', 'innovation', 'native American sign language', 'phonology', 'practical application', 'research study', 'syntax', 'teacher']",NIDCD,PURDUE UNIVERSITY,R01,2008,476784,0.09004333373404944
"Biological and Information Processing Mechanisms Underlying Autism    DESCRIPTION (provided by applicant): This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34.           n/a",Biological and Information Processing Mechanisms Underlying Autism,7479861,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2008,1788188,0.0222167115564145
"Modeling the Nonmanuals in American Sign Language    DESCRIPTION (provided by applicant): This revised proposal describes a project to systematically investigate the facial components, combinations of components, and interactions of components that constitute facial expressions (nonmanual markers) in the grammar of American Sign Language (ASL). Some of these components have already been shown to differ in significant ways from those used by the general hearing population. They may carry semantic, prosodic, pragmatic, and syntactic information that may not be provided by the manual signing itself. We will compile an inventory of facial articulations, construct a database of video images of these in isolation and in context, and use these data and innovative computational tools to construct a model of facial behavior in ASL. To successfully accomplish this, we propose an innovative integrated linguistic and computational approach to the study of nonmanuals. Our goal in this project is to construct an initial phonological model of ASL nonmanuals. We have targeted a relevant set of facial features and have identified 4 experiments to obtain appropriate information on each of them. A necessary step in preparation for these experiments is to develop computer vision and pattern recognition algorithms that automatically extract these facial features from a large quantity of videos. These algorithms will be capable of processing data more accurately and efficiently than can be done by hand. Finally, by comparing these results with those obtained from native ASL signers in a series of perceptual studies, we can determine what further modifications are still needed. The study of facial expressions in ASL has very practical applications to several areas affecting the lives of Deaf individuals. The absence of clear information on the facial components makes teaching them to individuals trying to learn ASL, such as parents, deaf children, future teachers and interpreters, a pedagogical nightmare. Another important practical application is the development of systems that automatically recognize ASL. Such a system is not feasible without the ability to handle ASL nonmanuals, which carry grammatical information.         n/a",Modeling the Nonmanuals in American Sign Language,7151198,R01DC005241,"['Affect', 'Algorithms', 'Archives', 'Area', 'Articulators', 'Behavior', 'Child', 'Code', 'Communication', 'Computer Systems Development', 'Computer Vision Systems', 'Computers', 'Data', 'Databases', 'Development', 'Education', 'Educational process of instructing', 'Emotions', 'Equipment and supply inventories', 'Face', 'Facial Expression', 'Future', 'Goals', 'Guidelines', 'Hand', 'Hearing', 'Hearing Impaired Persons', 'Image', 'Individual', 'Investigation', 'Joints', 'Learning', 'Life', 'Lighting', 'Linguistics', 'Manuals', 'Modeling', 'Modification', 'Nightmare', 'Parents', 'Pattern Recognition', 'Population', 'Preparation', 'Process', 'Regulation', 'Research Design', 'Semantics', 'Series', 'Sign Language', 'Social Interaction', 'Speech', 'Stimulus', 'Structure', 'System', 'Testing', 'Translations', 'Trees', 'Work', 'computerized data processing', 'computerized tools', 'direct application', 'innovation', 'native American sign language', 'phonology', 'practical application', 'research study', 'syntax', 'teacher']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,2007,471644,0.09004333373404944
"Biological and Information Processing Mechanisms Underlying Autism    DESCRIPTION (provided by applicant): This center focuses on elucidating fundamental information processing and neurobiological mechanisms causing autism. It incorporates 5 themes: 1) autism as a disorder of complex information processing abilities WITH intact or enhanced basic abilities; 2) autism as a disturbance in cerebral connectivity with under connectivity of long-distance connections and over-connectivity of compensatory local connections; 3) autism as a developmental disorder whose brain-behavioral manifestations change with each stage of brain maturation from childhood to adulthood; 4) that the pathognomonic social impairments are best understood in terms of underlying information processing mechanisms; and 5) that emotional and affective impairments are a major component of the syndrome and significant contributor to impaired function and serious behavior issues. These goals will be addressed using behavioral, cognitive, fMRI, DTI, DTT, and neuropathologic studies of: infant siblings, first-diagnosed toddlers with autism, and groups of children, adolescents, and adults with and without autism. Project I: Development of Categorization & Facial Knowledge in Low & High Functioning Autism; Project II: Disturbances of Affective Contact: Development of Brain Mechanisms for Emotion Processing; Project III: Systems Connectivity & Brain Activation: Imaging Studies of Language & Perception; Project IV: Diffusion Tensor MRI & Histopathology of Brain Microstructure & Fiber Pathways. Project I focuses on understanding the earliest manifestations of autism and the development of earlier diagnostic tools; it also focuses on information processing mechanisms underlying social and cognitive symptoms. Project II focuses on elucidating emotion processing mechanisms and the maturational disturbances from childhood through adulthood; these studies will clarify how individuals with autism experience, understand and regulate emotion, and will also examine their self-awareness of emotion. Genetic modifiers of emotionality will also be determined. Project III focuses on understanding disturbances in functional brain connectivity that underlie the impaired processing of information and in turn the cognitive and behavioral impairments in autism. The project also includes innovative machine-learning studies of how the brain identifies and categorizes words and computational modeling of cortical function. Project IV looks at structural connectivity of specific white matter pathways and the histopathology of intra-cortical white matter projections. This project will help reconcile the functional under-connectivity and early structural overgrowth in autism. The identification of fundamental information processing mechanisms underlying the behavior of autism will lead to the development of more specific and effective interventions; similarly identification of connectivity alterations can lead to targeted cognitive interventions that increase neural connectivity. This research addresses Autism Research Matrix goals # 1,2,3,6,16,17,19,22,23,26,31,and 34.           n/a",Biological and Information Processing Mechanisms Underlying Autism,7277432,P50HD055748,[' '],NICHD,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,P50,2007,1928731,0.0222167115564145
"Modeling the Nonmanuals in American Sign Language    DESCRIPTION (provided by applicant): This revised proposal describes a project to systematically investigate the facial components, combinations of components, and interactions of components that constitute facial expressions (nonmanual markers) in the grammar of American Sign Language (ASL). Some of these components have already been shown to differ in significant ways from those used by the general hearing population. They may carry semantic, prosodic, pragmatic, and syntactic information that may not be provided by the manual signing itself. We will compile an inventory of facial articulations, construct a database of video images of these in isolation and in context, and use these data and innovative computational tools to construct a model of facial behavior in ASL. To successfully accomplish this, we propose an innovative integrated linguistic and computational approach to the study of nonmanuals. Our goal in this project is to construct an initial phonological model of ASL nonmanuals. We have targeted a relevant set of facial features and have identified 4 experiments to obtain appropriate information on each of them. A necessary step in preparation for these experiments is to develop computer vision and pattern recognition algorithms that automatically extract these facial features from a large quantity of videos. These algorithms will be capable of processing data more accurately and efficiently than can be done by hand. Finally, by comparing these results with those obtained from native ASL signers in a series of perceptual studies, we can determine what further modifications are still needed. The study of facial expressions in ASL has very practical applications to several areas affecting the lives of Deaf individuals. The absence of clear information on the facial components makes teaching them to individuals trying to learn ASL, such as parents, deaf children, future teachers and interpreters, a pedagogical nightmare. Another important practical application is the development of systems that automatically recognize ASL. Such a system is not feasible without the ability to handle ASL nonmanuals, which carry grammatical information.         n/a",Modeling the Nonmanuals in American Sign Language,6985348,R01DC005241,"['artificial intelligence', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'communication behavior', 'communication disorder aid', 'computational biology', 'computer data analysis', 'computer simulation', 'computer system design /evaluation', 'face expression', 'human subject', 'information systems', 'paralinguistic behavior', 'phonology', 'semantics', 'sign language', 'video recording system', 'videotape /videodisc']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,2006,475410,0.09004333373404944
"Facial Expression Analysis by Image Processing    DESCRIPTION (provided by applicant): Facial expression provides cues about emotional response, regulates interpersonal behavior, and communicates aspects of psychopathology. Human-observer based methods for measuring facial expression are labor intensive, qualitative, and difficult to standardize. Our interdisciplinary team of computer and behavioral scientists has developed the CMU/Pitt Automated Facial Image Analysis (AFA) system that is capable of automatically recognizing facial action units and analyzing their timing in facial behavior. The quantitative measurement achieved by AFA represents a major advance over manual and subjective measurement without requiring the use of invasive sensors. We envision to use AFA's reliable, valid, and efficient measurement of emotion expression and related nonverbal behavior for assessment of symptom severity in depression. Current methods of clinical assessment of depression depend almost entirely on verbal report (clinical interview and/or questionnaire). They lack systematic and efficient ways of incorporating behavioral observations that are .strong indicators of depressive symptoms, especially those related to the timing of dyadic interaction between clinician and patient, much of which may occur outside the awareness of either individual. AFA is capable of extracting both the type and timing of nonverbal indicators of depression. Our hypothesis is that quantitative measures of the configuration and timing of facial expression, head motion, and gaze obtainable by AFA will improve clinical assessment of symptom severity and evaluation of treatment outcomes when combined with information from interviews and self-report questionnaires. We propose to test this hypothesis in 40 participants participating in a treatment intervention study for major depression. Interview, questionnaire, and video data will be collected at regular intervals over the course of treatment. To measure social dynamics, both patient and interviewer will be video recorded and processed using AFA. Longitudinal multilevel modeling will be used to test study hypotheses. We will improve further algorithms and capabilities of AFA to meet evaluation goals and prepare AFA for use by the scientific and clinical community.         n/a",Facial Expression Analysis by Image Processing,7049026,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'clinical research', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2006,480876,0.06376606503024217
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6881434,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2005,358843,0.1458380363505368
"Modeling the Nonmanuals in American Sign Language    DESCRIPTION (provided by applicant): This revised proposal describes a project to systematically investigate the facial components, combinations of components, and interactions of components that constitute facial expressions (nonmanual markers) in the grammar of American Sign Language (ASL). Some of these components have already been shown to differ in significant ways from those used by the general hearing population. They may carry semantic, prosodic, pragmatic, and syntactic information that may not be provided by the manual signing itself. We will compile an inventory of facial articulations, construct a database of video images of these in isolation and in context, and use these data and innovative computational tools to construct a model of facial behavior in ASL. To successfully accomplish this, we propose an innovative integrated linguistic and computational approach to the study of nonmanuals. Our goal in this project is to construct an initial phonological model of ASL nonmanuals. We have targeted a relevant set of facial features and have identified 4 experiments to obtain appropriate information on each of them. A necessary step in preparation for these experiments is to develop computer vision and pattern recognition algorithms that automatically extract these facial features from a large quantity of videos. These algorithms will be capable of processing data more accurately and efficiently than can be done by hand. Finally, by comparing these results with those obtained from native ASL signers in a series of perceptual studies, we can determine what further modifications are still needed. The study of facial expressions in ASL has very practical applications to several areas affecting the lives of Deaf individuals. The absence of clear information on the facial components makes teaching them to individuals trying to learn ASL, such as parents, deaf children, future teachers and interpreters, a pedagogical nightmare. Another important practical application is the development of systems that automatically recognize ASL. Such a system is not feasible without the ability to handle ASL nonmanuals, which carry grammatical information.         n/a",Modeling the Nonmanuals in American Sign Language,6841137,R01DC005241,"['artificial intelligence', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'communication behavior', 'communication disorder aid', 'computational biology', 'computer data analysis', 'computer simulation', 'computer system design /evaluation', 'face expression', 'human subject', 'information systems', 'paralinguistic behavior', 'phonology', 'semantics', 'sign language', 'video recording system', 'videotape /videodisc']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,2005,473795,0.09004333373404944
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6907787,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,42260,0.1458380363505368
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6736314,R01MH051435,"['artificial intelligence', 'behavioral /social science research tag', 'bioimaging /biomedical imaging', 'computer program /software', 'computer system design /evaluation', 'digital imaging', 'emotions', 'face expression', 'human subject', 'image processing', 'interpersonal relations', 'statistics /biometry', 'videotape /videodisc', 'visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2004,305282,0.1458380363505368
"Modeling the Nonmanuals in American Sign Language    DESCRIPTION (provided by applicant): This revised proposal describes a project to systematically investigate the facial components, combinations of components, and interactions of components that constitute facial expressions (nonmanual markers) in the grammar of American Sign Language (ASL). Some of these components have already been shown to differ in significant ways from those used by the general hearing population. They may carry semantic, prosodic, pragmatic, and syntactic information that may not be provided by the manual signing itself. We will compile an inventory of facial articulations, construct a database of video images of these in isolation and in context, and use these data and innovative computational tools to construct a model of facial behavior in ASL. To successfully accomplish this, we propose an innovative integrated linguistic and computational approach to the study of nonmanuals. Our goal in this project is to construct an initial phonological model of ASL nonmanuals. We have targeted a relevant set of facial features and have identified 4 experiments to obtain appropriate information on each of them. A necessary step in preparation for these experiments is to develop computer vision and pattern recognition algorithms that automatically extract these facial features from a large quantity of videos. These algorithms will be capable of processing data more accurately and efficiently than can be done by hand. Finally, by comparing these results with those obtained from native ASL signers in a series of perceptual studies, we can determine what further modifications are still needed. The study of facial expressions in ASL has very practical applications to several areas affecting the lives of Deaf individuals. The absence of clear information on the facial components makes teaching them to individuals trying to learn ASL, such as parents, deaf children, future teachers and interpreters, a pedagogical nightmare. Another important practical application is the development of systems that automatically recognize ASL. Such a system is not feasible without the ability to handle ASL nonmanuals, which carry grammatical information.         n/a",Modeling the Nonmanuals in American Sign Language,6733239,R01DC005241,"['artificial intelligence', 'behavioral /social science research tag', 'biomedical automation', 'clinical research', 'communication behavior', 'communication disorder aid', 'computational biology', 'computer data analysis', 'computer simulation', 'computer system design /evaluation', 'face expression', 'human subject', 'information systems', 'paralinguistic behavior', 'phonology', 'semantics', 'sign language', 'video recording system', 'videotape /videodisc']",NIDCD,PURDUE UNIVERSITY WEST LAFAYETTE,R01,2004,499766,0.09004333373404944
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6639029,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2003,297999,0.1458380363505368
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6538699,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2002,290602,0.1458380363505368
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING   DESCRIPTION (adapted from investigator's abstract): Facial expression provides       sensitive cues about emotional response and plays a critical role in the             regulation of interpersonal behavior. Human-observer based methods for               measuring facial expression are labor intensive, qualitative, and difficult to       standardize across laboratories and over time. To make feasible more rigorous,       quantitative measurement of facial expression in diverse applications, the           investigators formed an interdisciplinary research group that covers expertise       in facial expression analysis and computer vision. In August 1995, the research      group received initial funding for Facial Expression Analysis by Computer Image      Processing (NIMH #IRO1MH51435). With NIMH support, they created a large              representative database for method development and developed and validated a         face analysis system that tracks gaze and facial features in digitized image         sequences of infant, child, and adult subjects. In near frontal views, the Face      Analysis System has achieved concurrent validity with manual FACS coding for 16      of 44 FAGS action units and more than 30 combinations in which they occur. For       the competing renewal, the investigators will (1) Increase system robustness to      head orientation and head motion (2) Increase the number of recognizable action      units to include all 30 of those that have a specific anatomic basis, and (3)        Conduct a systematic, large-scale, comprehensive test of the developed Face          Analysis System by using multiple databases. These databases encompass subjects      of various ages and backgrounds and varying types of emotion induction, head         orientation, head motion, size of the face in pixels, and presence of speech.        The databases were initially collected to answer substantive questions about         emotion processes; they represent the types of data that the Face Analysis           System will encounter in psychology research and clinical applications. n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6287970,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,334747,0.1458380363505368
"COMPUTER VISION ANALYSIS OF DYNAMIC FACIAL BEHAVIOR DESCRIPTION (Applicant's abstract): Facial expressions provide an important behavioral measure for the study of emotion, cognitive processes, and social interaction, and contain diagnostic information for neurological disorders and depression. Facial expression measurement from video is less intrusive than EEG, EMO, ANS or brain imaging measurements as an indicator of emotional activity. The measurement is presently performed by human experts. The goal of this research is to develop an automatic system for recognition, measurement, and coding of facial expressions from video using computer vision technology. This project will utilize probabilistic models of dynamical systems to mod& the facial behavior underlying the observed image sequences. These techniques include hidden Markov models, and a new stochastic modeling technique developed by Movellan and colleagues called diffusion networks [38]. Diffusion networks offer the advantage over traditional dynamical models of allowing continuous time dynamics and continuous states. An automated system would make facial expression measurement more widely accessible as a research tool in behavioral science and investigations of the neural substrates of emotion.  n/a",COMPUTER VISION ANALYSIS OF DYNAMIC FACIAL BEHAVIOR,6391721,F32MH012417,"['behavior test', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' clinical research', ' cognition', ' computer assisted diagnosis', ' depression', ' emotions', ' face expression', ' human subject', ' image processing', ' mathematical model', ' social integration']",NIMH,UNIVERSITY OF CALIFORNIA SAN DIEGO,F32,2001,41996,0.13010145326736108
"COMPUTER VISION ANALYSIS OF DYNAMIC FACIAL BEHAVIOR DESCRIPTION (Applicant's abstract): Facial expressions provide an important behavioral measure for the study of emotion, cognitive processes, and social interaction, and contain diagnostic information for neurological disorders and depression. Facial expression measurement from video is less intrusive than EEG, EMO, ANS or brain imaging measurements as an indicator of emotional activity. The measurement is presently performed by human experts. The goal of this research is to develop an automatic system for recognition, measurement, and coding of facial expressions from video using computer vision technology. This project will utilize probabilistic models of dynamical systems to mod& the facial behavior underlying the observed image sequences. These techniques include hidden Markov models, and a new stochastic modeling technique developed by Movellan and colleagues called diffusion networks [38]. Diffusion networks offer the advantage over traditional dynamical models of allowing continuous time dynamics and continuous states. An automated system would make facial expression measurement more widely accessible as a research tool in behavioral science and investigations of the neural substrates of emotion.  n/a",COMPUTER VISION ANALYSIS OF DYNAMIC FACIAL BEHAVIOR,6185479,F32MH012417,"['behavior test', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' clinical research', ' cognition', ' computer assisted diagnosis', ' depression', ' emotions', ' face expression', ' human subject', ' image processing', ' mathematical model', ' social integration']",NIMH,UNIVERSITY OF CALIFORNIA SAN DIEGO,F32,2000,37516,0.13010145326736108
"FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING DESCRIPTION (Applicant's Abstract):  Facial expression communicates              information about emotional response and plays a critical role in the            regulation of interpersonal behavior.  Current human-observer based methods      for measuring facial expression are labor intensive, qualitative, and            difficult to standardize across laboratories and over time.  To make             feasible more rigorous, quantitative measurement of facial expression in         diverse applications, we formed an interdisciplinary research group which        covers expertise in facial expression analysis and image processing.  In the     funding period, we developed and demonstrated the first version of an            automated system for measuring facial expression in digitized images.  The       system can discriminate nine combinations of FACS action units in the upper      and lower face, quantity the timing and topography of action unit intensity      in the brow region; and geometrically normalize image sequences within a         range of plus or minus 20 degrees of out of-plane.                                                                                                                In the competing renewal, we will increase the number of action unit             combinations that are recognized, implement convergent methods of                quantifying action unit intensity, increase the generalizability of action       unit estimation to a wider range of image orientations, test facial image        processing (FIP) in image sequences from directed facial action tasks and        laboratory studies of emotion regulation, and facilitate the integration of      FIP into existing data management and statistical analysis software for use      by behavioral science researchers and clinicians.  With these goals              completed, FIP will eliminate the need for human observers in coding facial      expression, promote standardize measurement, make possible the collection        and processing of larger, more representative data sets, and open new areas      of investigation and clinical application.                                        n/a",FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING,6185949,R01MH051435,"['artificial intelligence', ' behavioral /social science research tag', ' bioimaging /biomedical imaging', ' computer program /software', ' computer system design /evaluation', ' digital imaging', ' emotions', ' face expression', ' human subject', ' image processing', ' interpersonal relations', ' statistics /biometry', ' videotape /videodisc', ' visual tracking']",NIMH,MELLON PITTS CORPORATION (MPC CORP),R01,2000,233574,0.09459855901003604
"Creating an artificial intelligence therapy-to-data feedback loop for child developmental healthcare Project Summary There is a sharp and increasing imbalance between the number of children with autism in need of care and the availability of specialists certified to treat the disorder in its multi-faceted manifestations. The autism community faces a dual clinical challenge: how to direct scarce specialist resources to service the diverse array of phenomes and how to monitor and validate best practices in treatment. Clinicians must now look to solutions that scale in a decentralized fashion, placing data capture, remote monitoring, and therapy increasingly into the hands of families. Using artificial intelligence (AI) and large amounts of labeled human emotion computer vision data, we have developed a solution for automatic facial expression recognition that runs on Google Glasses and Android smartphones to deliver real time social cues to individuals with autism in the child’s natural environment. We hypothesize that this informatic system can provide real-time therapy in a way that scales to meet the demand of the growing population of autism families, including underserved minorities, while growing data that can be used to measure progress over time and in the development of novel AI. Our first aim will focus on the development of a deep learning model that enables dynamic emotion recognition in the real world, and on domain adaptation procedures that enable minimal manual labeling to personalize the model for optimal accuracy on the individuals with whom the child will interact most regularly at home. Our second aim will focus on the human computer interface, namely the design of the user experience with the Android application that controls the sessions run on the Google Glass wearable. We will work our clinical colleagues and with groups of autism families to develop and enhance a set of games and activity modes that create social engagements ideal for emotion therapy, including an emotion capture and a charades game. The third aim will test our central hypothesis that the Glass system can create a therapy-to-data feedback loop that delivers clinical care while growing data for measurement and model development. We will work with up to 200 children ages 4-8 who have recent autism diagnoses and do not have access to standard behavioral therapy. We will build a community of autism families through crowdsourcing techniques, befitting the mobile paradigm embodied by our work, and through close collaboration with behavioral therapy providers, the autism outreach organization Autism Speaks, and the digital healthcare company, Cognoa. The families will work with us on design and refinement of our “Superpower Glass” system for fit, engagement, and function of use for both therapy and data capture. Importantly, we will send units home with families to use the device for at least 3 twenty-minute sessions per week for a minimum of 6 weeks. This remote period will generate a massive database to quantify overall social learning, emotion comprehension, eye contact, and sustained social acuity. In all, our work program will show that mobile wearable AI can bring the social learning process out of the clinic and into the real world for faster and more adaptive intervention. Project Narrative There is a sharp and growing imbalance between the number of clinical care providers available and the number of children with an autism diagnosis that leave most children without therapy until after critical periods in development have passed. We intend to address this problem through creation of a machine learning-enabled wearable that brings effective care to the home and empowers both parents, patients, and clinicians with mobile solutions that personalize care delivery to dramatically improve children’s outcomes. The Superpower Glass system, which delivers social cues to children during real-time interactions and provides several engagement modes for families, is a promising solution that enables greater access to care for families across the US, and potentially, across the globe.!",Creating an artificial intelligence therapy-to-data feedback loop for child developmental healthcare,9936445,R01LM013083,"['Address', 'Affect', 'Age', 'Android', 'Artificial Intelligence', 'Award', 'Awareness', 'Behavior Therapy', 'Car Phone', 'Caregivers', 'Caring', 'Cellular Phone', 'Child', 'Child Support', 'Classification', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Comprehension', 'Computer Vision Systems', 'Computer software', 'Control Groups', 'Cues', 'Data', 'Databases', 'Decentralization', 'Development', 'Devices', 'Diagnosis', 'Disease', 'Emotions', 'Environment', 'Eye', 'Face', 'Facial Expression', 'Facial Expression Recognition', 'Family', 'Feedback', 'Future', 'Glass', 'Goals', 'Hand', 'Health Services Accessibility', 'Healthcare', 'Home environment', 'Human', 'Individual', 'Informatics', 'Intervention', 'Label', 'Learning', 'Machine Learning', 'Manuals', 'Measurement', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Outcome', 'Parents', 'Participant', 'Patients', 'Persons', 'Population', 'Procedures', 'Process', 'Provider', 'Resources', 'Running', 'Secure', 'Self-Direction', 'Services', 'Severities', 'Social Interaction', 'Socialization', 'Specialist', 'Stream', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic', 'Time', 'Training', 'User-Computer Interface', 'Variant', 'Waiting Lists', 'Work', 'adaptive intervention', 'applied behavior analysis', 'autism spectrum disorder', 'autistic children', 'base', 'care delivery', 'care providers', 'clinical care', 'combat', 'critical period', 'crowdsourcing', 'deep learning', 'design', 'digital', 'experience', 'improved', 'mobile application', 'mobile computing', 'model development', 'novel', 'outreach', 'personalized care', 'phenome', 'practical application', 'programs', 'prototype', 'skills', 'smartphone Application', 'social', 'social engagement', 'social learning', 'standard of care', 'tool', 'underserved minority', 'user-friendly']",NLM,STANFORD UNIVERSITY,R01,2020,664314,0.019868676670375198
"Thought disorder and social cognition in clinical risk states for schizophrenia Project Summary  In an effort to intervene before psychosis onset and prevent morbidity, a major recent focus in schizophrenia research has been the identification of young people during a putative prodromal period, so as to develop safe and effective interventions to modify disease course. Over the past decade, studies at Columbia and elsewhere have evaluated clinical high-risk (CHR) individuals across a wide range of cognitive processes to try to identify core deficits of schizophrenia evident before psychosis onset. Subthreshold thought disorder and impaired emotion recognition have emerged as profound deficits that predate, rather than follow, psychosis onset and thus may be indicators of schizophrenia liability, consistent with studies in other risk cohorts, including genetic high risk. Further, subthreshold thought disorder and emotion recognition deficit are significantly correlated, suggesting shared neural substrates in temporoparietal regions.  This study aims to identify the neural mechanisms that underlie subthreshold thought disorder and emotion recognition deficit in 125 CHR individuals followed prospectively for psychosis outcome. CHR cohorts are enriched with early cases of schizophrenia, as 20-25% develop schizophrenia and related psychotic disorders within 1-2 years. CHR cohorts may be optimal for studying core characteristics of illness as they otherwise have low-level symptoms, less illness chronicity and minimum exposure to antipsychotics. 25 individuals with schizophrenia and 50 healthy volunteers are included for comparison.  Subthreshold thought disorder and emotion recognition deficits will be studied across behavioral, physiological and circuit levels. For thought disorder, we will use automated speech analysis approaches developed in collaboration with IBM to identify constituent impairments in semantics and syntax, and a listening task that elicits reliable activation in language circuits. Our automated machine-learning approach to speech analysis, informed by artificial intelligence, derives the semantic meaning of words and phrases by drawing on a large corpus of text, similar to how humans assign meaning to what they read or hear. Emotion recognition will be measured using standard tasks, naturalistic tasks with dynamic face stimuli and parametric face morph tasks that discriminate between perception and appraisal; task-related BOLD activity will be used to identify relevant circuits. Associations with basic sensory impairment will be tested, including novel auditory mismatch negativity paradigms. Resting state functional connectivity (RSFC) methods will be used for circuit-level analysis of language production and emotion recognition across stages of illness, to determine unique and shared substrates of these constructs in early schizophrenia. If successful, this proposal will identify neural targets for remediation of cognitive impairments. Project Narrative Schizophrenia is an important public health concern. Core characteristics of schizophrenia that predate psychosis onset include subtle thought disorder and profound deficits in recognizing emotions in others' faces and voices. This proposal will evaluate mechanisms underlying these language and social cognitive deficits through the use of neuroimaging, electrophysiology and automated speech analysis, in order to develop new preventive strategies for schizophrenia.  ",Thought disorder and social cognition in clinical risk states for schizophrenia,9920230,R01MH107558,"['Adolescence', 'Age', 'Antipsychotic Agents', 'Artificial Intelligence', 'Auditory', 'Behavior', 'Behavioral', 'Biological Assay', 'Brain', 'Characteristics', 'Chronic', 'Clinical', 'Cognitive', 'Cognitive deficits', 'Cognitive remediation', 'Collaborations', 'Data', 'Deltastab', 'Development', 'Disease', 'Electroencephalography', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event-Related Potentials', 'Exposure to', 'Face', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Genetic', 'Goals', 'Hearing', 'Human', 'Impaired cognition', 'Impairment', 'Individual', 'Inferior', 'Language', 'Language Disorders', 'Link', 'Logistic Regressions', 'Logistics', 'Machine Learning', 'Measures', 'Mentored Patient-Oriented Research Career Development Award', 'Methods', 'Morbidity - disease rate', 'Natural Language Processing', 'Neurocognitive', 'Neurodevelopmental Disorder', 'Neuronal Dysfunction', 'Outcome', 'Parietal', 'Pattern', 'Perception', 'Phase', 'Phenotype', 'Physiological', 'Predictive Value', 'Prevalence', 'Prevention strategy', 'Preventive Intervention', 'Production', 'Prospective Studies', 'Psychotic Disorders', 'Public Health', 'Research', 'Rest', 'Risk', 'Schizophrenia', 'Semantics', 'Sensory', 'Severities', 'Social Functioning', 'Speech', 'Stimulus', 'Symptoms', 'Testing', 'Text', 'Visual', 'Voice', 'Withdrawal', 'Work', 'auditory processing', 'automated analysis', 'career', 'clinical risk', 'cognitive process', 'cohort', 'connectome', 'deviant', 'effective intervention', 'healthy volunteer', 'high risk', 'indexing', 'insight', 'language impairment', 'language processing', 'natural language', 'neural patterning', 'neuroimaging', 'neuromechanism', 'novel', 'phrases', 'predictive modeling', 'prevent', 'prognostic tool', 'prognostic value', 'prospective', 'relating to nervous system', 'remediation', 'social', 'social cognition', 'syntax', 'visual processing', 'young adult']",NIMH,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2020,676651,0.061599684698335426
"Emotion Regulation in Distress Disorders: Elucidating the Role of Cognitive Processes and Person-Situation Fit in the Laboratory and Daily Life ABSTRACT: Emotion regulation (ER) is a key transdiagnostic process and treatment target, with particular relevance to distress disorders (depression, generalized anxiety disorder). Distress disorders, which are characterized by heightened negative emotions, are prevalent conditions with considerable public health burden and comparatively lower response to treatment. The efficacy of interventions for distress disorders may be improved by investigating components of effective ER, both as instructed in the lab (ER capacity) and as measured naturalistically in daily life (ER tendency). Effective ER is based on key components such as accurate perceptions of one's emotions (i.e., emotional awareness) as well sensitivity to the environmental context (i.e., contextual sensitivity), which facilitate the selection and implementation of the appropriate ER strategy. For individuals with distress disorders, elevated levels of perseverative negative thinking (PNT) interferes with these processes, contributing to ER deficits and increased symptoms. This contextualized and integrative model of ER in distress disorders has not been tested, and very little is known about the predictors or outcomes of effective ER tendency—a critical gap given that ER capacity is irrelevant if ER is not employed skillfully in daily life. Furthermore, examinations of ER tendency can show how individuals, considering their particular characteristics and abilities, can optimally match particular ER strategies to the specific situations they encounter. The goal of this application is to evaluate components of ineffective ER— both in the lab and in daily life— that influence the severity and course of distress disorders and functioning. Consistent with NIMH strategic objectives and the RDoC framework, this project incorporates multimodal assessment, dimensional symptom measurement, and machine learning approaches. The proposed study will be comprised of 300 adults, oversampled for elevated PNT (50%). Participants will complete a lab assessment to measure predictors of ER capacity, followed by a 10- day ecological momentary assessment study examining predictors of ER tendency in daily life. To better capture negative emotions as they occur, reports in daily life will be physiologically-triggered with algorithms that detect potential episodes of psychological stress. Effective ER will be operationalized in multiple ways in the lab and in daily life, using self-reported changes in affect, physiological indices, and perceived ER success. Effective ER capacity and tendency will then be examined as predictors of distress symptom, functioning, and well-being trajectories assessed monthly for 12 months. Additionally, an exploratory aim is to create predictive models from a multifaceted battery of theoretically motivated variables that impact ER tendency and subsequent clinical outcomes. To this end, machine learning will be used to build a clinically-relevant framework for how person- level, situation-level, and ER strategy use interact to predict optimal ER. Overall, this project contributes to the long-term goal of identifying and refining targets for personalized interventions, by more precisely isolating key mechanisms of effective ER for specific individuals and the contexts they encounter in their daily lives. Project Narrative The proposed study seeks to examine predictors (i.e., emotional awareness, contextual sensitivity, perseverative negative thinking) of dysfunctional emotion regulation that contribute to depression and anxiety, as well as to provide an initial framework for optimal emotion regulation in individualized daily life contexts. This study uses multiple measurements (behavioral, psychophysiological, self-report, clinical interview) in the laboratory and in naturalistic settings in daily life, and it focuses on dimensional and transdiagnostic features that underlie multiple disorders. Consistent with NIMH's mission, findings will be relevant to public health because they can help refine intervention targets and inform personalized treatment for depression and anxiety, which are common and impairing conditions.",Emotion Regulation in Distress Disorders: Elucidating the Role of Cognitive Processes and Person-Situation Fit in the Laboratory and Daily Life,9993553,R01MH118218,"['Adult', 'Affect', 'Algorithms', 'Anxiety', 'Applications Grants', 'Arousal', 'Awareness', 'Behavioral', 'Cardiovascular system', 'Characteristics', 'Clinical', 'Complex', 'Dimensions', 'Disease', 'Distress', 'Ecological momentary assessment', 'Emotional', 'Emotions', 'Generalized Anxiety Disorder', 'Goals', 'Impairment', 'Individual', 'Individual Differences', 'Intervention', 'Interview', 'Laboratories', 'Life', 'Machine Learning', 'Major Depressive Disorder', 'Measurement', 'Measures', 'Mental Depression', 'Mission', 'Modeling', 'National Institute of Mental Health', 'Outcome', 'Participant', 'Patient Self-Report', 'Pattern', 'Perception', 'Personal Satisfaction', 'Persons', 'Physiological', 'Process', 'Psychological Stress', 'Psychophysiology', 'Public Health', 'Reporting', 'Research Domain Criteria', 'Risk', 'Role', 'Severities', 'Symptoms', 'Testing', 'Thinking', 'Time', 'Treatment Efficacy', 'Work', 'adaptive intervention', 'base', 'clinically relevant', 'cognitive process', 'comparative', 'emotion regulation', 'improved', 'improved outcome', 'indexing', 'innovation', 'multimodality', 'novel', 'personalized intervention', 'personalized medicine', 'predictive modeling', 'psychosocial', 'random forest', 'response', 'success', 'treatment response']",NIMH,UNIVERSITY OF WESTERN AUSTRALIA,R01,2020,441529,0.047230859363702485
"Neurocomputational Approaches to Emotion Representation Maintaining an adaptive balance of emotions is central to well-being, and dysregulated emotions contribute broadly to clinical disorders that impart high personal and societal burdens. Recognizing the transdiagnostic importance of emotion to mental health, the National Institute of Health's Research Domain Criteria (RDoC) matrix contains overarching domains of Negative Valence, Positive Valence, and Arousal. However, the matrix underspecifies how specific affective states like sadness, anxiety, or craving are organized within and across these domains, in part because it is unknown whether representations of discrete emotions are reliably differentiated. Other RDoC constructs, such as rumination and worry, modify the temporal parameters of emotions that confer psychopathology risk and exacerbate symptom maintenance. Nonetheless, it is unknown how these processes interface with emotional brain circuits to impact affect dynamics, particularly as they often occur spontaneously during mind wandering. The proposed research promises to improve the RDoC depiction of these emotion-related constructs by taking an affective computing approach. During combined recording of psychophysiology and functional magnetic resonance imaging (fMRI), adult participants will experience emotions to vignettes and movie clips spanning the arousal and valence dimensions, and will report on their spontaneous emotions during resting-state fMRI scans. Machine learning algorithms will decode emotion- specific signals across the levels of analysis, which will be integrated using Bayesian state-space modeling. An analysis of classifier errors will test competing predictions from emotion theories regarding the optimal structure of affective space. Using graph theoretic tools, we will characterize the neural network architecture of the discrete emotion representations to identify provincial and connector hubs that can be used as novel targets for future symptom-specific or co-morbid neuromodulation interventions, respectively. We will apply the emotion-specific maps to resting-state data from the same participants to create neurophysiological indices of spontaneous emotions and to relate their frequencies to measures of trait and state affect as a validation step. Using stochastic modeling of the resting-state data, we will derive temporal dynamics metrics to test the hypothesis that rumination and worry promote emotional inertia during mind wandering. Finally, we will use existing data repositories to demonstrate that our novel indices of affect dynamics transdiagnostically differentiate resting-state fMRI activity patterns in mental health disorders from healthy controls. The proposed research will improve upon current RDoC formulations of Negative Affect, Positive Affect, and Arousal domains by informing how discrete emotions are organized within and across these domains, by integrating emotion representations across multiple RDoC units of analysis, by informing how rumination and worry impact neurophysiological signatures of spontaneous emotions, and by establishing the clinical utility of computationally-derived metrics of emotion dynamics. PROJECT NARRATIVE  The overall goal of this project is to characterize how emotions are represented in the brain and autonomic nervous system. Computational modeling approaches will be applied to integrate the data across multiple sources and tasks, to test competing theories about how emotions are organized, and to derive new metrics of emotion dynamics. The outcome of the work will inform how emotions should be conceptualized within a broader research framework of mental health domains.",Neurocomputational Approaches to Emotion Representation,10059052,R01MH124112,"['Adult', 'Affect', 'Affective', 'Age', 'Anxiety', 'Arousal', 'Autonomic nervous system', 'Basic Science', 'Behavioral', 'Brain', 'Categories', 'Classification', 'Clinical', 'Clip', 'Code', 'Computer Models', 'Data', 'Depressed mood', 'Dimensions', 'Disease', 'Emotional', 'Emotions', 'Equilibrium', 'Exhibits', 'Formulation', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Future', 'Goals', 'Graph', 'Human', 'Individual', 'Individual Differences', 'Intervention', 'Link', 'Machine Learning', 'Maintenance', 'Maps', 'Measures', 'Mental Health', 'Mental disorders', 'Methods', 'Mind', 'Modeling', 'Negative Valence', 'Outcome', 'Participant', 'Patient Self-Report', 'Pattern', 'Personal Satisfaction', 'Physiological', 'Population', 'Positive Valence', 'Process', 'Psychopathology', 'Psychophysiology', 'Reporting', 'Research', 'Research Domain Criteria', 'Rest', 'Risk', 'Role', 'Signal Transduction', 'Source', 'Space Models', 'Structure', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Validation', 'Work', 'affective computing', 'anxious', 'anxious individuals', 'base', 'biobehavior', 'comorbidity', 'craving', 'data warehouse', 'experience', 'functional MRI scan', 'improved', 'indexing', 'machine learning algorithm', 'markov model', 'movie', 'negative affect', 'neural network architecture', 'neurophysiology', 'neuroregulation', 'novel', 'organizational structure', 'relating to nervous system', 'repository', 'theories', 'tool', 'trait']",NIMH,DUKE UNIVERSITY,R01,2020,774444,0.14278125426715405
"Bioethics of syndrome diagnosis using 3D image analysis Project Summary/Abstract This supplement will address the unintended consequences and collateral damage that arise when facial recognition software is used for medical purposes, such as for syndrome diagnosis as in our Facebase-funded project. In Aim 1, we will determine whether the accuracy of this technology varies based on self-reported race, sex and age. In this aim, we examine our existing database for evidence of bias based on self-reported race, sex or age. We further determine the extent to which these variables influence classification performance. To the extent sample sizes allow, we will carry this analysis to the level of specific syndromes. Finally, we will use anonymized reference datasets of non-syndromic faces to compare false positive rates based on NIH race definitions, sex and age. The outcome of this aim is to objectively establish bias and estimate the effects of under-representation across race, age and sex categories within our data. In Aim 2, we will determine how the reports of race-, sex- and age-based bias in facial recognition technology may influence views of the technology and its application amongst researchers and clinicians. This aim will establish the extent to which the storing of large databases of facial images and the application of machine learning processes to them for diagnostic purposes may raise privacy concerns. The concerns investigated will include potential hacks into protected health information; fear relating to the bias in some facial recognition software (and, potentially, in the Facebase database); and fear of discrimination in the application of the technology, such as by insurers. The outcome will be a white paper that targets a high-profile journal, summarizing the findings and defining crucial issues that should guide the development of facial imaging for disease diagnosis and clinical usage. Project Narrative Our supplement application will address the important question of unintended consequences and collateral damage when facial recognition software is used for medical purposes, such as for syndrome diagnosis as in our Facebase-funded project. The use of large facial recognition databases in medicine represents a frontier that arrives with tremendous potential but undeniable risks. Our central aims are: (1) determine whether the accuracy of this technology varies based on self-reported race, sex and age; and (2) determine how the reports of race-, sex- and age-based bias in facial recognition technology may influence views of the technology and its application amongst researchers and clinicians.",Bioethics of syndrome diagnosis using 3D image analysis,10132648,U01DE028729,"['3-Dimensional', 'Address', 'Age', 'Authoritarianism', 'Bioethical Issues', 'Bioethics', 'Biometry', 'Categories', 'Classification', 'Clinic', 'Clinical', 'Clustered Regularly Interspaced Short Palindromic Repeats', 'Computational Biology', 'Country', 'Data', 'Data Science', 'Data Set', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Discrimination', 'Ethics', 'Face', 'FaceBase', 'Fright', 'Funding', 'General Hospitals', 'Generations', 'Genes', 'Genetic Diseases', 'Health', 'Hereditary Disease', 'Image', 'Image Analysis', 'Insurance Carriers', 'Journals', 'Libraries', 'Machine Learning', 'Medical', 'Medicine', 'Outcome', 'Paper', 'Pathology', 'Patient Self-Report', 'Patient imaging', 'Performance', 'Privacy', 'Private Sector', 'Process', 'Psychiatry', 'Public Sector', 'Race', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Sample Size', 'San Francisco', 'Secure', 'Syndrome', 'Technology', 'Three-Dimensional Image', 'United States National Institutes of Health', 'Universities', 'Work', 'base', 'craniofacial', 'disease diagnosis', 'facial recognition software', 'frontier', 'human data', 'intervention cost', 'new technology', 'professor', 'repository', 'sex', 'tool']",NIDCR,UNIVERSITY OF SOUTHERN CALIFORNIA,U01,2020,152200,0.04302713152744313
"Can fidgeting lead to enhanced attention and emotional regulation in adult ADHD? Fidgeting is a highly common behavior, with excessive fidgeting associated with attention-deficit/hyperactivity disorder (ADHD). Studies from our laboratory (1) and colleagues (2) suggest physical movement can enhance cognitive performance in children with ADHD. Hyper-sensorimotor behavior may be related to impaired regulation of arousal in the noradrenergic and dopaminergic systems (3). This project will assess if frequency and characteristics of sensorimotor behavior relates to cognitive and emotional response in adults with ADHD, in a fine-grained manner, unlike other studies. We will test if intrinsic fidgeting (Aim 1) and access to a specially designed fidget device (Aim 2) modulates behavioral and physiological response in cognitively and emotionally-demanding contexts. The hype of the commercially available Fidget Cube, its competitors and fidget spinners suggest it might, but there is no systematic evidence to inform consumers, a gap, we aim to fill. ADHD is a significant problem in adulthood, with estimates of 4.4% in the population (4). It is associated with higher rates of substance use disorders, traffic accidents and employment challenges and a national annual economic impact ranging from $143 to $266 billion (5). While overt hyperactivity is mostly associated with childhood, subtler, finer-grained frequent movements (e.g., leg movements, doodling, clicking objects, tapping) are highly common in adult ADHD. Little is known about the characteristics of fidgeting in adulthood or whether it can be harnessed to enhance self-regulation with the use of an external device. Our aims are as follows: Aim 1: Assess in a randomized controlled study if a) intrinsic fidgeting and b) use of a smart fidget device improves attention, working memory, processing speed and emotional regulation in adult ADHD; Aim 2: Identify specific touch characteristics associated with cognitive and emotional regulation in adult ADHD using behavioral coding and a prototype fidget ball (developed by Co-I Isbister) with embedded pressure sensors on the fidget surface transmitting real time data to a computer for data analysis; Exploratory Aim 3: Conduct a machine learning analysis of fidgeting behavior in relation to cognitive performance and emotional regulation to: 1) automate recognition of touch features present in fidgeting in adult ADHD; 2) correlate touch sequences with cognitive performance measures and; 3) recommend fidgeting strategies that should prove effective in a given situation. This project will build upon prior work by PI Schweitzer, with her expertise in ADHD, clinical translational research and cognitive neuroscience and Co-I Isbister, with expertise in computer science and engineering, who developed sensor-enabled, smart fidget devices with the goal of improving self-regulation of mood and attention (6-9); Co-I Shapiro (10) with machine learning expertise in analyzing fidgeting behavior. This project is highly responsive to NIMH Strategic Plan Objective 3 as it will inform researchers working to develop new interventions based on behavioral and physiological markers, tailored to the individual. This project will study how fidgeting relates to cognitive and emotional functioning in adults with attention- deficit/hyperactivity disorder (ADHD). It will determine, in a laboratory setting, whether movement and access to a “fidget device” providing sensory and motor stimulation can improve cognitive and emotional regulation (including on physiological measures) in adult ADHD. We will also acquire pilot data for machine learning analyses to be used in future, large scale studies to identify gestures and touch characteristics associated with improved cognitive and emotional regulation to see if we can predict and subsequently develop recommendations to improve performance and emotional control in natural settings (e.g., home, office, college classroom) for adult ADHD.",Can fidgeting lead to enhanced attention and emotional regulation in adult ADHD?,10064501,R21MH121901,"['Accidents', 'Adult', 'Affective', 'Arousal', 'Attention', 'Attention deficit hyperactivity disorder', 'Behavior', 'Behavioral', 'Biological Markers', 'Boredom', 'Brain region', 'Characteristics', 'Child', 'Childhood', 'Clinical', 'Code', 'Cognition', 'Cognitive', 'Cues', 'Data', 'Devices', 'Disease', 'Effectiveness', 'Emotional', 'Emotional Stress', 'Employment', 'Engineering', 'Exploratory/Developmental Grant', 'Frequencies', 'Future', 'Gestures', 'Goals', 'Grain', 'Home environment', 'Human', 'Hyperactive behavior', 'Impairment', 'Individual', 'Informal Social Control', 'Intervention', 'Laboratories', 'Lead', 'Leg', 'Machine Learning', 'Measures', 'Mental disorders', 'Methods', 'Microprocessor', 'Motor', 'Motor Activity', 'Movement', 'National Institute of Mental Health', 'Occupational', 'Outcome', 'Patient Self-Report', 'Pattern', 'Performance', 'Physiological', 'Physiology', 'Population', 'Prediction of Response to Therapy', 'Reaction Time', 'Readability', 'Recommendation', 'Recovery', 'Regulation', 'Reporting', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Schools', 'Sensory', 'Short-Term Memory', 'Strategic Planning', 'Stress', 'Substance Use Disorder', 'Surface', 'System', 'Tactile', 'Task Performances', 'Taxes', 'Testing', 'Therapeutic Intervention', 'Time', 'Touch sensation', 'Toy', 'Traffic accidents', 'Translational Research', 'Update', 'Video Recording', 'Work', 'alertness', 'base', 'citizen science', 'cognitive enhancement', 'cognitive function', 'cognitive neuroscience', 'cognitive performance', 'cognitive reappraisal', 'college', 'computer data analysis', 'computer science', 'cost', 'design', 'digital', 'economic impact', 'emotion regulation', 'emotional functioning', 'heart rate variability', 'improved', 'large datasets', 'learning strategy', 'mood regulation', 'noradrenergic', 'novel', 'pressure sensor', 'processing speed', 'prototype', 'randomized controlled study', 'response', 'sensor', 'sensorimotor system', 'social relationships', 'success', 'theories', 'young adult']",NIMH,UNIVERSITY OF CALIFORNIA AT DAVIS,R21,2020,417969,0.028474580463570928
"Affective science and smoking cessation: Real time real world assessment Tobacco use plays a causal role in almost 20 different types of cancer, and although smoking cessation is a cornerstone of cancer risk reduction, the vast majority of smoking quit attempts fail. Numerous conceptual models, as well as a large body of empirical evidence, underscore that affect is a potent determinant of smoking lapse. Unfortunately, very little is known about how the constellation and temporal dynamics of distinct emotions and other factors play out in real time in the real world to influence lapse risk. This lack of knowledge severely hampers both our conceptual models and our ability to optimally intervene. Thus, the overarching objectives of this research are to create a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. The proposed research directly addresses several objectives from the PAR including the influence of distinct emotions and their time course on cancer risk behaviors, whether the role of distinct emotions is altered by the presence of other emotions (e.g., “blended” emotional states), and how the influence of affective experience is modified by context. The proposed longitudinal cohort study among 300 smokers attempting to quit is guided by a conceptual framework grounded in affective science and conceptual models of self-regulation and addiction. Participants will be followed from 1 week prior to their quit date through 6 months post-quit date. They will be assessed from 1 week pre-quit date through 2 weeks post-quit date using AutoSense, geographic positioning system (GPS), and ecological momentary assessment (EMA). AutoSense, GPS, and EMA collect real time data in natural environments, communicate wirelessly with each other, and data are processed in real time on a smartphone. AutoSense detects specific behavioral and physiologic “signatures” of smoking (the primary outcome) and self regulatory capacity (an intermediate outcome; assessed using high frequency heart rate variability) in real time. GPS real time spatial tracking will be linked with spatially and temporally relevant characteristics of the environment using geographic information system (GIS) data. EMAs assess self-reported emotions, cognition, and context. Analyses utilize advanced dynamic risk prediction models and machine learning approaches to model the dynamics of real time, real world associations among distinct emotions, SRC, and lapse. Tobacco use is the leading preventable cause of death and disability in the U.S. The proposed study will yield a more detailed and comprehensive conceptual model of the role of distinct emotions in self-regulation, as well as the technical, empirical, and analytic foundation necessary to develop more effective interventions for smoking cessation and other cancer risk behaviors that can target real time, real world mechanisms. This knowledge can be utilized to reduce the public health burden of tobacco use.",Affective science and smoking cessation: Real time real world assessment,9839496,R01CA224537,"['Address', 'Affect', 'Affective', 'Algorithms', 'Anger', 'Behavioral', 'Cause of Death', 'Cellular Phone', 'Characteristics', 'Cognition', 'Complement', 'Complex', 'Data', 'Depressed mood', 'Ecological momentary assessment', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Foundations', 'Frequencies', 'Geographic Information Systems', 'Geography', 'Home environment', 'Informal Social Control', 'Intervention', 'Knowledge', 'Link', 'Longitudinal cohort study', 'Machine Learning', 'Measurement', 'Mediating', 'Mediator of activation protein', 'Modeling', 'Outcome', 'Participant', 'Patient Self-Report', 'Physiological', 'Play', 'Positioning Attribute', 'Process', 'Public Health', 'Real-Time Systems', 'Reporting', 'Research', 'Risk', 'Risk Behaviors', 'Risk Reduction', 'Role', 'Science', 'Smoker', 'Smoking', 'Smoking Cessation Intervention', 'Stress', 'System', 'Testing', 'Time', 'Tobacco', 'Tobacco use', 'Volition', 'Wireless Technology', 'adaptive intervention', 'addiction', 'advanced analytics', 'base', 'cancer prevention', 'cancer risk', 'cancer type', 'disability', 'effective intervention', 'experience', 'heart rate variability', 'knowledge base', 'mobile computing', 'negative affect', 'primary outcome', 'risk prediction model', 'role model', 'smoking cessation', 'success']",NCI,UNIVERSITY OF UTAH,R01,2020,617352,0.04673414117981004
"Predicting complicated grief from grief processing PROJECT SUMMARY Most people grieving the loss of a loved one will experience a period of intense pain and focusing on the loss lasting around 6 months, which is known as acute grief. Complicated grief (CG) occurs when the experiences of acute grief extend well past 6-months post-loss. Thoughts and feelings about the loss (i.e. grief processing) occurring during acute grief may play a role in healthy grieving and protect against CG development. Identification of the cognitive and emotional mechanisms of grief processing that contribute to healthy grief resolution would advance knowledge of the goals of grieving and assist the development of interventions for complicated grief. Two core components of grief processing are top-down regulation and balanced loss confrontation. Top-down pursue related emotional representations and recruit proportion regulation is the ability to suppress processing of intrusive emotional information to a stated goal. Top-down regulation may facilitate healthy grieving by allowing reprieve from intense loss thinking. Balanced loss confrontation refers to the processing of the loss in a way that protects against overload. Confrontation with the l oss may assist in the process of reforming one's mental of the deceased. This tudy will test extrinsic and intrinsic measures of top-down regulation balanced loss confrontation during acute grieving as predictors of CG development a year later We will a sample at high-risk for CG, the suicide-bereaved, in order to maximize the likeliness that a significant of the sample develops CG. The s . findings produced by this study may advance the knowledge of how CG develops, assist in the identification of people at high-risk for developing CG and potentially form the basis for targeted interventions.  The following K23 presents a research and training program that will support the applicant on the path of becoming an independent investigator of the role of grief processing in the development of complicated grief. The research mentorship, coursework, hands-on experience, seminars and classes ingrained in this training and plan will propel the applicant to independence in the domains of1) Clinical Research, 2) Psychometric Assessment of Grief Processing, 3) Machine Learning analysis of fMRI, 4) Biostatistics, 5) Scientific Independence. team independent and The combination of the environment, t raining plan, research strategy and mentorship will not only provide the candidate with a spectrum of new methods and skills that will establish him as an research scientist, but will also produce a body of knowledge that will clarify the specific cognitive emotional grief processes that contribute to the development of CG. PROJECT NARRATIVE Complicated grief describes an inability to adjust to the loss of a loved one over the course of the first year following the death. This study will identify cognitive, emotional and neural processes occurring in the early grieving period (3 to 5-months post-loss) that predict or protect against the development of complicated grief a year later in suicide bereaved subjects, a sample at high-risk for developing complicated grief. These findings may advance understanding of the process of grief, facilitate early identification of high-risk grievers and potentially form the basis for targeted treatment of complicated grief.",Predicting complicated grief from grief processing,9888433,K23MH114021,"['Acute', 'Age', 'Attention', 'Biometry', 'Cessation of life', 'Clinical', 'Clinical Research', 'Cognitive', 'Data', 'Depressed mood', 'Development', 'Down-Regulation', 'Early identification', 'Emotional', 'Emotions', 'Environment', 'Failure', 'Family member', 'Feeling', 'Functional Magnetic Resonance Imaging', 'Gender', 'Goals', 'Grief reaction', 'Guilt', 'High Prevalence', 'Individual', 'Instruction', 'Intervention', 'Interview', 'Knowledge', 'Machine Learning', 'Measures', 'Mental Depression', 'Mentorship', 'Methods', 'Modeling', 'Pain', 'Pathogenesis', 'Pattern', 'Play', 'Process', 'Psyche structure', 'Psychometrics', 'Questionnaires', 'Rain', 'Reaction Time', 'Recording of previous events', 'Regulation', 'Reporting', 'Research', 'Research Personnel', 'Research Training', 'Resolution', 'Role', 'Sampling', 'Scientist', 'Severities', 'Shame', 'Stimulus', 'Suicide', 'Techniques', 'Testing', 'Thinking', 'Time', 'Training', 'Training Programs', 'Trauma', 'Unconscious State', 'Validation', 'attentional bias', 'base', 'experience', 'high risk', 'indexing', 'intense pain', 'loved ones', 'neural patterning', 'recruit', 'relating to nervous system', 'response', 'sex', 'skills', 'sustained attention', 'targeted treatment', 'therapy development']",NIMH,NEW YORK STATE PSYCHIATRIC INSTITUTE,K23,2020,199800,0.05377112980576799
"Computational and brain predictors of emotion cue integration The purpose of this project is to develop computational and brain-based models of emotion cue integration: people’s inferences about others’ emotions based on dynamic, multimodal cues. Observers often decide how targets feel based on cues such as facial expressions, prosody, and language. Such inferences scaffold healthy social interaction, and abnormal inference both marks and exacerbates social deficits in numerous psychiatric disorders. Psychologists and neuroscientists have studied emotion inference for decades, but the vast majority of this work employs simplified social cues, such as vignettes or static images of faces. By contrast, “real world” emotion cues are complex, dynamic, and multimodal. Cue integration—inference based on naturalistic emotion information—likely differs from simpler inference at cognitive and neural levels, but this phenomenon remains poorly understood. This means that scientists lack a clear model of how observers adaptively process complex emotion cues, and how that processing goes awry in mental illness. Especially lacking are mechanistic models that can describe the computations and brain processes involved in cue integration with sufficient precision to predict inference in new cases, observers, and samples. This project will merge tools from social psychology, computer science, and neuroscience to generate a novel and rigorous model of emotion cue integration. We have demonstrated that in the face of complex emotion cues, observers dynamically “weight” cues from each modality (e.g., visual, linguistic) over time, a process that (i) tracks shifts in brain activity and connectivity; and (ii) can be captured using Bayesian models. Here, we will expand this work in several ways. First, we will develop precise computational tools to isolate features of emotion cues—such as facial movements, prosody, and linguistic sentiment—that track observers’ use of each cue modality during integration. Second, we will develop multi-region “signatures” of brain activity and connectivity that track emotion inference in each modality. We will use these signatures in conjunction with machine learning to predict unimodal emotion inference and cue integration in new observers and samples, based on brain data alone. Third, we will explore the context-dependence of naturalistic emotion inference by testing whether reinforcement learning can bias observers’ cue integration and accompanying brain signatures. Finally, we will model computational and neural abnormalities associated with cue integration in patients with Major Depressive Disorder and Bipolar Disorder. At the level of basic science, these data will generate a fundamentally new—and more naturalistic—approach to the neuroscience of emotion inference. The computational and brain metrics we produce will also be made publically available to facilitate the open and cumulative study of emotion inference across labs. At a translational level, we will provide a mechanistic, rich account of abnormal emotion inference in mood disorders, paving the way for computational and brain markers that can be used to assess social dysfunction and treatment efficacy in these and other mental illnesses. PROJECT NARRATIVE The proposed research will use methods from social psychology, cognitive neuroscience, and computer science to (i) precisely model people’s inferences about others’ emotions based on complex, dynamic cues, (ii) generate multi-region, brain-based predictors of these inferences, and (iii) characterize abnormalities in inference among individuals with mood disorders. Several psychiatric and neurodevelopmental disorders are characterized by difficulties understanding others’ emotions, which in turn worsen social functioning in patients. In addition to providing new insights about social processes—a core target within the NIMH’s research domain criteria (RDoC) framework—the proposed research will offer powerful, novel computational and neural targets through which to assess and treat difficulties in emotion inference, and to eventually reduce the social burden faced by people with mental illness on a broad scale.",Computational and brain predictors of emotion cue integration,9923725,R01MH112560,"['Affect', 'Agreement', 'Base of the Brain', 'Basic Science', 'Bayesian Modeling', 'Bipolar Disorder', 'Brain', 'Brain region', 'Classification', 'Cognitive', 'Complex', 'Computer Models', 'Cues', 'Data', 'Dependence', 'Emotional', 'Emotional disorder', 'Emotions', 'Event', 'Exhibits', 'Face', 'Face Processing', 'Facial Expression', 'Functional Magnetic Resonance Imaging', 'Future', 'Image', 'Individual', 'Language', 'Lateral', 'Learning', 'Life', 'Linguistics', 'Machine Learning', 'Major Depressive Disorder', 'Measures', 'Mental disorders', 'Methods', 'Modality', 'Modeling', 'Mood Disorders', 'Moods', 'Movement', 'National Institute of Mental Health', 'Neurodevelopmental Disorder', 'Neurosciences', 'Observer Variation', 'Participant', 'Patients', 'Pattern', 'Perception', 'Process', 'Psychological reinforcement', 'Psychologist', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Running', 'Sampling', 'Scanning', 'Scientist', 'Sensory', 'Social Functioning', 'Social Interaction', 'Social Processes', 'Social Psychology', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Testing', 'Time', 'Training', 'Treatment Efficacy', 'Visual', 'Weight', 'Work', 'affective computing', 'base', 'brain abnormalities', 'cognitive neuroscience', 'computer science', 'computerized tools', 'executive function', 'insight', 'language comprehension', 'multimodality', 'neuroimaging', 'novel', 'recruit', 'relating to nervous system', 'response', 'scaffold', 'social', 'social deficits', 'tool']",NIMH,STANFORD UNIVERSITY,R01,2020,451783,0.13157813104076466
"Mapping connectomes for disordered emotional states PROJECT SUMMARY/ABSTRACT Our objective is to use HCP protocols to acquire and make public a large dataset of imaging, behavioral, and symptom data from patients with disordered emotional states. We will also develop and make public new methods for examining how connectome disorganization gives rise to these disordered states at the level of the individual patient. Psychopathology arising from enhanced negative emotion or from the loss of positive emotional experience affects over 400 million people globally. Such states of disordered emotion cut across multiple diagnostic categories and are compounded by accompanying disruptions in cognitive function. Not surprisingly, therefore, these forms of psychopathology are a leading cause of disability. To address these issues our investigative strategy is informed by the Research Domain Criteria (RDoC) initiative spearheaded by NIMH. We focus on three RDoC domains and constructs: 1) acute threat within the Negative Valence System (NVS) domain, a construct relevant to automatic reactions to fear and physical symptoms of anxiety; 2) reward valuation and responsiveness within the Positive Valence System (PVS) domain, a construct involving incentive salience, hedonic responses and symptoms of anhedonia; and 3) working memory within the Cognitive System (CS) domain, a construct that implicates top-down regulation of cognitive rumination and worry. Our approach is grounded in strict adherence to HPC protocols and a strong commitment to data sharing. We unite complementary expertise, including (1) state-of-the-art MRI technology and data management systems; (2) a field-leading Center for Reproducible Neuroscience; (3) a track record in leading large-scale neuroradiology consortia; (4) leaders in RDoC-informed approaches to large-scale imaging in depression and anxiety; and (5) pioneering statistical approaches for high-dimensional data. Our aims are to (1) use the HCP protocols to acquire multi-modal data for 300 people aged 22-25 years of age who are experiencing varying degrees of acute threat, loss of reward valuation/responsiveness, and difficulties in working memory, (2) elucidate the nature of the relations among connectomes, symptoms, and behavior based on networks related to the RDoC constructs of interest, and (3) to develop data-driven, machine-learning methods to discover how connectomes for these constructs combine together to form naturally organized clusters of people. Our data will advance a neurobiological model that maps network dysfunctions to specific behaviors and symptoms. This model will provide a foundation for ultimately guiding more classifications and treatment choices according to types of neural dysfunction rather than relying on diagnostic categories that are agnostic to neurobiology. PROJECT NARRATIVE Psychopathology arising from a disruption of emotional function affects over 400 million people globally, yet we lack a neurobiological model to guide classification and treatment. We propose to use Human Connectome Project protocols to develop and disseminate a brain network model of disordered emotional states.",Mapping connectomes for disordered emotional states,9925811,U01MH109985,"['Acute', 'Address', 'Adherence', 'Affect', 'Age', 'Age-Years', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Anhedonia', 'Anterior', 'Anxiety', 'Behavior', 'Behavioral', 'Behavioral Symptoms', 'Brain', 'Categories', 'Classification', 'Cognitive', 'Corpus striatum structure', 'Data', 'Data Management Resources', 'Data Set', 'Diagnostic', 'Diffusion', 'Dimensions', 'Disease', 'Dorsal', 'Down-Regulation', 'Emotional disorder', 'Emotions', 'Evaluation', 'Foundations', 'Fright', 'Functional Imaging', 'Human', 'Image', 'Insula of Reil', 'Magnetic Resonance Imaging', 'Maps', 'Medial', 'Mental Depression', 'Methods', 'Modeling', 'National Institute of Mental Health', 'Nature', 'Negative Valence', 'Neurobiology', 'Neuronal Dysfunction', 'Neurosciences', 'Parietal Lobe', 'Participant', 'Patient Self-Report', 'Patients', 'Performance', 'Positive Valence', 'Precentral gyrus', 'Prefrontal Cortex', 'Principal Component Analysis', 'Protocols documentation', 'Psychopathology', 'Reaction', 'Reproducibility', 'Research Domain Criteria', 'Resources', 'Rewards', 'Sampling', 'Seeds', 'Short-Term Memory', 'Structure', 'Subgroup', 'Symptoms', 'System', 'Taxonomy', 'Technology', 'United States National Institutes of Health', 'aged', 'anxiety symptoms', 'base', 'burden of illness', 'cognitive function', 'cognitive reappraisal', 'cognitive system', 'cohesion', 'connectome', 'data sharing', 'disability', 'disability burden', 'emotional experience', 'emotional functioning', 'executive function', 'experience', 'follow-up', 'hedonic', 'human imaging', 'incentive salience', 'individual patient', 'interest', 'large datasets', 'machine learning method', 'multidimensional data', 'multimodal data', 'network dysfunction', 'network models', 'outcome prediction', 'physical symptom', 'predict clinical outcome', 'recruit', 'response', 'social', 'treatment choice', 'white matter']",NIMH,STANFORD UNIVERSITY,U01,2020,745640,0.06348538773156562
"Neurophysiology underlying neural representations of value A range of behavioral, physiological, and cognitive responses (e.g. approach and avoidance, autonomic reactivity, and subjective feelings) reflects a subject's emotional state. The cognitive regulation of emotion refers to the capacity to regulate these emotional responses in a flexible manner according to a cognitive operation. Deficits in the cognitive regulation of emotional processes characterize many psychiatric disorders. In everyday life, however, particular sensory stimuli and/or actions can elicit different emotional responses depending upon the situation or context. Contexts often rely on a cognitive understanding of one's current situation in the absence of explicit cues. These types of contexts may be referred to as “abstract” contexts. This grant studies a type of abstract context where the context is determined by a task set. A task set is the set of stimulus- response-outcome mappings (or rules) that dictate correct performance for trials within a particular block. Previous research demonstrates the capacity of primates to learn these abstract contexts, and neural representations of abstract contexts exist in the amygdala and two areas in the prefrontal cortex (PFC), the anterior cingulate and orbitofrontal cortices (ACC and OFC). This grant seeks to understand the mechanisms that underlie the formation and maintenance of these representations of contexts. In contrast to supervised learning driven by error signals, we hypothesize that the occurrence of temporally associated trial types triggers unsupervised learning, presumably through a Hebbian mechanism involving activity-dependent plasticity. This learning could underlie formation of representations of abstract contexts defined by task sets, which will be explored with electrophysiological recordings in Aim 1. The creation of a representation of a task set requires combining information about the current trial with information about the trials that have occurred recently. Brain structures that provide memory traces of recent events and/or that combine information over time could create representations of a task set prior to the emergence of the representations observed in amygdala, OFC, and ACC. Our next experiments therefore target the hippocampus and dorsolateral PFC (DLPFC), which are implicated in memory processes, working memory, and executive functions. We will compare and contrast the encoding of task sets in hippocampus, DLPFC, OFC, and ACC during and after learning about task sets (Aim 2). Finally, we will use causal methods to determine if PFC input to the amygdala and the hippocampus acts to maintain these context representations, which could be a vital mechanism for the cognitive regulation of emotion (Aim 3). Overall, these experiments promise to illuminate neurophysiological mechanisms critical for normal adaptive emotional health. ! The aim of this proposal is to understand brain mechanisms responsible for the cognitive regulation of emotion. Since many psychiatric disorders, like anxiety and mood disorders as well as schizophrenia, autism, and addiction, involve cognitive dysfunction mediated by neural circuits in the brain areas under study, this project promises to provide new insights about neural network function critical for developing new treatments.",Neurophysiology underlying neural representations of value,9819777,R01MH082017,"['Amygdaloid structure', 'Anterior', 'Anxiety Disorders', 'Area', 'Behavioral', 'Brain', 'Cognitive', 'Cues', 'Development', 'Electrophysiology (science)', 'Emotional', 'Emotions', 'Event', 'Feeling', 'Grant', 'Health', 'Hippocampus (Brain)', 'Impaired cognition', 'Knowledge', 'Lead', 'Learning', 'Life', 'Maintenance', 'Mediating', 'Memory', 'Mental disorders', 'Methods', 'Monkeys', 'Mood Disorders', 'Outcome', 'Perception', 'Performance', 'Physiological', 'Play', 'Prefrontal Cortex', 'Primates', 'Process', 'Psychological reinforcement', 'Research', 'Response to stimulus physiology', 'Reversal Learning', 'Rewards', 'Role', 'Schizophrenia', 'Short-Term Memory', 'Signal Transduction', 'Stimulus', 'Structure', 'Task Performances', 'Testing', 'Time', 'Update', 'Work', 'addiction', 'autism spectrum disorder', 'cognitive control', 'cognitive process', 'cognitive reappraisal', 'emotion regulation', 'emotional experience', 'executive function', 'expectation', 'experimental study', 'flexibility', 'insight', 'memory process', 'neural circuit', 'neural network', 'neurophysiology', 'operation', 'optogenetics', 'prevent', 'relating to nervous system', 'response', 'sensory stimulus', 'statistics', 'supervised learning', 'unsupervised learning']",NIMH,COLUMBIA UNIVERSITY HEALTH SCIENCES,R01,2020,698101,0.05840088934701091
"CRCNS: Optimization of closed-loop control of gamma oscillations Throughout the brain, specialized systems carry out different but complementary functions, sometimes  independently but often in cooperation. However, we do not understand how their activity is dynamically  coordinated, and dysregulation of this is associated with many mental health conditions. Neuronal oscillations, which are detectable in local field potentials (LFPs) at various frequencies, are a promising  target for this coordination. Gamma oscillations (40-100 Hz) in particular have been singled out since they enhance stimulus responses, facilitate interactions between brain regions, and are expressed ubiquitously across cortical and subcortical regions. Indeed, gamma oscillations occur in the basolateral  nucleus of the amygdala (BL), an important regulator of emotional behaviors. BL gamma oscillations are  enhanced during periods of heightened vigilance during a foraging task, following emotionally salient experiences, and upon presentation of socially-relevant stimuli. The variety of circumstances that engage  it make it a promising target for interventions affecting emotional behaviors in general. However, technical challenges abound because gamma manifests as brief intermittent oscillatory bursts, layered atop numerous ongoing activities in other frequency bands. This precludes manipulating gamma exclusively with traditional pharmacological, optogenetic, or chemogenetic approaches, since these have substantial  effects on ongoing non-gamma activities, and are delivered irrespective of whether gamma bursts are present or absent. To overcome this, a closed-loop algorithm was developed that monitors the LFP in  real-time for gamma oscillations and delivers precisely timed optogenetic stimulation capable of enhancing or suppressing gamma strength on a cycle-by-cycle basis. While this improves upon the status  quo,, further refinement is needed. Aim 1 of this proposal seeks to clarify how the gamma modulation technique operates via biophysically detailed modeling of the local circuits in the BL that generate gamma, the effects of optogenetic stimulation, and the closed-loop algorithm. Aim 2 designs better signal  processing routines for detecting and parameterizing gamma in real-time. Aim 3 develops an approach to  create customized biophysical models that reproduce the properties of gamma observed in individual  subjects, which when combined with the results of Aims 1 and 2 should allow for optimized control over gamma oscillations in individual subjects. RELEVANCE (See instructions):  Gamma oscillations occur in the basolateral amygdala, a brain region implicated in emotional regulation.  By developing improved methods to manipulate these oscillations, we hope to better understand their  function and improve our ability to control emotional states and behaviors. n/a",CRCNS: Optimization of closed-loop control of gamma oscillations,10002297,R01MH122023,"['Accounting', 'Address', 'Affect', 'Algorithms', 'Amygdaloid structure', 'Anatomy', 'Behavior', 'Behavioral', 'Biophysics', 'Brain', 'Brain region', 'Cell Nucleus', 'Cells', 'Collaborations', 'Communication', 'Custom', 'Detection', 'Emotional', 'Frequencies', 'Genetic', 'Goals', 'Implant', 'Individual', 'Individual Differences', 'Instruction', 'Interneurons', 'Intervention', 'Machine Learning', 'Mental Health', 'Methods', 'Modeling', 'Monitor', 'Neurons', 'Performance', 'Pharmacology', 'Phase', 'Physiological', 'Property', 'Response to stimulus physiology', 'Rodent Model', 'Route', 'Scheme', 'Stimulus', 'Structure', 'System', 'Techniques', 'Testing', 'Time', 'Twin Multiple Birth', 'biophysical model', 'cognitive function', 'design', 'emotion regulation', 'emotional behavior', 'experience', 'experimental study', 'improved', 'in vivo', 'individual variation', 'insight', 'neurophysiology', 'novel', 'optogenetics', 'predictive modeling', 'response', 'signal processing', 'social', 'vigilance']",NIMH,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,284005,0.05257777699072336
"Cerebellar Interactions with the Amygdala and Prefrontal Cortex during Learning Learning is how organisms adapt to changes in their environment and involves the coordination of neural systems mediating cognition, emotion, and motor control. The major goal of the proposed research program is to elucidate the neural circuit mechanisms underlying interactions between cognitive, emotional, and motor systems during associative learning. Interactions between these neural systems are particularly important because the context and emotional significance of stimuli provide essential information for acquisition and performance of motor responses. The breakdown of interactions between cognitive, emotional, and motor systems in various neurological disorders can therefore have devastating consequences for learned behaviors. The prefrontal cortex, amygdala, and cerebellum play significant roles in cognition, emotional responses, and motor learning, respectively. The proposed research program constitutes a comprehensive analysis of cerebellar interactions with the amygdala and prefrontal cortex during associative motor learning. Our general conceptual framework is that the cerebellum receives inputs from the amygdala and prefrontal cortex via the pons regarding which stimuli are important and when they occur, and the cerebellum then sends error-driven feedback to these forebrain systems to facilitate learning about important events. This conceptual framework takes into account the bidirectional relationship between the cerebellum and the relevant forebrain systems as well as interactions between forebrain systems. Multi-site electrophysiology, pathway-specific optogenetics, and precise behavioral analyses will be combined to investigate circuit-level interactions between the cerebellum, amygdala, and prefrontal cortex during associative learning and extinction (inhibitory learning) training. The proposed studies would significantly advance understanding of the neural circuit mechanisms underlying cerebellar interactions with the forebrain. This would be a substantial contribution to the field because it has been known that the cerebellum must interact with the forebrain in many contexts that are crucial for everyday life such as learning, memory, planning, control of emotions, and communication, but very little is known mechanistically about how the cerebellum interacts with the amygdala and prefrontal cortex. Memory deficits are found with many neurological disorders such as Alzheimer's disease and following stroke; the breakdown of interactions between memory systems can be particularly debilitating. This proposal examines the neural mechanisms underlying interactions between emotional and motor memory systems during associative learning. Elucidating the neural mechanisms underlying memory system interactions is potentially important for developing treatments for memory deficits.",Cerebellar Interactions with the Amygdala and Prefrontal Cortex during Learning,9893633,R01NS088567,"['Affect', 'Alzheimer&apos', 's Disease', 'Amygdaloid structure', 'Anatomy', 'Animals', 'Area', 'Attention', 'Behavioral', 'Blinking', 'Cerebellum', 'Cognition', 'Cognitive', 'Communication', 'Conditioned Stimulus', 'Data', 'Electrophysiology (science)', 'Elements', 'Emotional', 'Emotions', 'Environment', 'Etiology', 'Event', 'Extinction (Psychology)', 'Feedback', 'Funding', 'Goals', 'Image', 'Learning', 'Life', 'Medial', 'Mediating', 'Memory', 'Memory impairment', 'Methods', 'Motor', 'Nature', 'Organism', 'Output', 'Pathway interactions', 'Performance', 'Physiological', 'Play', 'Pontine structure', 'Prefrontal Cortex', 'Process', 'Prosencephalon', 'Rattus', 'Research', 'Role', 'Safety', 'Short-Term Memory', 'Signal Transduction', 'Site', 'Stimulus', 'Stroke', 'Structure', 'System', 'Testing', 'Training', 'classical conditioning', 'conditioned fear', 'granule cell', 'indexing', 'learned behavior', 'learning extinction', 'machine learning algorithm', 'motor control', 'motor learning', 'nervous system disorder', 'neural circuit', 'neuromechanism', 'novel', 'optogenetics', 'programs', 'relating to nervous system', 'response', 'two-photon']",NINDS,UNIVERSITY OF IOWA,R01,2020,374781,0.03480698595848512
"Mechanisms of Dynamic Neural Coupling during Face-to-Face Expressions of Emotion PROJECT SUMMARY Little is known about the neural mechanisms that regulate natural dynamic cues during human social and emotional interactions, although these mechanisms are impaired in many psychiatric and neurological disorders. Although it is widely understood that social signals such as facial expressions carry salient, but implicit, emotional and social cues, these “real-time” pathways have not been investigated with dual-brain neuroimaging techniques. This unmet need is largely due to technological limitations that prevent neuroimaging of two or more individuals during natural interactive situations. We overcome this technical “roadblock” with recent advances in an emerging human brain imaging technology, functional near-infrared spectroscopy (fNIRS). This non-invasive technique detects active neural tissue based on hemodynamic signals measured by variations in the absorption spectra associated with oxyhemoglobin and de-oxyhemoglobin. Because detectors and emitters are surface mounted on the head, absent a high magnetic field, they are relatively insensitive to head movement and thus successfully applied to dyadic experiments. The focus of this proposal is to gain a comprehensive understanding of the mechanisms that underlie dynamic cross-brain neural coupling during real interpersonal interactions. Cross-brain neural coupling is defined as the correlation between the temporal patterns of the signals of two brains. It has been proposed that these matched patterns represent shared neural processes including dynamic exchanges of information. However, the basic assumptions of shared information and temporal resonance patterns between specific brain-to-brain regions has not been tested. We pioneer tests of these hypothesis using eye-to-eye contact as a metric of shared information and predict that dynamic neural coupling between the two brains will increase with increasing numbers of eye-to-eye contact events. Mimicry of facial expressions is also a metric of emotional contagion as well as shared information between brains. We further test the hypothesis that neural coupling will increase with the level of mimicry also by virtue of the shared information Confirmation of the hypothesis that neural coupling represents shared information between the two brains would provide a singular advance for understanding mechanisms for dynamic interactions. Both approaches include variations of emotive expressions to test the additional hypothesis that content as well as shared information might influence dynamic coupling mechanisms. Findings from these studies are expected to open a new direction for the study of live and dynamic interactions between individuals, and provide foundational components to a general framework for models of face-to-face interactions. A long-term goal is to understand the neural underpinnings of affective disorders as they present in clinically-relevant and real-world situations. Project Narrative How do two brains interact? We pioneer a novel neuroimaging technology, near infrared spectroscopy, to investigate basic mechanisms of neural coupling between two dynamically interacting individuals. Using these new methods we test the fundamental hypothesis that neural coupling (temporal synchrony between two brains) represents shared information across the two brains.",Mechanisms of Dynamic Neural Coupling during Face-to-Face Expressions of Emotion,9883842,R01MH119430,"['Affective', 'Arousal', 'Brain', 'Brain imaging', 'Brain region', 'Classification', 'Code', 'Communication', 'Coupled', 'Coupling', 'Cues', 'Detection', 'Development', 'Elements', 'Emotional', 'Event', 'Eye', 'Eyebrow structure', 'Face', 'Face Processing', 'Facial Expression', 'Feeling', 'Foundations', 'Goals', 'Head', 'Head Movements', 'Human', 'Imaging technology', 'Impairment', 'Individual', 'International', 'Interpersonal Relations', 'Machine Learning', 'Measures', 'Mental disorders', 'Methods', 'Modeling', 'Mood Disorders', 'Near-Infrared Spectroscopy', 'Negative Valence', 'Oral cavity', 'Oxyhemoglobin', 'Participant', 'Pathway interactions', 'Pattern', 'Positioning Attribute', 'Positive Valence', 'Process', 'Shapes', 'Signal Transduction', 'Smiling', 'Social Interaction', 'Stimulus', 'Surface', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissues', 'Variant', 'absorption', 'analog', 'base', 'clinically relevant', 'contagion', 'detector', 'dyadic interaction', 'emotional reaction', 'experimental study', 'hemodynamics', 'magnetic field', 'mimicry', 'nervous system disorder', 'neural correlate', 'neuroimaging', 'neuromechanism', 'novel', 'prevent', 'relating to nervous system', 'response', 'showing emotion', 'social']",NIMH,YALE UNIVERSITY,R01,2020,400249,0.04929243783582008
"Dynamics of Large-Scale Networks During Emotional and Social Processing The goal of this application is to develop a research program that, as stated in the call entitled The Neural Mechanisms of Multi-Dimensional Emotional and Social Representation (RFA-MH- 17-300), incorporates innovative approaches designed to move the fields of affective and social neuroscience beyond single region-based, modular, and static models of brain function and behavior. The RFA calls for research that is multi-dimensional, that is, that investigates the role of (among others) complex contexts, as well as distributed and/or dynamic processes that unfold over time. The objective of the present application is to jointly investigate emotional and social processes in a richly multi-dimensional manner. Aim 1: Network organization and evolution during emotional and social processing. The objective of this aim is to uncover how large-scale brain networks are organized and evolve temporally during emotional and social processing. Networks will include brain regions that robustly respond to the tasks proposed, and regions from well characterized networks, including the salience, executive control, and task-negative networks. Aim 2: Naturalistic processing during emotional and social processing. The objective of this aim is to understand naturalistic processing of emotional and social information. Although standard experimental designs afford great control over experimental conditions, they lack ecological validity and restrict the experiments that can be studied. We propose to investigate continuous (“naturalistic”) processing during movie watching involving emotional and social content. Continuous processing will be investigated via intersubject correlation analysis, which measures the extent to which signals are correlated across participants. Aim 3: Development of network organization/evolution and naturalistic processing. The objective of this aim is to investigate multi-dimensional emotional and social processes from a developmental perspective. Most developmental research in emotion has focused on observing amygdala responses and those of a few other brain regions during face perception. We focus on a question largely neglected in prior research, specifically sustained threat processing and the involvement of the bed nucleus of the stria terminalis. In the context of social processing, this aim examines the development of intersubject synchrony. Across the emotional and social domains, we propose to study middle childhood (8-9 y), early adolescence (12-13 y), and young adulthood (18-19 y). The goal of this application is to investigate the neural mechanisms of multi-dimensional emotional and social representation. Functional MRI experiments are proposed to investigate the following general questions: organization and evolution of brain networks; naturalistic processing during movie watching; and development.",Dynamics of Large-Scale Networks During Emotional and Social Processing,9883647,R01MH112517,"['Adolescence', 'Adopted', 'Affective', 'Amygdaloid structure', 'Anxiety', 'Behavior', 'Brain', 'Brain region', 'Complex', 'Data', 'Development', 'Developmental Process', 'Disease', 'Emotional', 'Emotions', 'Evolution', 'Experimental Designs', 'Functional Magnetic Resonance Imaging', 'Goals', 'Individual', 'Individual Differences', 'Lead', 'Link', 'Machine Learning', 'Measures', 'Methods', 'Modeling', 'Monitor', 'Mood Disorders', 'Participant', 'Pathway Analysis', 'Persons', 'Process', 'Property', 'Research', 'Risk', 'Role', 'Series', 'Signal Transduction', 'Social Development', 'Social Environment', 'Social Interaction', 'Social Processes', 'Stimulus', 'Structure', 'Structure of terminal stria nuclei of preoptic region', 'Testing', 'Time', 'Work', 'affective neuroscience', 'age group', 'autism spectrum disorder', 'base', 'design', 'early adolescence', 'executive function', 'experimental study', 'face perception', 'graph theory', 'improved', 'innovation', 'middle childhood', 'movie', 'neglect', 'neurobiological mechanism', 'neuromechanism', 'novel', 'peer', 'positive mood', 'programs', 'relating to nervous system', 'response', 'social', 'social neuroscience', 'young adult']",NIMH,"UNIV OF MARYLAND, COLLEGE PARK",R01,2020,675503,0.10500034482042042
"Interaction of Emotional Perception and Visual Attention The understanding of the brain mechanisms of emotion and motivation has grown steadily in the past two decades. The majority of the work in the rodent and human literatures has relied heavily on particular paradigms, such as the perception of faces with emotion content in humans, and aversive conditioning procedures in both humans and animals. The present application proposes a series of experimental paradigms that are ecologically inspired, semi- naturalistic, and dynamic.  The objective of this application is to investigate the dynamics of threat processing during escape behaviors, as well as the dynamics of threat/reward processing during approach- avoid conflict paradigms. Across experiments, we seek to both characterize and test specific hypotheses centered around a set of brain regions implicated in aversive and appetitive processing. We seek to characterize their contributions individually but also as distributed circuits that collectively and dynamically support behaviors.  The proposed work is organized around two aims. Aim 1 investigates the circuits supporting threat escape. Rodent studies suggest that the ventral striatum/accumbens is an important node during escape behaviors. Whereas some work in humans (e.g., avoidance conditioning) supports this notion, other work with ethologically inspired paradigms has also revealed the participation of regions such as the mid-cingulate cortex and the anterior hippocampus. Experiments will test the contributions of these and other brain regions to escape mechanisms. Aim 2 investigates the circuits involved in aversive and appetitive interactions during dynamic threat and reward processing. Experiments will employ dynamic stimuli where the proximity to threat and reward vary dynamically.  Collectively, the work addresses a set of basic research questions aimed at understanding how emotion/motivation circuits outlined in the past decades supports dynamic processing. The potential results may inform the clinically-oriented human literature, which has been heavily informed and inspired by research with standard experimental paradigms. This project aims to study emotion and motivation in the human brain of typical participants with functional magnetic resonance imaging. We will study mechanisms of escaping from threat and approaching reward and how they interact. The project attempts to contribute to the understanding to processes that support many human behaviors, and may inform potential treatments of conditions such as anxiety disorders and substance abuse.",Interaction of Emotional Perception and Visual Attention,10058351,R01MH071589,"['Address', 'Amygdaloid structure', 'Animals', 'Anterior', 'Anxiety Disorders', 'Area', 'Aversive Stimulus', 'Basic Science', 'Behavior', 'Behavioral', 'Brain', 'Brain region', 'Calcium', 'Clinical', 'Cognition', 'Coin', 'Collaborations', 'Communities', 'Conflict (Psychology)', 'Data', 'Data Analyses', 'Desire for food', 'Development', 'Dimensions', 'Emotional', 'Emotions', 'Functional Magnetic Resonance Imaging', 'Grant', 'Graph', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Insula of Reil', 'Knowledge', 'Link', 'Literature', 'Location', 'Machine Learning', 'Motivation', 'Movement', 'Neurons', 'Participant', 'Peer Review', 'Perception', 'Positioning Attribute', 'Prefrontal Cortex', 'Procedures', 'Process', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Rewards', 'Rodent', 'Role', 'Safety', 'Series', 'Stimulus', 'Structure of terminal stria nuclei of preoptic region', 'Substance abuse problem', 'Techniques', 'Testing', 'Time', 'Ventral Striatum', 'Vision', 'Visual attention', 'Work', 'aversive conditioning', 'brain behavior', 'cingulate cortex', 'classical conditioning', 'conditioning', 'experience', 'experimental study', 'face perception', 'gang', 'midbrain central gray substance', 'reward processing', 'stem', 'virtual']",NIMH,"UNIV OF MARYLAND, COLLEGE PARK",R01,2020,719764,0.019602534923465517
"Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease PROJECT ABSTRACT/SUMMARY The prevalence of psychiatric disorders has reached nearly epidemic proportions. Rates of common affective diseases (unipolar depression, anxiety and stress disorders) are high across the lifespan and these diseases place a tremendous social and economic burden on the individual and society. Clear evidence indicates that most affective disorders emerge at the intersection of pre-existing vulnerability and significant, highly stressful, life-events. However, current models of emotion-related risk do not adequately account for this confluence of biological, historical, and situational factors. In this investigation, we build upon our prior work demonstrating broad associations between flexible emotion processing and psychological health and adjustment, and in-flexible emotion and psychological risk and affective disease. Specifically, we will recruit 400 adults in hospital following a potentially traumatic event (e.g., accident, violence, fire, etc.) in order to model the influence of early emotion processing on trajectories of adjustment. We focus our investigation on the super-ordinate construct of Emotion flexibility (EF) which encompasses the ability to generate or up-regulate emotions, as well as to shift or down-regulate emotions according to needs and/or environmental demands. EF is well-suited to inform models of emotion-related risk and adjustment as it characterizes an optimal balance of two biologically-based, constituent dimensions: “bottom-up” threat-related processing and “top-down” cognitive control increasingly recognized as central to all emotion processing. We propose rigorous methods to assess EF and related processing in-vivo in lab and via experience sampling. Moreover, we will follow participants to 18 months post event so as to effectively model the association between emotion processing and trajectories of adjustment, while also considering established influences such as physical health status, psychiatric history, childhood maltreatment, daily stress/hassles, and social support. In particular, we will incorporate recent developments in advanced statistical modelling to better characterize the complex and interactive influence of historical and contemporary factors on moment-level emotion processing, EF and adjustment. Broadly, this project is in line with the most recent NIMH strategic plan and will contribute to more complex models of the most common affective diseases, including facilitating the charting of illness trajectories to help determine when, where, and how to intervene. Moreover, this research will directly examine how variation in key systems can influence emotion-processing and adjustment to aversive life events, fitting complex influences more directly into models of risk for the most common and burdensome affective diseases. PUBLIC HEALTH RELEVANCE/NARRATIVE Emotion-related psychiatric disorders, including depression and anxiety, affect a considerable portion of adults in this country and rank as many of the most burdensome diseases worldwide. In this investigation, we will follow an at-risk sample of adults in order to better understand how one key pathway, relating to how individuals process emotion, influences risk for emotion-related diseases over time. In addition, we test the role by which certain other factors, both contemporary and historical (physical health, life stress, social support, psychiatric treatment history, or childhood experiences) may increase or decrease risk via this particular pathway.",Unpacking Emotion Inflexibility and Prospective Prediction of Affective Disease,9840518,R01MH113622,"['Accidents', 'Adult', 'Affect', 'Affective', 'Anxiety', 'Anxiety Disorders', 'Behavioral', 'Biological', 'Categories', 'Child Abuse and Neglect', 'Childhood', 'Clinical', 'Clinical Sciences', 'Complex', 'Country', 'Derivation procedure', 'Development', 'Diagnostic', 'Dimensions', 'Disease', 'Disease Progression', 'Early Intervention', 'Economic Burden', 'Emotions', 'Epidemic', 'Equilibrium', 'Event', 'Fire - disasters', 'Health', 'Health Status', 'Heritability', 'Hospitals', 'Individual', 'Inherited', 'Investigation', 'Life', 'Life Stress', 'Longevity', 'Mental Depression', 'Mental Health', 'Mental disorders', 'Methodology', 'Methods', 'Modeling', 'Mood Disorders', 'National Institute of Mental Health', 'Nature', 'Participant', 'Pathway interactions', 'Patients', 'Pattern', 'Phenotype', 'Post-Traumatic Stress Disorders', 'Prevalence', 'Process', 'Psychiatric therapeutic procedure', 'Psychological adjustment', 'Qualifying', 'Recording of previous events', 'Regulation', 'Research', 'Research Domain Criteria', 'Risk', 'Risk Adjustment', 'Risk Factors', 'Role', 'Sampling', 'Shapes', 'Sleep disturbances', 'Social support', 'Societies', 'Statistical Models', 'Strategic Planning', 'Stress', 'Symptoms', 'System', 'Testing', 'Time', 'Unipolar Depression', 'Variant', 'Violence', 'Work', 'base', 'childhood adversity', 'clinical diagnostics', 'clinically relevant', 'cognitive control', 'emotion regulation', 'experience', 'flexibility', 'improved', 'in vivo', 'indexing', 'laboratory experience', 'longitudinal design', 'machine learning method', 'negative mood', 'network models', 'physical conditioning', 'post-traumatic stress', 'prospective', 'psychologic', 'public health relevance', 'recruit', 'response', 'social', 'stress disorder', 'tool', 'traumatic event']",NIMH,KENT STATE UNIVERSITY,R01,2020,573248,0.1142494328799957

text,title,id,project_number,terms,administration,organization,mechanism,year,funding
"Transcriptional Regulatory Networks of Craniofacial Development Abstract Human craniofacial development is a complex process and frequently goes awry to cause a major class of birth defects, orofacial clefting, which affects approximately 1 in 700 live births. Proper facial development in mouse and human requires three sets of paired facial prominences coming together by growth, morphogenesis, and fusion. Embryonic facial development is strikingly similar in human and mouse, making the mouse the best available model system for human. Previous studies have shown that the expression of many thousands of genes changes across tissue layer, age, and/or prominence, as well as cell population during early mouse facial development. However, we still only have a rudimentary understanding of how these changes are regulated by the interaction of transcriptional modulators in the developing face. To understand how genes are transcriptionally regulated during facial development, this research seeks to construct transcriptional regulatory networks in a temporospatial manner by in silico analysis of publicly available multi- omic datasets. Aim 1 will focus on the identification and verification of transcriptional regulatory networks operating in facial mesenchyme with a focus on super-enhancers. Aim 2 will adopt a similar approach to study the ectoderm which acts as a vital signaling center for the mesenchyme. Finally, in Aim 3 I will apply knowledge from Aims 1 and 2 to build transcriptional regulatory networks at the single cell level. These aims will take advantage of available RNA-seq, ATAC-seq, histone marker ChIP-seq, transcription factor ChIP-seq, bulk and single cell RNA-seq data from wild-type or mutant mice, as well as facial enhancer expression databases. Accomplishment of these studies will predict how genes are transcriptionally regulated in a temporospatial manner during facial development and discover sets of core transcription factors and super- enhancers controlling facial development. These transcriptional regulatory networks will be relevant to the genetic and molecular underpinnings of human orofacial clefting, and will provide clear testable predictions about transcription factor function and the consequences of aberrant expression. Performance and accomplishment of these Aims will also act as a major component of my career development plan, in which my goal is to obtain and independent tenure-track faculty position and serve as a mentor to the next generation of scientists. A major aspect of my career development plan is to build on my growing strength in bioinformatics by learning more advanced techniques in this specialty alongside new computational based approaches, such as machine learning. In this respect, my Aims and career development plan are aligned with a Notice of Special Interest (NOSI) of NIDCR in Supporting Dental, Oral, and Craniofacial Research Using Bioinformatic, Computational, and Data Science Approaches (NOT-DE-20-006) for which this application is targeted. I have recruited a mentorship team with specialties in craniofacial biology, bioinformatics, machine learning, and career development to help me achieve these goals. Project Narrative Human craniofacial development is a complex process and requires suites of genes to be switched on and off at appropriate times and spaces during formation of the embryo. There is now a wealth of data available in public databases concerning which genes are expressed when and where during mammalian facial development, including for the transcription factors which are the proteins that regulate these critical expression programs, but we have not yet begun to connect this information together to derive logical predictions about the critical networks responsible for craniofacial development. This proposal will address that gap using both computational and laboratory-based methods to derive and verify transcriptional regulatory networks relevant to the genetic and molecular underpinnings of normal facial development as well as how these are disrupted to cause defects such as human orofacial clefting.",Transcriptional Regulatory Networks of Craniofacial Development,10284443,K01DE030923,"['ATAC-seq', 'Address', 'Adopted', 'Affect', 'Age', 'Automobile Driving', 'Bioinformatics', 'Biological Assay', 'Biological Models', 'Biology', 'Cartilage', 'Cells', 'ChIP-seq', 'Complex', 'Computational Science', 'Computer Models', 'Congenital Abnormality', 'Data', 'Data Science', 'Data Set', 'Databases', 'Defect', 'Dental', 'Dependence', 'Development', 'Development Plans', 'Ectoderm', 'Embryo', 'Enhancers', 'Face', 'FaceBase', 'Faculty', 'Foundations', 'Future', 'Gene Expression', 'Genes', 'Genetic', 'Genetic Models', 'Genetic Transcription', 'Genome', 'Genomics', 'Goals', 'Growth', 'Histones', 'Human', 'Human Genetics', 'Instruction', 'Knowledge', 'Laboratories', 'Learning', 'Live Birth', 'Machine Learning', 'Mentors', 'Mentorship', 'Mesenchymal', 'Mesenchyme', 'Methods', 'Molecular', 'Morphogenesis', 'Mus', 'Muscle', 'Mutant Strains Mice', 'National Institute of Dental and Craniofacial Research', 'Oral', 'Pathology', 'Pattern', 'Performance', 'Population', 'Positioning Attribute', 'Process', 'Proteins', 'Regulatory Element', 'Research', 'Resources', 'Scientist', 'Signal Pathway', 'Signal Transduction', 'Solid', 'Study models', 'System', 'Techniques', 'Technology', 'Time', 'Tissues', 'Transcriptional Regulation', 'Transgenic Organisms', 'Validation', 'Wild Type Mouse', 'base', 'bone', 'career', 'career development', 'cell type', 'craniofacial', 'craniofacial development', 'critical period', 'design', 'differential expression', 'in silico', 'interest', 'medical specialties', 'mouse model', 'multiple omics', 'network models', 'next generation', 'orofacial cleft', 'programs', 'promoter', 'recruit', 'research and development', 'single-cell RNA sequencing', 'spatiotemporal', 'tenure track', 'transcription factor', 'transcriptome', 'transcriptome sequencing']",NIDCR,UNIVERSITY OF COLORADO DENVER,K01,2021,130135
"Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases ï»¿    DESCRIPTION (provided by applicant): American Sign Language (ASL) grammar is specified by the manual sign (the hands) and by the nonmanual components, which include the face. Our general hypothesis is that nonmanual facial articulations perform significant semantic and syntactic functions by means of a more extensive set of facial expressions than that seen in other communicative systems (e.g., speech and emotion). This proposal will systematically study this hypothesis. Specifically, we will study the following three hypotheses needed to properly answer the general hypothesis stated above: First, we hypothesize (H1) that the facial muscles involved in the production of clause-level grammatical facial expressions in ASL and/or their intensity of activation are more extensive than those seen in speech and emotion. Second, we hypothesize (H2) that the temporal structure of these facial configurations are more extensive than those seen in speech and emotion. Finally, we hypothesize (H3) that eliminating these ASL nonmanual makers from the original videos, drastically reduces the chances of correctly identifying the clause type of the signed sentence. To test these three hypotheses, we define a highly innovative approach based on the design of computational tools for the analysis of nonmanuals in signing. In particular, we will examine the following three specific aims. In Aim 1, we will build a series of computer algorithms that allow us to automatically (i.e., without the need of any human intervention) detect the face, its facial features as well as the automatic detection of the movements of the facial muscles and their intensity of activation. These tools will be integrated into ELAN, a standard software used for linguistic analysis. These tools will then be used to test six specific hypotheses to successfully study H1. In Aim 2, we define computer vision and machine learning algorithms to identify the temporal structure of ASL facial configurations and examine how these compare to those seen in speech and emotion. We will study six specific hypotheses to successfully address H2. Alternative hypotheses are defined in both aims. Finally, in Aim 3 we define algorithms to automatically modify the original videos of facial expression in ASL to eliminate the identified nonmanual markers. Native users of ASL will complete behavioral experiments to examine H3 and test potential alternative hypotheses. Comparative analysis with non-signer controls will also be completed. These studies will thus further validate H1 and H2. We provide evidence of our ability to successfully complete the tasks in each of these aims. These aims address a critical need; at present, the study of nonmanuals must be carried out by hand. To be able to draw conclusive results, it is necessary to study thousands of videos. The proposed computational approach supposes at least a 50-fold reduction in time compared to methods done by hand. PUBLIC HEALTH RELEVANCE: Deafness limits access to information, with consequent effects on academic achievement, personal integration, and life-long financial situation, and also inhibits valuable contributions by Deaf people to the hearing world. The public benefit of our research includes: (1) the goal of a practical and useful device to enhance communication between Deaf and hearing people in a variety of settings; and (2) the removal of a barrier that prevents Deaf individuals from achieving their full potential. An understanding of the nonmanuals will also change how ASL is taught, leading to an improvement in the training of teachers of the Deaf, sign language interpreters and instructors, and crucially parents of deaf children.",Computational Methods for the Study of American Sign Language Nonmanuals Using Very Large Databases,9841303,R01DC014498,"['3-Dimensional', 'Academic achievement', 'Access to Information', 'Address', 'Agreement', 'Algorithms', 'American Sign Language', 'Articulation', 'Behavioral', 'Child', 'Code', 'Communication', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Computing Methodologies', 'Databases', 'Detection', 'Devices', 'Emotions', 'Excision', 'Face', 'Facial Expression', 'Facial Muscles', 'Goals', 'Hand', 'Head', 'Hearing', 'Human', 'Image', 'Individual', 'Intervention', 'Life', 'Linguistics', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Movement', 'Parents', 'Pattern Recognition', 'Production', 'Research', 'Research Personnel', 'Science', 'Semantics', 'Series', 'Shapes', 'Sign Language', 'Signal Transduction', 'Specific qualifier value', 'Speech', 'Structure', 'System', 'Teacher Professional Development', 'Technology', 'Testing', 'Time', 'Visual', 'Visual system structure', 'base', 'body position', 'comparative', 'computerized tools', 'deaf', 'deafness', 'design', 'experience', 'experimental study', 'face perception', 'innovation', 'instructor', 'interest', 'machine learning algorithm', 'prevent', 'public health relevance', 'reconstruction', 'showing emotion', 'syntax', 'tool']",NIDCD,OHIO STATE UNIVERSITY,R01,2021,317844

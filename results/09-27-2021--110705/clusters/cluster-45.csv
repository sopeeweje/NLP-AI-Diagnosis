text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9747977,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,115051,0.0023838122583164055
"Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images ABSTRACT Manual analysis of biomedical images by researchers and pathologists has the potential to introduce bias and error that compromise the reliability of research and clinical findings. These problems are significant barriers to delivering the most beneficial evidence-based medicine, developing effective medical treatments, and promoting confidence in scientific inquiry. Identification of biomarkers and cellular targets following microscopy requires manual analysis of biomedical images, which is time intensive, difficult, and prone to bias and errors. Unintentional bias and attentional limitations during analysis of biomarkers can underlie poor reproducibility of findings in biomedical research and potentially introduce error in clinical diagnostics. We recently developed a “beta” software package designed to improve automation and standardization of image analysis, called “PIPSQUEAK” (Perineuronal net Intensity Program for the Standardization and Quantification of Extracellular matrix Analysis Kit). Since its publication in 2016, PIPSQUEAK beta has amassed approximately 1,300 users worldwide who use it to quantify the intensity and number of perineuronal nets and other neural markers in the brain. This technology significantly increases data reliability between image raters and decreases the time required for analysis by more than 100-fold. However, PIPSQUEAK beta currently uses target detection algorithms that require high-contrast images to automatically identify neurons as clusters of bright pixels on dark backgrounds. A significant current limitation to PIPSQUEAK beta, and other available imaging programs, is that detection of biomarkers can be difficult unless image conditions are ideal. Suboptimal conditions, like high background staining, off-target structures, overlapping or clustered biomarkers, and atypical morphologies, can lead to artifacts and consequently to inaccurate results and erroneous conclusions. Here, we propose to develop a user-friendly artificial intelligence (AI) platform for the automated detection of targeted biomarkers in digital microscopy that reduces this error by learning to distinguish between true cellular biomarkers and artifacts. We propose to integrate AI capabilities into our PIPSQUEAK technology to produce an adaptive, high-throughput, biomedical image analysis platform that quickly and accurately identifies biomarker targets from bench to bedside. A key advantage is that this AI program will be user friendly and available online, making it highly accessible to basic researchers and to technicians and clinicians identifying human pathologies. Thus, successful development of our AI program has a high translational potential. The goal of this proposal is 1) to develop and validate a machine learning model that is capable of detecting common histological marker morphologies in digital microscopy, and 2) to test the feasibility of adapting our AI platform to new biomarker datasets with minimal additional supervised training. Our end goal is to advance the reliability and speed of research findings and clinical diagnoses by making this technology widely available to researchers and clinicians. PROJECT NARRATIVE Manual analysis of biomedical images by researchers and pathologists has the potential to introduces bias and error that compromise the reliability of research and clinical findings; problems which are significant barriers to delivering the most beneficial evidence-based medicine and developing effective medical treatments. Application of artificial intelligence for the detection of disease or cellular targets has the potential to improve the reliability of research findings and clinical diagnoses, while reducing waste, time, and expense. We propose a method to improve the quality of biomedical research reproducibility and clinical diagnoses by developing a high-throughput, adaptive artificial intelligence platform for automated analysis of cellular and disease targets in digital microscopy images, which will be made available to scientists and clinicians as a user-friendly analysis platform.",Development of an adaptive machine learning platform for automated analysis of biomarkers in biomedical images,9845994,R43GM134789,"['Abbreviations', 'Algorithms', 'Artificial Intelligence', 'Attention', 'Automation', 'Biological Markers', 'Biomedical Research', 'Brain', 'Cell Line', 'Cell model', 'Cellular Morphology', 'Clinical', 'Computer software', 'Confocal Microscopy', 'Coupled', 'Custom', 'Data', 'Data Set', 'Detection', 'Development', 'Disease', 'Evidence Based Medicine', 'Extracellular Matrix', 'FOS gene', 'Fluorescence', 'Future', 'Glial Fibrillary Acidic Protein', 'Goals', 'Histologic', 'Histology', 'Human Pathology', 'Image', 'Image Analysis', 'Immunoassay', 'Immunohistochemistry', 'Lead', 'Learning', 'Location', 'Machine Learning', 'Manuals', 'Measurement', 'Medical', 'Methods', 'Microscopy', 'Modeling', 'Morphologic artifacts', 'Morphology', 'Neurons', 'Nuclear', 'Pathologist', 'Performance', 'Procedures', 'Psychological Transfer', 'Publications', 'Rattus', 'Reproducibility', 'Reproducibility of Results', 'Research', 'Research Personnel', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Shapes', 'Speed', 'Stains', 'Standardization', 'Structure', 'Supervision', 'Techniques', 'Technology', 'Testing', 'Time', 'Tissue Banks', 'Tissue Model', 'Tissue imaging', 'Tissues', 'Training', 'Zebrafish', 'automated analysis', 'base', 'bench to bedside', 'bioimaging', 'biomarker identification', 'cell type', 'cellular targeting', 'clinical Diagnosis', 'clinical diagnostics', 'contrast imaging', 'design', 'digital', 'digital imaging', 'extracellular', 'histological specimens', 'histological stains', 'imaging biomarker', 'imaging program', 'improved', 'interest', 'lateral line', 'microscopic imaging', 'predictive marker', 'programs', 'relating to nervous system', 'software as a service', 'statistics', 'targeted biomarker', 'tool', 'user-friendly', 'wasting']",NIGMS,"REWIRE NEUROSCIENCE, LLC",R43,2019,224915,-0.0009150105444169612
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9771473,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Hazard Models', 'Health system', 'Individual', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning algorithm', 'learning strategy', 'multimodality', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'prognostic', 'repository', 'response', 'risk prediction model', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2019,251983,0.02606704824256014
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9645433,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2019,92070,-0.02353007994929919
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9743225,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2019,528283,0.01596308472639645
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9636581,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2019,489919,0.01647639787577346
"2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT SUMMARY/ABSTRACT A 1.5-day symposium, StoneLab will be held December 6-7, 2019 at AUA Headquarters in Linthicum, Maryland, near BWI Airport. The novel StoneLab Symposium will be a meeting where kidney stone researchers can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, medical device design and development. We plan to encourage researchers to “think outside the box” in the search for advances in kidney stone treatment. The target audience for this meeting includes: urologists, nephrologists, basic scientists, early-career investigators, residents, research fellows, and project management members of urology research teams in the area of nephrolithiasis and related technology development. It is essential that the urology community, and especially trainees, early-career physician- scientists, and researchers be provided with accessible opportunities to gain understanding of new knowledge and recent advances in not only their own fields, but in related fields that they may not otherwise be aware of, and leverage this for new research frameworks that can help improve the treatment of kidney stone disease. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons. The intent of these discussions is to spur the development of collaborations and the generation of new ideas and avenues for exploration. Another goal of the meeting is to help identify future funding needs and growth areas for kidney stone research. The R13 support requested in this application will encourage early investigators to participate in the workshop and stimulate their development as researchers and surgeon- scientists. This symposium will have a major impact by bringing together premier kidney stone researchers, along with experts in nephrology, biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development, to explore solutions for the most pressing translational science issues facing kidney stone researchers today. The Principal Investigator (PI), Carolyn J.M. Best, PhD, is AUA Director of Research. The Program Planning Committee consists of a diverse multidisciplinary team of distinguished scientists and urologists, most of whom also serve in leadership roles of several AUA-affiliated subspecialty societies: Khurshid Ghani, MD, MS (Chair); Ben H. Chew, MD, MSc (Co-Chair); Thomas Chi, MD; Benjamin Canales, MD, MPH; Gary Curhan, MD, ScD; Amy Krambeck, MD; Dirk Lange, PhD; Manoj Monga, MD, FACS; Kristina Penniston, PhD,RD; and Aria Olumi, MD (ex-officio). 2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE  PROJECT NARRATIVE/PUBLIC HEALTH RELEVANCE STATEMENT This application seeks support to enable the American Urological Association (AUA) to invite early-career investigators (and late-stage trainees) to participate in a novel interdisciplinary symposium where premier kidney stone researchers, along with experts in nephrology, can come together with experts in biomedical engineering, physics, chemistry, artificial intelligence, and medical device design and development with the aim of “thinking outside the box” in the search for advances in kidney stone treatment. This meeting will explore solutions for the most pressing translational science issues facing kidney stone researchers today, research in interdisciplinary fields that might be leveraged for new research to improve treatment of kidney stone disease, and help identify future funding needs and growth areas for kidney stone research. A critical aspect of the StoneLab Sympoisum is the delivery of talks from outside the normal realm of kidney stone research by leading basic scientists, followed by discussions led by kidney stone researchers and surgeons with the intent to spur the development of collaborations and the generation of new ideas and avenues for exploration.",2019 STONE LAB SCIENTIFIC SYMPOSIUM: THINKING OUTSIDE THE BOX FOR KIDNEY STONE DISEASE,9914630,U13DK124023,"['American', 'Area', 'Artificial Intelligence', 'Award', 'Awareness', 'Basic Science', 'Big Data', 'Big Data Methods', 'Biology', 'Biomedical Engineering', 'Chemistry', 'Collaborations', 'Communities', 'Development', 'Development Plans', 'Device or Instrument Development', 'Discipline', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Fostering', 'Funding', 'Future', 'Generations', 'Genomics', 'Goals', 'Grant', 'Growth', 'Kidney Calculi', 'Knowledge', 'Lead', 'Leadership', 'Machine Learning', 'Maryland', 'Medical', 'Medical Device Designs', 'Methodology', 'Miniaturization', 'Nephrolithiasis', 'Nephrology', 'North America', 'Operative Surgical Procedures', 'Optics', 'Physicians', 'Physics', 'Prevalence', 'Principal Investigator', 'Privatization', 'Research', 'Research Personnel', 'Robotics', 'Role', 'Science', 'Scientist', 'Societies', 'Surgeon', 'Technology', 'Translational Research', 'Travel', 'United States', 'Urologist', 'Urology', 'career', 'career development', 'improved', 'innovation', 'interest', 'medical specialties', 'meetings', 'member', 'multidisciplinary', 'next generation', 'next generation sequencing', 'novel', 'programs', 'public health relevance', 'symposium', 'technology development', 'urologic']",NIDDK,AMERICAN UROLOGICAL ASSOCIATION,U13,2019,10000,-0.0015916593467909122
"New approaches to optimizing the application and measuring the impact of community-based tuberculosis interventions PROJECT SUMMARY Globally, tuberculosis (TB) kills more people each year than any other infectious disease, but around 40% of TB cases go undiagnosed each year. This leads to continued transmission and a slow rate of decline in global TB incidence. Community-based TB screening interventions are one strategy for increasing the early diagnosis and treatment of people with tuberculosis. Knowledge is lacking on when community-based TB screening interventions are likely to be fruitful and on the impact of these interventions on TB transmission. This study addresses the first of these knowledge gaps by identifying epidemiologic signals that predict high yields for community-based TB screening, and by assessing barriers to optimal implementation of community-based screening programs. It addresses the second knowledge gap by measuring transmission within communities that receive TB screening interventions, and by identifying potential sites of transmission both within and outside the community. Aim 1 applies random forest regression to data from a community-based screening program to create a decision tree that uses information about past TB patients to predict whether a commnity is likely to have high levels of undiagnosed TB in the present. Aim 2 is a mixed-methods study focusing on people who were missed by TB screening interventions, which will help understand the barriers to successful implementation. Aim 3 uses whole-genome sequencing of clinical isolates to determine the proportion of TB cases attributable to recent transmission within intervention communities, using this metric to evaluate the impact of screening interventions on transmission. Aim 4 uses social network analysis to identify sites of transmission within and outside intervention communities. The long-term goal of this work is to reduce global TB morbidity through improved community-based screening. PROJECT NARRATIVE This study seeks to understand (a) when community-based tuberculosis screening interventions are likely to be fruitful and (b) the impact these interventions have on tuberculosis transmission. This knowledge will improve community-based tuberculosis screening interventions and thus contribute to the reduction of global tuberculosis morbidity.",New approaches to optimizing the application and measuring the impact of community-based tuberculosis interventions,9781984,DP2MD015102,"['Address', 'Communicable Diseases', 'Communities', 'Data', 'Decision Trees', 'Early Diagnosis', 'Early treatment', 'Epidemiology', 'Goals', 'Incidence', 'Intervention', 'Knowledge', 'Measures', 'Methods', 'Morbidity - disease rate', 'Pathway Analysis', 'Patients', 'Signal Transduction', 'Site', 'Social Network', 'Tuberculosis', 'Work', 'base', 'clinical sequencing', 'community intervention', 'improved', 'novel strategies', 'random forest', 'screening', 'screening program', 'transmission process', 'whole genome']",NIMHD,BRIGHAM AND WOMEN'S HOSPITAL,DP2,2019,2353629,0.00113907712963479
"Identifying individuals at risk of progression to active tuberculosis Project Summary Almost 2 billion people are infected with Mycobacterium tuberculosis (Mtb), the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no clinical test to distinguish those that will progress to active TB disease, from those that will not. If we are to realize the World Health Organization's (WHO) goal of a world free of TB by 2035, the massive reservoir of TB infection must be addressed with a cost-effective, ethical therapy for preventing progression, based on treating only those most likely to progress. A diagnostic test that can accurately predict the risk of progression is critical for treating these high-risk individuals and the eradication of TB. Our goal is to develop such an assay. Our central hypothesis is that five independent host immune biomarkers, combined into a single multimetric signature will predict progression from latent to active TB with at least 90% sensitivity and specificity. We will test this hypothesis and achieve our goal by implementing the following specific aims: Aim 1: Compile a comprehensive dataset of biomarkers in a prospective cohort of individuals who are at risk of progressing to active TB. Working with the Moldova Ministry of Health's National TB Program, we will enroll 3,685 close contacts of active TB cases. All participants will be followed for two years to determine who progresses to active TB. We expect to identify ≥ 140 progressors. We will assess three previously established blood-based predictors of active TB progression, and two novel assays. We will verify the performance of previously published biomarkers in this population to discriminate progressors from non-progressors and identify new candidate biomarkers using RNA-Seq of antigen stimulated PBMC and detection of Mtb-peptides by NanoDisk MS. Aim 2: Use a discovery set of samples to develop predictive models of progression to active TB. Using data from 140 progressors and 140 non-progressors from Aim 1 we will (1) Verify the performance of existing biomarkers, (2) Use a cross-validation to identify new candidate biomarkers, and (3) derive predictive models using logistic regression and machine learning methods to identify optimal biomarker signatures that best predict progression to active TB within 12 months. Aim 3: Verify the ability of the model to predict progression to active TB disease. Using the same approach as Aim 1, we will enroll a new set of 1,340 household contacts of active TB and identify at least 60 progressors and 60 matched non-progressors and verify clinically the sensitivity/specificity of our models and biosignatures (Aim 2) to predict progression to active disease. A combined host biomarker signature that can predict TB progression from a small blood volume will have significant impact on the WHO End TB Program. PROJECT NARRATIVE Almost 2 billion people are infected with Mycobacterium tuberculosis, the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no test to distinguish those that will progress from those that will not. We propose to develop a multimetric signature of host biomarkers that together will have a sensitivity and specificity of ≥ 90% for predicting progression to active TB in one year, a critical first step to developing cost-effective and ethical treatment plans in order to reach the World Health Organization goal of Ending TB by 2035.",Identifying individuals at risk of progression to active tuberculosis,9654700,R01AI137681,"['Address', 'Antigens', 'Biological Assay', 'Biological Markers', 'Blood', 'Blood Volume', 'Cells', 'Characteristics', 'Child', 'Clinical', 'Clinical Sensitivity', 'Data', 'Data Set', 'Detection', 'Diagnostic tests', 'Disease', 'Enrollment', 'Ethics', 'Event', 'Filtration', 'Flow Cytometry', 'Foundations', 'Freezing', 'Frequencies', 'Gender', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Goals', 'Health', 'Household', 'Immune', 'Immune response', 'Immunologic Markers', 'Individual', 'Interferons', 'Logistic Regressions', 'Lymphocyte', 'Machine Learning', 'Modeling', 'Moldova', 'Mycobacterium tuberculosis', 'Mycobacterium tuberculosis antigens', 'National Health Programs', 'Organizational Objectives', 'Outcomes Research', 'Participant', 'Patients', 'Peptide Fragments', 'Peptides', 'Performance', 'Peripheral Blood Mononuclear Cell', 'Plasma', 'Population', 'Procedures', 'Production', 'Prospective cohort', 'Proteins', 'Publications', 'Publishing', 'RNA', 'Research Personnel', 'Risk', 'Sampling', 'Sensitivity and Specificity', 'Specificity', 'T cell response', 'T-Lymphocyte', 'Testing', 'Tuberculosis', 'Validation', 'World Health Organization', 'age group', 'base', 'biobank', 'biomarker performance', 'biosignature', 'blood-based biomarker', 'candidate marker', 'classification algorithm', 'clinical Diagnosis', 'cohort', 'cost effective', 'deep neural network', 'enzyme linked immunospot assay', 'falls', 'follow-up', 'high risk', 'indexing', 'innovation', 'learning strategy', 'monocyte', 'nanodisk', 'novel', 'novel diagnostics', 'predictive modeling', 'predictive test', 'prevent', 'programs', 'random forest', 'research clinical testing', 'transcriptome sequencing', 'transmission process', 'treatment planning']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2019,715056,0.012964602675237098
"IEEE International Symposium on Biomedical Imaging Project Summary This proposal requests funds to provide travel support for graduates to attend and participate in the IEEE Inter- national Symposium on Biomedical Imaging (ISBI) 2019 conference, Venice, Italy on April 08-11, 2019. The main objective of the IEEE ISBI is to bring together researchers with interests in mathematical and computa- tional aspects of biomedical imaging, with a focus on addressing problems of significance to the development and application of imaging systems across spatial scales, from microscopy to whole-body imaging. ISBI partici- pants – on the order of 600-700 from across the world are involved in biomedical imaging research and development in academic institutions, government laboratories, or R&D departments of private companies. ISBI is co-sponsored by two IEEE societies: Signal Processing Society (SPS) and Engineering in Medicine and Biology Society (EMBS), representing academia, industry, and healthcare, and considered the world's foremost societies in biomedical engineering and imaging. SPS and EMBS publish IEEE Transactions on Medical Imag- ing, Transactions on Image Processing, Transactions on Biomedical Engineering, Transactions on Computational Imaging, and IEEE Journal of Biomedical and Health Informatics, among others. Since incep- tion in 2002, ISBI has become the leading international conference bringing together researchers from diverse algorithmic fields, applications, modalities, and size scales, to facilitate cross-fertilization of ideas across imag- ing modalities and scales. Conference topics include physical, biological and statistical modeling, image formation and reconstruction, computational and statistical image analysis, visualization and image quality as- sessment, and artificial intelligence and machine learning for big image data. ISBI, like other IEEE SPS and EMBS conferences, requires submission and review of a 4-page paper. Peer reviews are handled by a 50-mem- ber editorial board (area editors) of leading experts in the community, who in turn assign papers to well- qualified reviewers. All oral and poster papers are published in IEEExplore as Proceedings of ISBI. If awarded, IEEE anticipates the primary impact of this R13 grant will be increased attendance of U.S.-based students, postdoctoral fellows, and early career faculty. By offering to cover a significant portion of attendee's travel expenses, the cost-benefit ratio for attending ISBI 2019 will be extremely favorable. Furthermore, IEEE will award travel grants based on need and scientific excellence, creating opportunities for those early career researchers who have accepted papers (of which less than 50% are accepted to ISBI) and who have limited means to travel. IEEE will be particularly supportive in providing travel awards to women, under-represented groups, and persons with disabilities. Benefits can largely be summarized as “exposure” and education. ISBI provides opportunity for student exposure to many more areas of computational imaging research than generally available in her/his home institution, and concurrently provides opportunity for students to interact with leaders in the field through tutorials, plenary, oral, and poster presentations, and individual discussions. This proposal requests funds to provide travel support for graduate to attend and participate in the IEEE International Symposium on Biomedical Imaging (ISBI) 2019 conference, to be held in Venice, Italy on April 08-11, 2019. The main objective of the IEEE international Symposium on Biomedical Imaging is to bring together researchers with interests in the mathematical and computational aspects of biomedical imaging, with a focus on addressing problems of significance to the development and application of imaging systems across spatial scales, from microscopy to whole-body imaging. The conference covers biomedical imaging problems of high relevance to human health, and hence is of high relevance to the interests of the National Institute of Health.",IEEE International Symposium on Biomedical Imaging,9685477,R13EB027566,"['Academia', 'Address', 'Algorithms', 'Appointment', 'Area', 'Artificial Intelligence', 'Award', 'Biological Models', 'Biology', 'Biomedical Computing', 'Biomedical Engineering', 'Breeding', 'Budgets', 'Communities', 'Complement', 'Computational algorithm', 'Computer Simulation', 'Computer software', 'Costs and Benefits', 'Data', 'Development', 'Disabled Persons', 'Education', 'Engineering', 'Exposure to', 'Fertilization', 'Funding', 'Future', 'Generations', 'Goals', 'Government', 'Grant', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Imaging problem', 'Individual', 'Industry', 'Institution', 'International', 'Italy', 'Journals', 'Laboratories', 'Location', 'Machine Learning', 'Mathematics', 'Medical', 'Medical Imaging', 'Medicine', 'Methodology', 'Microscopic', 'Microscopy', 'Modality', 'Modeling', 'Oral', 'Paper', 'Participant', 'Peer Review', 'Postdoctoral Fellow', 'Privatization', 'Public Health Informatics', 'Publishing', 'Recommendation', 'Request for Proposals', 'Research', 'Research Personnel', 'Series', 'Societies', 'Statistical Models', 'Students', 'System', 'Training', 'Transact', 'Travel', 'Underrepresented Groups', 'United States National Institutes of Health', 'Woman', 'authority', 'base', 'biocomputing', 'bioimaging', 'biomedical informatics', 'body system', 'career', 'computerized tools', 'cost', 'early-career faculty', 'editorial', 'graduate student', 'image processing', 'imaging modality', 'imaging system', 'innovation', 'interest', 'meetings', 'member', 'physical model', 'posters', 'programs', 'reconstruction', 'research and development', 'signal processing', 'student participation', 'success', 'supportive environment', 'symposium', 'whole body imaging']",NIBIB,UNIVERSITY OF IOWA,R13,2019,10000,-0.011612117922779488
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,9885663,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Traction', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'success', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2019,206139,0.01013573337073919
"NextGen Random Forests Project Summary/Abstract Building from the PI's current R01, we propose next generation random forests (RF) designed for unprecedented accuracy and computational scalability to meet the challenges of today's complex and big data in the health sciences. Superior accuracy is achieved using super greedy trees which circumvent limitations on local adaptivity imposed by classical tree splitting. We identify a key quantity, forest weights, and show how these can be leveraged for further improvements and generalizability. In one application, improved survival estimators are applied to worldwide esophageal cancer data to develop guidelines for clinical decision making. Richer RF inference is another issue explored. Cutting edge machine learning methods rarely consider the problem of estimating variability. For RF, bootstrapping currently exists as the only tool for reliably estimating conﬁdence intervals, but due to heavy computations is rarely applied. We introduce tools to rapidily calculate standard errors based on U-statistic theory. These will be used to increase robustness of esophageal clinical recommendations and to investigate survival temporal trends in cardiovascular disease. In another application, we make use of our new massive data scalability for discovery of tumor and immune regulators of immunotherapy in cancers. This project will set the standard for RF computational performance. Building from the core libraries of the highly accessed R-package randomForestSRC (RF-SRC), software developed under the PIs current R01, we develop open source next generation RF software, RF-SRC Everywhere, Big Data RF-SRC, and HPC RF-SRC. The software will be deployable on a number of popular machine learning workbenches, use distributed data storage technologies, and be optimized for big-p, big-n, and big-np scenarios. Project Narrative We introduce next generation random forests (RF) designed for unprecedented accuracy for complex and big data encountered in the health sciences.",NextGen Random Forests,9706046,R01GM125072,"['Atrophic', 'Benchmarking', 'Big Data', 'Biological Response Modifiers', 'Blood', 'Cancer Patient', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Code', 'Combined Modality Therapy', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Storage and Retrieval', 'Databases', 'Development', 'Esophageal', 'Flow Cytometry', 'Guidelines', 'Health Sciences', 'Heart failure', 'Human', 'Hybrids', 'Immune', 'Immunotherapy', 'In Vitro', 'Interagency Registry for Mechanically Assisted Circulatory Support', 'Internet', 'Java', 'Laboratories', 'Language', 'Libraries', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Neoadjuvant Therapy', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Performance', 'Population', 'Pump', 'Receptor Activation', 'Recommendation', 'Resistance', 'Subgroup', 'T-Lymphocyte', 'Technology', 'Therapeutic', 'Thrombosis', 'Time', 'Time trend', 'Trees', 'Weight', 'base', 'clinical decision-making', 'clinical practice', 'design', 'distributed data', 'forest', 'immune checkpoint blockade', 'improved', 'in vivo', 'learning strategy', 'lymph nodes', 'mouse model', 'next generation', 'novel', 'open source', 'outcome forecast', 'parallel processing', 'pre-clinical', 'predicting response', 'predictive modeling', 'random forest', 'receptor', 'response', 'software development', 'statistics', 'theories', 'therapeutic target', 'tool', 'tumor', 'tumor progression']",NIGMS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2019,347834,-0.00023184537950160603
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,9712304,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Online Systems', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'learning strategy', 'lung basal segment', 'lung cancer screening', 'mHealth', 'model development', 'novel', 'novel strategies', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistics', 'stem', 'tool']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2019,657823,0.0015158920907329836
"Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization Project Summary  This application is being submitted in response to NOT-RM-19-009 as a supplement to the parent award U54NS093793.  The Common Fund supports a number of resources that can significantly enhance gene and variant prioritization for study in the Model Organisms Screening Center of the Undiagnosed Diseases Network and beyond. To facilitate the use of these resources, we propose to create a tool that can be easily accessed by clinical geneticists and model organism scientists alike.  MARRVEL (Model organism Aggregated Resources for Rare Variant ExpLoration) was created two years ago because important data that is necessary for rare variant analysis for personalized medicine is spread throughout the internet in tens of different locations. To improve efficiency and streamline access to these data sources, we created a web-tool that allows users to query tens of data sources at once, including GTEx, and links to IMPC, the display portal for KOMP2.  In this proposal, our goal is to develop version 2 of MARRVEL to promote the use of Common Fund resources in the rare disease research community for manual and automated data analysis. This goal will be accomplished by developing MARRVEL 2.0 by integrating KOMP2 (IMPC) and PHAROS data and using the aggregated dataset to develop a machine-assisted gene and variant prioritization for diagnosis and animal model generation.  Our goals align with those of the NIH Common Fund to increase the utility of resources for broader use in the biomedical community. Project Narrative  We aim to promote the use of Common Fund resources and facilitate the diagnosis of rare diseases and the subsequent generation of animal models for the Undiagnosed Diseases Network and beyond. This goal will be accomplished by developing the web resource, MARRVEL 2.0.",Common Fund Data Supplement: Integration of KOMP2 (IMPC) and PHAROS into MARRVEL 2.0 for machine learning-assisted rare variant prioritization,9984757,U54NS093793,"['Affect', 'Animal Model', 'Artificial Intelligence', 'Award', 'Clinical', 'Collaborations', 'Communities', 'Country', 'Data', 'Data Analyses', 'Data Display', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Discipline', 'Disease', 'Disease model', 'Drosophila genus', 'Drug Targeting', 'Expert Systems', 'Family', 'Funding', 'Generations', 'Genes', 'Genetic Diseases', 'Genotype-Tissue Expression Project', 'Goals', 'Growth', 'Healthcare Systems', 'Human Genetics', 'Individual', 'Internet', 'Investigation', 'Knowledge', 'Link', 'Location', 'Machine Learning', 'Manuals', 'Medical', 'Medical Genetics', 'Modeling', 'Mus', 'Parents', 'Pathogenicity', 'Pharmaceutical Preparations', 'Phenotype', 'Process', 'Proteins', 'Rare Diseases', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Suggestion', 'Symptoms', 'System', 'Testing', 'Therapeutic', 'Therapeutic Studies', 'Time', 'Training', 'United States National Institutes of Health', 'Variant', 'Visit', 'Yeasts', 'Zebrafish', 'base', 'data wrangling', 'design', 'experimental study', 'feeding', 'fly', 'genetic disorder diagnosis', 'genetic variant', 'human data', 'improved', 'interest', 'learning community', 'machine learning algorithm', 'model organisms databases', 'online resource', 'personalized medicine', 'phenotypic data', 'rare genetic disorder', 'rare variant', 'response', 'screening', 'supervised learning', 'tool', 'web-based tool']",NINDS,BAYLOR COLLEGE OF MEDICINE,U54,2019,320000,-0.02444546973955957
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,9798401,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Machine Learning', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'learning strategy', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2019,377200,-0.0141802206189185
"Genomic sequencing to aid diagnosis in pediatric and prenatal practice: Examining clinical utility, ethical implications, payer coverage, and data integration in a diverse population. PROJECT SUMMARY I propose to gain experience in what are becoming the next big topics in genomic medicine – the integration of “big data” using data science in order to achieve “precision health” – what could be summed up as “data science of the future”. These topics emerge from - but go beyond - the narrower concept of “precision medicine” as the use of genetic information for treatment decisions. The goal is to develop experience in data science and precision health so that my work can serve as a bridge between my field of economics and these fields - and begin to prepare for the future challenges as they emerge. PROJECT NARRATIVE Both data science and precision health will be critical components of future health care interventions and impact patients, providers, and society. “Big Data” using data science includes the aggregation and analysis of data across platforms (includes information from, e.g. genetic testing, biosensors, wearables, and electronic health records, with such data analyzed using, e.g. artificial intelligence and machine learning). Precision Health uses a “big data” approach to focus on disease prevention and detection throughout one’s lifetime.","Genomic sequencing to aid diagnosis in pediatric and prenatal practice: Examining clinical utility, ethical implications, payer coverage, and data integration in a diverse population.",9929780,U01HG009599,"['Artificial Intelligence', 'Big Data', 'Biosensor', 'Childhood', 'Clinical', 'Data Analyses', 'Data Science', 'Detection', 'Diagnosis', 'Economics', 'Electronic Health Record', 'Ethics', 'Future', 'Genetic screening method', 'Genomic medicine', 'Genomics', 'Goals', 'Health', 'Healthcare', 'Intervention', 'Machine Learning', 'Patients', 'Population Heterogeneity', 'Precision Health', 'Provider', 'Societies', 'Source', 'Sum', 'Work', 'data integration', 'disorder prevention', 'experience', 'formal learning', 'genetic information', 'informal learning', 'precision medicine', 'prenatal']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2019,166973,-0.015352216985327695
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9791176,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Infrastructure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'feature detection', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2019,1339073,0.004470181959272346
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9642618,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Effectiveness', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Imagery', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Testing', 'Training', 'Universities', 'Virginia', 'absorption', 'artificial neural network', 'base', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2019,375602,-0.009718176789694789
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval - (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9797689,R44MH118815,"['Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Digital Imaging and Communications in Medicine', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Infrastructure', 'Institutes', 'Institution', 'Intelligence', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'Stem cells', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'collaborative environment', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'prototype', 'research and development', 'treatment strategy', 'two-dimensional', 'usability', 'virtual reality', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2019,748584,0.0036697748908127064
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9857305,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'machine learning algorithm', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2019,249000,-0.005722160581391081
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9772886,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics\xa0tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2019,663419,0.020554902630061146
"Brain Science Compute Cluster Computational requirements of contemporary brain science research typically exceed financial and resource management limits of individual investigator laboratories. Many brain science research projects require analysis of large data sets with advanced statistical methods and anatomical reconstruction techniques. These methods require high speed computational and graphics engines operating in a multiple processor environments equipped with large capacity, high speed storage devices. An ongoing limitation in the Brown brain science effort at understanding neural processing is the lack of a contemporary and readily accessible high-speed computational resource.  We plan to replace an existing, but 5 year old and now outmoded, central computational resource that has outdated graphic processing units (GPU) and central processing units (CPU) and and limited storage that will serve the computational needs of a core group of brain science investigators at Brown without compromising individual access to stand-alone workstations. The requested computation equipment comprises 13 GPU nodes (total of 52 cores), 12 CPU nodes (288 cores) and 1.2 petabytes of disk storage, which will serve the needs of the assembled brain science researchers. The equipment will become integrated into Brown's high performance Compute Cluster, which has system software that automatically balances GPU and CPU usage, thereby ensuring maximum access to the computational resource for all users. Intensive 3D graphics are off- loaded either to GPUs or to client workstations, thereby further reducing the central computational load. Commercial or open-source software with an open operating environment will be used for analysis using standard and novel statistical and machine learning approaches to assess significance of large data sets.  This proposal details the architecture and benefits of a contemporary computational resource for the major and minor users, and more generally the Brown brain science community. The resource was designed to fill immediate and near-term computational and storage needs of a core group of Brown brain scientists. The system can be readily expansion as needs, either computational, storage, or new users, arise. Expansion of the existing core investigators group can occur easily since the computational power or storage capacity of the system can be readily enhanced at relatively low cost.  The flexible nature of the system will serve a variety of research needs of the Brown brain science community. The computational resource is expected to bring together researchers at Brown working on the common problem of neural processing. Relevance Gaining insights into the neural and brain mechanisms that underlie normal brain function and malfunction in disease requires analysis of large and complex data sets related to behavior and brain physiology, chemistry and structure. This proposals requests support to develop a computing infrastructure devoted to researchers in Brown's Carney Institute for Brain Science who investigate brain structure and function.",Brain Science Compute Cluster,9708078,S10OD025181,"['3-Dimensional', '5 year old', 'Anatomy', 'Architecture', 'Brain', 'Client', 'Communities', 'Computer software', 'Data Set', 'Devices', 'Ensure', 'Environment', 'Equilibrium', 'Equipment', 'High Performance Computing', 'Individual', 'Laboratories', 'Machine Learning', 'Methods', 'Minor', 'Nature', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Speed', 'Statistical Methods', 'System', 'Techniques', 'cluster computing', 'computing resources', 'cost', 'design', 'flexibility', 'novel', 'open source', 'petabyte', 'reconstruction', 'relating to nervous system', 'software systems']",OD,BROWN UNIVERSITY,S10,2019,300000,0.004390797254120025
"C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative ABSTRACT The BRAIN Initiative is designed to leverage sophisticated neuromodulation, electrophysiological recording, and macroscale neuroimaging techniques in human and non-human animal models in order to develop a multilevel understanding of human brain function. However, the necessary tools for organizing, processing and analyzing neuroimaging data generated through these efforts are not widely available as coherent and easy-to- use software packages. Gaps are particularly apparent for nonhuman data (i.e., monkey, rodent), as most of the existing processing and analytic software packages are specifically designed for human imaging. Methods have been proposed for addressing the challenges inherent to the processing of nonhuman data (e.g., brain extraction, tissue segmentation, spatial normalization, brain parcellation, temporal denoising); to date, these have not been readily integrated into an easy-to-use, robust, and reproducible analysis package. Similarly, many of the sophisticated machine learning and modeling methods developed for neuroimaging analyses are inaccessible to most researchers because they have not been integrated into easy-to-use pipeline software. As a result, translational and comparative neuroimaging researchers patch together neuroinformatics pipelines that use various combinations of disparate software packages and in-house code. We propose to extend the Configurable Pipeline for the Analysis of Connectomes (C-PAC) open-source software to provide robust and reproducible pipelines for functional and structural MRI data. We will integrate the various disparate image processing and analysis methods used to handle the challenges of nonhuman imaging data, into a single, open source, configurable, easy-to-use end-to-end analysis pipeline package that is accessible locally or via the cloud. The end product will not only improve the quality, transparency and reproducibility of nonhuman translational and comparative imaging, but also enable new avenues of scientific inquiry through our inclusion of methods that are yet to be applied to nonhuman imaging data (e.g., gradient- based cortical parcellation methods, hyperalignment). Specific aims of the proposed work include to: 1) Integrate neuroimaging processing and analysis methods optimized for BRAIN Initiative data, 2) Implement strategies for carrying out comparative studies of human and non-human populations, and 3) Extend C-PAC to include cutting-edge analytical strategies for identifying mechanisms of brain function. All development will occur “in the open” using GitHub and other collaborative tools to maximally involve participation in the C-PAC project. Annual hackathons will be held to collaborate with investigators from BRAIN Initiative awards and other neuroinformatics development projects to integrate their tools with C-PAC. Hands-on training will be held to train investigators on optimal use of the newly developed tools. NARRATIVE New neuroimaging analysis software is needed to process and analyze the various human and non-human neuroimaging data collected through the BRAIN Initiative. We will address this need by extending the already mature C-PAC human brain imaging data analysis pipeline to include support for animal data, with a particular focus on providing methods for conducting comparative studies between species. The proposed work will also include a toolbox for helping to align electrophysiological data that is commonly collected in non-human studies, with the brain imaging data.","C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative",9766371,R24MH114806,"['Address', 'Adoption', 'Anatomy', 'Animal Model', 'Architecture', 'Award', 'Behavior', 'Brain', 'Brain imaging', 'Capital', 'Code', 'Communities', 'Comparative Study', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Documentation', 'Electrodes', 'Electrophysiology (science)', 'Environment', 'Funding', 'High Performance Computing', 'Human', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Measures', 'Methods', 'Modeling', 'Modification', 'Monkeys', 'Outcome', 'Output', 'Pattern', 'Persons', 'Phenotype', 'Population', 'Process', 'Pythons', 'Readability', 'Reproducibility', 'Research Personnel', 'Rodent', 'Scientific Inquiry', 'Software Design', 'Statistical Methods', 'Structure', 'Techniques', 'Testing', 'Text', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validity of Results', 'Work', 'analysis pipeline', 'animal data', 'base', 'brain research', 'cloud based', 'comparative', 'computing resources', 'connectome', 'cost', 'data sharing', 'data structure', 'denoising', 'design', 'flexibility', 'graphical user interface', 'hackathon', 'human imaging', 'image processing', 'improved', 'innovative neurotechnologies', 'investigator training', 'learning strategy', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuroregulation', 'open source', 'software as a service', 'supervised learning', 'tool', 'unsupervised learning']",NIMH,"CHILD MIND INSTITUTE, INC.",R24,2019,545648,0.012287430430504091
"C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative ABSTRACT The BRAIN Initiative is designed to leverage sophisticated neuromodulation, electrophysiological recording, and macroscale neuroimaging techniques in human and non-human animal models in order to develop a multilevel understanding of human brain function. However, the necessary tools for organizing, processing and analyzing neuroimaging data generated through these efforts are not widely available as coherent and easy-to- use software packages. Gaps are particularly apparent for nonhuman data (i.e., monkey, rodent), as most of the existing processing and analytic software packages are specifically designed for human imaging. Methods have been proposed for addressing the challenges inherent to the processing of nonhuman data (e.g., brain extraction, tissue segmentation, spatial normalization, brain parcellation, temporal denoising); to date, these have not been readily integrated into an easy-to-use, robust, and reproducible analysis package. Similarly, many of the sophisticated machine learning and modeling methods developed for neuroimaging analyses are inaccessible to most researchers because they have not been integrated into easy-to-use pipeline software. As a result, translational and comparative neuroimaging researchers patch together neuroinformatics pipelines that use various combinations of disparate software packages and in-house code. We propose to extend the Configurable Pipeline for the Analysis of Connectomes (C-PAC) open-source software to provide robust and reproducible pipelines for functional and structural MRI data. We will integrate the various disparate image processing and analysis methods used to handle the challenges of nonhuman imaging data, into a single, open source, configurable, easy-to-use end-to-end analysis pipeline package that is accessible locally or via the cloud. The end product will not only improve the quality, transparency and reproducibility of nonhuman translational and comparative imaging, but also enable new avenues of scientific inquiry through our inclusion of methods that are yet to be applied to nonhuman imaging data (e.g., gradient- based cortical parcellation methods, hyperalignment). Specific aims of the proposed work include to: 1) Integrate neuroimaging processing and analysis methods optimized for BRAIN Initiative data, 2) Implement strategies for carrying out comparative studies of human and non-human populations, and 3) Extend C-PAC to include cutting-edge analytical strategies for identifying mechanisms of brain function. All development will occur “in the open” using GitHub and other collaborative tools to maximally involve participation in the C-PAC project. Annual hackathons will be held to collaborate with investigators from BRAIN Initiative awards and other neuroinformatics development projects to integrate their tools with C-PAC. Hands-on training will be held to train investigators on optimal use of the newly developed tools. NARRATIVE New neuroimaging analysis software is needed to process and analyze the various human and non-human neuroimaging data collected through the BRAIN Initiative. We will address this need by extending the already mature C-PAC human brain imaging data analysis pipeline to include support for animal data, with a particular focus on providing methods for conducting comparative studies between species. The proposed work will also include a toolbox for helping to align electrophysiological data that is commonly collected in non-human studies, with the brain imaging data.","C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative",9894275,R24MH114806,"['Address', 'Adoption', 'Anatomy', 'Animal Model', 'Architecture', 'Award', 'Behavior', 'Brain', 'Brain imaging', 'Capital', 'Code', 'Communities', 'Comparative Study', 'Computer software', 'Consumption', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Documentation', 'Electrodes', 'Electrophysiology (science)', 'Environment', 'Funding', 'High Performance Computing', 'Human', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Measures', 'Methods', 'Modeling', 'Modification', 'Monkeys', 'Outcome', 'Output', 'Pattern', 'Persons', 'Phenotype', 'Population', 'Process', 'Pythons', 'Readability', 'Reproducibility', 'Research Personnel', 'Rodent', 'Scientific Inquiry', 'Software Design', 'Statistical Methods', 'Structure', 'Techniques', 'Testing', 'Text', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validity of Results', 'Work', 'analysis pipeline', 'animal data', 'base', 'brain research', 'cloud based', 'comparative', 'computing resources', 'connectome', 'cost', 'data sharing', 'data structure', 'denoising', 'design', 'flexibility', 'graphical user interface', 'hackathon', 'human imaging', 'image processing', 'improved', 'innovative neurotechnologies', 'investigator training', 'learning strategy', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuroregulation', 'open source', 'software as a service', 'supervised learning', 'tool', 'unsupervised learning']",NIMH,"CHILD MIND INSTITUTE, INC.",R24,2019,23100,0.012287430430504091
"Big Omics Data Engine 2 Supercomputer Computational and data science has transformed biomedical scientific discovery: its approaches are embedded into a wide range of workflows for diseases such as schizophrenia, depression, Alzheimer's, epilepsy, influenza, autism, drug addiction, pediatric cardiac care, Inflammatory Bowel Disease, prostate cancer and multiple myleloma. Sixty-one basic and translational researchers at Mount Sinai representing over $100 million in NIH funding, along with their collaborators from 75 external institutions, have utilized the Big Omics Data Engine (BODE) supercomputer to elucidate significant scientific findings in over 167 publications, including high impact journals such as Nature and Science, with 2,427 citations in three years. These researchers have also shared the data generated on BODE throughout their consortia and into national data sharing repositories. BODE is nearing the end of its vendor maintainable life, and researchers need increased computational throughput and storage space. To empower researchers to not only continue their inquiries, but to also tackle more complex scientific questions with decreased time to solution, we propose the Big Omics Data Engine 2 Supercomputer (BODE2). BODE2 will contain a total of 3,200 Intel Cascade Lake cores with 15 terabytes of memory and 14 petabytes of raw storage, and will leverage an existing 250 terabytes of SSDs. An instrument of this size is not available elsewhere affordably. With the proposed instrument, researchers will be able to take advantage of three major benefits: (1) the ability to receive results faster for overall greater scientific throughput; (2) the ability to increase the fidelity of their simulations and analyses; and (3) the ability to migrate research applications seamlessly to the software environment for greater scientific productivity. As with data produced on BODE, BODE2 data products will also be shared with the broader scientific community. BODE2 will provide the critical infrastructure needed by the wide range of researchers and clinicians for the genetics and population analysis, gene expression, machine learning and structural and chemical biology approaches used to make advances in these diseases. A specialized Big Omics Data Engine 2 Supercomputer instrument will provide necessary computational and data science infrastructure for 61 research projects with 75 collaborating institutions in diverse areas such as Alzheimer's, autism, schizophrenia, drug addiction, influenza, pediatric cardiac care, depression, epilepsy, prostate cancer and multiple myeloma. Data generated from this instrument will be shared in national databases.",Big Omics Data Engine 2 Supercomputer,9708160,S10OD026880,"['Alzheimer&apos', 's Disease', 'Biology', 'Cardiac', 'Caring', 'Chemicals', 'Childhood', 'Communities', 'Complex', 'Computational Science', 'Computer software', 'Data', 'Data Science', 'Disease', 'Drug Addiction', 'Environment', 'Epilepsy', 'Funding', 'Gene Expression', 'Inflammatory Bowel Diseases', 'Influenza', 'Infrastructure', 'Institution', 'Journals', 'Life', 'Machine Learning', 'Malignant neoplasm of prostate', 'Memory', 'Mental Depression', 'Nature', 'Population Analysis', 'Productivity', 'Publications', 'Research', 'Research Personnel', 'Schizophrenia', 'Science', 'Structure', 'Time', 'United States National Institutes of Health', 'Vendor', 'autism spectrum disorder', 'data sharing', 'genetic analysis', 'instrument', 'petabyte', 'repository', 'simulation', 'supercomputer', 'terabyte', 'translational scientist']",OD,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,S10,2019,1998264,-0.007483054111732366
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9827788,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,33190,0.02091435468128292
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9693030,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,36510,0.02091435468128292
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9625823,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,343157,0.02091435468128292
"Precision immunoprofiling to reveal diagnostic biomarkers of latent TB infection PROJECT SUMMARY  Tuberculosis (TB) is among the leading causes of mortality worldwide with an estimated 2 billion individuals currently infected. Latent tuberculosis infection (LTBI) is the most common form of TB infection affecting 13 million Americans. While many with LTBI remain asymptomatic, an estimated 10% of immunocompetent patients with LTBI will reactivate to active TB, and will become infectious. LTBI is treatable with a prolonged antibiotic treatment; however, potential side effects motivate the development of new diagnostic approaches that can identify with high specificity patients at the highest risk of reactivation, for who therapy would be most beneficial.  The tuberculin skin test (TST) and interferon-γ release assays (IGRAs) are commonly used for TB and LTBI screening. Both tests provide good measures of TB exposure; however, neither is effective at diagnosing LTBI (positive predictive values <5%). Moreover, neither provide any prognostic stratification based upon reactivation risk. Both the TST and IGRAs probe immunological memory to TB-related antigen challenges and we hypothesize that a more nuanced and personalized approach to monitoring immune responses to both TB- specific and non-specific antigens might reveal new approaches to LTBI diagnosis and patient stratification.  Enabling a new, individualized approach to LTBI diagnostics, we propose to combine high throughput, multiplexed inflammatory biomarker detection strategies and powerful bioinformatics tools that allow for the identification of previously obscured multi-marker diagnostic signatures of LTBI status and reactivation risk. Silicon photonic microring resonators are an enabling technology for biomarker analysis due to their intrinsic scalability and multiplexing capabilities. Applied to the detection of cytokine panels, this technology supports the rapid immune profiling of individual samples under both TB-specific and non-specific antigen stimulation conditions. Machine learning algorithms will be utilized to analyze the resulting dense data streams to facilitate selection of key diagnostic signatures forming the basis for predictive model development and deployment. This powerful analytical combination is supplemented by deep expertise in clinical diagnosis and treatment of TB and LTBI, and an enabling collaboration and connection to subjects from an international location with high TB burden and exposure in a healthcare worker population subjected to regularly-scheduled and repeated LTBI screening.  The resulting diagnostic workflow and machine learning feature selection approaches will reveal multiplexed biomarker signatures that have strong positive predictive correlation with LTBI status (+ or -). This approach will also further stratify LTBI+ subjects on the basis of reactivation potential, thus providing a fundamentally new approach to identifying subjects that are most likely to benefit from therapeutic intervention. The end result of this project will be a new precision medicine-based diagnostic strategy that is vastly superior to the current state-of-the-art and offers the potential to transform current clinical practice. PROJECT NARRATIVE Tuberculosis (TB) affects an estimated one third of the world’s population and an asymptomatic latent state of tuberculosis infection (LTBI) is extremely common. Unfortunately, there are not any good clinical tests that can definitely diagnose LTBI, making it difficult to identify patients that should be treated to prevent reactivation to active TB, which is infectious. We will integrate cutting edge measurement technologies and machine learning bioinformatic approaches to identify and test multiplexed biomarker signatures that will transform clinical TB management by enabling personalized diagnosis of LTBI and the stratification of individuals with the highest potential for reactivation.",Precision immunoprofiling to reveal diagnostic biomarkers of latent TB infection,9819449,R01AI141591,"['Affect', 'Algorithms', 'American', 'Antibiotic Therapy', 'Antibiotics', 'Antigens', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Clinical', 'Clinical Treatment', 'Collaborations', 'Complex', 'Cytokine Network Pathway', 'Data', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Eligibility Determination', 'Generations', 'Goals', 'Gold', 'Health Personnel', 'Immune', 'Immune response', 'Immunocompetent', 'Immunologic Markers', 'Immunologic Memory', 'Immunologic Monitoring', 'Individual', 'Infection', 'Inflammatory', 'Informatics', 'Interferons', 'International', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Modeling', 'Patients', 'Peripheral', 'Peripheral Blood Mononuclear Cell', 'Plasma', 'Population', 'Predictive Value', 'Prevention strategy', 'Regimen', 'Residual state', 'Risk', 'Sampling', 'Schedule', 'Silicon', 'Specificity', 'Stratification', 'Stream', 'Technology', 'Testing', 'Therapeutic Intervention', 'Translations', 'Tuberculin Test', 'Tuberculosis', 'Whole Blood', 'antigen challenge', 'base', 'bioinformatics tool', 'clinical Diagnosis', 'clinical practice', 'cytokine', 'diagnostic accuracy', 'diagnostic biomarker', 'high risk', 'immune function', 'immunoregulation', 'improved', 'individual variation', 'latent infection', 'machine learning algorithm', 'model development', 'monocyte', 'mortality', 'novel diagnostics', 'novel strategies', 'patient stratification', 'personalized approach', 'personalized diagnostics', 'photonics', 'precision medicine', 'predictive marker', 'predictive modeling', 'prevent', 'prognostic', 'prospective', 'response', 'screening', 'side effect', 'targeted treatment', 'tool', 'treatment strategy', 'tuberculosis treatment']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2019,754111,0.010981072977778233
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9911854,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2019,15000,-0.017377221527266545
"Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack Project Summary: Tufts University physics professor Allan Cormack pioneered the field of tomography. His seminal work, from 1963 and 1964, provided both the mathematical foundations of computerized tomography (CT), and tangible proof-of-concept by engineering a rudimentary CT scanner. Taken together, this effort represented the first practical method to ""see into"" an object without physically breaking it open. Along with the engineer Godfrey Hounsfield, he won the 1979 Nobel Prize in Physiology or Medicine for these contributions. Since then, tomography has broadened to include a wide range of modalities and problems. This field is unique for the rich interplay among applications in medicine, security, earth sciences, industry, physics, and the mathematics required to solve these problems. This international conference at Tufts, “Modern Challenges in Imaging: In the Footsteps of Allan Cormack” will honor the achievements of Cormack and reflect this diversity in the field by gathering top international researchers in mathematics, engineering, science, and medicine to communicate the most current research and challenges in the field. This will include work on mathematical models of emerging modalities, tomographic machine learning, dynamic methods, and spectral imaging with applications include medicine and security. The best research from the conference will be disseminated in a special issue of the journal Inverse Problems. Talks will be posted on the conference website. The organizers will recruit a diverse set of experienced participants and trainees, and the conference will be advertised in a range of publications reflecting the scientific and demographic diversity of the field. This conference is unique in that it combines high-level mathematical participants with experts in medical and industrial CT. It is structured to encourage participants from different fields to talk with each other, broaden their horizons, and make connections between problems and methodologies in the various fields. Several of the plenary talks will provide introductions to the areas. Trainees will be integrated into the conference through an informal welcome lunch and a poster session to introduce them to researchers in the field. This supports goals 1, 4, and 5, of the NIBIB: Researchers will present innovative biomedical technologies, engineering solutions, and mathematical methods to better image the body and objects more generally. The synergy between research areas will support the translation of technologies from the academic sphere to medical utility. The training opportunities for graduate students and beginners support the training of the next generation of diverse scientists. Project Narrative This conference will bring together medical, scientific, engineering, and applied mathematical researchers to present their newest research for a range of tomographic problems. Graduate students and beginners will be encouraged to participate and learn by being offered introductory talks, a student poster session, a welcome event, and an informal atmosphere. The conference will be structured so researchers will learn about important challenges in practical tomography as well as new techniques and methods, thereby creating synergies and research connections among the areas.",Conference on Modern Challenges in Imaging in the Footsteps of Allan Cormack,9837131,R13EB028700,"['Achievement', 'Advertising', 'Algorithms', 'Area', 'Biomedical Technology', 'Communication', 'Development', 'Earth science', 'Engineering', 'Environment', 'Event', 'Fertilization', 'Foundations', 'Goals', 'Image', 'Individual', 'Industrialization', 'Industry', 'International', 'Journals', 'Lead', 'Learning', 'Lightning', 'Machine Learning', 'Mathematics', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modality', 'Modernization', 'National Institute of Biomedical Imaging and Bioengineering', 'Nobel Prize', 'Outcome', 'Participant', 'Physics', 'Physiology', 'Population', 'Problem Solving', 'Publications', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Security', 'Seminal', 'Societies', 'Structure', 'Students', 'Techniques', 'Technology', 'Time', 'Training Support', 'Translations', 'Underrepresented Groups', 'Universities', 'Work', 'X-Ray Computed Tomography', 'cohort', 'demographics', 'design', 'experience', 'graduate student', 'higher level mathematics', 'informal atmosphere', 'innovation', 'mathematical methods', 'mathematical model', 'meetings', 'member', 'next generation', 'posters', 'professor', 'recruit', 'spectrograph', 'symposium', 'synergism', 'tomography', 'training opportunity', 'web site']",NIBIB,TUFTS UNIVERSITY MEDFORD,R13,2019,10000,-0.03294089400224252
"Summer Institute in Neuroimaging and Data Science Project Summary/Abstract The study of the human brain with neuroimaging technologies is at the cusp of an exciting era of Big Data. Many data collection projects, such as the NIH-funded Human Connectome Project, have made large, high- quality datasets of human neuroimaging data freely available to researchers. These large data sets promise to provide important new insights about human brain structure and function, and to provide us the clues needed to address a variety of neurological and psychiatric disorders. However, neuroscience researchers still face substantial challenges in capitalizing on these data, because these Big Data require a different set of technical and theoretical tools than those that are required for analyzing traditional experimental data. These skills and ideas, collectively referred to as Data Science, include knowledge in computer science and software engineering, databases, machine learning and statistics, and data visualization.  The Summer Institute in Data Science for Neuroimaging will combine instruction by experts in data science methodology and by leading neuroimaging researchers that are applying data science to answer scientiﬁc ques- tions about the human brain. In addition to lectures on the theoretical background of data science methodology and its application to neuroimaging, the course will emphasize experiential hands-on training in problem-solving tutorials, as well as project-based learning, in which the students will create small projects based on openly available datasets. Summer Institute in Neuroimaging and Data Science: Project Narrative The Summer Institute in Neuroimaging and Data Science will provide training in modern data science tools and methods, such as programming, data management, machine learning and data visualization. Through lectures, hands-on training sessions and team projects, it will empower scientists from a variety of backgrounds in the use of these tools in research on the human brain and on neurological and psychiatric brain disorders.",Summer Institute in Neuroimaging and Data Science,9650637,R25MH112480,"['Address', 'Adopted', 'Big Data', 'Brain', 'Brain Diseases', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Databases', 'Discipline', 'Face', 'Faculty', 'Fostering', 'Funding', 'Habits', 'Home environment', 'Human', 'Image', 'Institutes', 'Institution', 'Instruction', 'Internet', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mental disorders', 'Methodology', 'Methods', 'Modernization', 'Neurologic', 'Neurosciences', 'Participant', 'Positioning Attribute', 'Problem Solving', 'Psychology', 'Reproducibility', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Software Engineering', 'Software Tools', 'Structure', 'Students', 'Technology', 'Testing', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'career', 'classification algorithm', 'computer science', 'connectome', 'data management', 'data visualization', 'design', 'e-science', 'experimental study', 'high dimensionality', 'insight', 'instructor', 'interdisciplinary collaboration', 'knowledge base', 'lectures', 'nervous system disorder', 'neurogenetics', 'neuroimaging', 'novel', 'open source', 'prediction algorithm', 'programs', 'project-based learning', 'skills', 'statistics', 'success', 'summer institute', 'theories', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,R25,2019,199118,-0.019657892222836894
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9741121,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'side effect', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2019,281932,0.002150120464280882
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9634069,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2019,293003,0.00018853204253641557
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,9775212,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Axon', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional']",NIMH,UNIVERSITY OF WASHINGTON,R01,2019,628430,-0.03923074850331925
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9747967,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2019,514129,0.008085287265877551
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9786847,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2019,747591,-0.014055909041774992
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9734597,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'learning strategy', 'lectures', 'lens', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,249789,-0.028533088397126205
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,9851583,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'learning strategy', 'mHealth', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2019,741688,-0.019109602782406185
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9676043,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,308000,-0.0011648546474007262
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. n/a",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9882672,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2019,149810,0.0010819140432230976
"BRAIN INITIATIVE RESOURCE: DEVELOPMENT OF A HUMAN NEUROELECTROMAGNETIC DATA ARCHIVE AND TOOLS RESOURCE (NEMAR) To take advantage of recent and ongoing advances in intensive and large-scale computational methods, and to preserve the scientific data created by publicly funded research projects, data archives must be created as well as standards for specifying, identifying, and annotating deposited data. The value of and interest in such archives among researchers can be greatly increased by adding to them an active computational capability and framework of analysis and search tools that support further analysis as well as larger scale meta-analysis and large scale data mining. The OpenNeuro.org archive, begun as a repository for functional magnetic resonance imaging (fMRI) data, is such an archive. We propose to build a gateway to OpenNeuro for human electrophysiology data (EEG and MEG, as well as intracranial data recorded from clinical patients to plan brain surgeries or other therapies) – herein we refer to these modalities as neuroelectromagnetic (NEM) data. The Neuroelectromagnetic Data Archive and Tools Resource (NEMAR) at the San Diego Supercomputer Center will act as a gateway to OpenNeuro for NEM data research. Such data uploaded to NEMAR at SDSC will be deposited in the OpenNeuro archive. Still- private NEM data in OpenNeuro will, on user request, be copied to the NEMAR gateway for further user processing using the XSEDE high-performance resources at SDSC in conjunction with The Neuroscience Gateway (nsgportal.org), a freely available and easy to use portal to use of high-performance computing resources for neuroscience research. Publicly available OpenNeuro NEM data will be able to be analyzed by running verified analysis applications on the OpenNeuro system. In this project we will build an application to evaluate the quality of uploaded NEM data, and another to visualize the data, for EEG and MEG at both the scalp and brain source levels, including time-domain and frequency-domain dynamics time locked to sets of experimental events learned from the BIDS- and HED-formatted data annotations. The NEMAR gateway will take a major step toward applying machine learning methods to a large store of carefully collected and stored human electrophysiologic brain data to spur new developments in basic and clinical brain research. The NEMAR gateway to the OpenNeuro.org human neuroimaging data archive will build tools to add human electrical and magnetic brain activity records to the archive, to evaluate its quality for users and visualize its features. The resulting facility will allow applications of new machine learning methods to research on human brain dynamics that can be expected to lead to breakthroughs in understanding how the human brain supports our awareness and behavior in both health and disease.",BRAIN INITIATIVE RESOURCE: DEVELOPMENT OF A HUMAN NEUROELECTROMAGNETIC DATA ARCHIVE AND TOOLS RESOURCE (NEMAR),9795341,R24MH120037,"['Archives', 'Awareness', 'BRAIN initiative', 'Base of the Brain', 'Behavior', 'Brain', 'Brain imaging', 'Cell Nucleus', 'Clinical', 'Cloud Computing', 'Communities', 'Computing Methodologies', 'Custom', 'Data', 'Data Analyses', 'Data Quality', 'Data Set', 'Data Storage and Retrieval', 'Deposition', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Documentation', 'Educational workshop', 'Electrophysiology (science)', 'Engineering', 'Environment', 'Evaluation', 'Event', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Funding', 'Grant', 'Health', 'High Performance Computing', 'Human', 'Infrastructure', 'Internet', 'Laboratories', 'Lead', 'Libraries', 'Machine Learning', 'Magnetic Resonance', 'Magnetism', 'Magnetoencephalography', 'Meta-Analysis', 'Methods', 'Mining', 'Modality', 'Neurosciences', 'Neurosciences Research', 'Patients', 'Performance', 'Privatization', 'Process', 'Quality Control', 'Records', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Running', 'Scalp structure', 'Science', 'Source', 'Specific qualifier value', 'Spottings', 'Staging', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'Visualization software', 'base', 'brain research', 'brain surgery', 'built environment', 'computational platform', 'computerized data processing', 'computing resources', 'cyber infrastructure', 'data archive', 'data format', 'data mining', 'data structure', 'data submission', 'hackathon', 'interest', 'learning strategy', 'neuroimaging', 'preservation', 'repository', 'response', 'sensor', 'supercomputer', 'support tools', 'tool', 'tool development', 'web interface', 'web services', 'web site']",NIMH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R24,2019,926107,-0.018185127008561487
"Acquisition of a next-generation computing cluster We request funds to purchase our next-generation computing cluster to support computationally intensive NIH-funded research at Washington University in St. Louis. This system will become the foundation of the Center for High Performance Computing (CHPC) to support our active, diverse user community. It has been designed to meet our current and future computing needs. It adds additional capabilities to support emerging fields such as “Deep Learning”. The CHPC currently supports over 775 users from 300 different groups across 33 departments. 58 papers have cited the CHPC. The Center has a proven funding model and is economically sustainable. The Center has partnered with other University organizations to offer training workshops, not only on the use of the cluster, but also on introductory programming for users with no prior programming experience. If this proposal is funded, we will be able to continue to support this ever-growing diverse community of researchers. The proposed system would replace critical components including the management node, the login nodes, the storage, and upgrade the Infiniband networking. We would add substantial upgrades to our computing power with state-of-the-art processors, increased memory capacity for growing jobs, General Purpose Graphical Processing units (GPGPUs), and new capabilities for “Deep Learning”. Nearly all fields of NIH-funded research are faced with increasingly large data sets that require additional computing power to analyze. We propose building a next-generation computing cluster to support this research. Our Center has a proven track record in supporting a large, diverse group of users in all aspects of their computationally demanding research.",Acquisition of a next-generation computing cluster,9707936,S10OD025200,"['Communities', 'Educational workshop', 'Foundations', 'Funding', 'Future', 'High Performance Computing', 'Memory', 'Modeling', 'Occupations', 'Paper', 'Research', 'Research Personnel', 'System', 'Training', 'United States National Institutes of Health', 'Universities', 'Washington', 'cluster computing', 'deep learning', 'design', 'experience', 'next generation']",OD,WASHINGTON UNIVERSITY,S10,2019,597200,0.012776844938860641
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,9882822,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Simulation', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2019,390806,-0.00717427821364156
"SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies ﻿    DESCRIPTION (provided by applicant): Efforts to reduce the burden of Tuberculosis (TB) are challenged by the persistent social inequalities in health, the limited number of local healthcare professionals, and the weak healthcare infrastructure found in resource-poor communities. Reducing the TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the TB epidemic in high-burden areas. The main objective of this proposal is to expedite the TB diagnosis process by developing novel image processing and machine learning techniques to analyze chest X-ray images thus reducing patient wait times for being diagnosed with TB. The study will be conducted in the district of Carabayllo, a densely occupied, high-burden TB area in Lima, the capital of Perú. Efforts to develop the proposed user-centered, mobile device-based computing system are aligned with the mission of the National Institute of Biomedical Imaging and Bioengineering (NIBIB) and its strategic goals 2 and 4 in particular-the proposed socio-technical intervention aims at developing biomedical imaging techniques (i.e. wireless and image sensing/analyzing) to enable a point-of-care mobile device-based computing system for TB screening and diagnostic. Anticipated outcomes include a) a large-scale, real-world, well-annotated, and public available chest X-ray image database for TB screening, b) development of new image analysis techniques for X-ray image capturing and pre- processing, and c) novel learning-based feature extraction and classification algorithms. This  interdisciplinary effort, involving community, university, hospitals and health care establishments in all stages of the research, responds to the need for increased partnerships between academia and community stakeholders, and the potential for building capacity in biomedical and technology solutions for health in both directions (North-South, South-North). Its scientific contribution lies in the intersection of three NIBIB scientific program areas including image processing, telehealth, and biomedical informatics. PUBLIC HEALTH RELEVANCE: This project is highly relevant to public and global health because it offers a socio-technical solution for resource-poor communities severely affected by TB. Outcomes of this project will contribute significantly to improving specific healthcare processes affecting hard-to-reach communities that are socially excluded and lack the benefits of technological advances while broadening our understanding about effective human centered designs to improve healthcare systems with mobile computing technologies.",SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies,9746721,R01EB021900,"['Academia', 'Address', 'Affect', 'Algorithms', 'Area', 'Benchmarking', 'Biomedical Technology', 'Capital', 'Cessation of life', 'Chest', 'Chronic Disease', 'Cities', 'Classification', 'Clinic', 'Communicable Diseases', 'Communities', 'Community Health', 'Complex', 'Computer Assisted', 'Computer Systems', 'Computer software', 'Computers', 'Data', 'Data Analytics', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Discipline', 'Engineering', 'Epidemic', 'Evaluation', 'Female', 'Goals', 'Health', 'Health Professional', 'Health Sciences', 'Health Technology', 'Healthcare', 'Healthcare Systems', 'Hispanics', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging Techniques', 'Infrastructure', 'Intervention', 'Learning', 'Lung nodule', 'Machine Learning', 'Medical', 'Minority', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Outcome', 'Patients', 'Peru', 'Process', 'Public Health', 'Reader', 'Recruitment Activity', 'Reporting', 'Research', 'Resources', 'Running', 'Sensitivity and Specificity', 'Software Tools', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Training', 'Treatment Protocols', 'Tuberculosis', 'Underrepresented Students', 'University Hospitals', 'Vaccines', 'Wait Time', 'Wireless Technology', 'Woman', 'World Health Organization', 'accurate diagnosis', 'base', 'bioimaging', 'biomedical informatics', 'classification algorithm', 'clinical practice', 'community involvement', 'compliance behavior', 'data exchange', 'design', 'digital imaging', 'disadvantaged population', 'disease transmission', 'global health', 'handheld mobile device', 'image processing', 'improved', 'mHealth', 'mobile computing', 'novel', 'open source', 'point of care', 'programs', 'public health relevance', 'reproductive', 'screening', 'social', 'social inequality', 'telehealth', 'tool', 'tuberculosis diagnostics']",NIBIB,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2019,334204,0.03217961611059152
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9831333,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Community Health Centers', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2019,4776211,-0.0013185417169192081
"The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers Project Summary  One of the most glaring yet easily addressable gaps in our current scientific workflow and publication system is improving the way that methods are reported, in particular, the lack of key methodological details necessary for interpreting and reproducing a study. Most authors continue to cite the name of the reagent, like an antibody using the vendor, and the city where the vendor is located, but omit the catalog and lot number making antibodies very difficult to track down, thereby reducing reproducibility of the paper. The Resource Identification, RRID, Initiative has successfully implemented a solution to this lack of identification, by asking authors to include a persistent unique identifier (RRID) for each antibody along with a standard syntax that includes the lot information. This syntax is now required in about 100 journals and accepted in at least 400, was accepted into the EQUATOR network of standards and is under consideration by the JATS committee, the NISO standard for journal article metadata. For antibodies, RRIDs are assigned by the AntibodyRegistry.org, which accepts full catalogs from antibody companies and individual antibody records from authors, who are unable to locate the record for the antibody that they used in a study or one which they created in their lab. This process should be made as easy as possible for authors, and through text analysis we have devised a set of tools that should help authors create better records with less work.  The AntibodyRegistry.org was created as part of an academic project and it has successfully incorporated millions of antibody records, thousands of which have now been cited in scientific papers using the RRID syntax. The use of RRID is growing, and in order to support the longer term sustainability of the AntibodyRegistry.org, a core community authority for RRIDs, this resource needs to be enhanced to align with the available commercial and non-commercial funding sources. We propose the addition of features, valuable to antibody companies and journals, to improve market intelligence and reporting around antibodies. We also propose to auto-generate antibody entries for authors and curators when submitting/curating an antibody to decrease the time it takes to complete the task, thereby reducing the barrier to entry and cost. Project Narrative The AntibodyRegistry.org is a catalog of antibodies used in research and serves as the antibody identifier source for the Research Resource Identifier (RRID) initiative. The use of these identifiers improves rigor and transparency in compliance with both journal and NIH standards. As the RRID initiative grows, the AntibodyRegistry.org is becoming an increasingly well-used and valuable resource; requiring increased attention from curation staff. Improvements to the AntibodyRegistry.org are needed to increase commercial value and thereby its sustainability, decrease curation cost, and provide a higher level of service to the scientific community.",The Antibody Registry: A Community Authority for Antibody Research Resource Identifiers,9680073,R41GM131551,"['Adopted', 'Antibodies', 'Asia', 'Attention', 'Award', 'Businesses', 'California', 'Catalogs', 'Cities', 'Cloud Computing', 'Communities', 'Data', 'Data Analytics', 'Databases', 'Electronic Mail', 'Ensure', 'Environment', 'Europe', 'Feedback', 'Funding', 'Funding Agency', 'Generations', 'Glare', 'Goals', 'Individual', 'Intelligence', 'International', 'Journals', 'Machine Learning', 'Marketing', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Names', 'Neurosciences', 'Paper', 'Performance', 'Phase', 'Process', 'Provider', 'Publications', 'Reagent', 'Records', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Services', 'Small Business Technology Transfer Research', 'Source', 'System', 'Text', 'Time', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Work', 'authority', 'base', 'cost', 'improved', 'information framework', 'journal article', 'syntax', 'text searching', 'tool']",NIGMS,"SCICRUNCH, INC.",R41,2019,149373,-0.006998118306706181
"Administrative Supplement to the OAIC Pepper Center Coordinating Center We wish to advantage of 2 new key opportunities that could significantly enhance achievement of the overall goals of the OIAC Coordinating Center (OAIC CC) and 2 key, unexpected administrative needs. Project 1) Develop, test and implement an innovative set of tools to perform Integrative Data Analysis (IDA) for combining and analyzing independent data sets across the OAIC network An over-arching goal of the OAIC CC is to build collaborations between OAICs that unlock synergy. Each of the OAICs has many small/medium-sized completed studies relevant to the OAIC theme, and that have measured key domains of physical function. Combining these studies could provide large, powerful databases for answering critical questions not possible with individual studies. However, this is currently not possible because different measurement instruments are often used across centers and across studies. This project overcomes this critical limitation by taking advantage of 2 newly available technologies and an ongoing study. IDA is a set of strategies in which two or more independent data sets which contain measures addressing similar domains but using different measurement instruments are combined into one and then statistically analyzed. The proposed project is timely because it leverages an ongoing clinical study to validate new procedures for harmonizing measures of physical and cognitive function across 20 Pepper center studies. The resources created by the project will significantly enhance collaboration across the OAIC program network, benefiting researchers at all OAICs, and can be disseminated to other NIA center programs. Project 2) Develop a robust, interactive database of OAIC Program accomplishments that will automatically be updated via an efficient, streamlined, electronic annual reporting process.  It is widely believed that the NIA-funded Pepper Center program has been highly productive. However, there is no means of assessing the overall effectiveness of the Pepper Center, or of ‘cataloging’ its impressive accomplishments. This project will take advantage of new open-source technology to efficiently develop a robust, comprehensive, searchable, interactive database of past accomplishments. It will also develop a streamlined electronic Annual Directory Report template, and link it to the new OAIC database so that it is automatically updated each year. Achieving the goals of this project will reduce administrative burden for sites, facilitate NIA review of performance of centers, and create an annually updated database of OAIC accomplishments, projects, publications, and outcomes, and facilitate collaborations between centers and investigators across NIA programs. This application also requests support for 2 key, unexpected administrative needs that have arisen: 1) Increase in funding amount for the annual OAIC CC Multi-center pilot project. 2) Support for additional Pepper Centers that will soon be added to the OAIC network. Relevance Statement for OAIC Coordinating Center Administrative Supplement The Coordinating Center of the OAIC coordinates the activities of all the individual centers in the NIA- funded, OAIC network; its over-arching goal is to build collaborations between the individual OAICs and thereby unlock synergy and enable projects that could not be undertaken by any single OAIC center. This administrative supplement application proposes 2 developmental projects that will significantly enhance the capabilities of the OAIC to achieve these goals and which takes advantage of newly available methods and technology. This also includes additional support for the possible increase in the number of Pepper Centers and an increase in the pilot award budget.",Administrative Supplement to the OAIC Pepper Center Coordinating Center,9961004,U24AG059624,"['Achievement', 'Address', 'Administrative Supplement', 'Aging', 'Annual Reports', 'Award', 'Budgets', 'Capsicum', 'Cataloging', 'Catalogs', 'Clinical', 'Clinical Research', 'Cognition', 'Collaborations', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Directories', 'Effectiveness', 'Elderly', 'Equipment and supply inventories', 'Evaluation', 'Funding', 'Goals', 'Health', 'Individual', 'Link', 'Machine Learning', 'Measurement', 'Measures', 'Methods', 'Online Systems', 'Outcome', 'Participant', 'Performance', 'Physical Function', 'Pilot Projects', 'Procedures', 'Process', 'Psychometrics', 'Publications', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Site', 'Source', 'Statistical Data Interpretation', 'Technology', 'Testing', 'Time', 'Update', 'Walking', 'analytical method', 'analytical tool', 'base', 'cognitive function', 'cost effective', 'data modeling', 'forest', 'innovation', 'instrument', 'interest', 'lifestyle intervention', 'new technology', 'novel', 'open source', 'programs', 'recruit', 'response', 'synergism', 'theories', 'tool']",NIA,WAKE FOREST UNIVERSITY HEALTH SCIENCES,U24,2019,149775,0.004161840966739961
"LATTICE: A Software Platform for Prospective and Retrospective Image Based Translational Research PROJECT SUMMARY Imaging forms the backbone of living subjects research. Living subjects research is both essential to the progress of translational medicine and very expensive. The research community actively seeks to develop and validate new clinical endpoints to solve a range of etiology, natural history, diagnostic and prognostic problems. This project aims to develop and commercialize LATTICE, an Electronic Research Record, Image Management and Sharing Solution, and Deep Learning Platform. LATTICE is designed to increase the efficiency of imaging-driven biomedical research and clinical trials. This efficiency is accomplished first through a structured workflow that includes protocol management, subject scheduling, and records collection from multiple imaging modalities. Access to imaging and associated data within the same workflow simplifies the process for the research team. Structuring the data into a de-identified, privacy-managed Image Bank enables sharing for collaboration and re-use for retrospective research. Image processing algorithms connected to the Image Bank facilitate batch analysis, while the system also provides a platform for the development of new image-based outcome measures and clinical endpoints. A key objective of LATTICE is to enable investigators and collaborators to accelerate the translation of insights to the clinic with maximum efficiency. Successful translation requires structuring the workflow, record keeping, and protocols into a rigorous, transparent, reproducible and validated process. LATTICE is designed to reduce the friction in translating successful research projects to the clinic. Researchers in the Advanced Ocular Imaging Program (AOIP) at the Medical College of Wisconsin developed elements of LATTICE as separate technologies. The Specific Aims of this proposal are directed to an integrated workflow addressing a broader set of objectives. The AOIP LATTICE Electronic Research Record will be translated into a commercially managed repository and brought under regulatory Design Control. The current AOIP Image Bank containing 3,000,000 de- identified retinal images will be integrated into the LATTICE workflow. Critically, this integration will allow the sharing of the Image Bank with external researchers. Three retinal image process algorithms that operate on retinal images will integrate into this workflow. These algorithms include analysis of adaptive optics images of the fundus, analysis of the foveal avascular zone from optical coherence tomography angiography (OCTA), and model-based analysis of the fovea imaged with OCT. A computational deep learning workflow will also be prototyped using a cloud-based architecture. This final workflow will be constructed to demonstrate the feasibility of deploying a collaborative deep learning environment for the development of new clinical endpoints using shared, de-identified images. LATTICE will be a unique system for both prospective and retrospective translational research. LATTICE will make a profound impact on the cost of managing image-based research and add leverage to translational research expenditures for moving insights into the clinic. PROJECT NARRATIVE LATTICE is an innovative electronic research record and development platform for image-based ophthalmic research. LATTICE is designed to reduce the cost of translational research, promote the re- use of images, and simplify the development and application of new techniques to analyze medical images. LATTICE will integrate research workflow tools with a database of 3,000,000 retinal images and advanced image processing software to accelerate the process of translating eye research insights from the lab to the clinic.",LATTICE: A Software Platform for Prospective and Retrospective Image Based Translational Research,9777970,R43EY030408,"['Address', 'Algorithmic Software', 'Algorithms', 'Angiography', 'Architecture', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Trials', 'Code', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Diagnostic', 'Documentation', 'Elements', 'Etiology', 'Expenditure', 'Eye', 'Friction', 'Future', 'Health Insurance Portability and Accountability Act', 'Image', 'Libraries', 'Medical Imaging', 'Methods', 'Modeling', 'Morphology', 'Natural History', 'Optical Coherence Tomography', 'Outcome Measure', 'Output', 'Privacy', 'Process', 'Protocols documentation', 'Recording of previous events', 'Records', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Scanning', 'Schedule', 'Secure', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Translating', 'Translational Research', 'Translations', 'Vertebral column', 'Wisconsin', 'Writing', 'adaptive optics', 'base', 'cloud based', 'cost', 'deep learning', 'design', 'educational atmosphere', 'fovea centralis', 'fundus imaging', 'image processing', 'imaging modality', 'imaging platform', 'imaging program', 'innovation', 'insight', 'medical schools', 'ocular imaging', 'operation', 'prognostic', 'programs', 'prospective', 'prototype', 'repository', 'retinal imaging', 'system architecture', 'tool', 'translational medicine', 'web services', 'wiki']",NEI,"TRANSLATIONAL IMAGING INNOVATIONS, INC.",R43,2019,299999,-0.006343516251929422
"IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP The purpose of this contract is to provide bioinformatic support to researchers in the Divisions of National Toxicology Program (DNTP) and Intramural Research (DIR) at the National Institute of Environmental Health Sciences (NIEHS). NIEHS researchers conduct studies that produce large amounts of data, varying in size and complexity. Fields of scientific study are diverse and include toxicology, genomics, transcriptomics, high throughput screening (HTS) data and data extraction from diverse text resources. The variety and complexity of NIEHS scientific studies dictates the need for innovative analytical techniques and the development of new software tools. Bioinformatic data analyses are required to support accurate and precise interpretation of study results. Specific bioinformatics needs include data analysis, data mining, creating bioinformatics pipelines for gene expression and pathway analysis and computational support for the vast amount of data collected through studies conducted at NIEHS and NIEHS contract laboratories. n/a",IGF::OT::IGF  BIOINFORMATICS SUPPORT FOR THE NIEHS IN DIR & DNTP,9915697,73201700001C,"['Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'ChIP-seq', 'Chemical Exposure', 'Chemicals', 'Contractor', 'Contracts', 'DNA Methylation', 'DNA Sequence', 'DNA sequencing', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Epigenetic Process', 'Evaluation', 'Exons', 'Gene Expression', 'Genes', 'Genomics', 'Informatics', 'Intramural Research', 'Knowledge', 'Laboratories', 'Literature', 'Measures', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Output', 'Pathway Analysis', 'Peer Review', 'Privatization', 'Programming Languages', 'Proteomics', 'Publications', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Sampling', 'Scientific Evaluation', 'Scientist', 'Series', 'Software Tools', 'Specific qualifier value', 'Technology', 'Text', 'Toxicogenomics', 'Toxicology', 'analysis pipeline', 'bioinformatics tool', 'bisulfite sequencing', 'cheminformatics', 'computational intelligence', 'data integration', 'data mining', 'differential expression', 'high throughput screening', 'innovation', 'meetings', 'metabolomics', 'method development', 'next generation sequencing', 'physical property', 'programs', 'screening', 'technique development', 'transcriptomics', 'whole genome']",NIEHS,"SCIOME, LLC",N01,2019,2464037,-0.014651511475462392
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9788202,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2019,114747,-0.007240429844719558
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9703267,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'tool']",NICHD,DUKE UNIVERSITY,R25,2019,161462,-0.021053079230440214
"Omics for TB:  Response to Infection and Treatment Abstract – Overview With about 10 million new cases of active disease and 1.8 million deaths annually, TB is a global health emergency. A distinguishing feature of TB disease is its biological heterogeneity, which manifests at the clinical level chiefly in 2 forms: disease progression and treatment response. The premise of this Program is that the heterogeneous outcomes of TB infection and treatment are determined by the interplay of competing regulatory networks between the pathogen and the host. Our primary goal is to apply systems biology approaches to elucidate the biological control underlying the variability of disease outcome and response to treatment. Our first specific aim is to define novel host regulators of TB disease progression in vivo, and the innate and adaptive networks they control. We will also seek to define novel Mtb regulators of TB treatment response, and the Mtb regulatory networks that they control. This work will allow us to produce and validate host and Mtb models of TB disease progression and treatment response. Altogether, this program addresses key unanswered questions that stymie efforts to combat the TB pandemic. Our team has perfected the required platforms and scientific approaches to execute this ambitious research plan in a timely and cost- effective manner. All the participating investigators have strong records of interacting productively, and of disseminating their data and reagents to the scientific community. Project Narrative - Omics for TB: Response to Infection and Treatment Mycobacterium tuberculosis causes ~10 million new cases of active disease and 1.8 million deaths each year, and our tools to combat tuberculosis (TB) disease are universally outdated and overmatched. This project combines separate advances in systems biology and network modeling to produce experimentally grounded and verifiable systems-level models of the host and MTB regulatory networks that affect disease progression and response to treatment.",Omics for TB:  Response to Infection and Treatment,9646321,U19AI135976,"['Address', 'Affect', 'Bacteria', 'Biological', 'Cessation of life', 'Clinical', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Data', 'Data Set', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug-sensitive', 'Eicosanoids', 'Elements', 'Genetic Transcription', 'Goals', 'Human', 'Immunologic Receptors', 'Infection', 'Inflammatory Response', 'Infrastructure', 'Machine Learning', 'Mass Spectrum Analysis', 'Methodology', 'Modeling', 'Molecular Profiling', 'Mouse Strains', 'Multiplexed Ion Beam Imaging', 'Mus', 'Mycobacterium tuberculosis', 'Network-based', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Predisposition', 'Proteomics', 'Reagent', 'Receptor Activation', 'Records', 'Regulator Genes', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Symptoms', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Time', 'Treatment outcome', 'Tuberculosis', 'Vaccines', 'Work', 'base', 'biological heterogeneity', 'chemotherapy', 'combat', 'cost effective', 'design', 'global health emergency', 'high risk', 'in vivo', 'member', 'metabolomics', 'mouse model', 'network models', 'novel', 'pandemic disease', 'pathogen', 'predictive signature', 'programs', 'protein protein interaction', 'response', 'tool', 'transcriptomics', 'treatment response', 'tuberculosis treatment']",NIAID,SEATTLE CHILDREN'S HOSPITAL,U19,2019,3359502,0.009195080837275781
"COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: Decentralized, Scalable Analysis of Loosely Coupled Data",9938885,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'Intelligence', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'base', 'commune', 'computational platform', 'computer framework', 'computing resources', 'connectome', 'cost', 'data anonymization', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'preservation', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2019,585151,0.022189618890644275
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future. Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9999822,R24MH114788,"['Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data pipeline', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'machine learning algorithm', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2019,102318,-0.012380558661627178
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9748608,R24MH114788,"['Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data pipeline', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'machine learning algorithm', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2019,1263611,-0.012380558661627178
"Laboratory of Neuro Imaging Resource (LONIR) PROJECT SUMMARY - OVERALL The LONIR is focused on developing innovative solutions for the investigation of imaging, genetics, behavioral and clinical data. The LONIR structure is designed to facilitate studies of dynamically changing anatomic frameworks, e.g., developmental, neurodegenerative, traumatic, and metastatic, by providing methods for the comprehensive understanding of the nature and extent of these processes. Specifically, TR&D1 (Data Science) focuses on methodological developments for the management and informatics of brain and related data. This project will develop and issue new methods for robust scientific data management to create an environment where scientific analyses can be reproduced and/or enhanced, data can be easily discovered and reused, and analysis results can be visualized and made publicly searchable. TR&D2 (Diffusion MRI and Connectomics) seeks to advance the study of brain connectivity using diffusion imaging and its powerful extensions. This project will go beyond traditional tensor models of diffusion for assessing tissue and fiber microstructure and connectivity, develop tract-based statistical analysis tools using Deep Learning, introduce novel adaptive connectivity mapping approaches, using L1 fusion of multiple tractography methods, and provide mechanisms to study connectivity and diffusion imaging over 10,000 subjects. (This technology and these methods will be managed and executed by the TR&D1 framework to distributed datasets totaling over 10,000 subjects). Lastly, our TR&D3 (Intrinsic Surface Mapping) develops a general framework for surface mapping in the high dimensional Laplace-Beltrami embedding space via the mathematical optimization of their Riemannian metric. Our approach here overcomes fundamental limitations in existing methods based on spherical registration by eliminating the metric distortion during the parameterization step, thus achieving much improved accuracy in mapping brain anatomy. Coupled with a mature and efficient administrative structure and comprehensive training and dissemination, this program serves a wide and important need in the scientific community. PROJECT NARRATIVE - OVERALL The comprehensive suite of technologies include algorithmic and computational methods for image management, processing, data analysis and visualization. The technologies are ideally suited to enable holistic studies of the interactions between different imaging data modalities, phenotypic population characteristics, and physiological brain connectivity.",Laboratory of Neuro Imaging Resource (LONIR),9700661,P41EB015922,"['AIDS/HIV problem', 'Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Atlases', 'Award', 'Behavioral', 'Books', 'Brain', 'Brain Mapping', 'Brain imaging', 'Clinical Data', 'Clinical Research', 'Communities', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Database Management Systems', 'Databases', 'Dementia', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Environment', 'Equilibrium', 'Evolution', 'Fiber', 'Funding', 'Image', 'Informatics', 'Infrastructure', 'Ingestion', 'Investigation', 'Laboratories', 'Manuscripts', 'Mathematics', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Parkinson Disease', 'Peer Review', 'Phenotype', 'Physiological', 'Population', 'Population Characteristics', 'Process', 'Protocols documentation', 'Publications', 'Publishing', 'Reproducibility', 'Research Activity', 'Research Personnel', 'Resources', 'Schizophrenia', 'Science', 'Services', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Students', 'Surface', 'System', 'Technology', 'Tissues', 'Training', 'United States National Institutes of Health', 'Visualization software', 'algorithmic methodologies', 'analysis pipeline', 'autism spectrum disorder', 'base', 'brain shape', 'cohort', 'computer grid', 'computer infrastructure', 'computerized data processing', 'data archive', 'data management', 'data resource', 'data visualization', 'deep learning', 'design', 'high dimensionality', 'imaging genetics', 'imaging modality', 'improved', 'innovation', 'morphometry', 'neuroimaging', 'new technology', 'novel', 'programs', 'symposium', 'synergism', 'technology research and development', 'tool', 'tractography', 'translational study', 'web services']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,P41,2019,1217435,-0.0018138236350510636
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive, PhysioBank, was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. PhysioToolkit, its software collection, supports exploration and quantitative analyses of PhysioBank and similar data with a wide range of well-documented, rigorously tested open-source software that can be run on any platform. PhysioNet's team of researchers leverages results of other funded projects to drive the creation and enrichment of: i) Data collections that provide increasingly comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC III (Medical Information Mart for Intensive Care) Database of critical care patients; ii) Analytic methods that lead to more timely and accurate diagnoses of major public health problems (such as life-threatening cardiac arrhythmias, infant apneas, fall risk in older individuals and those with neurologic disease, and seizures), and iii) Elucidation of dynamical changes associated with a variety of pathophysiologic processes and aging (such as cardiopulmonary interactions during sleep disordered breathing syndromes); User interfaces, reference materials and services that add value and improve accessibility to PhysioNet's data and software (such as PhysioNetWorks, a virtual laboratory for data sharing). Impact: Cited in The White House Fact Sheet on Big Data Across the Federal Government (March 29, 2012), PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are inaccessible otherwise. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world- wide, growing community of researchers, clinicians, educators, students, and medical instrument and software developers, retrieve about 380 GB of data per day. By providing free access to its unique and wide-ranging data and software collections, PhysioNet is invaluable to studies that currently result in an impressive average of nearly 250 new scholarly articles per month by academic, clinical, and industry-affiliated researchers worldwide. Over the next year we aim to sustain and enhance PhysioNet's impact with new technology and data; and complete the 2019 PhysioNet/Computing in Cardiology Challenge on sepsis. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,9993811,R01GM104987,"['Aging', 'Algorithms', 'Apnea', 'Area', 'Arrhythmia', 'Big Data', 'Biomedical Research', 'Boston', 'Bypass', 'Cardiology', 'Cardiopulmonary', 'Categories', 'Clinical', 'Clinical Data', 'Cloud Service', 'Collection', 'Communities', 'Community Outreach', 'Complex', 'Computer software', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Dedications', 'Development', 'Diagnostic radiologic examination', 'Entropy', 'FAIR principles', 'Federal Government', 'Functional disorder', 'Funding', 'Grant', 'Imagery', 'Individual', 'Industry', 'Infant', 'Infrastructure', 'Intensive Care', 'Israel', 'Journals', 'Laboratories', 'Lead', 'Licensing', 'Life', 'Link', 'Machine Learning', 'Maintenance', 'Medical', 'Medical center', 'Methods', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase Transition', 'Physiological', 'Process', 'Public Health', 'Publishing', 'Radiology Specialty', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Roentgen Rays', 'Role', 'Running', 'Seizures', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Students', 'Switzerland', 'Syndrome', 'Testing', 'Thoracic Radiography', 'Time', 'United States National Institutes of Health', 'University Hospitals', 'Visit', 'accurate diagnosis', 'analytical method', 'clinical application', 'computerized data processing', 'computing resources', 'data archive', 'data sharing', 'experience', 'fall risk', 'heart rate variability', 'improved', 'innovation', 'instrument', 'instrumentation', 'interest', 'member', 'nervous system disorder', 'new technology', 'open source', 'preservation', 'repository', 'signal processing', 'software repository', 'symposium', 'time interval', 'virtual laboratory']",NIGMS,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2019,409563,0.0032579725481301867
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9765194,R44CA206782,"['Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Monitor', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'data warehouse', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'off-patent', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2019,238768,0.020345519130626096
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9762963,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Infrastructure', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2019,10000,-0.001603630888927633
"Mentoring in Immunometabolic Dysregulation in TB and TB/HIV Tuberculosis (TB) is the leading cause of death among people living with HIV (PLWH) worldwide. Despite recent scientific advances, significant gaps remain in our understanding of the immune mechanisms responsible for control and eradication of Mycobacterium tuberculosis (Mtb) infection. PLWH with latent TB infection (LTBI) have a ~10% annual risk of progressing to TB disease, however currently available tests for LTBI diagnosis have reduced sensitivity in this population and are not able to predict which latently infected individuals are at highest risk for developing TB for targeted preventive therapy. Emerging data from clinically relevant animal models suggest that LTBI and active TB represent a spectrum of immune responses and host pathology, with increasing metabolic changes and immune dysregulation during the transition to TB disease. We have identified unique serum metabolite and microRNA (miRNA) profiles that are able to discriminate between patients with TB and those with non-TB lung disease. However, these novel TB signatures have not been assessed prospectively to identify PLWH and HIV-negative persons with LTBI who are at increased risk for TB progression. In order to address this significant knowledge gap, in Aim 1 of the current research program, trainees will leverage the Indian and South African RePORT longitudinal biorepositories of household contacts of TB index cases to test the hypothesis that TB is a chronic inflammatory disease associated with profound changes in immune regulation and metabolism prior to the onset of clinical signs and symptoms. Another major barrier to global TB eradication efforts is the lengthy and complicated current anti-tubercular regimen, which is associated with medical nonadherence and the emergence of drug resistance. Recently, attention has focused on host-directed adjunctive therapies aimed at optimizing immune responses to the pathogen and improving lung damage. Lipid-laden macrophages (foam cells) are central to maintaining chronic TB infection by providing a favorable niche in which antimicrobial functions are down-regulated, and by inducing caseation and tissue damage. Recent work has shown that foam-cell-rich and necrotic areas of TB granulomas are particularly enriched in triglycerides. Mtb infection is associated with dysregulation of two cellular pathways involved in triglyceride homeostasis: a pro-lipogenic pathway involving protein kinase B and mTOR complex 1 (Akt/mTORC1), and an anti- lipogenic pathway involving AMP-activated protein kinase and the sirtuins (AMPK/SIRT). In Aim 2, trainees will use longitudinal clinical samples from RePORT study participants and experimental infections ex vivo to characterize: (i) the relationship between activation of these pathways and control of clinical Mtb infection, and the effect of anti-lipogenic treatments on antimycobacterial functions of human macrophages infected ex vivo. The research aims will be integrated with a mentoring strategy for mentees that fosters development of high impact patient-oriented research with a pathway to independence. Tuberculosis (TB) remains among the most deadly infections worldwide, especially among people living with HIV. Current available tests are not able to accurately detect persons at the highest risk of developing TB, and curing the disease requires at least 6 months of therapy because the TB bacteria can avoid being killed by the immune system and currently available drugs. In the current proposal, physician scientists will receive training in a variety of complementary disciplines, and use several cutting-edge experimental and modeling techniques to analyze samples from patients with TB and TB/HIV, with the ultimate goal of identifying new biomarkers that can predict TB disease and host-directed therapies that can shorten TB treatment.",Mentoring in Immunometabolic Dysregulation in TB and TB/HIV,9846869,K24AI143447,"['5&apos', '-AMP-activated protein kinase', 'Address', 'Animal Model', 'Antibiotics', 'Antimycobacterial Agents', 'Archives', 'Area', 'Attention', 'Automobile Driving', 'Bacteria', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Cause of Death', 'Chronic', 'Clinical', 'Clinical Data', 'Complex', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline', 'Disease', 'Drug resistance', 'Environment', 'Experimental Models', 'FRAP1 gene', 'Foam Cells', 'Fostering', 'Goals', 'HIV', 'HIV Seronegativity', 'HIV Seropositivity', 'HIV/TB', 'Homeostasis', 'Household', 'Human', 'Immune', 'Immune response', 'Immune system', 'Individual', 'Infection', 'Inflammatory', 'International', 'Knowledge', 'Lesion', 'Lipid-Laden Macrophage', 'Lipids', 'Lung diseases', 'Machine Learning', 'Medical', 'Mentors', 'Metabolic', 'Metabolism', 'MicroRNAs', 'Mycobacterium tuberculosis', 'Necrosis', 'Outcome', 'Participant', 'Pathology', 'Pathway interactions', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Prevention', 'Preventive therapy', 'Proto-Oncogene Proteins c-akt', 'Pulmonary Tuberculosis', 'Regimen', 'Reporting', 'Research', 'Resources', 'Risk', 'Sampling', 'Scientific Advances and Accomplishments', 'Scientist', 'Serum', 'Signs and Symptoms', 'Sirtuins', 'South African', 'Specimen', 'Systems Biology', 'Techniques', 'Testing', 'Tissues', 'Training', 'Triglycerides', 'Tuberculosis', 'Validation', 'Whole Blood', 'Work', 'World Health Organization', 'antimicrobial', 'biobank', 'biosignature', 'career', 'clinically relevant', 'cohort', 'cytokine', 'high risk', 'immunoregulation', 'improved', 'indexing', 'lifetime risk', 'lung injury', 'macrophage', 'monocyte', 'mycobacterial', 'novel', 'pathogen', 'patient oriented research', 'peripheral blood', 'prevent', 'programs', 'prospective', 'tool', 'transcriptome', 'transcriptome sequencing', 'tuberculosis granuloma', 'tuberculosis treatment']",NIAID,JOHNS HOPKINS UNIVERSITY,K24,2019,191195,-0.025378156051191557
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9607599,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2019,559237,0.02752511428024238
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine No abstract available PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,10063300,U01LM012675,[' '],NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2019,375751,-0.004744109237262562
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),9850367,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip region structure', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2019,644134,0.013901090309570674
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9747930,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,258870,0.0020426651608675193
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9992241,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2019,76935,0.0020426651608675193
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9752596,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2019,323659,-0.029070203374494993
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,9916138,R01EB029272,"['Address', 'Aging', 'Biophysics', 'Brain', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Machine Learning', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'learning strategy', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2019,215134,-0.0005907115390546822
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9733348,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Infrastructure', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'data sharing', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual technology']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2019,379357,0.0014401049620740623
"Pathology Image Informatics Platform for visualization, analysis and management ﻿    DESCRIPTION (provided by applicant): With the advent of whole slide digital scanners, histopathology slides can be digitized into very high-resolution digital images, realizing a new ""big data"" stream that can potentially rival ""omics data"" in size and complexity. Just as with the analysis of high-throughput genetic and expression data, the application of sophisticated image analytic tools and data pipelines can render the often passive data of digital pathology (DP) archives into a powerful source for: (a) rich quantitative insights into cancer biology and (b) companion diagnostic decision support tools for precision medicine. Digital pathology enabled companion diagnostic tests could yield predictions of cancer risk and aggressiveness in a manner similar to molecular diagnostic tests. However, prior to widespread clinical adoption of DP, extensive evaluation of clinical interpretation of DP imaging (DPI) and accompanying decision support tools needs to be undertaken. Wider acceptance of DPI by the cancer community (clinical and research) is hampered by lack of a publicly available, open access image informatics platform for easily viewing, managing, and quantitatively analyzing DPIs. While some commercial platforms exist for viewing and analyzing DPI data, none of these platforms are freely available. Open source image viewing/management platforms that cater to the radiology (e.g. XNAT) and computational biology communities are typically not conducive to handling very large file sizes as encountered with DPI datasets.  This multi-PI U24 proposal seeks to expand on an existing, freely available pathology image viewer (Sedeen Image Viewer) to create a pathology informatics platform (PIIP) for managing, annotating, sharing, and quantitatively analyzing DPI data. Sedeen was designed as a universal platform for DPI (by addressing several proprietary scanner formats and ""big data"" challenges), to provide (1) reliable and useful image annotation tools, and (2) for image registration and analysis of DPI data. Additionally, Sedeen has become an application for cropping large DPIs so that they can be input into programs such as Matlab or ImageJ. Sedeen has been freely available to the public for three years, with over 160 unique users from over 20 countries.  Building on the initial successes of Sedeen and its existing user base, our intent is to massively increase dissemination of DPI and algorithms in the cancer research community and clinical trial efforts, as well as to contribute towards the adoption of a rational and standardized set of DP operational conventions. This unique project will allow end users with different needs and technical backgrounds to seamlessly (a) archive and manage, (b) share, and (c) visualize their DPI data, acquired from different sites, formats, and platforms. The PIIP will provide a unified user interface for third party algorithms (nuclear segmentation, color normalization, biomarker quantification, radiology-pathology fusion) and will allow for algorithmic evaluation upon data arising from a plurality of source sites. By partnering with professional societies, we envision that the PIIP user base will expand to include the oncology, pathology, radiology, and pharmaceutical communities. PUBLIC HEALTH RELEVANCE: This grant will result in the further development of advanced functionality of the already existing digital pathology image informatics platform (PIIP) with an established user-base for cancer research. Such an enhanced platform will provide the much-needed foundation for advancing (a) routine clinical adoption of digital pathology for primary diagnosis and (b) training and validation of companion diagnostic decision support systems based off histopathology. Thus, the project is aligned with the NCI's goal to foster innovative research strategies and their applications as a basis for ultimately protecting and improving human health.","Pathology Image Informatics Platform for visualization, analysis and management",9784742,U24CA199374,"['Address', 'Adoption', 'Advanced Development', 'Algorithmic Analysis', 'Algorithms', 'American', 'Archives', 'Big Data', 'Biological Markers', 'Cancer Biology', 'Cancer Prognosis', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Color', 'Communities', 'Community Clinical Oncology Program', 'Community Trial', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computer software', 'Country', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Storage and Retrieval', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Evaluation', 'Felis catus', 'Fostering', 'Foundations', 'Genetic', 'Goals', 'Grant', 'Health', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Informatics', 'Institution', 'International', 'Language', 'Length', 'Letters', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical Imaging', 'Medical Students', 'Molecular Diagnostic Testing', 'Morphology', 'Nuclear', 'Ontology', 'Optics', 'Pathologist', 'Pathology', 'Pharmacologic Substance', 'Professional Organizations', 'Protocols documentation', 'Pythons', 'Radiology Specialty', 'Research', 'Resolution', 'Scientist', 'Site', 'Slide', 'Societies', 'Source', 'Standardization', 'Stream', 'Training', 'Training and Education', 'Validation', 'analytical tool', 'annotation  system', 'anticancer research', 'base', 'biomarker discovery', 'biomedical scientist', 'cancer diagnosis', 'cancer imaging', 'cancer risk', 'clinical research site', 'companion diagnostics', 'computer human interaction', 'data exchange', 'data integration', 'data pipeline', 'data sharing', 'design', 'digital', 'digital imaging', 'digital pathology', 'drug discovery', 'high throughput analysis', 'image archival system', 'image registration', 'imaging informatics', 'improved', 'in vivo imaging', 'innovation', 'insight', 'interest', 'malignant breast neoplasm', 'oncology', 'open source', 'pathology imaging', 'photonics', 'precision medicine', 'programs', 'public health relevance', 'quantitative imaging', 'radiological imaging', 'repository', 'research clinical testing', 'success', 'support tools', 'symposium', 'tool', 'tumor', 'user friendly software', 'validation studies']",NCI,CASE WESTERN RESERVE UNIVERSITY,U24,2019,533529,-0.010557693554260415
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9731544,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'repository', 'research and development', 'software development', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2019,350620,-0.005553148915553494
"INvestigations In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT) Project Summary Gout impacts around 4% of the U.S. adult population and is the most common form of inflammatory arthritis in men. The incidence of gout is increasing worldwide. Gout significantly burdens the healthcare system, and is associated with both decreased work productivity and quality of life. With the ever-increasing impact of gout and its associated comorbidities on the population, investigation of novel translational mechanisms mediating gout flares as well as approaches to improving gout and hyperuricemia outcomes comprise an unmet and urgent medical need. When funded, the INvestigationS In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT) included 4 active research projects and an Administrative Core focused on the theme, “Gout, Hyperuricemia, and Associated Comorbidities”. Projects include studies to: determine if adenosine monophosphate-activated protein kinase (AMPK) activity metabolomics have promise as gout biomarkers independent of serum urate (P1), examine the influence of key gene-environment interactions within an internet study of gout flares (P2), unravel the functional genomics of urate transporter genes identified in previously reported genome wide association studies (P3), and investigate the mechanism of urate lowering therapy on renal function within VA STOP-GOUT (P4). Projects range from basic research translation of underlying genetics and inflammatory pathobiology of gout, to understanding mechanisms of CKD progression in gout, and translation of genetic interaction with environmental factors and medications, to precision medicine. The proposed revision project (P5) aims to utilize a novel emergency department based intervention to test methods to improve the care gout patients in the Deep South, with a secondary goal of concurrently enhancing participation of minorities in research. The revised INSIGHT CORT aims to: (1) Conduct five outstanding, innovative, and synergistic research projects drawing on the unique strengths of multidisciplinary research teams at our four major centers: University of Alabama at Birmingham, Harvard University, University of California San Diego, and now Vanderbilt University Medical Center; (2) Foster the development of pilot and feasibility projects and the development and application of new translational methods to research in gout and hyperuricemia and their associated major comorbidities, particularly CKD and metabolic syndrome; and (3) Promote training of translational investigators in current methods of research applicable to gout and hyperuricemia through enrichment activities overseen by our Administrative Core. The proposed revision project directly ties to the CORT through patient enrollment into the Gout CORT Registry and Biorepository, which contributes clinical data and biological samples to projects. The INSIGHT CORT is a multi-disciplinary translational research program at UAB and partner institutions. We have assembled an outstanding team and are uniquely prepared and strongly committed to scientific rigor, innovation, and development of knowledge and translational techniques. Project Narrative The prevalence of gout has been steadily increasing over several decades and is correlated with the rising burden of obesity, chronic cardiac and renal disease; all conditions overrepresented in the Southeastern U.S. – particularly in African Americans. Through a novel emergency department led intervention we aim to improve the care patients with gout receive, both during acute exacerbations and long-term. A secondary goal of the project is to concurrently enhance participation of minorities in biomedical research in the Deep South.","INvestigations In Gout, Hyperuricemia, and comorbidiTies (INSIGHT) Center of Research Translation (CORT)",9902085,P50AR060772,"['Absenteeism', 'Academic Medical Centers', 'Accident and Emergency department', 'Acute', 'Address', 'Adenosine Monophosphate', 'Adult', 'Affect', 'African American', 'Alabama', 'Basic Science', 'Biological', 'Biological Markers', 'Biomedical Research', 'Biometry', 'California', 'Caring', 'Chronic', 'Chronic Disease', 'Chronic Kidney Failure', 'Clinical Data', 'Clinical Informatics', 'Clinical Trials', 'Communities', 'Comorbidity', 'Computerized Medical Record', 'Continuity of Patient Care', 'Deep South', 'Development', 'Diabetes Mellitus', 'Disease Progression', 'Doctor of Philosophy', 'Early identification', 'Educational Intervention', 'Emergency Department-based Intervention', 'Emergency Medicine', 'Enrollment', 'Environmental Risk Factor', 'Epidemic', 'Epidemiology', 'Face', 'Flare', 'Fostering', 'Frequencies', 'Funding', 'Future', 'Genes', 'Genetic', 'Genetic Translation', 'Goals', 'Gout', 'Health', 'Healthcare', 'Healthcare Systems', 'Heart Diseases', 'Hyperuricemia', 'Incidence', 'Inflammatory', 'Inflammatory Arthritis', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'International', 'Internet', 'Intervention', 'Investigation', 'Kidney Diseases', 'Knowledge', 'Laboratories', 'Link', 'Mediating', 'Medical', 'Metabolic syndrome', 'Methodology', 'Methods', 'Minority', 'Minority Participation', 'Mission', 'Modernization', 'National Institute of Arthritis and Musculoskeletal and Skin Diseases', 'Natural Language Processing', 'Nephrology', 'Obesity', 'Outcome', 'Patient Care', 'Patient Education', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Phenotype', 'Population', 'Premature Mortality', 'Prevalence', 'Productivity', 'Protein Kinase', 'Quality of life', 'Recommendation', 'Registries', 'Renal function', 'Reporting', 'Research', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatology', 'Sampling', 'Serum', 'Specimen', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Translational Research', 'Universities', 'Urate', 'Work', 'aging population', 'base', 'biobank', 'clinically relevant', 'functional genomics', 'gene environment interaction', 'genome wide association study', 'health care service utilization', 'health disparity', 'improved', 'innovation', 'men', 'metabolomics', 'minority health', 'multidisciplinary', 'novel', 'personalized medicine', 'precision medicine', 'productivity loss', 'recruit', 'tool', 'translational research program', 'translational scientist', 'urate transporter']",NIAMS,UNIVERSITY OF ALABAMA AT BIRMINGHAM,P50,2019,961116,-0.00449043250281314
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9789913,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2019,750000,-0.006690567338544198
"Predicting tuberculosis outcomes using genotypic and biomarker signatures PROJECT SUMMARY/ABSTRACT Tuberculosis (TB) is caused by an infectious pathogen, Mycobacterium tuberculosis (M.tb) in susceptible individuals, but we cannot yet classify or predict outcomes in those prone to pulmonary TB disease versus those prone to resistance. In part, this reflects knowledge gaps regarding genotypes that may increase susceptibility, and in validated disease correlates (e.g. serum of lung protein biomarkers) measured individually, or combined signatures. We address these knowledge gaps by using Diversity Outbred (DO) mice, a population with abundant genetic diversity and heterozygosity, like the human population. Also, like humans, a low dose M.tb infection of DO mice produces a spectrum of outcomes, from highly susceptible to highly resistant, and many intermediate outcomes. In this proposal, we use the DO population to: 1) Identify and test the capacity of genotypic (alleles and statistically significant loci) to predict outcomes such as diagnostic category (class); and 2) To identify and test lung and serum biomarker (protein) and granuloma signatures to determine diagnostic category (class); and 3) To identify and test serum biomarker (protein) signatures that can forecast disease onset, within a 3-week window before illness manifests clinically. The best performing signatures will be tested using samples from humans. Collectively, results from these studies will generate new translatable knowledge regarding correlates of pulmonary TB (useful for diagnostics), and genotypic and serum protein signatures (useful for prognostics). PROJECT NARRATIVE Mycobacterium tuberculosis (M.tb) causes tuberculosis (TB) in millions of susceptible humans each year. It is well known that humans respond variably to M.tb infection, yet we are unable to predict outcomes with accuracy. Here, we use the Diversity Outbred (DO) mouse population to identify and test genotypic, serum, and lung biomarker signatures to accurately predict outcomes. Findings are also validated in samples from humans.",Predicting tuberculosis outcomes using genotypic and biomarker signatures,9642291,R01HL145411,"['AIDS/HIV problem', 'Address', 'Adult', 'Aerosols', 'Alleles', 'Animal Model', 'Bacillus (bacterium)', 'Biological Markers', 'Blood', 'Categories', 'Classification', 'Clinical', 'Consensus', 'Data', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Dose', 'Genetic Variation', 'Genotype', 'Granuloma', 'Harvest', 'Heterozygote', 'Human', 'Image', 'Image Analysis', 'Inbred Strain', 'Individual', 'Infection', 'Intervention', 'Knowledge', 'Lung', 'Malaria', 'Malignant Neoplasms', 'Measures', 'Minority', 'Modeling', 'Morbidity - disease rate', 'Mus', 'Mycobacterium tuberculosis', 'Necrosis', 'Onset of illness', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Population', 'Predisposition', 'Process', 'Production', 'Proteins', 'Pulmonary Tuberculosis', 'Quantitative Trait Loci', 'Resistance', 'Sampling', 'Serum', 'Serum Proteins', 'Structure', 'Testing', 'Time', 'Training', 'Tuberculosis', 'Vehicle crash', 'base', 'human pathogen', 'improved', 'individual patient', 'learning algorithm', 'model development', 'novel diagnostics', 'novel marker', 'outcome forecast', 'outcome prediction', 'pathogen', 'predictive marker', 'predictive modeling', 'prognostic', 'protein biomarkers', 'public health intervention', 'response', 'supervised learning', 'survival outcome', 'tool', 'transmission process', 'tuberculosis diagnostics']",NHLBI,TUFTS UNIVERSITY BOSTON,R01,2019,617451,-0.029030742258978694
"Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND) Project Summary/Abstract  This Phase III (P-III) COBRE project will extend the cores that have been successfully leveraged in our Phase I (P-I) and Phase II (P-II) COBRE projects and sustain these unique resources in New Mexico through the im- plementation of a business plan. Over the past eight years we have built up infrastructure and created a cutting edge brain imaging center, our P-II project is just over half-way through and is even more successful than our P- I was at this point in time. The Mind Research Network (MRN) houses an Elekta Neuromag 306-channel MEG System, a high density EEG lab, a 3T Siemens Trio MRI scanner, and a mobile 1.5T Siemens Avanto MRI scanner. Additional resources include a centralized neuroinformatics system, a strong IT management plan, and state-of-the-art image analysis expertise and tools. This P-III COBRE center will continue our momentum and move the cores we have developed into a position of long term sustainability. We will continue with the technical cores established during the P-II project including multimodal data acquisition (MDA), algorithm and data analy- sis (ADA), and biostatistics and neuro-informatics (BNI). These cores have begun to serve MRN and the greater community, as well as other institutions including extensive collaborations with IDeA funded projects in New Mexico and other states. We believe this P-III COBRE is extremely well-positioned to establish and sustain New Mexico as one of the premier brain imaging sites. We include an extensive pilot project program (PPP) that is built on the successful pilot programs implemented as part of the earlier COBRE phases. This includes an ex- tensive educational, mentoring, and faculty development program to carefully mentor and position faculty who use the cores to maximize their potential to successfully compete for external funding, thus fulfilling the ultimate goals of the COBRE program. 2 Narrative  This Phase III COBRE project is a natural extension of our Phase I and II COBRE projects which were cen- tered on mentoring individual researchers along with building the necessary infrastructure to support multimodal neuroimaging in mental illness. During this time, cutting-edge cores were developed that facilitated not only our local projects but also research at multiple institutions across New Mexico; the cores served as neuroimaging facilities and training centers for others to utilize. The Phase III project will ensure the sustainability of these cores as they transition to being fully funded by a broad cadre of users with various funding sources. We propose three technical cores including a multimodal data acquisition (MDA) core, an algorithm and data analysis (ADA) core, and a biostatistics and neuro-informatics (BNI) core. These cores have already shown their utility and have begun to be leveraged by users outside the COBRE. In addition, we propose a robust pilot project program (PPP) to continue to seed and enable new users of the cores to ultimately grow and sustain world class brain imaging research within our IDeA state, thus fulfilling the ultimate goals of the COBRE program. 1",Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND),9700159,P30GM122734,"['Algorithmic Analysis', 'Appointment', 'Area', 'Awareness', 'Biology', 'Biometry', 'Bipolar Depression', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Businesses', 'Centers of Research Excellence', 'Chemistry', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Core Facility', 'Data', 'Data Analyses', 'Department of Energy', 'Development', 'Devices', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Electroencephalography', 'Engineering', 'Ensure', 'Environment', 'Equipment', 'Faculty', 'Functional Magnetic Resonance Imaging', 'Funding', 'Funding Agency', 'Genetic', 'Goals', 'Grant', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Infrastructure', 'Institution', 'Interdisciplinary Study', 'Leadership', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Magnetoencephalography', 'Major Depressive Disorder', 'Mental Depression', 'Mental disorders', 'Mentors', 'Methods', 'Mind-Body Method', 'Mission', 'Multimodal Imaging', 'Neurobiology', 'Neurologic', 'Neurosciences', 'New Mexico', 'Paper', 'Patients', 'Peer Review', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Program Development', 'Psychiatry', 'Publications', 'Recording of previous events', 'Research', 'Research Domain Criteria', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Schizophrenia', 'Seeds', 'Site', 'Structure', 'System', 'Teacher Professional Development', 'Time', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Vision', 'base', 'cohesion', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'deep learning', 'density', 'design', 'distinguished professor', 'improved', 'independent component analysis', 'meetings', 'multimodal data', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'neuropsychiatric disorder', 'programs', 'tool']",NIGMS,THE MIND RESEARCH NETWORK,P30,2019,1290517,-0.0052061631003017346
"Research and development of an open, extensible, web-based information extraction workbench for systematic review Project Summary  1 More than 4,000 systematic reviews are performed each year in the fields of environmental health and evidence-based  2 medicine, with each review requiring, on average, between six months to one year of effort to complete. One of the most  3 time consuming and repetitive aspects of this endeavor involves extraction of detailed information from a large number  4 of scientific documents. The specific data items extracted differ among disciplines, but within a given scientific domain,  5 certain data points are extracted repeatedly for each review conducted. Research on use of natural language processing  6 (NLP) for extracting individual data elements has shown that it has the potential to greatly reduce the laborious, time  7 intensive, and repetitive nature of this step. However, there is currently no integrated, automatic data extraction platform  8 that meets the needs of the systematic review community. We propose a web-based data extraction software platform  9 specifically designed for usage in the domain of systematic review. By combining multiple state-of-the-art data extraction 10 methods utilizing NLP, text mining and machine learning, into a single, unified user interface, we will thereby empower 11 the end-user with a powerful and novel tool for automating an otherwise arduous task. 12 The research we propose encompasses three specific aims: (1) develop new data extraction models using deep learning 13 and a new technique called “data programming”; (2) develop a web-based platform to semi-automate the process; (3) 14 design protocols and standards for packaging extraction models as software components and integrating work done by 15 other research groups and vendors. In the first aim, we will contribute novel data extraction modules designed and 16 trained specifically to extract data elements of interest to those conducting systematic reviews in the domain of 17 environmental health. For this research, we will employ state-of-the-art machine learning, NLP and text mining 18 methodologies to train and evaluate several novel extraction components. In our second aim, we will develop a web- 19 based workbench which will allow users to upload scientific documents for automated data extraction. Our system will 20 also be designed to allow for integration of data extraction approaches (components) from other research groups, thus 21 enabling end users to choose from a wide variety of advanced data extraction methodologies within one unified and 22 intuitive software environment. In our third aim, we will develop new protocols to standardize the inputs and outputs 23 for data extraction components. The resulting interface, which will enable seamless integration of third party extraction 24 components into the workbench, will also facilitate the incorporation of feedback from users such that extraction 25 components can be continuously improved based on real-time data. 26 Our overarching goal is to translate emerging semi-automated extraction technologies out of the lab and into practical 27 software and to bring to market both the software itself as well as several premium data extraction components. The 28 results of the research conducted for Aims 1-3 represent the first step in this direction and will provide the foundation for 29 future developments. These result will take us one step closer to the dream of creating “living systematic reviews,” which 30 are maintained using automated or semi-automated methods and updated regularly as new evidence becomes available. Project Narrative Systematic review is a formal process used widely in evidence-based medicine and environmental health research to identify, assess, and integrate the primary scientific literature with the goal of answering a specific, targeted question in pursuit of the current scientific consensus. By conducting research and development to build a flexible, extensible software system that automates the crucial and resource-intensive process of extracting key data elements from scientific documents, we will make an important contribution toward ongoing efforts to automate systematic review. These efforts will serve to make systematic reviews both more efficient to produce and less expensive to maintain, a result which will greatly accelerate the process by which scientific consensus is obtained in a variety of medical and health-related disciplines having great public significance.","Research and development of an open, extensible, web-based information extraction workbench for systematic review",9623091,R43ES029901,"['Communities', 'Computer software', 'Consensus', 'Data', 'Data Element', 'Development', 'Discipline', 'Dreams', 'Environment', 'Environmental Health', 'Evidence Based Medicine', 'Feedback', 'Foundations', 'Future', 'Goals', 'Health', 'Individual', 'Internet', 'Intuition', 'Literature', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'One-Step dentin bonding system', 'Online Systems', 'Output', 'Process', 'Protocols documentation', 'Research', 'Resources', 'Source', 'Standardization', 'Supervision', 'System', 'Techniques', 'Technology', 'Time', 'Training', 'Translating', 'Update', 'Vendor', 'Work', 'artificial neural network', 'base', 'data integration', 'deep learning', 'design', 'evidence base', 'flexibility', 'improved', 'innovation', 'interest', 'learning strategy', 'model design', 'novel', 'research and development', 'software systems', 'systematic review', 'text searching', 'tool']",NIEHS,"SCIOME, LLC",R43,2018,225000,-0.02958322981096572
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9527181,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2018,545116,0.0023838122583164055
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9532186,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'deep learning', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2018,259358,0.02606704824256014
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9614770,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genetic Diseases', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2018,538700,0.01596308472639645
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9523638,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Ecosystem', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Standardization', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data sharing', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2018,478806,0.01647639787577346
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9565646,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2018,293252,-0.004164776399974036
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9789424,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2018,293560,0.004470181959272346
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9633463,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Records', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2018,1583573,0.004470181959272346
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. Narrative There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9680657,R44MH118815,"['Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Institutes', 'Institution', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'Stem cells', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'professional atmosphere', 'prototype', 'research and development', 'treatment strategy', 'two-dimensional', 'usability', 'virtual reality', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2018,449918,0.0036697748908127064
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9566212,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'clinical decision support', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2018,1069680,0.005870999269093796
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9749413,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,155743,0.020554902630061146
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9560825,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2018,674602,0.020554902630061146
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9545836,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'deep learning', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2018,569784,0.014032196616017779
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9549126,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2018,81977,-0.005722160581391081
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9422475,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,356646,0.02091435468128292
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9478117,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Breast Cancer Risk Factor', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2018,897471,-0.0019104992430428723
"Device for real-time streaming of preclinical research data into a central cloud-based platform Project Summary BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. Traditionally researchers collect data from many disparate instruments and store data on PCs running single license software. Raw data is stored across several locations, analysis is restricted to the PC used to collect the data, and opportunities for collaboration, remote-participation, and data sharing are limited. BehaviorCloud is leveraging cloud data streaming and storage to overcome these barriers to discovery. In 2017 BehaviorCloud released a first version of the underlying cloud platform as well as BehaviorCloud Camera, an open-source “reference implementation” that demonstrates automated video tracking of animal behavior on the BehaviorCloud platform using a consumer-grade smartphone. This tool and the underlying platform are both in active use across academic and pharmaceutical labs. The aim of this Phase I SBIR application is to develop patent-pending “Bridge” technology that allows data streaming from third-party instrumentation into the central web platform. Researchers will bypass the original software and PCs associated with their instruments to control trials through their BehaviorCloud account and receive data back in real-time. BehaviorCloud will provide a public repository to aggregate all of these data and accelerate discovery by providing computational tools for large-scale meta-analysis and machine learning based predictive analytics. Project Narrative BehaviorCloud is a unified cloud platform where biomedical researchers can collect, analyze, and share preclinical research data – specifically behavioral and phenotype data from animal models. The goal of this Phase I SBIR application is to develop the technology to enable streaming of data from all kinds of behavioral and phenotyping instrumentation into the BehaviorCloud platform. BehaviorCloud will aggregate these data into a repository and accelerate discovery by providing tools for collaboration and meta-analysis.",Device for real-time streaming of preclinical research data into a central cloud-based platform,9621228,R43OD025448,"['Adoption', 'Animal Behavior', 'Animal Experimentation', 'Animal Model', 'Area', 'Back', 'Behavioral', 'Bypass', 'Carbon Dioxide', 'Cellular Phone', 'Collaborations', 'Computer software', 'Computers', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Sources', 'Data Storage and Retrieval', 'Development', 'Devices', 'Goals', 'Heart Rate', 'Information Systems', 'Internet', 'Intervention', 'Legal patent', 'Licensing', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Meta-Analysis', 'Pharmacologic Substance', 'Phase', 'Phenotype', 'Physiological', 'Positioning Attribute', 'Predictive Analytics', 'Process', 'Research', 'Research Contracts', 'Research Personnel', 'Resources', 'Running', 'Signal Transduction', 'Small Business Innovation Research Grant', 'Standardization', 'Stimulus', 'Stream', 'System', 'Technology', 'Temperature', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'cloud based', 'cloud platform', 'computerized tools', 'control trial', 'data management', 'data sharing', 'data warehouse', 'design', 'experimental study', 'instrument', 'instrumentation', 'laptop', 'open source', 'phenotypic data', 'pre-clinical', 'pre-clinical research', 'prototype', 'repository', 'tool', 'wasting', 'web interface']",OD,"BEHAVIORCLOUD, LLC",R43,2018,220420,0.012000196027797087
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9448918,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Autistic Disorder', 'Big Data', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Machine Learning', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'big biomedical data', 'cell type', 'computing resources', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'learning strategy', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2018,308503,0.00018853204253641557
"NextGen Random Forests Project Summary/Abstract Building from the PI's current R01, we propose next generation random forests (RF) designed for unprecedented accuracy and computational scalability to meet the challenges of today's complex and big data in the health sciences. Superior accuracy is achieved using super greedy trees which circumvent limitations on local adaptivity imposed by classical tree splitting. We identify a key quantity, forest weights, and show how these can be leveraged for further improvements and generalizability. In one application, improved survival estimators are applied to worldwide esophageal cancer data to develop guidelines for clinical decision making. Richer RF inference is another issue explored. Cutting edge machine learning methods rarely consider the problem of estimating variability. For RF, bootstrapping currently exists as the only tool for reliably estimating conﬁdence intervals, but due to heavy computations is rarely applied. We introduce tools to rapidily calculate standard errors based on U-statistic theory. These will be used to increase robustness of esophageal clinical recommendations and to investigate survival temporal trends in cardiovascular disease. In another application, we make use of our new massive data scalability for discovery of tumor and immune regulators of immunotherapy in cancers. This project will set the standard for RF computational performance. Building from the core libraries of the highly accessed R-package randomForestSRC (RF-SRC), software developed under the PIs current R01, we develop open source next generation RF software, RF-SRC Everywhere, Big Data RF-SRC, and HPC RF-SRC. The software will be deployable on a number of popular machine learning workbenches, use distributed data storage technologies, and be optimized for big-p, big-n, and big-np scenarios. Project Narrative We introduce next generation random forests (RF) designed for unprecedented accuracy for complex and big data encountered in the health sciences.",NextGen Random Forests,9547466,R01GM125072,"['Atrophic', 'Benchmarking', 'Big Data', 'Biological Response Modifiers', 'Blood', 'Cancer Patient', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Code', 'Combined Modality Therapy', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Storage and Retrieval', 'Databases', 'Development', 'Esophageal', 'Flow Cytometry', 'Guidelines', 'Health Sciences', 'Heart failure', 'Human', 'Hybrids', 'Immune', 'Immunotherapy', 'In Vitro', 'Interagency Registry for Mechanically Assisted Circulatory Support', 'Internet', 'Java', 'Laboratories', 'Language', 'Libraries', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Neoadjuvant Therapy', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Performance', 'Population', 'Pump', 'Receptor Activation', 'Recommendation', 'Resistance', 'Subgroup', 'T-Lymphocyte', 'Technology', 'Therapeutic', 'Thrombosis', 'Time', 'Time trend', 'Trees', 'Weight', 'base', 'clinical decision-making', 'clinical practice', 'design', 'distributed data', 'forest', 'immune checkpoint blockade', 'improved', 'in vivo', 'learning strategy', 'lymph nodes', 'mouse model', 'next generation', 'novel', 'open source', 'outcome forecast', 'parallel processing', 'pre-clinical', 'predicting response', 'predictive modeling', 'receptor', 'response', 'software development', 'statistics', 'theories', 'therapeutic target', 'tool', 'tumor', 'tumor progression']",NIGMS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2018,347834,-0.00023184537950160603
"C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative ABSTRACT The BRAIN Initiative is designed to leverage sophisticated neuromodulation, electrophysiological recording, and macroscale neuroimaging techniques in human and non-human animal models in order to develop a multilevel understanding of human brain function. However, the necessary tools for organizing, processing and analyzing neuroimaging data generated through these efforts are not widely available as coherent and easy-to- use software packages. Gaps are particularly apparent for nonhuman data (i.e., monkey, rodent), as most of the existing processing and analytic software packages are specifically designed for human imaging. Methods have been proposed for addressing the challenges inherent to the processing of nonhuman data (e.g., brain extraction, tissue segmentation, spatial normalization, brain parcellation, temporal denoising); to date, these have not been readily integrated into an easy-to-use, robust, and reproducible analysis package. Similarly, many of the sophisticated machine learning and modeling methods developed for neuroimaging analyses are inaccessible to most researchers because they have not been integrated into easy-to-use pipeline software. As a result, translational and comparative neuroimaging researchers patch together neuroinformatics pipelines that use various combinations of disparate software packages and in-house code. We propose to extend the Configurable Pipeline for the Analysis of Connectomes (C-PAC) open-source software to provide robust and reproducible pipelines for functional and structural MRI data. We will integrate the various disparate image processing and analysis methods used to handle the challenges of nonhuman imaging data, into a single, open source, configurable, easy-to-use end-to-end analysis pipeline package that is accessible locally or via the cloud. The end product will not only improve the quality, transparency and reproducibility of nonhuman translational and comparative imaging, but also enable new avenues of scientific inquiry through our inclusion of methods that are yet to be applied to nonhuman imaging data (e.g., gradient- based cortical parcellation methods, hyperalignment). Specific aims of the proposed work include to: 1) Integrate neuroimaging processing and analysis methods optimized for BRAIN Initiative data, 2) Implement strategies for carrying out comparative studies of human and non-human populations, and 3) Extend C-PAC to include cutting-edge analytical strategies for identifying mechanisms of brain function. All development will occur “in the open” using GitHub and other collaborative tools to maximally involve participation in the C-PAC project. Annual hackathons will be held to collaborate with investigators from BRAIN Initiative awards and other neuroinformatics development projects to integrate their tools with C-PAC. Hands-on training will be held to train investigators on optimal use of the newly developed tools. NARRATIVE New neuroimaging analysis software is needed to process and analyze the various human and non-human neuroimaging data collected through the BRAIN Initiative. We will address this need by extending the already mature C-PAC human brain imaging data analysis pipeline to include support for animal data, with a particular focus on providing methods for conducting comparative studies between species. The proposed work will also include a toolbox for helping to align electrophysiological data that is commonly collected in non-human studies, with the brain imaging data.","C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative",9591465,R24MH114806,"['Address', 'Adoption', 'Anatomy', 'Animal Model', 'Architecture', 'Award', 'Behavior', 'Brain', 'Brain imaging', 'Capital', 'Code', 'Communities', 'Comparative Study', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Documentation', 'Electrodes', 'Electrophysiology (science)', 'Environment', 'Funding', 'High Performance Computing', 'Human', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Measures', 'Methods', 'Modality', 'Modeling', 'Modification', 'Monkeys', 'Outcome', 'Output', 'Pattern', 'Persons', 'Phenotype', 'Population', 'Process', 'Pythons', 'Readability', 'Reproducibility', 'Research Personnel', 'Rodent', 'Scientific Inquiry', 'Software Design', 'Statistical Methods', 'Structure', 'Supervision', 'Techniques', 'Testing', 'Text', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validity of Results', 'Work', 'animal data', 'base', 'brain research', 'cloud based', 'comparative', 'computing resources', 'connectome', 'cost', 'data sharing', 'data structure', 'design', 'flexibility', 'graphical user interface', 'hackathon', 'human imaging', 'image processing', 'improved', 'innovative neurotechnologies', 'investigator training', 'learning strategy', 'neuroimaging', 'neuroinformatics', 'neuroregulation', 'open source', 'software as a service', 'tool', 'unsupervised learning']",NIMH,"CHILD MIND INSTITUTE, INC.",R24,2018,507643,0.012287430430504091
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,9594042,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Research Infrastructure', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional']",NIMH,UNIVERSITY OF WASHINGTON,R01,2018,661507,-0.03923074850331925
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",9517044,U54EB020403,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Big Data to Knowledge', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Science', 'Data Set', 'Diagnosis', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'computer science', 'connectome', 'diagnostic biomarker', 'hackathon', 'high dimensionality', 'imaging study', 'innovation', 'multidisciplinary', 'multimodality', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'organizational structure', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'web portal', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2018,790391,0.00191475177066529
"Summer Institute in Neuroimaging and Data Science Project Summary/Abstract The study of the human brain with neuroimaging technologies is at the cusp of an exciting era of Big Data. Many data collection projects, such as the NIH-funded Human Connectome Project, have made large, high- quality datasets of human neuroimaging data freely available to researchers. These large data sets promise to provide important new insights about human brain structure and function, and to provide us the clues needed to address a variety of neurological and psychiatric disorders. However, neuroscience researchers still face substantial challenges in capitalizing on these data, because these Big Data require a different set of technical and theoretical tools than those that are required for analyzing traditional experimental data. These skills and ideas, collectively referred to as Data Science, include knowledge in computer science and software engineering, databases, machine learning and statistics, and data visualization.  The Summer Institute in Data Science for Neuroimaging will combine instruction by experts in data science methodology and by leading neuroimaging researchers that are applying data science to answer scientiﬁc ques- tions about the human brain. In addition to lectures on the theoretical background of data science methodology and its application to neuroimaging, the course will emphasize experiential hands-on training in problem-solving tutorials, as well as project-based learning, in which the students will create small projects based on openly available datasets. Summer Institute in Neuroimaging and Data Science: Project Narrative The Summer Institute in Neuroimaging and Data Science will provide training in modern data science tools and methods, such as programming, data management, machine learning and data visualization. Through lectures, hands-on training sessions and team projects, it will empower scientists from a variety of backgrounds in the use of these tools in research on the human brain and on neurological and psychiatric brain disorders.",Summer Institute in Neuroimaging and Data Science,9491911,R25MH112480,"['Address', 'Adopted', 'Big Data', 'Brain', 'Brain Diseases', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Databases', 'Discipline', 'Face', 'Faculty', 'Fostering', 'Funding', 'Habits', 'Home environment', 'Human', 'Image', 'Institutes', 'Institution', 'Instruction', 'Internet', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mental disorders', 'Methodology', 'Methods', 'Modernization', 'Neurologic', 'Neurosciences', 'Participant', 'Positioning Attribute', 'Problem Solving', 'Psychology', 'Reproducibility', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Software Engineering', 'Software Tools', 'Structure', 'Students', 'Technology', 'Testing', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'career', 'computer science', 'connectome', 'data management', 'data visualization', 'design', 'e-science', 'experimental study', 'high dimensionality', 'insight', 'instructor', 'interdisciplinary collaboration', 'knowledge base', 'lectures', 'nervous system disorder', 'neurogenetics', 'neuroimaging', 'novel', 'open source', 'prediction algorithm', 'programs', 'project-based learning', 'skills', 'statistics', 'success', 'summer institute', 'theories', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,R25,2018,199622,-0.019657892222836894
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9572992,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Life Style Modification', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States Agency for Healthcare Research and Quality', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'sleep quality', 'symptomatic improvement', 'tool', 'wearable device']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2018,297237,0.002150120464280882
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9770622,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2018,380000,-0.01363002470758893
"Robust Control of the Stem Cell Niche Diana Arguijo has a unique background with a double major in biomedical engineering (BME) and electrical and computer engineering (ECE). Leveraging her strong mathematical background, she will develop computational techniques to identify patterns of epigenetic reprogramming during epithelial development and patterns of real- time electrical recoding of the GI tract reflective of sacral nerve modulation. Her work will provide insights into the robustness and plasticity of underlying biological control schemes. Diana Arguijo will develop machine-learning based computational techniques to analyze epigenetic reprogramming of epithelial development and electrical activities of the enteric nervous system. Her analyses will provide insights into the robustness and plasticity of tissue regulation.",Robust Control of the Stem Cell Niche,9731853,R35GM122465,"['Biological', 'Biomedical Engineering', 'Computational Technique', 'Computers', 'Development', 'Engineering', 'Enteric Nervous System', 'Epigenetic Process', 'Epithelial', 'Gastrointestinal tract structure', 'Machine Learning', 'Mathematics', 'Pattern', 'Regulation', 'Sacral nerve', 'Scheme', 'Time', 'Tissues', 'Work', 'base', 'insight', 'stem cell niche']",NIGMS,DUKE UNIVERSITY,R35,2018,42822,-0.04910707872693329
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9527186,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2018,516810,0.008085287265877551
"2018 Image Science Gordon Research Conference & Gordon Research Seminar Project Summary: The proposal requests support for early-career investigators to attend the 2018 Gordon Research Conference on Image Science. The unique feature of this conference in its third offering compared with others in medical imaging is the bringing together of renown speakers from disparate application areas, including astronomy, biology, medicine, remote sensing, and security and defense industries, in a forum that encourages each to describe their greatest challenges and most promising solutions. All speakers are invited based on their leadership in their field and their willingness to debate fundamental issues shared by everyone developing, evaluating, and applying imaging in medicine and biology. We believe the GRC format placed in the context of a small-college venue promotes the type of innovative interdisciplinary thinking that leads to breakthroughs. An environment where leading senior scientists debate core issues is valuable to young investigators trying to build successful independent careers in medical imaging in industry and academia. All attendees are invited to present a poster describing their research in poster sessions that are a key element of the Gordon Conference format. The June 17-22, 2018 GRC conference theme is “Image Science: Creating Knowledge from Information,” which is focused on appropriate acquisition and efficient uses of the massive volume of imaging information now collected from patients. Speakers give 40 minutes presentations in a single-track format with 20 minute discussions following each presentation that are led by experts in the field. Topic range from “Imaging in Brain Science Discovery” to “Advanced Machine Learning” and “Computational Imaging.” At the center of each presentation is a discussion of the core challenges shared by image scientists and novel techniques for acquiring and displaying information in a manner that maximizes decision performance. Given the success of the previous meeting, we will hold the first-ever, student-run Gordon Research Seminars (GRS) on Image Science June 16, 17, 2018. Our aim is to build Image Science as an independent field of study through detailed interdisciplinary discussions and by fostering the success of a new generation of image scientists. Project Narrative: Solutions to very difficult problems often emerge from discussions among experts in different fields of study being challenged by the same core problems. The 2018 GRC on Imaging Science strives to build a community of problem solvers by creating an environment for detailed discussions among senior investigators that involves young investigators at a time when they are building careers. This is a proposal to fund young investigators to attend the conference.",2018 Image Science Gordon Research Conference & Gordon Research Seminar,9461211,R13EB025662,"['Academia', 'Area', 'Astronomy', 'Big Data', 'Biology', 'Brain', 'Collaborations', 'Communities', 'Computational Science', 'Data', 'Data Analytics', 'Development', 'Disabled Persons', 'Discipline', 'Elements', 'Engineering', 'Ensure', 'Environment', 'Evaluation', 'Event', 'Fees', 'Female', 'Financial Support', 'Fostering', 'Funding', 'Housing', 'Human', 'Image', 'Industry', 'Information Sciences', 'Interdisciplinary Study', 'Knowledge', 'Leadership', 'Machine Learning', 'Medical', 'Medical Imaging', 'Medicine', 'Methods', 'Minority', 'Modeling', 'Patients', 'Performance', 'Play', 'Postdoctoral Fellow', 'Recruitment Activity', 'Request for Proposals', 'Research', 'Research Personnel', 'Resource Development', 'Risk-Taking', 'Role', 'Running', 'Science', 'Scientist', 'Security', 'Senior Scientist', 'Series', 'Societies', 'Source', 'Statistical Models', 'Students', 'Systems Development', 'Techniques', 'Thinking', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'career', 'college', 'cost', 'design', 'disabled students', 'educational atmosphere', 'field study', 'frontier', 'graduate student', 'image reconstruction', 'imaging scientist', 'imaging system', 'information display', 'innovation', 'instrument', 'meetings', 'minority student', 'multidisciplinary', 'next generation', 'novel', 'posters', 'preference', 'remote sensing', 'skills', 'success', 'symposium', 'training opportunity', 'virtual', 'willingness']",NIBIB,GORDON RESEARCH CONFERENCES,R13,2018,10000,-0.011407754313281082
"2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Summary /Abstract:  The 2018 OSA Biophotonics Congress: Biomedical Optics, 3-6 April 2018, Hollywood, FL, consists of four topical meetings. Two of these meetings, Optical Tomography and Spectroscopy (OTS) and Microscopy, Histopathology and Analytics (Microscopy) provide broad exposure to a very active multidisciplinary field in biomedical imaging and bioengineering focused on illness treatment and health enhancement. The interdisciplinary nature of the co- located meetings will provide cross-fertilization of concepts and techniques between fields with the resulting synergies obtained from such interactions. This proposal is to provide registration and travel support for students and early career professionals presenting at one of these topical meetings.  OTS will focus on new developments in diffuse optics, spectroscopy and other non- invasive tomographic imaging approaches, including the fields of diffuse optical tomography (DOT), photoacoustic tomography (PAT), optical coherence tomography (OCT), wavefront engineering to overcome scattering, as well as new developments in spectroscopic technologies.  Microscopy will include topics central to the development of optical microscopy and in vitro optical sensing for the clinic. Areas such as novel optical approaches, including computational optics, new image processing and segmentation techniques, development of decision-assistance algorithms via machine-learning and other strategies, testing technologies in pre-clinical models, applications to clinical samples, and validation in the clinic will be discussed. Optically enabled microfluidics are included in this track as well. The goal of these efforts should be towards clinical translation.  The general purpose of these meetings is to create an inclusive, open forum for the presentation of high-quality scientific research through plenary and technical sessions, short courses, panels, networking and special events. This method of face-to-face information sharing allows researchers to learn what others in their field and related disciplines are doing and to efficiently learn about new research, tools, and techniques that might be relevant to their work. It allows conversations with colleagues from different institutions around the world and engenders far reaching scientific collaborations – both domestic and international. FOA: PA-16-294 Opportunity Title: NIH Support for Conferences and Scientific Meetings (R13/U13) Agency: NIH - NIBIB Proposal Title: 2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and  Microscopy, Histopathology and Analytics. Discussing New Research in  Biomedical Imaging and Bioengineering. Principal Investigator: Gregory J. Quarles, Ph.D., Chief Scientist, The Optical Society  2010 Massachusetts Ave, NW, Washington, DC  gquarles@osa.org, 202-416-1954 Project Narrative The 2018 OSA Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics Topical Meetings will discuss important, highly interdisciplinary areas that focus on technological solutions to medical challenges and medical applications, and will cover a diversity of cutting-edge research and innovative new tools and techniques, especially in biomedical imaging and bioengineering. These Topical Meetings will bring together researchers working in all aspects of this field and will serve as a forum for discussion of existing and emerging techniques as well as future directions.","2018 OSA Topical Meetings: Optical Tomography and Spectroscopy; and Microscopy, Histopathology and Analytics. Discussing New Research in Biomedical Imaging and Bioengineering.",9543795,R13EB026325,"['Academic Training', 'Algorithms', 'Area', 'Biomedical Engineering', 'Biophotonics', 'Birds', 'Career Mobility', 'Clinic', 'Clinical', 'Collaborations', 'Congresses', 'Development', 'Diffuse', 'Digital Libraries', 'Discipline', 'Doctor of Philosophy', 'Engineering', 'Ensure', 'Event', 'Exhibits', 'Exposure to', 'Fertilization', 'Fostering', 'Future', 'Goals', 'Grant', 'Health', 'Hearing', 'Histopathology', 'In Vitro', 'Industry', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Machine Learning', 'Massachusetts', 'Medical', 'Methods', 'Microfluidics', 'Microscopy', 'National Institute of Biomedical Imaging and Bioengineering', 'Nature', 'Optical Coherence Tomography', 'Optical Tomography', 'Optics', 'Outcome', 'Paper', 'Participant', 'Peer Review', 'Physicians', 'Pre-Clinical Model', 'Principal Investigator', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Scientist', 'Services', 'Societies', 'Special Event', 'Spectrum Analysis', 'Students', 'Techniques', 'Technology', 'Testing', 'Time', 'Translating', 'Travel', 'Underrepresented Minority', 'United States National Institutes of Health', 'Validation', 'Washington', 'Work', 'academic standard', 'base', 'bioimaging', 'career', 'clinical application', 'clinical translation', 'diffuse optical tomography', 'graduate student', 'image processing', 'imaging Segmentation', 'imaging approach', 'indexing', 'innovation', 'meetings', 'multidisciplinary', 'novel', 'optoacoustic tomography', 'posters', 'programs', 'symposium', 'synergism', 'technique development', 'tomography', 'tool']",NIBIB,OPTICAL SOCIETY OF AMERICA,R13,2018,10000,0.0001265840476191803
"mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment Project Summary NIH is increasing its investment in large mutli-center brain MRI studies via projects such as the recently announced BRAIN initiative. The success of these studies depends on the quality of MRIs and the resulting image measurements, regardless of sample size. Even though quality control of MRIs and corresponding measurements could be outsourced, most neuroscience studies rely on in-house procedures that combine automatically generated scores with manually guided checks, such as visual inspection. Implementing these procedures typically requires combining several open-source software systems. For example, the NIH NIAAA and BD2K funded Data Analysis Resource (DAR) of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) uses XNAT to consolidate the structural, diffusion, and functional MRIs acquired across five sites, and has also developed their own custom software package to comply with study requirements that called for a multi-tier, quality control (QC) workflow. However, these custom, one-off tools lack support for multi-site QC workflows as that would require a unified platform, design that supports collaboration and sharing, and strong cohesion between technologies. To improve the effectiveness of QC efforts specific to multi-center neuroimaging studies, we will develop a widely accessible and broadly compatible software platform that supports simplified creation of custom QC workflows in compliance with study requirements, provides core functionality for performing QC of medical images, and automatically generates documentation compliant with the FAIR principle, i.e., making scientific findings findable, accessible, interoperable, and reusable.  Specifically, our multi-site open-source software platform for Medical Image Quality Assurance (mIQa) will enable efficient and accurate QC processing by leveraging open-source, state-of-the-art web interface technologies, such as a web-based dataset caching system, machine learning to aid in QC process, and an interactive electronic notebook platform. Users will be able to configure workflows that not only reflect the specific requirements of medical imaging studies but also minimize the time spent on labor-intensive operations, such as visually reviewing scans. Issue tracking technology will enhance communication between geographically-distributed team members, as they can easily share image annotations and receive automating notifications of outstanding QC issues. The system will be easy to deploy as it will be able to interface with various imaging storage backends, such as local file systems and XNAT. While parts of this functionality have been developed elsewhere, mIQa is unique as it provides a unified, standard interface for efficient QC setup, maintenance, and review for projects analyzing multiple, independently managed data sources.  The usefulness of this unique QC system will be demonstrated on increasing the efficiency of the diverse QC team of the multi-center NCANDA study. Narrative The goal of this proposal is to develop multi-site, open-source software for Medical Image Quality Assurance (mIQa) to address the QC needs of geographically diverse teams using small and large medical image-based studies alike. mIQa will enable efficient and accurate QC processing by levering open-source, state-of-the-art machine learning, data management, and web interface technologies. Our effort will minimize the time spent on labor-intensive review and analysis operations by supporting team-oriented reviewing that is guided by highly customizable workflows seamlessly interacting with existing data management systems.",mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment,9622218,R43MH119022,"['Active Learning', 'Address', 'Adolescence', 'Alcohols', 'BRAIN initiative', 'Big Data to Knowledge', 'Brain', 'Brain imaging', 'Collaborations', 'Communication', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Data Sources', 'Development', 'Diffusion', 'Documentation', 'Effectiveness', 'Ensure', 'Environment', 'Evaluation', 'FAIR principles', 'Four-dimensional', 'Funding', 'Geography', 'Goals', 'Image', 'Image Analysis', 'Imagery', 'International', 'Internet', 'Investments', 'Label', 'Libraries', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Manuals', 'Measurement', 'Medical', 'Medical Imaging', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences', 'Notification', 'Online Systems', 'Peer Review', 'Phase', 'Procedures', 'Process', 'Publications', 'Quality Control', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Scanning', 'Site', 'System', 'Techniques', 'Technology', 'Time', 'United States National Institutes of Health', 'Visual', 'Work', 'Writing', 'application programming interface', 'base', 'cohesion', 'cost', 'dashboard', 'data access', 'data management', 'design', 'experience', 'flexibility', 'image archival system', 'imaging study', 'improved', 'innovation', 'member', 'neurodevelopment', 'neuroimaging', 'open source', 'operation', 'prototype', 'quality assurance', 'research study', 'software systems', 'success', 'tool', 'web interface', 'web-enabled']",NIMH,"KITWARE, INC.",R43,2018,225001,0.008070038130963344
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9565669,R24MH114788,"['Algorithms', 'Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Machine Learning', 'Metadata', 'Modeling', 'Modification', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data resource', 'data submission', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'member', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2018,1293101,-0.012380558661627178
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9453640,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2018,305500,-0.0011648546474007262
"SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies ﻿    DESCRIPTION (provided by applicant): Efforts to reduce the burden of Tuberculosis (TB) are challenged by the persistent social inequalities in health, the limited number of local healthcare professionals, and the weak healthcare infrastructure found in resource-poor communities. Reducing the TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the TB epidemic in high-burden areas. The main objective of this proposal is to expedite the TB diagnosis process by developing novel image processing and machine learning techniques to analyze chest X-ray images thus reducing patient wait times for being diagnosed with TB. The study will be conducted in the district of Carabayllo, a densely occupied, high-burden TB area in Lima, the capital of Perú. Efforts to develop the proposed user-centered, mobile device-based computing system are aligned with the mission of the National Institute of Biomedical Imaging and Bioengineering (NIBIB) and its strategic goals 2 and 4 in particular-the proposed socio-technical intervention aims at developing biomedical imaging techniques (i.e. wireless and image sensing/analyzing) to enable a point-of-care mobile device-based computing system for TB screening and diagnostic. Anticipated outcomes include a) a large-scale, real-world, well-annotated, and public available chest X-ray image database for TB screening, b) development of new image analysis techniques for X-ray image capturing and pre- processing, and c) novel learning-based feature extraction and classification algorithms. This  interdisciplinary effort, involving community, university, hospitals and health care establishments in all stages of the research, responds to the need for increased partnerships between academia and community stakeholders, and the potential for building capacity in biomedical and technology solutions for health in both directions (North-South, South-North). Its scientific contribution lies in the intersection of three NIBIB scientific program areas including image processing, telehealth, and biomedical informatics. PUBLIC HEALTH RELEVANCE: This project is highly relevant to public and global health because it offers a socio-technical solution for resource-poor communities severely affected by TB. Outcomes of this project will contribute significantly to improving specific healthcare processes affecting hard-to-reach communities that are socially excluded and lack the benefits of technological advances while broadening our understanding about effective human centered designs to improve healthcare systems with mobile computing technologies.",SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies,9525950,R01EB021900,"['Academia', 'Address', 'Affect', 'Algorithms', 'Area', 'Benchmarking', 'Biomedical Technology', 'Capital', 'Cessation of life', 'Chest', 'Chronic Disease', 'Cities', 'Classification', 'Clinic', 'Communicable Diseases', 'Communities', 'Community Health', 'Complex', 'Computer Assisted', 'Computer Systems', 'Computer software', 'Computers', 'Data', 'Data Analytics', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Discipline', 'Engineering', 'Epidemic', 'Evaluation', 'Female', 'Goals', 'Health', 'Health Professional', 'Health Sciences', 'Health Technology', 'Healthcare', 'Healthcare Systems', 'Hispanics', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging Techniques', 'Intervention', 'Learning', 'Lung nodule', 'Machine Learning', 'Medical', 'Minority', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Outcome', 'Patients', 'Peru', 'Process', 'Public Health', 'Reader', 'Recruitment Activity', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Sensitivity and Specificity', 'Software Tools', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Training', 'Treatment Protocols', 'Tuberculosis', 'Underrepresented Students', 'University Hospitals', 'Vaccines', 'Wait Time', 'Wireless Technology', 'Woman', 'World Health Organization', 'accurate diagnosis', 'base', 'bioimaging', 'biomedical informatics', 'clinical practice', 'compliance behavior', 'data exchange', 'design', 'digital imaging', 'disadvantaged population', 'disease transmission', 'global health', 'handheld mobile device', 'image processing', 'improved', 'mHealth', 'mobile computing', 'novel', 'open source', 'point of care', 'programs', 'public health relevance', 'reproductive', 'screening', 'social', 'social inequality', 'telehealth', 'tool', 'tuberculosis diagnostics']",NIBIB,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2018,333515,0.03217961611059152
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9672008,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,347221,0.018533606466534625
"Towards a FAIR Digital Ecosystem in the Cloud Cloud Computing, Big Data Analytics, and Artificial Intelligence are transforming biomedical research. The NIH Data Commons will provide a common, cloud-agnostic, harmonized environment where these technologies can be deployed to serve NIH intra- and extra-mural researchers, implementing FAIR principles [Wilkinson2016]. Our proposal provides two essential capabilities: (1) Global Unique Identification (GUIDs) and (2) Digital Object Publication, Citation, Replication and Reuse. We propose a FAIR biomedical ecosystem where all primary and derived digital objects (e.g., datasets, software) receive GUIDs, assisting with Findability and Accessibility, Reusability, data provenance, reproducibility, and accountability of research outcomes. GUIDs will provide full Interoperability between DOIs and prefix: accession based Compact Identifiers through a common resolution services prefix registry. All digital objects will interoperate with multiple, hybrid clouds—open source and commercial—enabling researchers to select computing resources best matching their needs. 1 - The Global Unique Identifier (GUID) Capability provides a persistent, machine resolvable identifier platform for all FAIR objects in the Commons, fully aligned with community practices, recommendations, and metadata models. 2 - The Cloud Dataverse for Biomedical Digital Object Publication, Citation, Replication, and Reuse applies FAIR principles to primary and derived datasets, computational provenance, and software, making them fully FAIR compliant, while documenting the production processes. Cloud Dataverse integrates with multiple cloud computing solutions. As an example, with these capabilities, a researcher can extract a subset of data from TOPMed or GTEx and publish it in Cloud Dataverse, with its associated metadata, provenance, and terms of use. In publications, she can cite the data and software according to Data Citation Publishers guidelines [Cousijn2017] and Software Citation Principles [Smith2016]. Other researchers can access the dataset using the GUID in the citation, resolving the repository’s dataset landing page (as recommended by the Data Citation Principles [Martone2014, Fenner2017, Starr2015]). From this landing page, researchers can repeat the original calculation, perform new computations on the dataset, or use the provenance graph to learn how the dataset was created. The data and software published in the repository reflect the evolving nature of research; anyone can publish new versions with the provenance documenting the process. Our open-source software platforms used in production, adhere to FAIR principles, and provide the basis for the two capabilities: GUIDs and Cloud Dataverse enabling the use cases above. We will expand upon them, produce new tools, and reach a wider community. Currently GUIDs and provenance describe datasets; we will generalize these mechanisms to support other digital objects, focusing on software in the pilot phase. We will connect and harmonize DataCite, identifiers.org, and N2T/EZID services to provide GUIDs; and integrate the Dataverse repository software with the Massachusetts Open Cloud, built on top of the OpenStack cloud platform, and public commercial clouds (Microsoft Azure, Google Cloud) to provide Cloud Dataverse. Our past experience producing sustainable services for overlapping communities of developers and users demonstrates our ability to apply our expertise to supporting the larger and more diverse NIH Data Commons user community. n/a",Towards a FAIR Digital Ecosystem in the Cloud,9559873,OT3OD025456,"['Accountability', 'Artificial Intelligence', 'Big Data', 'Biomedical Research', 'Cloud Computing', 'Communities', 'Community Practice', 'Computer software', 'Data', 'Data Analytics', 'Data Provenance', 'Data Set', 'Ecosystem', 'Environment', 'FAIR principles', 'Genotype-Tissue Expression Project', 'Graph', 'Guidelines', 'Hybrids', 'Learning', 'Massachusetts', 'Metadata', 'Modeling', 'Nature', 'Outcomes Research', 'Phase', 'Process', 'Production', 'Publications', 'Publishing', 'Recommendation', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Resolution', 'Services', 'Technology', 'Trans-Omics for Precision Medicine', 'United States National Institutes of Health', 'base', 'cloud platform', 'computing resources', 'digital', 'digital ecosystem', 'experience', 'interoperability', 'open source', 'repository', 'software repository', 'tool']",OD,HARVARD UNIVERSITY,OT3,2018,300000,0.018533606466534625
"Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines Project​ ​Summary While standards in reporting of scientific methods are absolutely critical to producing reproducible science, meeting such standards is tedious and difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent compliance. Scientific journals and societies as well as the National Institutes of Health are now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods,​ ​but​ ​the​ ​trickier​ ​part​ ​is​ ​to​ ​train​ ​the​ ​biomedical​ ​community​ ​to​ ​use​ ​these​ ​standards​ ​to​ ​effectively. To support new standards in methods reporting, especially the RRID standard for Rigor and Transparency of Key Biological Resources, we propose to build Sci-Score a text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard already implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife and other Rigor and transparency standards put forward by the NIH. The innovation behind Sci-score is the provision of a score, which can be obtained by individual investigators or journals. This score reflects an aspect of quality of methods reporting. We posit that the score will serve as a tool that investigators can use to compete with themselves​ ​and​ ​each​ ​other,​ ​the​ ​way​ ​they​ ​currently​ ​compete​ ​on​ ​metrics​ ​of​ ​popularity,​ ​i.e.,​ ​the​ ​H-index. In Phase I of this project and before, our group has successfully developed a text mining algorithm that can detect antibodies, cell lines, organisms and digital resources (all 4 RRID types) and has created a preliminary score. We propose to extend this approach to all research inputs, like chemicals and plasmids that are requested as part of Cell press’ STAR Methods (http://www.cell.com/star-methods)​. We also propose to build a set of algorithms to detect whether authors discuss the major sources of irreproducibility outlined by NIH, including investigator blinding, proper randomization and sufficient reporting of sex and other biological variables. Resource identification along with other quality metrics will be used to score the quality of scientific methods section text. If successful, the tool could be used by editors, reviewers, and investigators to improve the​ ​quality​ ​of​ ​the​ ​scientific​ ​paper. Our Phase II specific aims include 1) enhancing and hardening the core natural language processing pipelines to recognize a broader range of sentences in near real time; 2) building a set of modular tools that will be provided for different groups of users to take advantage of the text mining capability we develop in aim 1. At the end of Phase II, we should have a commercially viable product that will be able to be licensed to serve the needs​ ​of​ ​the​ ​publishers​ ​and​ ​the​ ​broader​ ​research​ ​community. Standards​ ​for​ ​scientific​ ​methods​ ​reporting​ ​are​ ​absolutely​ ​critical​ ​to​ ​producing​ ​reproducible​ ​science,​ ​but​ ​meeting such​ ​standards​ ​is​ ​difficult.​ ​Checklists​ ​and​ ​instructions​ ​are​ ​tough​ ​to​ ​follow​ ​often​ ​resulting​ ​in​ ​low​ ​and​ ​inconsistent compliance.​ ​To​ ​support​ ​new​ ​standards​ ​in​ ​methods​ ​reporting,​ ​especially​ ​the​ ​RRID​ ​standard​ ​for​ ​Rigor​ ​and Transparency,​ ​we​ ​propose​ ​to​ ​build​ ​​Sci-Score​​ ​a​ ​text​ ​mining​ ​based​ ​tool​ ​suite​ ​to​ ​help​ ​authors​ ​meet​ ​the​ ​standard. Sci-Score​ ​will​ ​provide​ ​an​ ​automated​ ​check​ ​on​ ​compliance​ ​with​ ​the​ ​RRID​ ​standard​ ​implemented​ ​by​ ​over​ ​100 journals​ ​including​ ​Cell,​ ​Journal​ ​of​ ​Neuroscience,​ ​and​ ​eLife.​ ​Sci-score​ ​provides​ ​an​ ​automated​ ​rating​ ​the​ ​quality of​ ​methods​ ​reporting​ ​in​ ​submitted​ ​articles,​ ​which​ ​provides​ ​feedback​ ​to​ ​authors,​ ​reviewers​ ​and​ ​editors​ ​on​ ​how to​ ​improve​ ​compliance​ ​with​ ​RRIDs​ ​and​ ​other​ ​standards.","Sci-Score, a tool suite to support Rigor and Transparency NIH and Journal Guidelines",9621771,R44MH119094,"['Address', 'Adherence', 'Algorithms', 'Antibodies', 'Award', 'Biological', 'Cell Line', 'Cells', 'Chemicals', 'Communities', 'Databases', 'Ethics', 'Feedback', 'Guidelines', 'Health', 'Human', 'Individual', 'Instruction', 'Journals', 'Methods', 'Mission', 'Natural Language Processing', 'Neurosciences', 'Oligonucleotide Probes', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Plasmids', 'Publications', 'Publishing', 'Randomized', 'Reader', 'Reagent', 'Recommendation', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Science', 'Societies', 'Software Tools', 'Source', 'Specific qualifier value', 'System', 'Talents', 'Text', 'Time', 'Training', 'United States National Institutes of Health', 'base', 'complex biological systems', 'digital', 'improved', 'indexing', 'innovation', 'meetings', 'sex', 'sound', 'text searching', 'tool']",NIMH,"SCICRUNCH, INC.",R44,2018,748748,-0.014055909041774992
"Identifying individuals at risk of progression to active tuberculosis Project Summary Almost 2 billion people are infected with Mycobacterium tuberculosis (Mtb), the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no clinical test to distinguish those that will progress to active TB disease, from those that will not. If we are to realize the World Health Organization's (WHO) goal of a world free of TB by 2035, the massive reservoir of TB infection must be addressed with a cost-effective, ethical therapy for preventing progression, based on treating only those most likely to progress. A diagnostic test that can accurately predict the risk of progression is critical for treating these high-risk individuals and the eradication of TB. Our goal is to develop such an assay. Our central hypothesis is that five independent host immune biomarkers, combined into a single multimetric signature will predict progression from latent to active TB with at least 90% sensitivity and specificity. We will test this hypothesis and achieve our goal by implementing the following specific aims: Aim 1: Compile a comprehensive dataset of biomarkers in a prospective cohort of individuals who are at risk of progressing to active TB. Working with the Moldova Ministry of Health's National TB Program, we will enroll 3,685 close contacts of active TB cases. All participants will be followed for two years to determine who progresses to active TB. We expect to identify ≥ 140 progressors. We will assess three previously established blood-based predictors of active TB progression, and two novel assays. We will verify the performance of previously published biomarkers in this population to discriminate progressors from non-progressors and identify new candidate biomarkers using RNA-Seq of antigen stimulated PBMC and detection of Mtb-peptides by NanoDisk MS. Aim 2: Use a discovery set of samples to develop predictive models of progression to active TB. Using data from 140 progressors and 140 non-progressors from Aim 1 we will (1) Verify the performance of existing biomarkers, (2) Use a cross-validation to identify new candidate biomarkers, and (3) derive predictive models using logistic regression and machine learning methods to identify optimal biomarker signatures that best predict progression to active TB within 12 months. Aim 3: Verify the ability of the model to predict progression to active TB disease. Using the same approach as Aim 1, we will enroll a new set of 1,340 household contacts of active TB and identify at least 60 progressors and 60 matched non-progressors and verify clinically the sensitivity/specificity of our models and biosignatures (Aim 2) to predict progression to active disease. A combined host biomarker signature that can predict TB progression from a small blood volume will have significant impact on the WHO End TB Program. PROJECT NARRATIVE Almost 2 billion people are infected with Mycobacterium tuberculosis, the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no test to distinguish those that will progress from those that will not. We propose to develop a multimetric signature of host biomarkers that together will have a sensitivity and specificity of ≥ 90% for predicting progression to active TB in one year, a critical first step to developing cost-effective and ethical treatment plans in order to reach the World Health Organization goal of Ending TB by 2035.",Identifying individuals at risk of progression to active tuberculosis,9501395,R01AI137681,"['Address', 'Algorithms', 'Antigens', 'Biological Assay', 'Biological Markers', 'Blood', 'Blood Volume', 'Cells', 'Characteristics', 'Child', 'Classification', 'Clinical', 'Clinical Sensitivity', 'Data', 'Data Set', 'Detection', 'Diagnostic tests', 'Disease', 'Enrollment', 'Ethics', 'Event', 'Filtration', 'Flow Cytometry', 'Foundations', 'Freezing', 'Frequencies', 'Gender', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Goals', 'Health', 'Household', 'Immune', 'Immune response', 'Immunologic Markers', 'Individual', 'Interferon-alpha', 'Logistic Regressions', 'Lymphocyte', 'Machine Learning', 'Modeling', 'Moldova', 'Mycobacterium tuberculosis', 'Mycobacterium tuberculosis antigens', 'National Health Programs', 'Organizational Objectives', 'Outcomes Research', 'Participant', 'Patients', 'Peptide Fragments', 'Peptides', 'Performance', 'Peripheral Blood Mononuclear Cell', 'Plasma', 'Population', 'Procedures', 'Production', 'Prospective cohort', 'Proteins', 'Publications', 'Publishing', 'RNA', 'Research Personnel', 'Risk', 'Sampling', 'Sensitivity and Specificity', 'Specificity', 'T cell response', 'T-Lymphocyte', 'Testing', 'Tuberculosis', 'Validation', 'World Health Organization', 'age group', 'base', 'biobank', 'biomarker performance', 'biosignature', 'blood-based biomarker', 'candidate marker', 'clinical Diagnosis', 'cohort', 'cost effective', 'deep neural network', 'enzyme linked immunospot assay', 'falls', 'follow-up', 'forest', 'high risk', 'indexing', 'innovation', 'learning network', 'learning strategy', 'monocyte', 'nanodisk', 'novel', 'novel diagnostics', 'predictive modeling', 'predictive test', 'prevent', 'programs', 'research clinical testing', 'transcriptome sequencing', 'transmission process', 'treatment planning']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2018,798023,0.012964602675237098
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9534738,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'ontology development', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2018,431097,-0.02403713679087922
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),9571275,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Privacy', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Research Infrastructure', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data management', 'data portal', 'data sharing', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'social', 'success', 'theories', 'therapy design', 'tool']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2018,220751,-0.007240429844719558
"COINSTAC: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community be- comes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2). The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates a dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informat- ics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an inde- pendent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented stor- age vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and pri- vacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix fac- torization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. 4 Project Narrative  Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don’t have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this ‘missing data’ and allow for pooling of both open and ‘closed’ repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed compu- tational solution for a large toolkit of widely used algorithms. 3","COINSTAC: decentralized, scalable analysis of loosely coupled data",9717051,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,282320,0.021405332741774922
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9473021,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Source', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2018,649098,0.022456390414051043
"Omics for TB:  Response to Infection and Treatment Abstract – Overview With about 10 million new cases of active disease and 1.8 million deaths annually, TB is a global health emergency. A distinguishing feature of TB disease is its biological heterogeneity, which manifests at the clinical level chiefly in 2 forms: disease progression and treatment response. The premise of this Program is that the heterogeneous outcomes of TB infection and treatment are determined by the interplay of competing regulatory networks between the pathogen and the host. Our primary goal is to apply systems biology approaches to elucidate the biological control underlying the variability of disease outcome and response to treatment. Our first specific aim is to define novel host regulators of TB disease progression in vivo, and the innate and adaptive networks they control. We will also seek to define novel Mtb regulators of TB treatment response, and the Mtb regulatory networks that they control. This work will allow us to produce and validate host and Mtb models of TB disease progression and treatment response. Altogether, this program addresses key unanswered questions that stymie efforts to combat the TB pandemic. Our team has perfected the required platforms and scientific approaches to execute this ambitious research plan in a timely and cost- effective manner. All the participating investigators have strong records of interacting productively, and of disseminating their data and reagents to the scientific community. Project Narrative - Omics for TB: Response to Infection and Treatment Mycobacterium tuberculosis causes ~10 million new cases of active disease and 1.8 million deaths each year, and our tools to combat tuberculosis (TB) disease are universally outdated and overmatched. This project combines separate advances in systems biology and network modeling to produce experimentally grounded and verifiable systems-level models of the host and MTB regulatory networks that affect disease progression and response to treatment.",Omics for TB:  Response to Infection and Treatment,9455136,U19AI135976,"['Address', 'Affect', 'Bacteria', 'Biological', 'Cessation of life', 'Clinical', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Data', 'Data Set', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug-sensitive', 'Eicosanoids', 'Elements', 'Genetic Transcription', 'Goals', 'Human', 'Immunologic Receptors', 'Infection', 'Inflammatory Response', 'Machine Learning', 'Mass Spectrum Analysis', 'Methodology', 'Modeling', 'Molecular Profiling', 'Mouse Strains', 'Multiplexed Ion Beam Imaging', 'Mus', 'Mycobacterium tuberculosis', 'Network-based', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Predisposition', 'Proteomics', 'Reagent', 'Receptor Activation', 'Records', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Sampling', 'Symptoms', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Time', 'Treatment outcome', 'Tuberculosis', 'Vaccines', 'Work', 'base', 'biological heterogeneity', 'chemotherapy', 'combat', 'cost effective', 'design', 'global health emergency', 'high risk', 'in vivo', 'member', 'metabolomics', 'mouse model', 'network models', 'novel', 'pandemic disease', 'pathogen', 'predictive signature', 'programs', 'protein protein interaction', 'response', 'tool', 'transcriptomics', 'treatment response', 'tuberculosis treatment']",NIAID,SEATTLE BIOMEDICAL RESEARCH INSTITUTE,U19,2018,1648783,0.009195080837275781
"Laboratory of Neuro Imaging Resource (LONIR) PROJECT SUMMARY - OVERALL The LONIR is focused on developing innovative solutions for the investigation of imaging, genetics, behavioral and clinical data. The LONIR structure is designed to facilitate studies of dynamically changing anatomic frameworks, e.g., developmental, neurodegenerative, traumatic, and metastatic, by providing methods for the comprehensive understanding of the nature and extent of these processes. Specifically, TR&D1 (Data Science) focuses on methodological developments for the management and informatics of brain and related data. This project will develop and issue new methods for robust scientific data management to create an environment where scientific analyses can be reproduced and/or enhanced, data can be easily discovered and reused, and analysis results can be visualized and made publicly searchable. TR&D2 (Diffusion MRI and Connectomics) seeks to advance the study of brain connectivity using diffusion imaging and its powerful extensions. This project will go beyond traditional tensor models of diffusion for assessing tissue and fiber microstructure and connectivity, develop tract-based statistical analysis tools using Deep Learning, introduce novel adaptive connectivity mapping approaches, using L1 fusion of multiple tractography methods, and provide mechanisms to study connectivity and diffusion imaging over 10,000 subjects. (This technology and these methods will be managed and executed by the TR&D1 framework to distributed datasets totaling over 10,000 subjects). Lastly, our TR&D3 (Intrinsic Surface Mapping) develops a general framework for surface mapping in the high dimensional Laplace-Beltrami embedding space via the mathematical optimization of their Riemannian metric. Our approach here overcomes fundamental limitations in existing methods based on spherical registration by eliminating the metric distortion during the parameterization step, thus achieving much improved accuracy in mapping brain anatomy. Coupled with a mature and efficient administrative structure and comprehensive training and dissemination, this program serves a wide and important need in the scientific community. PROJECT NARRATIVE - OVERALL The comprehensive suite of technologies include algorithmic and computational methods for image management, processing, data analysis and visualization. The technologies are ideally suited to enable holistic studies of the interactions between different imaging data modalities, phenotypic population characteristics, and physiological brain connectivity.",Laboratory of Neuro Imaging Resource (LONIR),9491555,P41EB015922,"['AIDS/HIV problem', 'Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Atlases', 'Autistic Disorder', 'Award', 'Behavioral', 'Books', 'Brain', 'Brain Mapping', 'Brain imaging', 'Clinical Data', 'Clinical Research', 'Communities', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Databases', 'Dementia', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Environment', 'Equilibrium', 'Evolution', 'Fiber', 'Funding', 'Image', 'Informatics', 'Investigation', 'Laboratories', 'Manuscripts', 'Mathematics', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Parkinson Disease', 'Peer Review', 'Phenotype', 'Physiological', 'Population', 'Population Characteristics', 'Process', 'Protocols documentation', 'Publications', 'Publishing', 'Reproducibility', 'Research Activity', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schizophrenia', 'Science', 'Services', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Students', 'Surface', 'System', 'Technology', 'Tissues', 'Training', 'United States National Institutes of Health', 'Visualization software', 'algorithmic methodologies', 'base', 'brain shape', 'cohort', 'computer grid', 'computer infrastructure', 'computerized data processing', 'data archive', 'data management', 'data resource', 'data visualization', 'deep learning', 'design', 'high dimensionality', 'imaging genetics', 'imaging modality', 'improved', 'innovation', 'morphometry', 'neuroimaging', 'new technology', 'novel', 'programs', 'symposium', 'synergism', 'technology research and development', 'tool', 'tractography', 'translational study', 'web services']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,P41,2018,1717236,-0.0018138236350510636
"Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)? PROJECT SUMMARY The North Coast Conference on Precision Medicine is a national annual mid-sized conference series held in Cleveland, Ohio. The conference series aims to serve as a venue for the continuing education and exchange of scientific ideas related to the rapidly evolving and highly interdisciplinary landscape that is precision medicine research. The topics for each conference coincide with the national conversation and research agenda set by national research programs focused on precision medicine. The 2018 conference is a symposium that will focus on issues related to return of genomic results both in clinical and research settings with an emphasis on diverse populations. The conference will be organized as a traditional format with invited speakers from among national experts for topics ranging from issues returning research results to culturally diverse participants and family members, inclusion of diverse patient and participant populations in the Clinical Sequencing Evidence- Generating Research (CSER) consortium and the Trans-Omics for Precision Medicine (TOPMed) Program, pharmacogenomics-guided dosing and race/ethnicity, strategies used to return results, among others. 2019 and beyond conference topics are being considered from previous symposia attendees and trends in precision medicine research. Odd-numbered year conferences include a workshop component that has previously covered outcome and exposure variable extraction from electronic health records. Future workshop topics being considered include integration of multiple ‘omics, drug response in different populations, pharmacogenomics clinical implementation, precision medicine in cancer, data sharing and informed consent, and the use of apps for recruitment, diagnosis, follow-up, and treatment. Our second major objective of this conference series is the promotion of diversity in the biomedical workforce. It is well-known that the pipeline from training to full professor for women in biomedical research is leaky whereas the pipeline for under-represented minorities is practically non-existent. Drawing from national and local sources, we vet women and under-represented minorities for every invited speaker opportunity, thereby providing valuable career currency and networking opportunities. We will also encourage women and under-represented minorities, particularly at the trainee level, to attend and participate in this conference series to spur interest in pursuing precision medicine research as a career. Overall, the North Coast Conference on Precision Medicine series is a valuable addition to the national conference landscape, and with its unique location and low cost to participants, will serve as an important educational opportunity as precision medicine research accelerates in earnest. PROJECT NARRATIVE The North Coast Conference on Precision Medicine is a yearly fall conference series in Cleveland, Ohio designed as a continuing education forum in the burgeoning area of precision medicine research. The conference brings together national experts on a host of topics ranging from bioethics to bioinformatics to biomedical informatics to speak and lead workshops on timely challenges posed in translating complex genomic and health data into clinical practice. The conference series also serves to promote diversity in the biomedical workforce. This year’s symposium will focus issues related to return of genomic results in both clinical and research settings with an emphasis on diverse populations.",Sequencing and Genotyping in Diverse Populations:  Who Wants What Back (and When)?,9612854,R13HG010286,"['Academic Medical Centers', 'Acceleration', 'African American', 'Area', 'Back', 'Big Data', 'Bioethics', 'Bioinformatics', 'Biomedical Research', 'Clinic', 'Clinical', 'Clinical Research', 'Complex', 'Computational Biology', 'Computer Simulation', 'Continuing Education', 'Custom', 'Data', 'Databases', 'Diagnosis', 'Dose', 'Educational workshop', 'Electronic Health Record', 'Ensure', 'Ethnic Origin', 'Family member', 'Funding', 'Future', 'Generations', 'Genetic', 'Genome', 'Genomics', 'Genotype', 'Goals', 'Health system', 'Healthcare Systems', 'Hospitals', 'Incidental Findings', 'Informed Consent', 'Institution', 'Knowledge', 'Lead', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Mining', 'Names', 'Ohio', 'Outcome', 'PMI cohort', 'Participant', 'Pathogenicity', 'Patients', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Phenotype', 'Physicians', 'Population', 'Population Heterogeneity', 'Prevention', 'Process', 'Race', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Schedule', 'Science', 'Series', 'Source', 'Surveys', 'Technology', 'Time', 'Training', 'Trans-Omics for Precision Medicine', 'Translating', 'Travel', 'Underrepresented Groups', 'Underrepresented Minority', 'United States', 'United States Centers for Medicare and Medicaid Services', 'Variant', 'Veterans', 'Woman', 'base', 'big biomedical data', 'biomedical informatics', 'career', 'clinical care', 'clinical implementation', 'clinical practice', 'clinical sequencing', 'clinically relevant', 'cost', 'cost effective', 'data sharing', 'design', 'falls', 'follow-up', 'forging', 'frontier', 'genome-wide', 'genomic data', 'health data', 'health disparity', 'health information technology', 'incentive program', 'individual patient', 'interest', 'medical specialties', 'multiple omics', 'patient population', 'point of care', 'posters', 'precision medicine', 'programs', 'recruit', 'response', 'science education', 'senior faculty', 'symposium', 'trend']",NHGRI,CASE WESTERN RESERVE UNIVERSITY,R13,2018,10000,-0.001603630888927633
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9542295,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Overweight', 'Pathology', 'Personal Satisfaction', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'online resource', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'social', 'social model', 'societal costs', 'surgery outcome', 'tool', 'visiting scholar', 'wearable device']",NIBIB,STANFORD UNIVERSITY,U54,2018,955747,0.007216232451062741
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9404042,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2018,564487,0.02752511428024238
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9536962,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2018,264913,0.0020426651608675193
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9535429,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Meta-Analysis', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2018,315184,-0.029070203374494993
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9568023,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2018,386960,0.0014401049620740623
"Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation DESCRIPTION (provided by applicant): Multi-atlas label fusion (MALF) is a powerful new technology that can automatically detect and label anatomical structures in biomedical images. It is arguably the most successful general-purpose automatic image segmentation technique ever developed. Automatic segmentation is in high demand in clinical and research applications of medical imaging, since segmentation forms a crucial step towards extracting quantitative information from imaging data, and since manual and semi-automatic approaches are ill suited for today's increasingly large and complex imaging datasets. Despite a number of papers that demonstrated outstanding performance of MALF methods across a range of biomedical imaging applications, the broader biomedical imaging research community has been slow to adopt this technique. This can be explained by multiple factors, including the technique's high computational demands, lack of a turnkey software implementation, as well as scarcity of validation in clinical imaging datasets and in the presence of extensive pathology. The present application seeks to remove these barriers and to enable a broad range of clinicians and biomedical researchers to take advantage of MALF technology. It builds on our strong track record of innovation in the MALF field, including a novel redundancy-correcting MALF technique that led in segmentation grand challenges in the past two years. Aim 1 seeks to improve the computational performance of MALF by replacing dense deformable image registration, by far the most time consuming component of MALF, with faster and less constrained sparse registration strategies. We hypothesize that this will not only reduce the computational cost of MALF, but will also make it more robust to anatomical variability, in particular enabling its use for tumor and lesion segmentation. Aim 2 proposes algorithmic extensions to MALF that support automatic segmentation of dynamic and multi-modality imaging datasets, which have been largely overlooked in the MALF literature. Aim 3 will develop a turnkey open-source implementation of MALF methodology. Taking advantage of cloud computing technology, this software will allow users with minimal image processing expertise to take full advantage of MALF segmentation on their desktop. Aim 3 will also provide a set of publicly available atlases and the means for users to build new custom atlas sets from their own data. Aim 4 will perform extensive evaluation of the new methods and software in challenging real-world clinical imaging data, including brain and cardiac imaging. As part of this evaluation, we will quantify how well our MALF approach and competing techniques generalize to novel imaging datasets with heterogeneity in acquisition parameters and clinical phenotypes. PUBLIC HEALTH RELEVANCE: This research will make it possible for a wide community of researchers who collect and analyze medical imaging data to take advantage of a new class of computer algorithms that very accurately label and measure anatomical structures and pathological formations in medical images. By offering more accurate image-derived measurements, the project promises to improve the accuracy of diagnosis, reduce the costs of biomedical re- search studies and pharmaceutical trials, and accelerate scientific discovery.",Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation,9547416,R01EB017255,"['Address', 'Adopted', 'Affect', 'Algorithms', 'Anatomy', 'Atlases', 'Biomedical Research', 'Brain', 'Brain imaging', 'Cardiac', 'Clinical Data', 'Clinical Research', 'Cloud Computing', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Dementia', 'Diagnostic', 'Evaluation', 'Gold', 'Heterogeneity', 'High Performance Computing', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'International', 'Intervention', 'Joints', 'Label', 'Lead', 'Learning', 'Lesion', 'Literature', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Medial', 'Medical Imaging', 'Medical Research', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Multiple Sclerosis Lesions', 'Myocardium', 'Paper', 'Pathologic', 'Pathology', 'Patient Care', 'Performance', 'Pharmacologic Substance', 'Public Domains', 'Research', 'Research Infrastructure', 'Research Personnel', 'S-nitro-N-acetylpenicillamine', 'Scheme', 'Services', 'Structure', 'Techniques', 'Technology', 'Temporal Lobe', 'Temporal Lobe Epilepsy', 'Time', 'Training', 'Ultrasonography', 'Uncertainty', 'Validation', 'Work', 'aortic valve', 'base', 'bioimaging', 'clinical application', 'clinical imaging', 'clinical phenotype', 'clinical practice', 'cloud based', 'cluster computing', 'cohort', 'cost', 'diagnostic accuracy', 'experience', 'heart imaging', 'image processing', 'image registration', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'interest', 'multi-atlas segmentation', 'multidisciplinary', 'new technology', 'novel', 'open source', 'outreach', 'public health relevance', 'research study', 'success', 'targeted imaging', 'tool', 'tumor']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2018,597688,0.00706251108463568
"Omics for TB:  Response to Infection and Treatment No abstract available Project Narrative - Omics for TB: Response to Infection and Treatment Mycobacterium tuberculosis causes ~10 million new cases of active disease and 1.8 million deaths each year, and our tools to combat tuberculosis (TB) disease are universally outdated and overmatched. This project combines separate advances in systems biology and network modeling to produce experimentally grounded and verifiable systems-level models of the host and MTB regulatory networks that affect disease progression and response to treatment.",Omics for TB:  Response to Infection and Treatment,9816680,U19AI135976,"['Address', 'Affect', 'Bacteria', 'Biological', 'Cessation of life', 'Clinical', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Data', 'Data Set', 'Disease', 'Disease Outcome', 'Disease Progression', 'Drug-sensitive', 'Eicosanoids', 'Elements', 'Genetic Transcription', 'Goals', 'Human', 'Immunologic Receptors', 'Infection', 'Inflammatory Response', 'Machine Learning', 'Mass Spectrum Analysis', 'Methodology', 'Modeling', 'Molecular Profiling', 'Mouse Strains', 'Multiplexed Ion Beam Imaging', 'Mus', 'Mycobacterium tuberculosis', 'Network-based', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Predisposition', 'Proteomics', 'Reagent', 'Receptor Activation', 'Records', 'Regulator Genes', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Sampling', 'Symptoms', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Time', 'Treatment outcome', 'Tuberculosis', 'Vaccines', 'Work', 'base', 'biological heterogeneity', 'chemotherapy', 'combat', 'cost effective', 'design', 'global health emergency', 'high risk', 'in vivo', 'member', 'metabolomics', 'mouse model', 'network models', 'novel', 'pandemic disease', 'pathogen', 'predictive signature', 'programs', 'protein protein interaction', 'response', 'tool', 'transcriptomics', 'treatment response', 'tuberculosis treatment']",NIAID,SEATTLE CHILDREN'S HOSPITAL,U19,2018,1991315,-0.001916921565451296
"Pathology Image Informatics Platform for visualization, analysis and management ﻿    DESCRIPTION (provided by applicant): With the advent of whole slide digital scanners, histopathology slides can be digitized into very high-resolution digital images, realizing a new ""big data"" stream that can potentially rival ""omics data"" in size and complexity. Just as with the analysis of high-throughput genetic and expression data, the application of sophisticated image analytic tools and data pipelines can render the often passive data of digital pathology (DP) archives into a powerful source for: (a) rich quantitative insights into cancer biology and (b) companion diagnostic decision support tools for precision medicine. Digital pathology enabled companion diagnostic tests could yield predictions of cancer risk and aggressiveness in a manner similar to molecular diagnostic tests. However, prior to widespread clinical adoption of DP, extensive evaluation of clinical interpretation of DP imaging (DPI) and accompanying decision support tools needs to be undertaken. Wider acceptance of DPI by the cancer community (clinical and research) is hampered by lack of a publicly available, open access image informatics platform for easily viewing, managing, and quantitatively analyzing DPIs. While some commercial platforms exist for viewing and analyzing DPI data, none of these platforms are freely available. Open source image viewing/management platforms that cater to the radiology (e.g. XNAT) and computational biology communities are typically not conducive to handling very large file sizes as encountered with DPI datasets.  This multi-PI U24 proposal seeks to expand on an existing, freely available pathology image viewer (Sedeen Image Viewer) to create a pathology informatics platform (PIIP) for managing, annotating, sharing, and quantitatively analyzing DPI data. Sedeen was designed as a universal platform for DPI (by addressing several proprietary scanner formats and ""big data"" challenges), to provide (1) reliable and useful image annotation tools, and (2) for image registration and analysis of DPI data. Additionally, Sedeen has become an application for cropping large DPIs so that they can be input into programs such as Matlab or ImageJ. Sedeen has been freely available to the public for three years, with over 160 unique users from over 20 countries.  Building on the initial successes of Sedeen and its existing user base, our intent is to massively increase dissemination of DPI and algorithms in the cancer research community and clinical trial efforts, as well as to contribute towards the adoption of a rational and standardized set of DP operational conventions. This unique project will allow end users with different needs and technical backgrounds to seamlessly (a) archive and manage, (b) share, and (c) visualize their DPI data, acquired from different sites, formats, and platforms. The PIIP will provide a unified user interface for third party algorithms (nuclear segmentation, color normalization, biomarker quantification, radiology-pathology fusion) and will allow for algorithmic evaluation upon data arising from a plurality of source sites. By partnering with professional societies, we envision that the PIIP user base will expand to include the oncology, pathology, radiology, and pharmaceutical communities. PUBLIC HEALTH RELEVANCE: This grant will result in the further development of advanced functionality of the already existing digital pathology image informatics platform (PIIP) with an established user-base for cancer research. Such an enhanced platform will provide the much-needed foundation for advancing (a) routine clinical adoption of digital pathology for primary diagnosis and (b) training and validation of companion diagnostic decision support systems based off histopathology. Thus, the project is aligned with the NCI's goal to foster innovative research strategies and their applications as a basis for ultimately protecting and improving human health.","Pathology Image Informatics Platform for visualization, analysis and management",9548627,U24CA199374,"['Address', 'Adoption', 'Advanced Development', 'Algorithmic Analysis', 'Algorithms', 'American', 'Archives', 'Big Data', 'Biological Markers', 'Cancer Biology', 'Cancer Prognosis', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Color', 'Communities', 'Community Clinical Oncology Program', 'Community Trial', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computer software', 'Country', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Storage and Retrieval', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Evaluation', 'Felis catus', 'Fostering', 'Foundations', 'Genetic', 'Goals', 'Grant', 'Health', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Informatics', 'Institution', 'International', 'Language', 'Length', 'Letters', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical Imaging', 'Medical Students', 'Molecular Diagnostic Testing', 'Morphology', 'Nuclear', 'Ontology', 'Optics', 'Pathologist', 'Pathology', 'Pharmacologic Substance', 'Professional Organizations', 'Protocols documentation', 'Pythons', 'Radiology Specialty', 'Research', 'Resolution', 'Scientist', 'Site', 'Slide', 'Societies', 'Source', 'Standardization', 'Stream', 'Training', 'Training and Education', 'Validation', 'analytical tool', 'annotation  system', 'anticancer research', 'base', 'biomarker discovery', 'biomedical scientist', 'cancer diagnosis', 'cancer imaging', 'cancer risk', 'clinical research site', 'companion diagnostics', 'computer human interaction', 'data exchange', 'data integration', 'data sharing', 'design', 'digital', 'digital imaging', 'digital pathology', 'drug discovery', 'high throughput analysis', 'image archival system', 'image registration', 'imaging informatics', 'improved', 'in vivo imaging', 'innovation', 'insight', 'interest', 'malignant breast neoplasm', 'oncology', 'open source', 'pathology imaging', 'photonics', 'precision medicine', 'programs', 'public health relevance', 'quantitative imaging', 'radiological imaging', 'repository', 'research clinical testing', 'success', 'support tools', 'symposium', 'tool', 'tumor', 'user friendly software', 'validation studies']",NCI,CASE WESTERN RESERVE UNIVERSITY,U24,2018,573677,-0.010557693554260415
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9593059,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Simulation', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data management', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2018,750000,-0.006690567338544198
"Enabling Shared Analysis and Processing of Large Neurophysiology Data Project Summary / Abstract Understanding brain function is key to improving health care and advancing a number of scientific initiatives. The treatment of degenerative brain diseases such as Alzheimer's, Parkinson’s, and ALS is becoming increasingly important as the current US population ages and life expectancies increase. The costs of Alzheimer's and other dementias is estimated at over $200 billion in 2016 alone, not to mention the human devastation that these diseases incur. Autism, addiction, depression, epilepsy, traumatic brain injury, and pain treatment are just a few more of the critically important health concerns related to brain function. Cognitive science is also at the forefront of research into computational and autonomous systems, with the potential to revolutionize human computer interaction and tackle emerging global challenges. As a result of these and other significant opportunities the BRAIN (Brain Research through Advancing Innovative Neurotechnologies) Presidential initiative was created to improve the future health and competitiveness of the nation, with the fundamental goal of accelerating brain research. Consistent with this goal, the brain research community has developed the Neurodata Without Borders: Neurophysiology (NWB) file format and specification to support large-scale collaboration and research. This open format was created in 2014 and is already making an impact on cellular-based neurophysiology, with organizations such as the Allen Institute for Brain Science generating and sharing datasets such as the Allen Brain Observatory and the Allen Cell Types Database. Although this preliminary work is promising, progress in the research community is slowed by a lack of software tools to readily browse, process, analyse, and visualize NWB data, while promoting replicability. Thus this work aims to to produce such tools in support of the BRAIN initiative and other large-scale brain research programs by supporting and growing the NWB community. The work proposed here addresses three important workflows in cellular-based neurophysiology: o​ ptical physiology to image neurons under stimuli, silicon probe recordings to detect spike events from the surface of the cortex down through deeps​ structures, and​ in vitro slice electrophysiology to record t​ ime-varying stimulus and electrical response from a neuron​. Novel multiscale software tools will be created to enable efficient browsing, processing, analysis, and visualization of NWB-based brain data; linking experimental stimuli to observed responses. Conversion utilities will also be developed to convert existing data into NWB form. As the data is large, complex, and may be distributed across many sites, the software tools will be web-based, enabling researchers to remotely access and process data in a reproducible manner, and to use scalable cloud computing resources. The software will be released under open source licences and will be placed under formal software process to facilitate sharing across the research community. The tools will be conceived and created with the help of Allen scientists, who will also perform final validation using these three workflows. Project Narrative This research is aimed at better understanding brain function. Such knowledge may lead to improved therapies for degenerative brain diseases such as Alzheimer's, Parkinson’s, and ALS; and provide new treatments for autism, addiction, depression, epilepsy, traumatic brain injury, and pain treatment. These studies of brain science may also yield significant insights into computing areas such as robotics and computer vision, thereby providing both healthcare and economic benefits.",Enabling Shared Analysis and Processing of Large Neurophysiology Data,9515062,R44MH115731,"['Address', 'Adopted', 'Adoption', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Autistic Disorder', 'Base of the Brain', 'Behavior', 'Brain', 'Brain Diseases', 'Case Study', 'Cloud Computing', 'Cognition', 'Cognitive Science', 'Collaborations', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Epilepsy', 'Event', 'Foundations', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'Image', 'Imagery', 'In Vitro', 'Individual', 'Institutes', 'Knowledge', 'Laboratories', 'Lead', 'Licensing', 'Life Expectancy', 'Link', 'Mental Depression', 'Metadata', 'Methods', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Optics', 'Pain management', 'Parkinson Disease', 'Periodicity', 'Phase', 'Physiology', 'Pilot Projects', 'Population', 'Process', 'Pythons', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Robotics', 'Science', 'Scientist', 'Services', 'Silicon', 'Site', 'Slice', 'Software Tools', 'Standardization', 'Stimulus', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Traumatic Brain Injury', 'Validation', 'Visual', 'Visualization software', 'Work', 'addiction', 'analytical tool', 'base', 'brain research', 'cell type', 'community based participatory research', 'computer human interaction', 'computing resources', 'cost', 'data exchange', 'data format', 'data modeling', 'data sharing', 'data warehouse', 'design', 'distributed data', 'experimental study', 'falls', 'file format', 'hackathon', 'health care economics', 'improved', 'innovation', 'innovative neurotechnologies', 'insight', 'nervous system disorder', 'neurophysiology', 'novel', 'open source', 'prevent', 'programs', 'relating to nervous system', 'research and development', 'response', 'success', 'terabyte', 'tool', 'web interface', 'web-enabled']",NIMH,"KITWARE, INC.",R44,2018,728425,-0.01684205067506398
"Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND) Project Summary/Abstract  This Phase III (P-III) COBRE project will extend the cores that have been successfully leveraged in our Phase I (P-I) and Phase II (P-II) COBRE projects and sustain these unique resources in New Mexico through the im- plementation of a business plan. Over the past eight years we have built up infrastructure and created a cutting edge brain imaging center, our P-II project is just over half-way through and is even more successful than our P- I was at this point in time. The Mind Research Network (MRN) houses an Elekta Neuromag 306-channel MEG System, a high density EEG lab, a 3T Siemens Trio MRI scanner, and a mobile 1.5T Siemens Avanto MRI scanner. Additional resources include a centralized neuroinformatics system, a strong IT management plan, and state-of-the-art image analysis expertise and tools. This P-III COBRE center will continue our momentum and move the cores we have developed into a position of long term sustainability. We will continue with the technical cores established during the P-II project including multimodal data acquisition (MDA), algorithm and data analy- sis (ADA), and biostatistics and neuro-informatics (BNI). These cores have begun to serve MRN and the greater community, as well as other institutions including extensive collaborations with IDeA funded projects in New Mexico and other states. We believe this P-III COBRE is extremely well-positioned to establish and sustain New Mexico as one of the premier brain imaging sites. We include an extensive pilot project program (PPP) that is built on the successful pilot programs implemented as part of the earlier COBRE phases. This includes an ex- tensive educational, mentoring, and faculty development program to carefully mentor and position faculty who use the cores to maximize their potential to successfully compete for external funding, thus fulfilling the ultimate goals of the COBRE program. 2 Narrative  This Phase III COBRE project is a natural extension of our Phase I and II COBRE projects which were cen- tered on mentoring individual researchers along with building the necessary infrastructure to support multimodal neuroimaging in mental illness. During this time, cutting-edge cores were developed that facilitated not only our local projects but also research at multiple institutions across New Mexico; the cores served as neuroimaging facilities and training centers for others to utilize. The Phase III project will ensure the sustainability of these cores as they transition to being fully funded by a broad cadre of users with various funding sources. We propose three technical cores including a multimodal data acquisition (MDA) core, an algorithm and data analysis (ADA) core, and a biostatistics and neuro-informatics (BNI) core. These cores have already shown their utility and have begun to be leveraged by users outside the COBRE. In addition, we propose a robust pilot project program (PPP) to continue to seed and enable new users of the cores to ultimately grow and sustain world class brain imaging research within our IDeA state, thus fulfilling the ultimate goals of the COBRE program. 1",Phase III COBRE:  Multimodal Imaging of Neuropsychiatric Disorders (MIND),9281577,P30GM122734,"['Algorithmic Analysis', 'Appointment', 'Area', 'Awareness', 'Biology', 'Biometry', 'Bipolar Depression', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Businesses', 'Centers of Research Excellence', 'Chemistry', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Core Facility', 'Data', 'Data Analyses', 'Department of Energy', 'Development', 'Devices', 'Diffusion Magnetic Resonance Imaging', 'Dimensions', 'Disease', 'Electroencephalography', 'Engineering', 'Ensure', 'Environment', 'Equipment', 'Faculty', 'Functional Magnetic Resonance Imaging', 'Funding', 'Funding Agency', 'Genetic', 'Goals', 'Grant', 'Image', 'Image Analysis', 'Imaging technology', 'Individual', 'Institution', 'Interdisciplinary Study', 'Leadership', 'Magnetic Resonance Imaging', 'Magnetic Resonance Spectroscopy', 'Magnetoencephalography', 'Major Depressive Disorder', 'Mental Depression', 'Mental disorders', 'Mentors', 'Methods', 'Mind-Body Method', 'Mission', 'Multimodal Imaging', 'Neurobiology', 'Neurologic', 'Neurosciences', 'New Mexico', 'Paper', 'Patients', 'Peer Review', 'Phase', 'Pilot Projects', 'Positioning Attribute', 'Principal Investigator', 'Program Development', 'Psychiatry', 'Publications', 'Recording of previous events', 'Research', 'Research Domain Criteria', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Training', 'Resources', 'Role', 'Schizophrenia', 'Seeds', 'Site', 'Structure', 'System', 'Teacher Professional Development', 'Time', 'Training', 'Training Programs', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Vision', 'base', 'cohesion', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'deep learning', 'density', 'design', 'distinguished professor', 'improved', 'independent component analysis', 'meetings', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'neuropsychiatric disorder', 'programs', 'tool']",NIGMS,THE MIND RESEARCH NETWORK,P30,2018,1320387,-0.0052061631003017346
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9403171,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Quality', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Learning', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'Measures', 'Medical', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Subject Headings', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'repository', 'specific biomarkers']",NLM,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",U01,2017,548068,0.0023838122583164055
"QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data  The proposed research directly addresses the mission of NIH's BD2K initiative by developing appropriate tools to derive novel insights from available Big Data and by adapting sophisticated machine learning methodology to a framework familiar to biomedical researchers. This new methodology will be one of the first to enable use of machine learning techniques with time-to-event and continuous longitudinal outcome data, and will be the first such extension of the deep Poisson model. In essence, this undertaking builds the missing bridge between the need for advanced prognostic and predictive techniques among biomedical and clinical researchers and the unrealized potential of deep learning methods in the context of biomedical data collected longitudinally. To facilitate smooth adoption in clinical research, the results will be translated into terms familiar to applied practitioners through publications and well-described software packages. The application of the methodology developed will be illustrated using data from the NIH dbGAP repository, thereby further promoting the use of open access data sources. Optimal risk models are essential to realize the promise of precision medicine. This project develops novel machine learning methods for time-to-event and continuous longitudinal data to enhance risk model performance by exploiting correlations between large numbers of predictors and genetic data. This will enable biomedical researchers to better stratify patients in terms of their likelihood of response to multiple therapies.",QuBBD: Deep Poisson Methods for Biomedical Time-to-Event and Longitude Data ,9392642,R01EB025020,"['Address', 'Adoption', 'Advanced Development', 'Algorithms', 'Architecture', 'Big Data', 'Big Data to Knowledge', 'Blood Glucose', 'Blood Pressure', 'Categories', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Research', 'Comorbidity', 'Computer software', 'Data', 'Data Sources', 'Development', 'Electronic Health Record', 'Event', 'Factor Analysis', 'Formulation', 'Funding', 'Gaussian model', 'Genetic', 'Gray unit of radiation dose', 'Hazard Models', 'Health system', 'Individual', 'Learning', 'Link', 'Lipids', 'Machine Learning', 'Medical Genetics', 'Medical History', 'Metabolic', 'Methodology', 'Methods', 'Mission', 'Modality', 'Modeling', 'Noise', 'Outcome', 'Performance', 'Persons', 'Pharmacology', 'Principal Investigator', 'Publications', 'Recommendation', 'Research', 'Research Personnel', 'Risk', 'Risk Factors', 'Risk stratification', 'Specific qualifier value', 'Structure', 'Techniques', 'Time', 'Translating', 'Translations', 'United States National Institutes of Health', 'Work', 'analog', 'cardiovascular disorder epidemiology', 'data access', 'data modeling', 'database of Genotypes and Phenotypes', 'genetic information', 'hazard', 'insight', 'learning strategy', 'novel', 'patient stratification', 'practical application', 'precision medicine', 'predictive modeling', 'prognostic', 'repository', 'response', 'semiparametric', 'temporal measurement', 'time use', 'tool', 'treatment response']",NIBIB,DUKE UNIVERSITY,R01,2017,262150,0.02606704824256014
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9326367,R01LM012086,"['Age', 'Area', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Community Medicine', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'National Health Policy', 'Natural Language Processing', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Software Tools', 'Standardization', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'novel', 'open source', 'process optimization', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability', 'web services', 'web-based tool']",NLM,NORTHEASTERN UNIVERSITY,R01,2017,293503,-0.004164776399974036
"From genomics to natural language processing: A protected environment for research computing in the health science NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Health sciences researchers are often required to manage, mine, and analyze restricted patient data (Protected Health Information, PHI) to facilitate and advance their research aims. They are often required to do this without access to central information technology expertise or resources to facilitate their research aims. These researchers are often left to their own devices to “solve” their research compute and data needs and are challenged due to lack of available resources, barriers from central IT, and/or lack of knowledge of available resources. A further challenge is that “small” data sets— data that researchers could formerly handle on office resources—have morphed and grown into the big data domain through the explosion of technical advances and significant expansion in various research directions. Examples include: genomics research, image analysis, simulation, natural language processing, and mining of EMRs. Therefore, the need exists to develop a framework for managing and processing this data securely and reliably. This S10 equipment proposal is to replace the “protected environment” (PE) prototype the University of Utah’s Center for High Performance Computing (CHPC) and Department of Biomedical Informatics built six years ago and has operated since. The PE consists of both high performance computing and virtual machine (VM) components and associated storage sufficient to manage, protect and analyze HIPAA protected health information. This environment has been very successful and has grown significantly in scope. CHPC isolated this protected environment in the secured University of Utah Downtown Data Center and setup a network protected logical partition that provided research groups specific access to individual data sets. As the environment and technology developed, CHPC added additional security features such as two-factor authentication for entry and audit/monitoring. Unfortunately, the prototype has reached the point where demand is surpassing capability and all the hardware is aged and off-warranty. To give an idea of users of the virtual machine farm component, the Biomedical Informatics Core (BMIC) REDCap (Research Electronic Data Capture) environment for data collection has over 2,500 users in 1,500 projects supporting over $25M in NIH funding at the University of Utah, including support for more than 25 active NIH R-01 grants. Moreover, the HIPAA compliant protected environment was a key factor that aided passing the recent University of Utah HIPAA audit. The “protected environment” also helped the University of Utah Health Sciences Center and the BMIC justify the NCATS Center Clinical and Translational Science award (1ULTR001067). NIH S10 equipment proposal: From genomics to natural language processing:  A protected environment for research computing in the health sciences. Project Narrative: The proposed “Protected Environment” instrument will provide research computing and data management capabilities for health sciences researchers to properly manage, secure, and analyze HIPAA regulated protected health information. The technology will not only support a large number of clinical trials, but also enable research in Human Genetics and Natural Language Processing of electronic health records.",From genomics to natural language processing: A protected environment for research computing in the health science,9274445,S10OD021644,"['Award', 'Big Data', 'Clinical Sciences', 'Data', 'Data Collection', 'Data Set', 'Devices', 'Environment', 'Equipment', 'Explosion', 'Farming environment', 'Funding', 'Genomics', 'Grant', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'High Performance Computing', 'Image Analysis', 'Individual', 'Information Technology', 'Knowledge', 'Left', 'Mining', 'Monitor', 'Natural Language Processing', 'Patients', 'Research', 'Research Personnel', 'Resources', 'Secure', 'Security', 'Technology', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Utah', 'aged', 'biomedical informatics', 'computerized data processing', 'electronic data', 'prototype', 'simulation', 'virtual']",OD,UNIVERSITY OF UTAH,S10,2017,493595,-0.012537675289352533
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9282279,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Custom', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacology', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Source', 'Standardization', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2017,455519,-0.014922701636608905
"Adapting the Berkeley Big Data Analytics Stack to Genomics and Health Project Summary We propose building a computational platform based on the high performance Berkeley Big Data Analytics Stack (BDAS) to support a new ecosystem of Clinical Decision Support (CDS) applications. This platform will make it faster, easier, and less expensive to develop molecular Clinical Decision Support Systems. These systems require real-time queries of globally distributed data, efficient machine learning on large genomic datasets, and must be secure, fault-tolerant and scalable. BDAS and associated technologies are designed to help us meet these challenges and are therefore ideal building blocks to help us create our computational platform. To encourage the adoption of standards for the querying and sharing of large genomic datasets, we will adapt the BDAS stack to support the standards of the Global Alliance for Genomics and Health (GA4GH). Project Narrative Funding this work will help establish a production quality FOSS implementation of the important Global Alliance for Genomics and Health standards. Without such open-source implementations, a fragmented and proprietary platform ecosystem would slow down innovation as well as divert resources away from the practice of medicine.",Adapting the Berkeley Big Data Analytics Stack to Genomics and Health,9466681,R44GM119858,"['Adoption', 'Algorithms', 'Apache', 'Big Data', 'Big Data to Knowledge', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Cloud Computing', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Contractor', 'Data', 'Data Analytics', 'Data Set', 'Distributed Systems', 'Ecosystem', 'Ensure', 'Feedback', 'Funding', 'Genome', 'Genomics', 'Health', 'Individual', 'Industrialization', 'Industry', 'Ingestion', 'Institutes', 'International', 'Leadership', 'Letters', 'Machine Learning', 'Maintenance', 'Measures', 'Medicine', 'Molecular', 'Performance', 'Phase', 'Phenotype', 'Policies', 'Production', 'Provider', 'Publications', 'Resources', 'Running', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Software Tools', 'Source', 'System', 'Technology', 'Time', 'Training', 'Variant', 'Work', 'base', 'cloud platform', 'cluster computing', 'commercialization', 'design', 'distributed data', 'genomic data', 'health care delivery', 'individual patient', 'innovation', 'open source', 'operation', 'petabyte', 'precision medicine', 'symposium', 'web services', 'whole genome']",NIGMS,"CUROVERSE INNOVATIONS, INC.",R44,2017,1086725,0.005870999269093796
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,9384200,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Infrastructure', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2017,685783,0.020554902630061146
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9371707,K99HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Databases', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'comparative effectiveness', 'computer science', 'data sharing', 'design', 'digital', 'effectiveness research', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'learning strategy', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'permissiveness', 'point of care', 'predictive modeling', 'privacy protection', 'programs', 'public trust', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",K99,2017,93824,-0.005722160581391081
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9266344,U54AI117924,"['Address', 'Biological', 'Blood coagulation', 'Clinical', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'biomedical scientist', 'clinical investigation', 'clinical predictors', 'education research', 'graduate student', 'high dimensionality', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning', 'undergraduate student']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2017,229691,-0.0019104992430428723
"NextGen Random Forests Project Summary/Abstract Building from the PI's current R01, we propose next generation random forests (RF) designed for unprecedented accuracy and computational scalability to meet the challenges of today's complex and big data in the health sciences. Superior accuracy is achieved using super greedy trees which circumvent limitations on local adaptivity imposed by classical tree splitting. We identify a key quantity, forest weights, and show how these can be leveraged for further improvements and generalizability. In one application, improved survival estimators are applied to worldwide esophageal cancer data to develop guidelines for clinical decision making. Richer RF inference is another issue explored. Cutting edge machine learning methods rarely consider the problem of estimating variability. For RF, bootstrapping currently exists as the only tool for reliably estimating conﬁdence intervals, but due to heavy computations is rarely applied. We introduce tools to rapidily calculate standard errors based on U-statistic theory. These will be used to increase robustness of esophageal clinical recommendations and to investigate survival temporal trends in cardiovascular disease. In another application, we make use of our new massive data scalability for discovery of tumor and immune regulators of immunotherapy in cancers. This project will set the standard for RF computational performance. Building from the core libraries of the highly accessed R-package randomForestSRC (RF-SRC), software developed under the PIs current R01, we develop open source next generation RF software, RF-SRC Everywhere, Big Data RF-SRC, and HPC RF-SRC. The software will be deployable on a number of popular machine learning workbenches, use distributed data storage technologies, and be optimized for big-p, big-n, and big-np scenarios. Project Narrative We introduce next generation random forests (RF) designed for unprecedented accuracy for complex and big data encountered in the health sciences.",NextGen Random Forests,9383718,R01GM125072,"['Atrophic', 'Benchmarking', 'Big Data', 'Biological Response Modifiers', 'Blood', 'Cancer Patient', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Code', 'Combined Modality Therapy', 'Complex', 'Computer software', 'Confidence Intervals', 'Data', 'Data Storage and Retrieval', 'Databases', 'Development', 'Esophageal', 'Flow Cytometry', 'Guidelines', 'Health Sciences', 'Heart failure', 'Human', 'Hybrids', 'Immune', 'Immunotherapy', 'In Vitro', 'Interagency Registry for Mechanically Assisted Circulatory Support', 'Internet', 'Java', 'Laboratories', 'Language', 'Libraries', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Neoadjuvant Therapy', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Performance', 'Population', 'Pump', 'Receptor Activation', 'Recommendation', 'Resistance', 'Subgroup', 'T-Lymphocyte', 'Technology', 'Therapeutic', 'Thrombosis', 'Time', 'Time trend', 'Trees', 'Weight', 'base', 'clinical decision-making', 'clinical practice', 'design', 'distributed data', 'forest', 'immune checkpoint blockade', 'improved', 'in vivo', 'learning strategy', 'lymph nodes', 'mouse model', 'next generation', 'novel', 'open source', 'outcome forecast', 'parallel processing', 'pre-clinical', 'predicting response', 'predictive modeling', 'receptor', 'response', 'software development', 'statistics', 'theories', 'therapeutic target', 'tool', 'tumor', 'tumor progression']",NIGMS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2017,359872,-0.00023184537950160603
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community. PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.",The Big DIPA: Data Image Processing and Analysis,9295026,R25EB022366,"['Address', 'Architecture', 'Area', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Communities', 'Competence', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Education', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modality', 'Modernization', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Scientist', 'Signal Transduction', 'Site', 'Software Tools', 'Source', 'Stream', 'Systems Biology', 'Talents', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'big biomedical data', 'bioimaging', 'biological systems', 'biomedical scientist', 'computer science', 'computerized tools', 'cost', 'course implementation', 'data acquisition', 'data format', 'demographics', 'design', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging approach', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'learning materials', 'lecture notes', 'lectures', 'microscopic imaging', 'nervous system disorder', 'open source', 'outreach', 'programs', 'public health relevance', 'repository', 'skill acquisition', 'skills', 'statistics', 'theories', 'training opportunity']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2017,161997,-0.039858355682659816
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",9517179,U54EB020403,"['AIDS/HIV problem', 'Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Big Data to Knowledge', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Science', 'Data Set', 'Diagnosis', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'computer science', 'connectome', 'diagnostic biomarker', 'hackathon', 'high dimensionality', 'imaging study', 'innovation', 'multidisciplinary', 'multimodality', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'organizational structure', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'web portal', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2017,81900,0.00191475177066529
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",9302420,U54EB020403,"['AIDS/HIV problem', 'Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Big Data to Knowledge', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Science', 'Data Set', 'Diagnosis', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'computer science', 'connectome', 'diagnostic biomarker', 'hackathon', 'high dimensionality', 'imaging study', 'innovation', 'multidisciplinary', 'multimodality', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'organizational structure', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'web portal', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2017,2369482,0.00191475177066529
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9398728,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Development', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Methods', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'open source', 'practical application', 'predictive modeling', 'repository', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2017,546372,0.008085287265877551
"Summer Institute in Neuroimaging and Data Science Project Summary/Abstract The study of the human brain with neuroimaging technologies is at the cusp of an exciting era of Big Data. Many data collection projects, such as the NIH-funded Human Connectome Project, have made large, high- quality datasets of human neuroimaging data freely available to researchers. These large data sets promise to provide important new insights about human brain structure and function, and to provide us the clues needed to address a variety of neurological and psychiatric disorders. However, neuroscience researchers still face substantial challenges in capitalizing on these data, because these Big Data require a different set of technical and theoretical tools than those that are required for analyzing traditional experimental data. These skills and ideas, collectively referred to as Data Science, include knowledge in computer science and software engineering, databases, machine learning and statistics, and data visualization.  The Summer Institute in Data Science for Neuroimaging will combine instruction by experts in data science methodology and by leading neuroimaging researchers that are applying data science to answer scientiﬁc ques- tions about the human brain. In addition to lectures on the theoretical background of data science methodology and its application to neuroimaging, the course will emphasize experiential hands-on training in problem-solving tutorials, as well as project-based learning, in which the students will create small projects based on openly available datasets. Summer Institute in Neuroimaging and Data Science: Project Narrative The Summer Institute in Neuroimaging and Data Science will provide training in modern data science tools and methods, such as programming, data management, machine learning and data visualization. Through lectures, hands-on training sessions and team projects, it will empower scientists from a variety of backgrounds in the use of these tools in research on the human brain and on neurological and psychiatric brain disorders.",Summer Institute in Neuroimaging and Data Science,9278954,R25MH112480,"['Address', 'Adopted', 'Big Data', 'Brain', 'Brain Diseases', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Databases', 'Discipline', 'Face', 'Faculty', 'Fostering', 'Funding', 'Habits', 'Home environment', 'Human', 'Image', 'Institutes', 'Institution', 'Instruction', 'Internet', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mental disorders', 'Methodology', 'Methods', 'Modernization', 'Neurologic', 'Neurosciences', 'Participant', 'Positioning Attribute', 'Problem Solving', 'Psychology', 'Reproducibility', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Software Engineering', 'Software Tools', 'Structure', 'Students', 'Technology', 'Testing', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'career', 'computer science', 'connectome', 'data management', 'data visualization', 'design', 'e-science', 'experimental study', 'high dimensionality', 'insight', 'instructor', 'interdisciplinary collaboration', 'knowledge base', 'lectures', 'nervous system disorder', 'neurogenetics', 'neuroimaging', 'novel', 'open source', 'prediction algorithm', 'programs', 'project-based learning', 'skills', 'statistics', 'success', 'summer institute', 'theories', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,R25,2017,205185,-0.019657892222836894
"QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine  The purpose of this proposal is to develop a combination of innovative statistical and data visualization approaches using patient-generated health data, including mobile health (mHealth) data from wearable devices and smartphones, and patient-reported outcomes, to improve outcomes for patients with Inflammatory Bowel Diseases (IBDs). This research will offer new insights into how to process and transform patient-generated health data into precise lifestyle recommendations to help achieve remission of symptoms. The specific aims of this research are: 1) To develop new preprocessing methods for publicly available, heterogeneous, time-varied mHealth data to develop a high quality mHealth dataset; 2) To develop and apply novel machine learning methods to obtain accurate predictions and formal statistical inference for the influence of lifestyle features on disease activity in IBDs; and 3) To design and develop innovative, interactive data visualization tools for knowledge discovery. The methods developed in the areas of preprocessing of mHealth data, calibration for mHealth devices, machine learning, and interactive data visualization will be broadly applicable to other mHealth data, chronic conditions beyond IBDs, and other fields in which the data streams are highly variable, intermittent, and periodic. This work is highly relevant to the mission of the NIH BD2K initiative which supports the development of innovative and transformative approaches and tools to accelerate the integration of Big Data and data science into biomedical research. This project will also enhance training in the development and use of methods for biomedical Big Data science and mentor the next generation of multidisciplinary scientists. The proposed research is relevant to public health by seeking to improve symptoms for patients with inflammatory bowel diseases, which are chronic, life-long conditions with waxing and waning symptoms. Developing novel statistical and visualization methods to provide a more nuanced understanding of the precise relationship between physical activity and sleep to disease activity is relevant to BD2K's mission.",QuBBD: Statistical & Visualization Methods for PGHD to Enable Precision Medicine ,9394127,R01EB025024,"['Adrenal Cortex Hormones', 'Adult', 'Adverse effects', 'Affect', 'Americas', 'Area', 'Behavior', 'Behavior Therapy', 'Big Data', 'Big Data to Knowledge', 'Biomedical Research', 'Calibration', 'Caring', 'Cellular Phone', 'Characteristics', 'Chronic', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Development', 'Devices', 'Disease', 'Disease Outcome', 'Disease remission', 'Dose', 'Effectiveness', 'Flare', 'Foundations', 'Functional disorder', 'Funding', 'Health Care Research', 'Imagery', 'Immunosuppression', 'Individual', 'Inflammation', 'Inflammatory', 'Inflammatory Bowel Diseases', 'Institute of Medicine (U.S.)', 'Knowledge Discovery', 'Life', 'Life Style', 'Longitudinal Surveys', 'Longitudinal cohort study', 'Machine Learning', 'Mathematics', 'Measures', 'Mentors', 'Methods', 'Mission', 'Moderate Activity', 'Morbidity - disease rate', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patient-Focused Outcomes', 'Patients', 'Periodicity', 'Phenotype', 'Physical activity', 'Precision therapeutics', 'Process', 'Public Health', 'Quality of life', 'Recommendation', 'Reporting', 'Research', 'Research Institute', 'Schools', 'Scientist', 'Sleep', 'Sleep disturbances', 'Stream', 'Symptoms', 'Therapeutic', 'Time', 'Training', 'Ulcerative Colitis', 'United States National Institutes of Health', 'Visualization software', 'Waxes', 'Work', 'base', 'big biomedical data', 'clinical remission', 'comparative effectiveness', 'cost', 'data visualization', 'design', 'disorder risk', 'effectiveness research', 'health care quality', 'health data', 'improved', 'improved outcome', 'individual patient', 'innovation', 'insight', 'large bowel Crohn&apos', 's disease', 'learning strategy', 'lifestyle factors', 'mHealth', 'member', 'multidisciplinary', 'next generation', 'novel', 'precision medicine', 'symptomatic improvement', 'tool']",NIBIB,UNIV OF NORTH CAROLINA CHAPEL HILL,R01,2017,338637,0.002150120464280882
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9251828,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Modernization', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Standardization', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'interoperability', 'knowledge translation', 'learning progression', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool', 'translational impact']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2017,428512,0.016560937194873674
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  As part of its activities, the National Toxicology Program (NTP) at NIEHS conducts literature-based evaluations to identify the state of the science, evaluate hazards, and determine the effectiveness of NTP’s research. NTP lacks an intelligent automated approach to assist with this work. NIEHS has requested assistant from the Oak Ridge National Laboratory (ORNL), Department of Energy to assist with the research and development of publication and web mining tools for use in NTP’s evaluations. This assistance extends to the Division of Extramual Research and Training to automate the approach for capturing outcomes and impacts from NIEHS-funded research noted in scientific publications and grantees’ progress reports. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,9492481,ES16002001,"['Applications Grants', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Funding', 'Imagery', 'Internet', 'Laboratories', 'Literature', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Outcome', 'Program Effectiveness', 'Program Evaluation', 'Progress Reports', 'Publications', 'Research', 'Research Training', 'Science', 'Techniques', 'Work', 'base', 'hazard', 'learning strategy', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2017,446000,-0.006161710763960711
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9413774,R24MH114788,"['Algorithms', 'Archives', 'Atlases', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Ecosystem', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Imagery', 'Individual', 'Ingestion', 'Institution', 'Internet', 'Knowledge', 'Location', 'Machine Learning', 'Metadata', 'Modification', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data integration', 'data management', 'data modeling', 'data resource', 'data visualization', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'innovative neurotechnologies', 'member', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2017,1247018,-0.012380558661627178
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9285168,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Machine Learning', 'Medicine', 'Methods', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Structure', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'learning strategy', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'tool']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2017,267313,-0.0011648546474007262
"SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies ﻿    DESCRIPTION (provided by applicant): Efforts to reduce the burden of Tuberculosis (TB) are challenged by the persistent social inequalities in health, the limited number of local healthcare professionals, and the weak healthcare infrastructure found in resource-poor communities. Reducing the TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the TB epidemic in high-burden areas. The main objective of this proposal is to expedite the TB diagnosis process by developing novel image processing and machine learning techniques to analyze chest X-ray images thus reducing patient wait times for being diagnosed with TB. The study will be conducted in the district of Carabayllo, a densely occupied, high-burden TB area in Lima, the capital of Perú. Efforts to develop the proposed user-centered, mobile device-based computing system are aligned with the mission of the National Institute of Biomedical Imaging and Bioengineering (NIBIB) and its strategic goals 2 and 4 in particular-the proposed socio-technical intervention aims at developing biomedical imaging techniques (i.e. wireless and image sensing/analyzing) to enable a point-of-care mobile device-based computing system for TB screening and diagnostic. Anticipated outcomes include a) a large-scale, real-world, well-annotated, and public available chest X-ray image database for TB screening, b) development of new image analysis techniques for X-ray image capturing and pre- processing, and c) novel learning-based feature extraction and classification algorithms. This  interdisciplinary effort, involving community, university, hospitals and health care establishments in all stages of the research, responds to the need for increased partnerships between academia and community stakeholders, and the potential for building capacity in biomedical and technology solutions for health in both directions (North-South, South-North). Its scientific contribution lies in the intersection of three NIBIB scientific program areas including image processing, telehealth, and biomedical informatics. PUBLIC HEALTH RELEVANCE: This project is highly relevant to public and global health because it offers a socio-technical solution for resource-poor communities severely affected by TB. Outcomes of this project will contribute significantly to improving specific healthcare processes affecting hard-to-reach communities that are socially excluded and lack the benefits of technological advances while broadening our understanding about effective human centered designs to improve healthcare systems with mobile computing technologies.",SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies,9150601,R01EB021900,"['Academia', 'Address', 'Affect', 'Algorithms', 'Area', 'Benchmarking', 'Biomedical Technology', 'Capital', 'Cessation of life', 'Chest', 'Chronic Disease', 'Cities', 'Classification', 'Clinic', 'Communicable Diseases', 'Communities', 'Community Health', 'Complex', 'Computer Assisted', 'Computer Systems', 'Computer software', 'Computers', 'Data', 'Data Analytics', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Discipline', 'Engineering', 'Epidemic', 'Evaluation', 'Female', 'Goals', 'Health', 'Health Professional', 'Health Sciences', 'Health Technology', 'Healthcare', 'Healthcare Systems', 'Hispanics', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging Techniques', 'Intervention', 'Learning', 'Lung nodule', 'Machine Learning', 'Medical', 'Minority', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Outcome', 'Patients', 'Peru', 'Process', 'Public Health', 'Reader', 'Recruitment Activity', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Sensitivity and Specificity', 'Software Tools', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Time', 'Training', 'Treatment Protocols', 'Tuberculosis', 'Underrepresented Students', 'University Hospitals', 'Vaccines', 'Wireless Technology', 'Woman', 'World Health Organization', 'accurate diagnosis', 'base', 'bioimaging', 'biomedical informatics', 'clinical practice', 'compliance behavior', 'data exchange', 'design', 'digital imaging', 'disadvantaged population', 'disease transmission', 'global health', 'handheld mobile device', 'image processing', 'improved', 'mHealth', 'mobile computing', 'novel', 'open source', 'point of care', 'programs', 'public health relevance', 'reproductive', 'screening', 'social', 'social inequality', 'telehealth', 'tool']",NIBIB,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2017,326571,0.03217961611059152
"All of Us, Wisconsin Our application for OT-PM-16-003, “One in a Million – Precision Medicine Initiative Wisconsin” represents the collaborative efforts of three fully integrated regional healthcare systems to form a virtual state-wide integrated delivery network. The application originates at the Marshfield Clinic Health System (MCHS), one of the most productive and earliest adopters of what has become Precision Medicine. The work of the MCHS has been cited by NIH Director Dr. Francis Collins as a model for the Precision Medicine Initiative (PMI). To ensure comprehensive nearly state-wide coverage of participant health care, MCHS has partnered with the other academic integrated delivery networks in Wisconsin, (University of Wisconsin (UW) and Medical College of Wisconsin (MCW)), that together include 173 clinics, 13 hospitals, insurer partners and 5 Federally Qualified Health Centers (FQHCs). The Blood Center of Wisconsin (BCW) also partners to operate state-wide mobile units of staff and equipment for blood collection that may be purposed for this effort. Through an independent agreement with Aurora Health, the electronic health records of participants recruited by academic and FQHC sites will be made available, ensuring nearly complete coverage of health care records across an entire state, with special emphasis on those populations that are traditionally the most underserved and understudied. The academic sites and FQHC partners have a long history of research cooperation and robust community engagement demonstrated by Clinical and Translational Science Awards (CTSA) ties, the Wisconsin Genome Initiative, reciprocal IRB arrangements, multiple joint grants and other collaborative studies. We have pioneered and implemented data sharing platforms and have demonstrated their usefulness with numerous high impact publications over many years. All three academic centers are leaders in the details of community engagement, recruitment, tracking, return of results and data sharing across various platforms in national and international consortia using large cohorts (e.g., MCHS’s 20,000 participant Personalized Medicine Research Project). A major innovation of this application is that it aligns the major healthcare systems of an entire state to more fully capture each participant’s data wherever participants get their health care. Indeed, 80% of all health care in Wisconsin is captured by our footprint. The State-wide catchment area also includes the oldest, sickest and poorest regions in the US and represent rural and urban areas, including African Americans, Hispanics and Native Americans, all represented as community champions. Through the FQHC partners, the recruiting institutions will achieve the 40% African American, Hispanic and Native American participant rate, with long standing community engagement and history of successful recruitment, including Wisconsin’s 12 Native American Tribes. Lending active support are Community Champions and Community Advisory Boards. A second innovative approach will employ Machine Learning (ML) techniques to optimize recruitment and retention processes. Our scientists have long collaborated on efforts to enhance the breadth and reliability of information extracted from the electronic health record (EHR), using a range of data to identify elements including family pedigrees or the occurrence of clinical events that are susceptible to unreliable coding. These efforts employed state-of-the-art methods including random forests, support vector machines and statistical relational learning, among others. These advanced computational methods will be used to predict those who are most likely to participate and which methods of approach are most effective. A third innovative approach is the planned use of mobile recruitment labs, previously successful with the Survey of the Health of Wisconsin (SHOW). SHOW, began in 2008 provides a novel infrastructure for population health research recruitment, enabling engagement across the state, longitudinal follow-up of participants and community-specific studies. Key assets of SHOW are two mobile units staffed by research and healthcare professionals to facilitate on-site health studies in randomly selected or community-specific participants. SHOW is predicated on community engagement and the response rate has been outstanding (in one study more than 90% of ~4,000 participants consented to follow-up, DNA testing, or blood work). This is true across all racial/ethnic groups, including non-Hispanic Whites, African Americans, Hispanics and Native Americans. The fourth innovative approach is to work with our FQHC partners to recruit traditionally underserved populations. MCHS, UW and MCW will work with FQHC partners (Marshfield Family Health Center, Access Community Health Center, Milwaukee Health Services, Progressive Health Center and 16th Street Community Health Center). Importantly, we are ready to start. We have already informed and engaged our communities through press releases and newsletters. Processes are in place. Volunteers have already asked us to contact them. All the required personnel are trained and SOPs are in place to begin recruitment. We have a productive history of working with partners already funded for PMI cohort recruitment. For several years, we have worked closely (and published) with our colleagues at Northwestern and Columbia through the eMERGE Network and with the University of Pittsburgh through CTSA. Project Director Murray Brilliant has recently served as an advisor to the University of Arizona’s Precision Medicine Program. Thus, we are team-players who are ready, willing and able to be valuable collaborative partners in the effort to build a national engine to transform healthcare under PMI. n/a","All of Us, Wisconsin",9512099,OT2OD025286,"['African American', 'Agreement', 'Arizona', 'Award', 'Blood', 'Catchment Area', 'Clinic', 'Clinical', 'Clinical Sciences', 'Code', 'Collection', 'Communities', 'Community Health Centers', 'Computing Methodologies', 'Consent', 'DNA', 'Data', 'Electronic Health Record', 'Elements', 'Ensure', 'Equipment', 'Ethnic group', 'Event', 'Family', 'Family health status', 'Federally Qualified Health Center', 'Funding', 'Genome', 'Grant', 'Health', 'Health Professional', 'Health Services', 'Health Surveys', 'Health system', 'Healthcare', 'Healthcare Systems', 'Hispanic Americans', 'Hospitals', 'Human Resources', 'Institution', 'Insurance Carriers', 'International', 'Joints', 'Learning', 'Machine Learning', 'Methods', 'Modeling', 'Native Americans', 'Newsletter', 'Not Hispanic or Latino', 'PMI cohort', 'Participant', 'Population', 'Precision Medicine Initiative', 'Press Releases', 'Process', 'Publications', 'Publishing', 'Recording of previous events', 'Records', 'Recruitment Activity', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Scientist', 'Site', 'Techniques', 'Testing', 'Training', 'Translational Research', 'Tribes', 'Underserved Population', 'United States National Institutes of Health', 'Universities', 'Wisconsin', 'Work', 'cohort', 'data sharing', 'follow-up', 'forest', 'genetic pedigree', 'innovation', 'medical schools', 'novel', 'personalized medicine', 'population health', 'precision medicine', 'programs', 'racial and ethnic', 'response', 'rural area', 'urban area', 'virtual', 'volunteer']",OD,MARSHFIELD CLINIC RESEARCH FOUNDATION,OT2,2017,5360833,-0.009995090508576875
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9306202,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Manuals', 'Methods', 'Molecular Analysis', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2017,591990,-0.01929103910734604
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9357656,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Genome', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Internet', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Phenotype', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'base', 'dashboard', 'improved', 'insight', 'microbial community', 'peer', 'prospective', 'repository', 'social', 'tool', 'transcriptomics']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2017,572778,0.014032196616017779
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9360131,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Competence', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Manuals', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Modernization', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Readability', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'annotation  system', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'knowledge base', 'novel', 'permissiveness', 'personalized medicine', 'repository', 'response', 'stem', 'tool', 'translational pipeline']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2017,456710,-0.02403713679087922
"Sci-Score, a tool to support rigor and transparency guidelines Project Summary While  standards  in  reporting  of  scientific  methods  are  absolutely  critical  to  producing  reproducible  science,  meeting  such  standards  is  difficult.  Checklists  and  instructions  are  tough  to  follow  often  resulting  in  low  and inconsistent  compliance.  Scientific  journals  and  societies  as  well  as  the  National  Institutes  of  Health  are  now actively proposing general guidelines to address reproducibility issues, particularly in the reporting of methods  (e.g.,  http://www.cell.com/star-­methods),  but  the  trickier  part  will  be  to  train  the  biomedical  community  to  use these standards to effectively improve how scientific methods are communicated. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency of  Key Biological Resources, we propose to build Sci-­Score a text mining based tool suite to help authors meet the  standard.  Sci-­Score  will  provide  an  automated  check  on  compliance  with  the  RRID  standard  already  implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. The innovation behind Sci-­ score is the provision of a score, which can be obtained by individual investigators, which reflects a numerical  validation of the quality of their methods reporting. We posit that the score will serve as a tool that investigators  and journals can use to compete with themselves and each other, or in the very least allow them to see how  close they are to the average in meeting quality requirements.   Recently, our group has developed a text mining algorithm that has now been successfully been used to detect software tools and databases from the SciCrunch Registry in published papers. Digital tools are one of four resource types that the RRID standard identifies. We propose to extend this approach to the other types of entities: antibodies, cell lines and model organisms. Resource identification along with other quality metrics twill be used to train an algorithm to score the overall quality of the methods document. If successful, the tool could be used by editors, reviewers, and investigators to improve the number of RRIDs, therefore the quality of descriptors of key biological resources in published papers. This SBIR project will build a set of algorithms similar to the resource finding pipeline and develop it into an industrial robust and reconfigurable software system. Our Phase I specific aims include to 1) creating gold sets of data for each resource type and training a set of algorithms for each resource type; 2) designing and evaluating the scoring system; 3) designing and evaluating a report generating system based on the previous aims. In Phase II, we will develop a scalable backend infrastructure to serve the needs of scientific publishers and research community. Standards for scientific methods reporting are absolutely critical to producing reproducible science, but meeting  such standards is difficult. Checklists and instructions are tough to follow often resulting in low and inconsistent  compliance. To support new standards in methods reporting, specifically the RRID standard for Rigor and Transparency, we propose to build Sci-Score text mining based tool suite to help authors meet the standard. Sci-Score will provide an automated check on compliance with the RRID standard implemented by over 100 journals including Cell, Journal of Neuroscience, and eLife. Sci-Score will provide a score rating the quality of  methods reporting in submitted articles, which provides feedback to authors, reviewers and editors on how to improvecompliance with RRIDs and other standards. ","Sci-Score, a tool to support rigor and transparency guidelines",9345707,R43OD024432,"['Address', 'Agreement', 'Algorithms', 'Animal Model', 'Antibodies', 'Area', 'Big Data', 'Biological', 'California', 'Cell Line', 'Cell model', 'Cells', 'Communities', 'Data Set', 'Databases', 'Descriptor', 'Elements', 'Ensure', 'Evaluation', 'Feedback', 'Funding', 'Glare', 'Gold', 'Guidelines', 'Habits', 'Human', 'Individual', 'Industrialization', 'Instruction', 'Journals', 'Learning', 'Literature', 'Machine Learning', 'Manuscripts', 'Methods', 'National Institute of Diabetes and Digestive and Kidney Diseases', 'Neurosciences', 'Organism', 'Paper', 'Performance', 'Phase', 'Plagiarism', 'Process', 'Publications', 'Publishing', 'Readability', 'Reader', 'Reading', 'Reagent', 'Registries', 'Reporting', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Running', 'Sales', 'Science', 'Scoring Method', 'Services', 'Small Business Innovation Research Grant', 'Societies', 'Software Tools', 'System', 'Technology', 'Text', 'To specify', 'Training', 'United States National Institutes of Health', 'Universities', 'Validation', 'Work', 'Writing', 'base', 'biological systems', 'computerized tools', 'design', 'digital', 'improved', 'innovation', 'interest', 'meetings', 'prototype', 'software systems', 'sound', 'text searching', 'tool', 'vigilance', 'web site']",OD,"SCICRUNCH, INC.",R43,2017,221865,-0.0008407600396635662
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9268713,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithmic Analysis', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Data', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Set', 'Data Sources', 'Decentralization', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'International', 'Knowledge', 'Language', 'Letters', 'Linear Models', 'Location', 'Logistics', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Privatization', 'Procedures', 'Process', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'commune', 'computer framework', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2017,655080,0.022456390414051043
"Large-Scale Semiparametric Graphical Models with Applications to Neuroscience DESCRIPTION: The objective of this proposal is to develop and theoretically evaluate a unified set of statistical, computational, and software tools to address data mining and discovery science challenges in the analysis of existing vast amounts of publicly available neuroimaging data. In particular, we propose to develop scalable and robust semiparametric solutions for high-throughput estimation of resting-state brain connectivity networks, both at the individual and population levels, with the flexibility of incorporating covariate information.  The work will contribute meaningfully to the theory and methods for large-scale semiparametric graphical models and will apply these methods to the largest collections of resting-state fMRI data available. The proposed methods and theory include key directions of research for brain network estimation and mining. First, we pro- pose novel methods for subject-specific network estimation, such as would be needed for biomarker development in functional brain imaging. Secondly, we define and propose to evaluate and implement methods for studying population-level graphs, which study collections of graphs. Thirdly, we propose the use of estimated graphs in predictive modeling. Finally, all of these methods will have complementary software and web services development. Most notably, the idea of population graphs allows for the creation of functional brain network atlases.  In summary, the work of this proposal will result in a unified framework for the analysis of modern neuroimaging data via graphical models. Our methods will further be agnostic to intricacies of the technology, thus making it portable across settings and applicable outside of the field of functional brain imaging. The methods will be carefully evaluated via theory, simulation and data-based application evidence. PUBLIC HEALTH RELEVANCE: Modern neuroimaging data are often Big, Complex, Noisy and Dependent. We propose a systematic attempt on methodological development for the largely unexplored but practically important problem of network estimation and mining based on neuroimaging data. Our proposed work represents a significant step forward over the current methodology and has the potential to be applied to analyze a wide range of scientific problems beyond brain imaging data analysis.",Large-Scale Semiparametric Graphical Models with Applications to Neuroscience,9208798,R01MH102339,"['Address', 'Algorithmic Software', 'Algorithms', 'Atlases', 'Attention deficit hyperactivity disorder', 'Big Data', 'Brain', 'Brain Diseases', 'Brain imaging', 'Characteristics', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Discovery', 'Data Set', 'Databases', 'Dependence', 'Development', 'Dimensions', 'Disease', 'Documentation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Gaussian model', 'Graph', 'Heterogeneity', 'Image', 'Individual', 'Internet', 'Machine Learning', 'Mathematics', 'Methodology', 'Methods', 'Mining', 'Modality', 'Modeling', 'Modernization', 'Neurosciences', 'Online Systems', 'Pattern', 'Population', 'Population Study', 'Rest', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Software Tools', 'Statistical Methods', 'Tail', 'Technology', 'Work', 'base', 'biomarker development', 'brain research', 'cloud based', 'computerized tools', 'data mining', 'flexibility', 'high dimensionality', 'interest', 'neuroimaging', 'non-Gaussian model', 'novel', 'portability', 'predictive modeling', 'psychologic', 'public health relevance', 'semiparametric', 'simulation', 'theories', 'user friendly software', 'web services']",NIMH,PRINCETON UNIVERSITY,R01,2017,109672,0.004115823965635059
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9320865,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multimodal Imaging', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'analytical tool', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'high dimensionality', 'imaging Segmentation', 'imaging modality', 'imaging study', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'spatiotemporal', 'study population', 'terabyte', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2017,347156,-0.004110403771503552
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9443736,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,50000,0.020345519130626096
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9333122,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'online resource', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'sensor', 'social', 'social model', 'tool', 'visiting scholar']",NIBIB,STANFORD UNIVERSITY,U54,2017,401742,0.007216232451062741
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9536289,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Environmental Risk Factor', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Income', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Legal', 'Legal patent', 'Liquid substance', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'clinical data warehouse', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'data resource', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'migration', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2017,589741,0.020345519130626096
An Integrated Biomedical Dataset Discovery System for Immunological Research Databases  The The Contractor will develop a prototype of an integrated biomedical dataset discovery system for Immunological Research Databases (BIRD) to facilitate integrated search for data/knowledge/tools of interest from DAIT bioinformatics resources. n/a,An Integrated Biomedical Dataset Discovery System for Immunological Research Databases ,9574416,72201700019C,"['Bioinformatics', 'Contractor', 'Data', 'Data Discovery', 'Data Reporting', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Immune system', 'Immunology', 'Ingestion', 'Knowledge', 'Maps', 'Metadata', 'Modeling', 'Natural Language Processing', 'Ontology', 'Research', 'Resources', 'System', 'Techniques', 'Technology', 'Terminology', 'base', 'data integration', 'indexing', 'information organization', 'interest', 'prototype', 'tool', 'user-friendly']",NIAID,"TECHWAVE INTERNATIONAL, INC.",N43,2017,224960,0.013722332443389624
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9217457,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Quality', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'clinical development', 'data integration', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'natural language', 'next generation', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2017,578512,0.02752511428024238
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9302929,R25OD021880,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion Investigative Technique', 'Informatics', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",OD,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2017,271803,0.0020426651608675193
"SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy Project Abstract Advances in imaging have had a profound effect on our ability to generate high-resolution measurements of the brain’s structure. One of the major hurdles in processing modern neuroimaging datasets designed to produce large-scale maps of the connections and the organization of the brain lies in the sheer size of these data. For instance, electron microscopic (EM) images of a cubic millimeter of cortex occupies roughly 3 PBon disk, and lower resolution emerging X-ray microtomography (XRM) data can exceed 10 TB for a single mouse brain. When dealing with datasets of this size, the application of even simple algorithms becomes difficult. The size of datasets also exacerbates the considerable challenges for dissemination, reproducibility, and collaboration across laboratories. Addressing these challenges requires a new approach that leverages state-of-the-art computer science technology while remaining conscientious of the underlying bioinformatics. We propose Scalable Analytics for Brain Exploration Research (SABER), a user-friendly and portable framework that automates the retrieval, extraction, and analysis of large-scale imagery data to facilitate neuroscientific analyses. SABER aims to improve the reliability and reproducibility of neuroimagery research by providing a common substrate upon which algorithms may be developed. Leveraging SABER’s containers — a standardized packaging for software — this substrate can then be trivially transferred to other machines by the same researcher or by other teams aiming to reproduce or adapt the prior work, making sharing workflows and extracting knowledge commonplace. Using SABER will ensure that the analysis runs identically, regardless of by whom or where the workflow is executed. Because developing and deploying these analysis solutions for large image volumes are acute barriers to developing consistently reproducible workflows, SABER will further the neuroscientific analysis community by simplifying the workflow-development and workflow-execution steps. To demonstrate this, we plan to distribute two community-vetted, optimized workflows to convert large-scale EM and XRM volumetric imagery into maps of neuronal connectivity. Many neurological diseases are characterized by their impact on the density of cells and vessels, neuron death, connectivity, or other factors that are visible with imaging technologies. SABER will provide a framework for producing reproducible estimates of cell counts, vasculature density, and connectomes, thus enabling increased understanding of the impact of disease on the neuroanatomy of many brains. This work will enable the development of tools that can both be applied to massive data and shared amongst many scientists, which will in turn accelerate progress and neuroscientific discovery. Project Narrative: Our Johns Hopkins University Applied Physics Laboratory team leverages prior neuroscience analysis experience to present SABER: Scalable Analytics for Brain Exploration Research — a portable, easy-to-install framework that enables large-scale neuroanatomical data processing by providing a scaffold upon which highly-reproducible bioinformatics protocols may be built. To support emerging efforts to understand the biological basis of disease, we demonstrate turn-key pipelines to translate multi-terabyte electron microscopy and X-ray microtomography data volumes into maps of neuronal connectivity.",SABER: Scalable Analytics for Brain Exploration Research using X-Ray Microtomography and Electron Microscopy,9414126,R24MH114799,"['Acute', 'Address', 'Algorithms', 'Artificial Arm', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Brain', 'Cell Count', 'Cell Density', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Discovery', 'Data Set', 'Development', 'Disease', 'Educational workshop', 'Electron Microscopy', 'Electrons', 'Ensure', 'Environment', 'Evaluation', 'Grant', 'Human Resources', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Intelligence', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Magnetic Resonance Imaging', 'Maps', 'Measurement', 'Microscopic', 'Modality', 'Modernization', 'Mus', 'Neuroanatomy', 'Neurodegenerative Disorders', 'Neurons', 'Neurosciences', 'Optics', 'Physics', 'Pluto', 'Process', 'Protocols documentation', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resolution', 'Retrieval', 'Roentgen Rays', 'Running', 'Science', 'Scientist', 'Source', 'Standardization', 'Structure', 'System', 'Techniques', 'Technology', 'Tissue imaging', 'Tissues', 'Training', 'Translating', 'Traumatic Brain Injury', 'Universities', 'Work', 'brain research', 'brain tissue', 'computer science', 'computerized data processing', 'connectome', 'data access', 'data archive', 'data management', 'density', 'design', 'experience', 'experimental study', 'high resolution imaging', 'improved', 'innovative neurotechnologies', 'microscopic imaging', 'millimeter', 'nervous system disorder', 'neuroimaging', 'neuron loss', 'novel', 'novel strategies', 'portability', 'relating to nervous system', 'scaffold', 'software development', 'terabyte', 'tool', 'tool development', 'user-friendly', 'virtual']",NIMH,JOHNS HOPKINS UNIVERSITY,R24,2017,395542,0.0014401049620740623
"Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation DESCRIPTION (provided by applicant): Multi-atlas label fusion (MALF) is a powerful new technology that can automatically detect and label anatomical structures in biomedical images. It is arguably the most successful general-purpose automatic image segmentation technique ever developed. Automatic segmentation is in high demand in clinical and research applications of medical imaging, since segmentation forms a crucial step towards extracting quantitative information from imaging data, and since manual and semi-automatic approaches are ill suited for today's increasingly large and complex imaging datasets. Despite a number of papers that demonstrated outstanding performance of MALF methods across a range of biomedical imaging applications, the broader biomedical imaging research community has been slow to adopt this technique. This can be explained by multiple factors, including the technique's high computational demands, lack of a turnkey software implementation, as well as scarcity of validation in clinical imaging datasets and in the presence of extensive pathology. The present application seeks to remove these barriers and to enable a broad range of clinicians and biomedical researchers to take advantage of MALF technology. It builds on our strong track record of innovation in the MALF field, including a novel redundancy-correcting MALF technique that led in segmentation grand challenges in the past two years. Aim 1 seeks to improve the computational performance of MALF by replacing dense deformable image registration, by far the most time consuming component of MALF, with faster and less constrained sparse registration strategies. We hypothesize that this will not only reduce the computational cost of MALF, but will also make it more robust to anatomical variability, in particular enabling its use for tumor and lesion segmentation. Aim 2 proposes algorithmic extensions to MALF that support automatic segmentation of dynamic and multi-modality imaging datasets, which have been largely overlooked in the MALF literature. Aim 3 will develop a turnkey open-source implementation of MALF methodology. Taking advantage of cloud computing technology, this software will allow users with minimal image processing expertise to take full advantage of MALF segmentation on their desktop. Aim 3 will also provide a set of publicly available atlases and the means for users to build new custom atlas sets from their own data. Aim 4 will perform extensive evaluation of the new methods and software in challenging real-world clinical imaging data, including brain and cardiac imaging. As part of this evaluation, we will quantify how well our MALF approach and competing techniques generalize to novel imaging datasets with heterogeneity in acquisition parameters and clinical phenotypes. PUBLIC HEALTH RELEVANCE: This research will make it possible for a wide community of researchers who collect and analyze medical imaging data to take advantage of a new class of computer algorithms that very accurately label and measure anatomical structures and pathological formations in medical images. By offering more accurate image-derived measurements, the project promises to improve the accuracy of diagnosis, reduce the costs of biomedical re- search studies and pharmaceutical trials, and accelerate scientific discovery.",Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation,9350173,R01EB017255,"['Address', 'Adopted', 'Affect', 'Algorithms', 'Anatomy', 'Atlases', 'Biomedical Research', 'Brain', 'Brain imaging', 'Cardiac', 'Clinical Data', 'Clinical Research', 'Cloud Computing', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Dementia', 'Diagnostic', 'Evaluation', 'Gold', 'Heterogeneity', 'High Performance Computing', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'International', 'Intervention', 'Joints', 'Label', 'Lead', 'Learning', 'Lesion', 'Literature', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Medial', 'Medical Imaging', 'Medical Research', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multimodal Imaging', 'Multiple Sclerosis Lesions', 'Myocardium', 'Paper', 'Pathologic', 'Pathology', 'Patient Care', 'Performance', 'Pharmacologic Substance', 'Public Domains', 'Research', 'Research Infrastructure', 'Research Personnel', 'S-nitro-N-acetylpenicillamine', 'Scheme', 'Services', 'Structure', 'Techniques', 'Technology', 'Temporal Lobe', 'Temporal Lobe Epilepsy', 'Time', 'Training', 'Ultrasonography', 'Uncertainty', 'Validation', 'Work', 'aortic valve', 'base', 'bioimaging', 'cardiovascular visualization', 'clinical application', 'clinical imaging', 'clinical phenotype', 'clinical practice', 'cloud based', 'cluster computing', 'cohort', 'cost', 'diagnostic accuracy', 'experience', 'image processing', 'image registration', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'interest', 'multi-atlas segmentation', 'multidisciplinary', 'new technology', 'novel', 'open source', 'outreach', 'public health relevance', 'research study', 'success', 'targeted imaging', 'tool', 'tumor']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2017,597688,0.00706251108463568
"Enabling Shared Analysis and Processing of Large Neurophysiology Data Project Summary / Abstract Understanding brain function is key to improving health care and advancing a number of scientific initiatives. The treatment of degenerative brain diseases such as Alzheimer's, Parkinson’s, and ALS is becoming increasingly important as the current US population ages and life expectancies increase. The costs of Alzheimer's and other dementias is estimated at over $200 billion in 2016 alone, not to mention the human devastation that these diseases incur. Autism, addiction, depression, epilepsy, traumatic brain injury, and pain treatment are just a few more of the critically important health concerns related to brain function. Cognitive science is also at the forefront of research into computational and autonomous systems, with the potential to revolutionize human computer interaction and tackle emerging global challenges. As a result of these and other significant opportunities the BRAIN (Brain Research through Advancing Innovative Neurotechnologies) Presidential initiative was created to improve the future health and competitiveness of the nation, with the fundamental goal of accelerating brain research. Consistent with this goal, the brain research community has developed the Neurodata Without Borders: Neurophysiology (NWB) file format and specification to support large-scale collaboration and research. This open format was created in 2014 and is already making an impact on cellular-based neurophysiology, with organizations such as the Allen Institute for Brain Science generating and sharing datasets such as the Allen Brain Observatory and the Allen Cell Types Database. Although this preliminary work is promising, progress in the research community is slowed by a lack of software tools to readily browse, process, analyse, and visualize NWB data, while promoting replicability. Thus this work aims to to produce such tools in support of the BRAIN initiative and other large-scale brain research programs by supporting and growing the NWB community. The work proposed here addresses three important workflows in cellular-based neurophysiology: o​ ptical physiology to image neurons under stimuli, silicon probe recordings to detect spike events from the surface of the cortex down through deeps​ structures, and​ in vitro slice electrophysiology to record t​ ime-varying stimulus and electrical response from a neuron​. Novel multiscale software tools will be created to enable efficient browsing, processing, analysis, and visualization of NWB-based brain data; linking experimental stimuli to observed responses. Conversion utilities will also be developed to convert existing data into NWB form. As the data is large, complex, and may be distributed across many sites, the software tools will be web-based, enabling researchers to remotely access and process data in a reproducible manner, and to use scalable cloud computing resources. The software will be released under open source licences and will be placed under formal software process to facilitate sharing across the research community. The tools will be conceived and created with the help of Allen scientists, who will also perform final validation using these three workflows. Project Narrative This research is aimed at better understanding brain function. Such knowledge may lead to improved therapies for degenerative brain diseases such as Alzheimer's, Parkinson’s, and ALS; and provide new treatments for autism, addiction, depression, epilepsy, traumatic brain injury, and pain treatment. These studies of brain science may also yield significant insights into computing areas such as robotics and computer vision, thereby providing both healthcare and economic benefits.",Enabling Shared Analysis and Processing of Large Neurophysiology Data,9409114,R44MH115731,"['Address', 'Adopted', 'Adoption', 'Affect', 'Age', 'Alzheimer&apos', 's Disease', 'Area', 'Autistic Disorder', 'Base of the Brain', 'Behavior', 'Brain', 'Brain Diseases', 'Case Study', 'Cloud Computing', 'Cognition', 'Cognitive Science', 'Collaborations', 'Communities', 'Complex', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Analytics', 'Data Set', 'Data Sources', 'Databases', 'Dementia', 'Development', 'Disease', 'Electrophysiology (science)', 'Ensure', 'Epilepsy', 'Event', 'Foundations', 'Future', 'Goals', 'Health', 'Healthcare', 'Human', 'Image', 'Imagery', 'In Vitro', 'Individual', 'Institutes', 'Knowledge', 'Laboratories', 'Lead', 'Licensing', 'Life Expectancy', 'Link', 'Mental Depression', 'Metadata', 'Methods', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Online Systems', 'Optics', 'Pain management', 'Parkinson Disease', 'Periodicity', 'Phase', 'Physiology', 'Pilot Projects', 'Population', 'Process', 'Pythons', 'Reproducibility', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Robotics', 'Science', 'Scientist', 'Services', 'Silicon', 'Site', 'Slice', 'Software Tools', 'Standardization', 'Stimulus', 'Stream', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Traumatic Brain Injury', 'Validation', 'Visual', 'Visualization software', 'Work', 'addiction', 'analytical tool', 'base', 'brain research', 'cell type', 'community based participatory research', 'computer human interaction', 'computing resources', 'cost', 'data exchange', 'data format', 'data modeling', 'data sharing', 'design', 'distributed data', 'experimental study', 'falls', 'file format', 'hackathon', 'health care economics', 'improved', 'innovation', 'innovative neurotechnologies', 'insight', 'nervous system disorder', 'neurophysiology', 'novel', 'open source', 'prevent', 'programs', 'relating to nervous system', 'research and development', 'response', 'success', 'terabyte', 'tool', 'web interface', 'web-enabled']",NIMH,"KITWARE, INC.",R44,2017,747542,-0.01684205067506398
"Pathology Image Informatics Platform for visualization, analysis and management ﻿    DESCRIPTION (provided by applicant): With the advent of whole slide digital scanners, histopathology slides can be digitized into very high-resolution digital images, realizing a new ""big data"" stream that can potentially rival ""omics data"" in size and complexity. Just as with the analysis of high-throughput genetic and expression data, the application of sophisticated image analytic tools and data pipelines can render the often passive data of digital pathology (DP) archives into a powerful source for: (a) rich quantitative insights into cancer biology and (b) companion diagnostic decision support tools for precision medicine. Digital pathology enabled companion diagnostic tests could yield predictions of cancer risk and aggressiveness in a manner similar to molecular diagnostic tests. However, prior to widespread clinical adoption of DP, extensive evaluation of clinical interpretation of DP imaging (DPI) and accompanying decision support tools needs to be undertaken. Wider acceptance of DPI by the cancer community (clinical and research) is hampered by lack of a publicly available, open access image informatics platform for easily viewing, managing, and quantitatively analyzing DPIs. While some commercial platforms exist for viewing and analyzing DPI data, none of these platforms are freely available. Open source image viewing/management platforms that cater to the radiology (e.g. XNAT) and computational biology communities are typically not conducive to handling very large file sizes as encountered with DPI datasets.  This multi-PI U24 proposal seeks to expand on an existing, freely available pathology image viewer (Sedeen Image Viewer) to create a pathology informatics platform (PIIP) for managing, annotating, sharing, and quantitatively analyzing DPI data. Sedeen was designed as a universal platform for DPI (by addressing several proprietary scanner formats and ""big data"" challenges), to provide (1) reliable and useful image annotation tools, and (2) for image registration and analysis of DPI data. Additionally, Sedeen has become an application for cropping large DPIs so that they can be input into programs such as Matlab or ImageJ. Sedeen has been freely available to the public for three years, with over 160 unique users from over 20 countries.  Building on the initial successes of Sedeen and its existing user base, our intent is to massively increase dissemination of DPI and algorithms in the cancer research community and clinical trial efforts, as well as to contribute towards the adoption of a rational and standardized set of DP operational conventions. This unique project will allow end users with different needs and technical backgrounds to seamlessly (a) archive and manage, (b) share, and (c) visualize their DPI data, acquired from different sites, formats, and platforms. The PIIP will provide a unified user interface for third party algorithms (nuclear segmentation, color normalization, biomarker quantification, radiology-pathology fusion) and will allow for algorithmic evaluation upon data arising from a plurality of source sites. By partnering with professional societies, we envision that the PIIP user base will expand to include the oncology, pathology, radiology, and pharmaceutical communities. PUBLIC HEALTH RELEVANCE: This grant will result in the further development of advanced functionality of the already existing digital pathology image informatics platform (PIIP) with an established user-base for cancer research. Such an enhanced platform will provide the much-needed foundation for advancing (a) routine clinical adoption of digital pathology for primary diagnosis and (b) training and validation of companion diagnostic decision support systems based off histopathology. Thus, the project is aligned with the NCI's goal to foster innovative research strategies and their applications as a basis for ultimately protecting and improving human health.","Pathology Image Informatics Platform for visualization, analysis and management",9341177,U24CA199374,"['Address', 'Adoption', 'Advanced Development', 'Algorithmic Analysis', 'Algorithms', 'American', 'Archives', 'Big Data', 'Biological Markers', 'Cancer Biology', 'Cancer Prognosis', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Color', 'Communities', 'Community Clinical Oncology Program', 'Community Trial', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computer software', 'Country', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Storage and Retrieval', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Evaluation', 'Felis catus', 'Fostering', 'Foundations', 'Genetic', 'Goals', 'Grant', 'Health', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Informatics', 'Institution', 'International', 'Language', 'Length', 'Letters', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical Imaging', 'Medical Students', 'Molecular Diagnostic Testing', 'Morphology', 'Nuclear', 'Ontology', 'Optics', 'Pathologist', 'Pathology', 'Pharmacologic Substance', 'Professional Organizations', 'Protocols documentation', 'Pythons', 'Radiology Specialty', 'Research', 'Resolution', 'Scientist', 'Site', 'Slide', 'Societies', 'Source', 'Standardization', 'Stream', 'Training', 'Training and Education', 'Validation', 'analytical tool', 'annotation  system', 'anticancer research', 'base', 'biomarker discovery', 'biomedical scientist', 'cancer diagnosis', 'cancer imaging', 'cancer risk', 'clinical research site', 'companion diagnostics', 'computer human interaction', 'data exchange', 'data integration', 'data sharing', 'design', 'digital', 'digital imaging', 'drug discovery', 'high throughput analysis', 'image archival system', 'image registration', 'imaging informatics', 'improved', 'in vivo imaging', 'innovation', 'insight', 'interest', 'malignant breast neoplasm', 'oncology', 'open source', 'photonics', 'precision medicine', 'programs', 'public health relevance', 'quantitative imaging', 'radiological imaging', 'repository', 'research clinical testing', 'success', 'support tools', 'symposium', 'tool', 'tumor', 'user friendly software', 'validation studies']",NCI,CASE WESTERN RESERVE UNIVERSITY,U24,2017,579791,-0.010557693554260415
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9146381,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,457075,-0.014922701636608905
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims. PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,9243496,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Health', 'Informatics', 'Information Retrieval', 'Investments', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'crowdsourcing', 'data to knowledge', 'data wrangling', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'search engine', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2016,150865,-0.014922701636608905
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice. Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9145775,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'cost efficient', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'learning strategy', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,NORTHEASTERN UNIVERSITY,R01,2016,293874,-0.004164776399974036
"Summer Institute for Statistics of Big Data DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support. PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.",Summer Institute for Statistics of Big Data,9063061,R25EB020380,"['Academia', 'Applied Skills', 'Area', 'Big Data', 'Biology', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'data visualization', 'graduate student', 'human disease', 'improved', 'instructor', 'learning materials', 'lectures', 'massive open online courses', 'member', 'online course', 'open source', 'programs', 'skills', 'statistics', 'summer institute', 'teacher', 'teaching assistant', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2016,159605,-0.021132182023921198
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,9056632,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,73173,-0.0019104992430428723
"The Center for Predictive Computational Phenotyping-1 Overall DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application. PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.",The Center for Predictive Computational Phenotyping-1 Overall,9270103,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Science', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'education research', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2016,43143,-0.0019104992430428723
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community. PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.",The Big DIPA: Data Image Processing and Analysis,9150564,R25EB022366,"['Accounting', 'Address', 'Architecture', 'Area', 'Big Data', 'Big Data to Knowledge', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Cell physiology', 'Communities', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Modality', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Scientist', 'Senior Scientist', 'Signal Transduction', 'Site', 'Skills Development', 'Software Tools', 'Source', 'Staging', 'Stream', 'Systems Biology', 'TNFRSF5 gene', 'Time', 'Training', 'United States National Institutes of Health', 'Work', 'bioimaging', 'biological systems', 'biomedical scientist', 'computer science', 'computerized tools', 'cost', 'course implementation', 'data acquisition', 'data format', 'demographics', 'design', 'education research', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'learning materials', 'lecture notes', 'lectures', 'meetings', 'microscopic imaging', 'nervous system disorder', 'open source', 'outreach', 'programs', 'repository', 'skill acquisition', 'skills', 'statistics', 'theories', 'tissue processing', 'training opportunity']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2016,161997,-0.039858355682659816
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",9108710,U54EB020403,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Big Data to Knowledge', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Science', 'Data Set', 'Diagnosis', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'cluster merger', 'computer science', 'connectome', 'diagnostic biomarker', 'innovation', 'multidisciplinary', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'web portal', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2016,2369463,0.00191475177066529
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"". PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,9049511,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Health', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'crowdsourcing', 'design', 'innovation', 'interest', 'knowledge translation', 'learning progression', 'learning strategy', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2016,428512,0.016560937194873674
"An Intelligent Concept Agent for Assisting with the Application of Metadata PROJECT ABSTRACT Biomedical investigators are generating increasing amounts of complex and diverse data. This data varies tremendously, from genome sequences through phenotypic measurements and imaging data. If researchers and data scientists can tap into this data effectively, then we can gain insights into disease mechanisms and how to tackle them. However, the main stumbling block is that it is increasingly hard to find and integrate the relevant datasets due to the lack of sufficient metadata. A researcher studying Crohn's disease may miss a crucial dataset on how certain microbial communities affect gut histology due to the lack of descriptive tags on the data. Currently, applying metadata is difficult, time-consuming and error prone due to the vast sea of confusing and overlapping standards for each datatype. Often specialized `data wranglers' are employed to apply metadata, but even these experts are hindered by lack of good tools. Here we propose to develop an intelligent agent that researchers and data wranglers can use to assist them apply metadata. The agent is based around a personalized dashboard of metadata elements that can be collected from multiple specialized portals, as well as sites such as Wikipedia. These elements can be coupled with classifiers that can be used to self-identify datasets to which they may be relevant, making the selection of appropriate vocabularies easier for researchers. We will deploy the system for a number of targeted use cases, including annotation of the National Center for Biomedical Information Bio-Samples repository, and annotation of images within the Figshare repository. Project Narrative Biomedical data is being generated at an increasing rate, and it is becoming increasingly difficult for researchers to be able to locate and effectively operate over this data, which has negative impacts on the rate of new discoveries. One solution is to attach metadata (data about data) onto all information generated in a research project, but application of metadata is currently difficult and time consuming due to the diverse range of standards on offer, typically requiring the expertise of trained data wranglers. Here we propose to develop an intelligent concept assistant that will allow researchers to generate and share sets of metadata elements relevant to their project, and will use machine learning techniques to automatically apply this to data.",An Intelligent Concept Agent for Assisting with the Application of Metadata,9161233,U01HG009453,"['Address', 'Affect', 'Area', 'Categories', 'Classification', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Coupled', 'Crohn&apos', 's disease', 'Data', 'Data Science', 'Data Set', 'Databases', 'Deposition', 'Disease', 'Distributed Systems', 'Ecosystem', 'Elements', 'Environment', 'Fostering', 'Frustration', 'Histology', 'Human Microbiome', 'Image', 'Intelligence', 'Knowledge', 'Learning', 'Logic', 'Machine Learning', 'Maintenance', 'Manuals', 'Measurement', 'Metadata', 'Ontology', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Sampling', 'Sea', 'Site', 'Source', 'Structure', 'Suggestion', 'System', 'Techniques', 'Testing', 'Text', 'Time', 'Training', 'Vision', 'Vocabulary', 'Work', 'base', 'dashboard', 'empowered', 'genome sequencing', 'improved', 'insight', 'meetings', 'microbial community', 'peer', 'repository', 'social', 'tool', 'transcriptomics', 'web portal']",NHGRI,UNIVERSITY OF CALIF-LAWRENC BERKELEY LAB,U01,2016,575532,0.014032196616017779
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,9113614,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2016,601709,-0.01929103910734604
"Metadata applications on informed content to facilitate biorepository data regulation and sharing ABSTRACT Biorepositories are critical to enabling modern molecular-based research that will drive the development of a new generation of targeted diagnostics and therapies as well as personalized medicine to improve clinical outcomes for patients. The use of data and biospecimen resources collected during research are constrained by the informed consent that research participants give to research teams, research protocol documents, and the constraints imposed on the research by the IRB itself. Currently there is a lack of a common model of consent that limits how easily data (and research specimens) from multiple research projects, or multiple institutions can be combined for large-scale retrospective studies. Manually examining and reconciling potentially millions of informed consent forms from different biobanks becomes an expensive and possibly irreconcilable problem. The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. Additionally, much of the regulatory processes involved in research are still based on paper-based workflows. We posit that by developing suitable machine-based metadata representation of regulatory processes focusing on informed consents and the associated documents would enhance the ability of regulatory bodies such as Institutional Review Boards to 1) review proposals in a more streamlined fashion, 2) have the potential to provide a more comprehensive understanding of risk along multiple axes, and 3) provide a formal and computable basis for data sharing and information release policies. More specifically we will focus on three specific aims: 1) Develop standard-conforming metadata representations of informed consent; 2) Develop NLP-based automatic annotation tool for informed consent documents; and 3) Evaluate the metadata-ontology-based approach for semantically representing the domain and demonstrate its capacity for answering competency questions. Our proposed approach is novel for the following reasons: 1) it provides the first metadata ontology, to the best of our knowledge, to represent the informed consent space while considering the US common rules; 2) it combines natural language processing technologies with ontology-based approaches for semantic annotation as well as ontology enrichment; and 3) it engages stakeholders in the ontology development and evaluation process and uses competency questions to verify the coverage of the ontology. PROJECT NARRATIVE The application of suitable metadata in support of the complex set of regulatory, legal, privacy and security requirement processes and information flows involved in regulated research is a field in an early phases of development. The complicated legal and technical requirements involved in the processes challenge our ability to effectively build information systems that support sharing of research data, specimens and other research artifacts at scale. In response to RFA-CA-15-017, this proposed project will develop suitable machine-based metadata representation and automatic annotation of regulatory processes focusing on informed consents and the associated documents. The proposed approach will presumably enhance the ability of regulatory bodies such as Institutional Review Boards to (a) review proposals in a more streamlined fashion, (b) have the potential to provide a more comprehensive understanding of risk along multiple axes, and (c) provide a formal and computable basis for data sharing and information release policies.",Metadata applications on informed content to facilitate biorepository data regulation and sharing,9161167,U01HG009454,"['Address', 'Adherence', 'Automated Annotation', 'Charge', 'Classification', 'Clinical', 'Communities', 'Complex', 'Consent', 'Consent Forms', 'Data', 'Data Element', 'Decision Making', 'Derivation procedure', 'Development', 'Diagnostic', 'Elements', 'Ensure', 'Ethics', 'Evaluation', 'Future', 'Generations', 'Guidelines', 'Heterogeneity', 'Human', 'Individual', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Knowledge', 'Legal', 'Maps', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Morphologic artifacts', 'Natural Language Processing', 'Ontology', 'Paper', 'Participant', 'Patient-Focused Outcomes', 'Phase', 'Policies', 'Privacy', 'Process', 'Protocols documentation', 'Regulation', 'Research', 'Research Project Grants', 'Resources', 'Retrospective Studies', 'Rights', 'Risk', 'Science', 'Security', 'Semantics', 'Specimen', 'Support System', 'System', 'Techniques', 'Technology', 'Terminology', 'Text', 'Trust', 'Work', 'base', 'biobank', 'common rule', 'data integration', 'data modeling', 'data sharing', 'human subject', 'improved', 'information processing', 'knowledge base', 'novel', 'personalized medicine', 'repository', 'response', 'stem', 'tool']",NHGRI,UNIVERSITY OF TEXAS HLTH SCI CTR HOUSTON,U01,2016,471848,-0.02403713679087922
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9123422,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'Natural Products', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'novel therapeutics', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2016,419732,0.01295834131045745
"An Open Source Precision Medicine Platform for Cloud Operating Systems ﻿    DESCRIPTION (provided by applicant):  Rapid improvements in DNA sequencing and synthesis have the potential to usher in a new era of precision medicine. To realize this vision, however, we must re-imagine the computational and storage infrastructure used to manage and extract actionable results from the massive data sets made possible by widely available advances in DNA sequencing and synthetic biology. In conjunction with the Global Alliance for Genomics and Health (GA4GH), we propose to build the Arvados platform so that a new ecosystem of clinical decision support applications will be able to navigate petabytes of global biomedical data and search millions of genomes in real-time (seconds). Our team has a proven track record of commercial success and high impact scientific research. Commercialization of this free and open-source software (FOSS) platform, which will be greatly accelerated by this grant, will permit organizations to seamlessly span on-premise & hosted cloud- operating systems and vastly simplify data-management & computation, all while facilitating compliance with institutional policies and regulatory requirements.         PUBLIC HEALTH RELEVANCE:  The delivery of healthcare based on molecular data specific to an individual patient (i.e. precision medicine) will require the creation of a new ecosystem of Clinical Decision Support (CDS) applications. This work will provide a platform that will make the development of such applications faster, easier, and less expensive.        ",An Open Source Precision Medicine Platform for Cloud Operating Systems,9140741,R44GM109737,"['Address', 'Adopted', 'Big Data', 'Bioinformatics', 'Businesses', 'Capital', 'Clinical', 'Clinical Decision Support Systems', 'Collaborations', 'Communities', 'Computer software', 'Contractor', 'DNA Sequence', 'DNA biosynthesis', 'Data', 'Data Set', 'Databases', 'Development', 'Distributed Systems', 'Ecosystem', 'Feedback', 'Fostering', 'Funding', 'Galaxy', 'Genome', 'Genomics', 'Grant', 'Health', 'Healthcare', 'Human', 'Industry', 'Information Technology', 'Institutional Policy', 'International', 'Internet', 'Language', 'Length', 'Letters', 'Machine Learning', 'Maintenance', 'Manuscripts', 'Measures', 'Medicine', 'Memory', 'Molecular', 'Operating System', 'Phase', 'Policies', 'Production', 'Publications', 'Reproducibility', 'Research', 'Research Infrastructure', 'Resources', 'Secure', 'Services', 'Small Business Innovation Research Grant', 'Source Code', 'System', 'Technology', 'Time', 'Training Support', 'Vision', 'Work', 'base', 'big biomedical data', 'cloud platform', 'commercialization', 'data management', 'genome sequencing', 'genomic data', 'health care delivery', 'individual patient', 'meetings', 'new technology', 'next generation sequencing', 'open source', 'operation', 'petabyte', 'portability', 'precision medicine', 'public health relevance', 'repository', 'screening', 'success', 'symposium', 'synthetic biology', 'terabyte', 'web services', 'whole genome']",NIGMS,"CUROVERSE, INC.",R44,2016,985339,0.00405956539687685
"Large-Scale Semiparametric Graphical Models with Applications to Neuroscience DESCRIPTION: The objective of this proposal is to develop and theoretically evaluate a unified set of statistical, computational, and software tools to address data mining and discovery science challenges in the analysis of existing vast amounts of publicly available neuroimaging data. In particular, we propose to develop scalable and robust semiparametric solutions for high-throughput estimation of resting-state brain connectivity networks, both at the individual and population levels, with the flexibility of incorporating covariate information.  The work will contribute meaningfully to the theory and methods for large-scale semiparametric graphical models and will apply these methods to the largest collections of resting-state fMRI data available. The proposed methods and theory include key directions of research for brain network estimation and mining. First, we pro- pose novel methods for subject-specific network estimation, such as would be needed for biomarker development in functional brain imaging. Secondly, we define and propose to evaluate and implement methods for studying population-level graphs, which study collections of graphs. Thirdly, we propose the use of estimated graphs in predictive modeling. Finally, all of these methods will have complementary software and web services development. Most notably, the idea of population graphs allows for the creation of functional brain network atlases.  In summary, the work of this proposal will result in a unified framework for the analysis of modern neuroimaging data via graphical models. Our methods will further be agnostic to intricacies of the technology, thus making it portable across settings and applicable outside of the field of functional brain imaging. The methods will be carefully evaluated via theory, simulation and data-based application evidence. PUBLIC HEALTH RELEVANCE: Modern neuroimaging data are often Big, Complex, Noisy and Dependent. We propose a systematic attempt on methodological development for the largely unexplored but practically important problem of network estimation and mining based on neuroimaging data. Our proposed work represents a significant step forward over the current methodology and has the potential to be applied to analyze a wide range of scientific problems beyond brain imaging data analysis.",Large-Scale Semiparametric Graphical Models with Applications to Neuroscience,8998688,R01MH102339,"['Address', 'Algorithms', 'Atlases', 'Attention deficit hyperactivity disorder', 'Big Data', 'Brain', 'Brain Diseases', 'Brain imaging', 'Characteristics', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Discovery', 'Data Set', 'Databases', 'Dependence', 'Development', 'Disease', 'Documentation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Graph', 'Health', 'Heterogeneity', 'Image', 'Individual', 'Internet', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modality', 'Modeling', 'Neurosciences', 'Pattern', 'Population', 'Process', 'Rest', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Software Tools', 'Statistical Methods', 'Tail', 'Technology', 'Work', 'abstracting', 'base', 'biomarker development', 'brain research', 'cloud based', 'data mining', 'flexibility', 'interest', 'neuroimaging', 'novel', 'predictive modeling', 'psychologic', 'semiparametric', 'simulation', 'study population', 'theories', 'user friendly software', 'web services']",NIMH,PRINCETON UNIVERSITY,R01,2016,368829,0.004115823965635059
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities. PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.","COINSTAC: decentralized, scalable analysis of loosely coupled data",9100683,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'connectome', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging genetics', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'open data', 'peer', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2016,680020,0.022456390414051043
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,9115248,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'study population', 'terabyte', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2016,347156,-0.004110403771503552
"Semantic Data Lake for Biomedical Research Capitalizing on the transformative opportunities afforded by the extremely large and ever-growing volume, velocity, and variety of biomedical data being continuously produced is a major challenge. The development and increasingly widespread adoption of several new technologies, including next generation genetic sequencing, electronic health records and clinical trials systems, and research data warehouses means that we are in the midst of a veritable explosion in data production. This in turn results in the migration of the bottleneck in scientific productivity into data management and interpretation: tools are urgently needed to assist cancer researchers in the assembly, integration, transformation, and analysis of these Big Data sets. In this project, we propose to develop the Semantic Data Lake for Biomedical Research (SDL-BR) system, a cluster-computing software environment that enables rapid data ingestion, multifaceted data modeling, logical and semantic querying and data transformation, and intelligent resource discovery. SDL-BR is based on the idea of a data lake, a distributed store that does not make any assumptions about the structure of incoming data, and that delays modeling decisions until data is to be used. This project adds to the data lake paradigm methods for semantic data modeling, integration, and querying, and for resource discovery based on learned relationships between users and data resources. The SDL-BR System is a distributed computing software solution that enables research institutions to manage, integrate, and make available large institutional data sets to researchers, and that permits users to generate data models specific to particular applications. It uses state of the art cluster computing, Semantic Web, and machine learning technologies to provide for rapid data ingestion, semantic modeling and querying, and search and discovery of data resources through a sophisticated, Web-based user interface.",Semantic Data Lake for Biomedical Research,9200905,R44CA206782,"['Accelerometer', 'Acute', 'Address', 'Adoption', 'Area', 'Big Data', 'Biomedical Computing', 'Biomedical Research', 'Cataloging', 'Catalogs', 'Chronic Myeloid Leukemia', 'Clinical', 'Clinical Trials', 'Collection', 'Colorectal Cancer', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Discovery', 'Data Quality', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Decision Modeling', 'Demographic Factors', 'Development', 'Electronic Health Record', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genetic', 'Genetic Markers', 'High-Throughput Nucleotide Sequencing', 'Immigration', 'Individual', 'Informatics', 'Ingestion', 'Institution', 'Knowledge', 'Knowledge Extraction', 'Learning', 'Legal', 'Legal patent', 'Machine Learning', 'Malignant Neoplasms', 'Methods', 'Modeling', 'Non-Small-Cell Lung Carcinoma', 'Online Systems', 'Ontology', 'Phase', 'Policies', 'Precision therapeutics', 'Procedures', 'Process', 'Production', 'Productivity', 'Recommendation', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Retrieval', 'Risk', 'Secure', 'Security', 'Semantics', 'Services', 'Source', 'Specific qualifier value', 'Staging', 'Structure', 'System', 'Technology', 'Testing', 'Vocabulary', 'Work', 'base', 'cancer therapy', 'cluster computing', 'computer based Semantic Analysis', 'cost effective', 'data access', 'data exchange', 'data integration', 'data management', 'data modeling', 'design', 'disease heterogeneity', 'experience', 'genetic information', 'handheld mobile device', 'indexing', 'individualized medicine', 'melanoma', 'natural language', 'new technology', 'next generation', 'novel', 'precision medicine', 'prototype', 'success', 'systems research', 'targeted treatment', 'technology development', 'time use', 'tool']",NCI,"INFOTECH SOFT, INC.",R44,2016,221175,0.020345519130626096
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,9103879,U54EB020405,"['Accelerometer', 'Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Big Data to Knowledge', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Science', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Mental Depression', 'Methods', 'Mission', 'Modality', 'Modeling', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Walking', 'Work', 'base', 'big biomedical data', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'experience', 'flexibility', 'health data', 'improved', 'improved outcome', 'industry partner', 'insight', 'massive open online courses', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'reduce symptoms', 'role model', 'sensor', 'social', 'social model', 'tool', 'visiting scholar']",NIBIB,STANFORD UNIVERSITY,U54,2016,401742,0.007216232451062741
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8997510,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2016,510376,0.022610822738410575
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8987580,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'Thinking', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'light weight', 'next generation', 'novel', 'online community', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2016,526540,0.028248677935962842
"Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation DESCRIPTION (provided by applicant): Multi-atlas label fusion (MALF) is a powerful new technology that can automatically detect and label anatomical structures in biomedical images. It is arguably the most successful general-purpose automatic image segmentation technique ever developed. Automatic segmentation is in high demand in clinical and research applications of medical imaging, since segmentation forms a crucial step towards extracting quantitative information from imaging data, and since manual and semi-automatic approaches are ill suited for today's increasingly large and complex imaging datasets. Despite a number of papers that demonstrated outstanding performance of MALF methods across a range of biomedical imaging applications, the broader biomedical imaging research community has been slow to adopt this technique. This can be explained by multiple factors, including the technique's high computational demands, lack of a turnkey software implementation, as well as scarcity of validation in clinical imaging datasets and in the presence of extensive pathology. The present application seeks to remove these barriers and to enable a broad range of clinicians and biomedical researchers to take advantage of MALF technology. It builds on our strong track record of innovation in the MALF field, including a novel redundancy-correcting MALF technique that led in segmentation grand challenges in the past two years. Aim 1 seeks to improve the computational performance of MALF by replacing dense deformable image registration, by far the most time consuming component of MALF, with faster and less constrained sparse registration strategies. We hypothesize that this will not only reduce the computational cost of MALF, but will also make it more robust to anatomical variability, in particular enabling its use for tumor and lesion segmentation. Aim 2 proposes algorithmic extensions to MALF that support automatic segmentation of dynamic and multi-modality imaging datasets, which have been largely overlooked in the MALF literature. Aim 3 will develop a turnkey open-source implementation of MALF methodology. Taking advantage of cloud computing technology, this software will allow users with minimal image processing expertise to take full advantage of MALF segmentation on their desktop. Aim 3 will also provide a set of publicly available atlases and the means for users to build new custom atlas sets from their own data. Aim 4 will perform extensive evaluation of the new methods and software in challenging real-world clinical imaging data, including brain and cardiac imaging. As part of this evaluation, we will quantify how well our MALF approach and competing techniques generalize to novel imaging datasets with heterogeneity in acquisition parameters and clinical phenotypes. PUBLIC HEALTH RELEVANCE: This research will make it possible for a wide community of researchers who collect and analyze medical imaging data to take advantage of a new class of computer algorithms that very accurately label and measure anatomical structures and pathological formations in medical images. By offering more accurate image-derived measurements, the project promises to improve the accuracy of diagnosis, reduce the costs of biomedical re- search studies and pharmaceutical trials, and accelerate scientific discovery.",Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation,9119513,R01EB017255,"['Address', 'Adopted', 'Affect', 'Algorithms', 'Atlases', 'Biomedical Research', 'Brain', 'Brain imaging', 'Cardiac', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cloud Computing', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Dementia', 'Diagnostic', 'Evaluation', 'Gold', 'Health', 'Heterogeneity', 'High Performance Computing', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'International', 'Intervention Studies', 'Joints', 'Label', 'Lead', 'Learning', 'Lesion', 'Literature', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Medial', 'Medical Imaging', 'Medical Research', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multiple Sclerosis Lesions', 'Myocardium', 'Paper', 'Pathology', 'Patient Care', 'Performance', 'Pharmacologic Substance', 'Public Domains', 'Research', 'Research Infrastructure', 'Research Personnel', 'S-nitro-N-acetylpenicillamine', 'Scheme', 'Services', 'Structure', 'Techniques', 'Technology', 'Temporal Lobe', 'Temporal Lobe Epilepsy', 'Time', 'Training', 'Ultrasonography', 'Uncertainty', 'Validation', 'Work', 'aortic valve', 'base', 'bioimaging', 'cardiovascular visualization', 'clinical application', 'clinical phenotype', 'clinical practice', 'cloud based', 'cohort', 'cost', 'diagnostic accuracy', 'experience', 'image processing', 'image registration', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'interest', 'multi-atlas segmentation', 'new technology', 'novel', 'open source', 'outreach', 'research study', 'success', 'targeted imaging', 'tool', 'tumor']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2016,597688,0.00706251108463568
"Pathology Image Informatics Platform for visualization, analysis and management ﻿    DESCRIPTION (provided by applicant): With the advent of whole slide digital scanners, histopathology slides can be digitized into very high-resolution digital images, realizing a new ""big data"" stream that can potentially rival ""omics data"" in size and complexity. Just as with the analysis of high-throughput genetic and expression data, the application of sophisticated image analytic tools and data pipelines can render the often passive data of digital pathology (DP) archives into a powerful source for: (a) rich quantitative insights into cancer biology and (b) companion diagnostic decision support tools for precision medicine. Digital pathology enabled companion diagnostic tests could yield predictions of cancer risk and aggressiveness in a manner similar to molecular diagnostic tests. However, prior to widespread clinical adoption of DP, extensive evaluation of clinical interpretation of DP imaging (DPI) and accompanying decision support tools needs to be undertaken. Wider acceptance of DPI by the cancer community (clinical and research) is hampered by lack of a publicly available, open access image informatics platform for easily viewing, managing, and quantitatively analyzing DPIs. While some commercial platforms exist for viewing and analyzing DPI data, none of these platforms are freely available. Open source image viewing/management platforms that cater to the radiology (e.g. XNAT) and computational biology communities are typically not conducive to handling very large file sizes as encountered with DPI datasets.  This multi-PI U24 proposal seeks to expand on an existing, freely available pathology image viewer (Sedeen Image Viewer) to create a pathology informatics platform (PIIP) for managing, annotating, sharing, and quantitatively analyzing DPI data. Sedeen was designed as a universal platform for DPI (by addressing several proprietary scanner formats and ""big data"" challenges), to provide (1) reliable and useful image annotation tools, and (2) for image registration and analysis of DPI data. Additionally, Sedeen has become an application for cropping large DPIs so that they can be input into programs such as Matlab or ImageJ. Sedeen has been freely available to the public for three years, with over 160 unique users from over 20 countries.  Building on the initial successes of Sedeen and its existing user base, our intent is to massively increase dissemination of DPI and algorithms in the cancer research community and clinical trial efforts, as well as to contribute towards the adoption of a rational and standardized set of DP operational conventions. This unique project will allow end users with different needs and technical backgrounds to seamlessly (a) archive and manage, (b) share, and (c) visualize their DPI data, acquired from different sites, formats, and platforms. The PIIP will provide a unified user interface for third party algorithms (nuclear segmentation, color normalization, biomarker quantification, radiology-pathology fusion) and will allow for algorithmic evaluation upon data arising from a plurality of source sites. By partnering with professional societies, we envision that the PIIP user base will expand to include the oncology, pathology, radiology, and pharmaceutical communities. PUBLIC HEALTH RELEVANCE: This grant will result in the further development of advanced functionality of the already existing digital pathology image informatics platform (PIIP) with an established user-base for cancer research. Such an enhanced platform will provide the much-needed foundation for advancing (a) routine clinical adoption of digital pathology for primary diagnosis and (b) training and validation of companion diagnostic decision support systems based off histopathology. Thus, the project is aligned with the NCI's goal to foster innovative research strategies and their applications as a basis for ultimately protecting and improving human health.","Pathology Image Informatics Platform for visualization, analysis and management",9145647,U24CA199374,"['Accounting', 'Address', 'Adoption', 'Advanced Development', 'Algorithms', 'American', 'Archives', 'Big Data', 'Biological Markers', 'Cancer Biology', 'Cancer Prognosis', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Color', 'Communities', 'Community Clinical Oncology Program', 'Community Trial', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computer software', 'Country', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Storage and Retrieval', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Evaluation', 'Fostering', 'Foundations', 'Genetic', 'Goals', 'Grant', 'Health', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Informatics', 'Institution', 'International', 'Language', 'Length', 'Letters', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical Imaging', 'Medical Students', 'Molecular Diagnostic Testing', 'Morphology', 'Nuclear', 'Ontology', 'Optics', 'Pathologist', 'Pathology', 'Pharmacologic Substance', 'Professional Organizations', 'Protocols documentation', 'Pythons', 'Radiology Specialty', 'Research', 'Resolution', 'Scientist', 'Site', 'Slide', 'Societies', 'Source', 'Stream', 'Training', 'Training and Education', 'Validation', 'anticancer research', 'base', 'biomarker discovery', 'biomedical scientist', 'cancer diagnosis', 'cancer imaging', 'cancer risk', 'clinical research site', 'companion diagnostics', 'computer human interaction', 'data exchange', 'data integration', 'data sharing', 'design', 'digital', 'digital imaging', 'drug discovery', 'high throughput analysis', 'image archival system', 'image registration', 'imaging informatics', 'improved', 'in vivo imaging', 'innovation', 'insight', 'interest', 'malignant breast neoplasm', 'oncology', 'open source', 'photonics', 'precision medicine', 'programs', 'quantitative imaging', 'repository', 'research clinical testing', 'success', 'support tools', 'symposium', 'tool', 'tumor', 'user friendly software', 'validation studies']",NCI,CASE WESTERN RESERVE UNIVERSITY,U24,2016,605366,-0.010557693554260415
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8840267,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2015,490160,-0.009298802953753202
"MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine ﻿    DESCRIPTION (provided by applicant): The velocity, variety, volume and veracity of data from relevant information sources make it extremely challenging for oncologists to collect and review pertinent data that can support routine personalized treatment for their patients. There is an urgent need to develop data wrangling approaches including Natural Language Processing and information retrieval methods to extract and curate personalized-therapy related publications and clinical trials. Once curated, the structured data can be used by biomedical researchers to generate novel scientific hypotheses, design new studies, obtain a better understanding of biological mechanisms of disease, perform meta-analyses, and create clinical decision support systems. There is an urgent need to develop improved search interfaces specific to the field of personalized therapy, including ways to display, rank, and save results by end users. While several database and web-based keyword search engine algorithms exist, there is a lack of tools that meet the unique challenges of personalized medicine. There is also an urgent need to develop software that allows for verification and validation of information extracted and ranked through computational methods using subject matter expertise to improve the gold standard corpus that can be used for biomedical research into personalized therapies.  To address these issues, we will build an innovative software stack (MACE2K) to adapt and extend widely tested Biocreative natural language processing (NLP) tools to automatically retrieve and pre-process targeted therapy information from clinicaltrials.gov, PubMed abstracts as well as open access articles, and conference proceedings. We will build an entity extraction cartridge to accurately parse gene mutations, translocations, gene expression, protein expression, and protein phosphorylation. A marker disambiguation cartridge will be built to assess for trial inclusion or exclusion criteria and to determine marker-related primary endpoints. We will include a ranking cartridge that uses the disambiguated information on markers, drugs and trials to provide a rigorous scoring of trials and studies according to their relevance for personalized medicine. A novel gamification cartridge will be built to allow subject matter experts to verify and validate the information corpus. Our research leverages National Cancer Institute's investments in several programs (many of which we are involved in) including the NCI drug dictionary, National Cancer Informatics Program (NCIP), I-SPY trials, and Center for cancer systems biology (CCSB) to efficiently accomplish our aims.         PUBLIC HEALTH RELEVANCE: This project will develop new computational methods and software to retrieve targeted molecular and drug therapy information from multiple sources of big data including: clinicaltrials.gov, PubMed abstracts, open access articles, and conference proceedings. The software can be used by biomedical researchers to generate new hypotheses for research on personalized cancer treatment decisions based on enormous volumes of public data already in existence. A novel gamification component will be built to allow subject matter experts to verify and validate the information corpus to enhance accuracy of the software.            ",MACE2K - Molecular And Clinical Extraction: A Natural Language Processing Tool for Personalized Medicine,8874546,U01HG008390,"['Address', 'Algorithms', 'Big Data', 'Biological', 'Biomedical Research', 'Cancer Center', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Trials', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Aggregation', 'Databases', 'Dictionary', 'Disease', 'Exclusion Criteria', 'Gene Expression', 'Gene Mutation', 'Genome', 'Goals', 'Gold', 'Informatics', 'Information Retrieval', 'Investments', 'Knowledge', 'Letters', 'Literature', 'Malignant Neoplasms', 'Maps', 'Meta-Analysis', 'Methods', 'Molecular', 'Molecular Profiling', 'Molecular Target', 'Mutation', 'National Cancer Institute', 'Natural Language Processing', 'Oncologist', 'Online Systems', 'Outcome', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Pharmacotherapy', 'Phosphorylation', 'Process', 'PubMed', 'Publications', 'Recording of previous events', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Software Validation', 'Source', 'Structure', 'System', 'Systems Biology', 'Testing', 'Therapeutic', 'Time', 'United States National Institutes of Health', 'abstracting', 'base', 'design', 'improved', 'inclusion criteria', 'innovation', 'interest', 'knowledge base', 'meetings', 'novel', 'novel strategies', 'personalized cancer care', 'personalized cancer therapy', 'personalized medicine', 'programs', 'protein expression', 'public health relevance', 'software development', 'symposium', 'targeted treatment', 'tool', 'user friendly software', 'verification and validation']",NHGRI,GEORGETOWN UNIVERSITY,U01,2015,478724,-0.014922701636608905
"Semi-Automating Data Extraction for Systematic Reviews ﻿    DESCRIPTION (provided by applicant): Evidence-based medicine (EBM) looks to inform patient care with the totality of available relevant evidence. Systematic reviews are the cornerstone of EBM and are critical to modern healthcare, informing everything from national health policy to bedside decision-making. But conducting systematic reviews is extremely laborious (and hence expensive): producing a single review requires thousands of person-hours. Moreover, the exponential expansion of the biomedical literature base has imposed an unprecedented burden on reviewers, thus multiplying costs. Researchers can no longer keep up with the primary literature, and this hinders the practice of evidence-based care.      The long term aim of this work is to develop computational tools and methods that optimize the practice of EBM. The proposed work thus builds upon our previous successful efforts developing computational approaches that reduce the workload in EBM. More speciﬁcally, we aim to develop tools that semi-automate the laborious task of data extraction - identifying and extracting the information of interest (e.g., trial sample size, interventions and outcomes) from the free-texts of biomedical articles - via novel machine learning methods. Semi-automating this task will drastically reduce reviewer workload, thus enabling the practice of EBM in an age of information overload.      Previous efforts to automate data extraction from articles describing clinical trials have shown promise, but lack the accuracy and scope necessary for real-world use. These approaches have been impeded by the absence of a large corpus of annotated clinical trials, and by the difﬁculty of constructing models to automatically extract all of the variables necessary for synthesis. We describe methodological innovations to overcome these hurdles. First, to train our machine learning models we propose leveraging large existing databases that contain structured information about clinical trials, in lieu of the usual approach of collecting expensive manual annotations. Practically, this means we will be able to exploit a very large `pseudo-annotated' dataset that is an order of magnitude bigger than what has been used in previous efforts, thus substantially improving model performance. Our extensive preliminary work demonstrates the promise and feasibility of this approach. Second, we propose novel machine learning models appropriate for the tasks of article categorization and data extraction for EBM. These models will speciﬁcally be designed to perform extraction of multiple, correlated data elements of interest while simultaneously classifying articles into clinically salient categories useful for EBM.      We will rigorously evaluate the developed methods to assess their practical utility, speciﬁcally y comparing automated extraction accuracy to that achieved by trained systematic reviewers. And to make these methods useful to end-users (systematic reviewers), we will develop and evaluate open-source software and tools, including a web-based extraction tool that integrates our machine learning models to automatically extract information from uploaded articles (PDFs). We will conduct a user study to evaluate the utility and usability of this tool in practice.             Public Health Narrative  We propose to develop computational methods and tools that make the practice of evidence-based medicine (EBM) more efﬁcient, speciﬁcally by semi-automating data extraction from the full-texts of articles describing clinical trials. Such tools would drastically reduce the workload currently involved in producing evidence syntheses, ultimately enabling evidence- based care in an era of information overload.",Semi-Automating Data Extraction for Systematic Reviews,9028559,R01LM012086,"['Age', 'Area', 'Beds', 'Caring', 'Categories', 'Characteristics', 'Clinical', 'Clinical Trials', 'Collaborations', 'Communities', 'Complement', 'Computer software', 'Computing Methodologies', 'Data', 'Data Element', 'Data Set', 'Databases', 'Decision Making', 'Effectiveness of Interventions', 'Elements', 'Evidence Based Medicine', 'Evidence based practice', 'Exercise', 'Feedback', 'Goals', 'Growth', 'Healthcare', 'Hour', 'Human Resources', 'Interdisciplinary Study', 'Intervention', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Manuals', 'Medical', 'Methods', 'Modeling', 'National Health Policy', 'Online Systems', 'Outcome', 'Patient Care', 'Performance', 'Persons', 'Population Characteristics', 'Positioning Attribute', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Services', 'Side', 'Software Tools', 'Structure', 'System', 'Text', 'Training', 'Work', 'Workload', 'base', 'clinical practice', 'computerized tools', 'cost', 'data mining', 'design', 'evidence base', 'experience', 'improved', 'innovation', 'interest', 'member', 'natural language', 'novel', 'open source', 'study characteristics', 'systematic review', 'tool', 'trial design', 'usability']",NLM,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,317900,-0.004164776399974036
"Summer Institute for Statistics of Big Data DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support. PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.",Summer Institute for Statistics of Big Data,8935790,R25EB020380,"['Academia', 'Area', 'Big Data', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'biomedical scientist', 'data visualization', 'graduate student', 'human disease', 'improved', 'instructor', 'lectures', 'member', 'open source', 'programs', 'skills', 'statistics', 'teacher', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2015,159605,-0.021132182023921198
"The Big DIPA: Data Image Processing and Analysis ﻿    DESCRIPTION (provided by applicant): This proposal aims to establish a national short course in Big Data Image Processing & Analysis (BigDIPA) intended to increase the number and overall skills of competent research scientists now encountering large, complex image data sources derived from cutting edge biological/biomedical research approaches. Extraction of knowledge from these imaging sources requires specialized skills and an interdisciplinary mindset. Yet effective training opportunities of this sector of the ""Big Data"" science community are glaringly underappreciated and underserved compared to other big data fields such as omics. UC Irvine is ideally suited to host a short course to address this thematic training deficit on account of the synergistic colocalization between multiple facilities, renowned for development of numerous advanced imaging techniques, and the outstanding instructional environment provided by faculty with collaborative expertise in biological image processing and computer vision, bioinformatics and high performance computational approaches.  Specifically, our BigDIPA proposal assembles an interdisciplinary alliance of faculty experts that can leverage the preeminent imaging resource facilities, such as the Laboratory of Fluorescence Dynamics (LFD) and the Beckman Laser Institute, and fuse these to ongoing campus big data initiatives, e.g. UCI's Data Science Initiative, to create a top-rated training course designed for senior graduate students, postdoctoral researchers, faculty and industry scientists from diverse scientific disciplines who have nascent interests and needs to handle BIG DATA sources beyond their current level of competency.  The course theme is focused to utilize discreet examples drawn from the analysis of complex data acquired from different microscopy imaging modalities employed to investigate dynamics in cellular and tissue processes, including signal transduction networks, development, neuroscience and biomedical applications, and that hereto where hidden or inaccessible to standard methods of analysis. Participants will be guided along the complete acquisition- processing-analysis pipeline through exposure to a coherent progression of topics and issues typically encountered when handling BIG DATA. We believe this training approach will therefore be attractive to a broad and significant untapped pool of researchers from the biological disciplines, biomedical engineering, systems biology, math, biophysics, computer science, bioinformatics and statistics who possess some, but not all, of the requisite competencies to effectively traverse the BD2K landscape. We have designed the course such that skills and experience gained by trainees will be transferable to their own research interests.  The BigDIPA course format will combine didactic lectures on the theory and foundational frameworks that underpin each step, with practical instruction on implementation and hands-on tutorials in image acquisition, large data handling, basic scripting of computational tools, image processing on high performance computing architectures, as well as feature extraction, evaluation and visualization of results. The course is designed to offer an intense learning experience delivered in a compact time frame, and opportunities to foster interdisciplinary interactions through small team exercises. Participants will also be encouraged to take advantage of pre-courses - separate and distinct training opportunities not funded by this proposal - that will be coordinated to directly precede our course. This unique format provides multiple benefits: it provides an efficient mechanism to address individual participant training deficiencies to permit a more productive experience in the BigDIPA course, adds no-cost mutual benefits to independent but synergistic programs, and facilitates recruitment of applicants who frequently feel interested but intimidated due to a perceived lack of prior adequate training.  Beyond providing an intensive on-site training course, all course materials (lecture notes, video lectures and tutorials), tutorial exercises, open source software resources and sample datasets will be made freely available through on-line distribution to maximize outreach and encourage additional contributions of curated training resources solicited from the community.         PUBLIC HEALTH RELEVANCE: We propose to train and expand the cadre of researchers capable of effectively using the deluge of complex BIG DATA being generated by advanced biomedical imaging approaches. These data sources represent a rich source of complex information relevant to many scientific areas of inquiry, and are informative at multiple scales ranging from fundamental biological processes at the cellular level to patient diagnostics for diseases such as cancer or neurological disorders.            ",The Big DIPA: Data Image Processing and Analysis,9044533,R25EB022366,"['Accounting', 'Address', 'Architecture', 'Area', 'Big Data', 'Bioinformatics', 'Biological', 'Biological Process', 'Biological Sciences', 'Biomedical Engineering', 'Biomedical Research', 'Biomedical Technology', 'Biophysics', 'Cell physiology', 'Communities', 'Complex', 'Computer Analysis', 'Computer Vision Systems', 'Computer software', 'Data', 'Data Set', 'Data Sources', 'Development', 'Diagnostic', 'Discipline', 'Disease', 'Education', 'Educational Curriculum', 'Educational workshop', 'Environment', 'Evaluation', 'Exercise', 'Exposure to', 'Faculty', 'Fluorescence', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'High Performance Computing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging technology', 'Individual', 'Industry', 'Information Sciences', 'Institutes', 'Instruction', 'Interdisciplinary Communication', 'Knowledge', 'Knowledge Extraction', 'Laboratories', 'Lasers', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Mathematics', 'Methods', 'Microscopy', 'Modality', 'NIH Program Announcements', 'National Institute of General Medical Sciences', 'Neurosciences', 'Participant', 'Patients', 'Performance', 'Problem Solving', 'Process', 'Recruitment Activity', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Sampling', 'Schools', 'Science', 'Scientist', 'Senior Scientist', 'Signal Transduction', 'Site', 'Software Tools', 'Source', 'Staging', 'Stream', 'Systems Biology', 'TNFRSF5 gene', 'Time', 'Training', 'United States National Institutes of Health', 'Work', 'bioimaging', 'biological systems', 'biomedical scientist', 'citizen science', 'computer science', 'computerized tools', 'cost', 'data acquisition', 'data format', 'demographics', 'design', 'experience', 'flexibility', 'graduate student', 'image processing', 'imaging modality', 'interdisciplinary collaboration', 'interest', 'lecture notes', 'lectures', 'meetings', 'nervous system disorder', 'open source', 'outreach', 'programs', 'public health relevance', 'repository', 'skills', 'statistics', 'theories', 'tissue processing']",NIBIB,UNIVERSITY OF CALIFORNIA-IRVINE,R25,2015,161997,-0.039858355682659816
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8935748,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'clinical investigation', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2015,73173,-0.0019104992430428723
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",8935792,U54EB020403,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genetic study', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Internet', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'cluster merger', 'computer science', 'innovation', 'multidisciplinary', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2015,2366897,0.00191475177066529
"Developing Cloud-based tools for Big Neural Data DESCRIPTION (provided by applicant): Big data has the potential to dramatically advance the electrophysiology biodata sciences in similar ways that it has transformed Genetics. Differences between these two areas dictate separate approaches to apply Big Data tools, and methods in order to provide successful assets to the research community. For one, neural datasets are very heterogeneous by nature. The data is difficult to interpret without knowing specifics about the data acquisition protocol, the experimental paradigm and the physiological state of the recorded subject. Many neural datasets are complemented with complex meta-data sets, which should be an integral component in any effort to integrate and share these data with other researchers. The goal of this project is to develop novel, generalizable Big Data tools to facilitate cloud-base analysis of complex multi-scale neural data. Epilepsy research will be used as a specific use case to guide the development of the tools. A cohort of established senior investigators performing epilepsy research will use and validate these tools in their laboratories. Epilepsy research is currently limited by its narrow focus on single models (animal or human) in individual centers and laboratories. Just as Genetics was revolutionized through Big Data techniques, so too can Epilepsy research be transformed through novel approaches to standardize, share, and mine data across groups of investigators. Over the past several years I have co-developed a NINDS funded cloud-based data platform, ://ieeg.org, giving me a central role in developing Big Data solutions for neural data, such as customized data sharing, large-scale cloud-based data analysis, and search and interrogation techniques for complex data and metadata. My scientific objectives for this project are: (1) to develop generalizable tools to curate, analyze, and interrogate multi-scale neural data, and (2) to create a platform that will galvanize a research community focused on sharing data, and methods to advance Big Data research in the basic and translational neurosciences. Equally important to this proposal, I present a training plan to prepare me for an academic career focused on Big Data in the neurosciences. This plan supplements my background in bioengineering and statistical modeling of neural data with broader data-science expertise in data integration and machine learning, and deeper domain knowledge of the clinical neurosciences. I have assembled a group of collaborators, basic investigators and clinician scientists, who will use the tools developed in this project to analyze and validate their data and methods. I will use the results of this project as the foundation for a R01 Grant application, in which I will expand the developed platform and tools to target other research domains (TBI, Emergency Care, Cardiac), as well as integrate other data-modalities such as Imaging, and Genomics. OMB No. 0925-0001/0002 (Rev. 08/12 Approved Through 8/31/2015) Page Continuation Format Page PUBLIC HEALTH RELEVANCE: The goal of this proposal is to advance Big Data research in the neurosciences by developing tools and techniques to interrogate electrophysiology data sets from animal models of human neurological disorders. Development of these tools requires close collaboration between domain experts in Neuroscience, Machine Learning, Statistics and Computer Science. When developed, this platform and these tools will allow investigators to share, collaborate, annotate, standardize and analyze large, complex, multiscale data sets that are a crucial first step in advancing this field.",Developing Cloud-based tools for Big Neural Data,8935817,K01ES025436,"['Animal Model', 'Applications Grants', 'Area', 'Big Data', 'Biomedical Engineering', 'Cardiac', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational Technique', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Electrophysiology (science)', 'Emergency Care', 'Epilepsy', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Incentives', 'Individual', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Mining', 'Modality', 'National Institute of Neurological Disorders and Stroke', 'Nature', 'Neurosciences', 'Organism', 'Performance', 'Physiological', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Series', 'Solutions', 'Standardization', 'Statistical Models', 'Techniques', 'Time', 'Training', 'base', 'career', 'cloud based', 'cohort', 'comparative', 'computer science', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'improved', 'nervous system disorder', 'novel', 'novel strategies', 'relating to nervous system', 'statistics', 'tool', 'tool development', 'translational neuroscience']",NIEHS,UNIVERSITY OF PENNSYLVANIA,K01,2015,192201,-0.008361971229818311
"Boosting the Translational Impact of Scientific Competitions by Ensemble Learning ﻿    DESCRIPTION (provided by applicant): ""Big data"" such as those arising from sequencing, imaging, genomics and other emerging technologies are playing a critical role in modern biology and medicine. The generation of hypotheses about biological processes and disease mechanisms is now increasingly being driven by the production and analysis of large and complex datasets. Advanced computational methods have been developed for the robust analysis of these datasets, and the growth in number and sophistication of these methods has closely tracked the growth in volume and complexity of biomedical data. In such a crowded environment of diverse computational methods and data, it is difficult to judge how generalizable the performance of these methods is from one setting to another. Crowdsourcing-based scientific competitions, or challenges, have now become popular mechanisms for the rigorous, blinded and unbiased evaluation of the performance of these methods and the identification of best-performing methods for biomedical problems. However, despite the benefits of these challenges to the biomedical research enterprise, the impact of their findings has been remarkably limited in laboratory and clinical settings. This is likely due to two important aspects of current challenges: (i) their over-emphasis on identifying the ""best"" solutions rather than tryig to comprehensively assimilate the knowledge embedded in all the submitted solutions, and (ii) the absence of a stable channel of communication and collaboration between problem and solution providers due to a lack of sufficient incentives to do so. The aim of this project is to boost the translational impact of scientific challenges through a combination of novel machine learning methods, development of novel scalable software and unique collaborations with disease experts to ensure the effective translation of knowledge accrued in challenges to real clinical settings and practice. These novel methods and software are designed to effectively assimilate the knowledge embedded in all the submissions to challenges into ""ensemble"" solutions. In a first of its kind effort, the ensemble solutions derived from disease-focused challenges under the DREAM project will be brought directly to scientists and clinicians that are experts in these disease areas. Initial effort in this project will focus on active DREAM challenges aiming at the accurate prediction of drug response and clinical outcomes respectively in Rheumatoid Arthritis (RA) and Acute Myeloid Leukemia (AML). Both these diseases are difficult to treat and thus they pose major medical and public health concerns. In collaboration with RA and AML experts, the ensemble solutions learnt in these challenges will be validated in independent patient cohorts and carefully designed clinical studies. This second-level validation is essential to judge the clinical applicability of any method, but is rarely done As the methodology is general, similar efforts will be made for other diseases in later stages of the project. Overall, using a smart combination of crowdsourcing-based challenges and computational methods and software, we aim to demonstrate a unique pathway for studying and treating disease by truly leveraging the ""wisdom of the crowds"".         PUBLIC HEALTH RELEVANCE: Crowdsourcing-based scientific competitions, or challenges, have become a popular mechanism to identify innovative solutions to complex biomedical problems. However, the collective effort of all the challenge participants has been under utilized, and the overall impact on actual clinical and laboratory practice has been remarkably limited. Using novel computational methods and novel ""big data""-friendly software implementation, we plan to demonstrate how biomedical challenges, combined with our approach, can influence clinical practice in Acute Myeloid Leukemia and Rheumatoid Arthritis, as well as rigorously validate our approach.                 ",Boosting the Translational Impact of Scientific Competitions by Ensemble Learning,8864679,R01GM114434,"['Acute Myelocytic Leukemia', 'Address', 'Adopted', 'Advanced Development', 'Architecture', 'Area', 'Big Data', 'Biological', 'Biological Process', 'Biology', 'Biomedical Research', 'Blinded', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Research', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Crowding', 'Data', 'Data Set', 'Discipline', 'Disease', 'Emerging Technologies', 'Ensure', 'Environment', 'Evaluation', 'Explosion', 'Generations', 'Genomics', 'Genotype', 'Goals', 'Growth', 'Heterogeneity', 'High Performance Computing', 'Image', 'Incentives', 'Knowledge', 'Laboratories', 'Learning', 'Life', 'Machine Learning', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Mining', 'Nature', 'Outcome', 'Participant', 'Pathway interactions', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Phenotype', 'Play', 'Problem Solving', 'Production', 'Provider', 'Public Health', 'Publications', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Running', 'Science', 'Scientist', 'Software Design', 'Solutions', 'Source', 'Staging', 'Synapses', 'System', 'Time', 'Translating', 'Translations', 'Validation', 'Variant', 'base', 'clinical application', 'clinical practice', 'cohort', 'computer science', 'design', 'innovation', 'interest', 'knowledge translation', 'meetings', 'method development', 'novel', 'open source', 'predictive modeling', 'prospective', 'public health relevance', 'response', 'stem', 'tool']",NIGMS,ICAHN SCHOOL OF MEDICINE AT MOUNT SINAI,R01,2015,445887,0.016560937194873674
"Protect Privacy of Healthcare Data in the Cloud DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier. Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8925916,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Laplacian', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'privacy protection', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2015,192613,0.007421434757105086
"SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies ﻿    DESCRIPTION (provided by applicant): Efforts to reduce the burden of Tuberculosis (TB) are challenged by the persistent social inequalities in health, the limited number of local healthcare professionals, and the weak healthcare infrastructure found in resource-poor communities. Reducing the TB diagnosis delay is critical in mitigating disease transmission and minimizing the reproductive rate of the TB epidemic in high-burden areas. The main objective of this proposal is to expedite the TB diagnosis process by developing novel image processing and machine learning techniques to analyze chest X-ray images thus reducing patient wait times for being diagnosed with TB. The study will be conducted in the district of Carabayllo, a densely occupied, high-burden TB area in Lima, the capital of Perú. Efforts to develop the proposed user-centered, mobile device-based computing system are aligned with the mission of the National Institute of Biomedical Imaging and Bioengineering (NIBIB) and its strategic goals 2 and 4 in particular-the proposed socio-technical intervention aims at developing biomedical imaging techniques (i.e. wireless and image sensing/analyzing) to enable a point-of-care mobile device-based computing system for TB screening and diagnostic. Anticipated outcomes include a) a large-scale, real-world, well-annotated, and public available chest X-ray image database for TB screening, b) development of new image analysis techniques for X-ray image capturing and pre- processing, and c) novel learning-based feature extraction and classification algorithms. This  interdisciplinary effort, involving community, university, hospitals and health care establishments in all stages of the research, responds to the need for increased partnerships between academia and community stakeholders, and the potential for building capacity in biomedical and technology solutions for health in both directions (North-South, South-North). Its scientific contribution lies in the intersection of three NIBIB scientific program areas including image processing, telehealth, and biomedical informatics.         PUBLIC HEALTH RELEVANCE: This project is highly relevant to public and global health because it offers a socio-technical solution for resource-poor communities severely affected by TB. Outcomes of this project will contribute significantly to improving specific healthcare processes affecting hard-to-reach communities that are socially excluded and lack the benefits of technological advances while broadening our understanding about effective human centered designs to improve healthcare systems with mobile computing technologies.            ",SCH: INT A.Soclotechnilcal Systems Systems  Approach for Improving Tuberculosis Diagnostics Using Mobile Health Technologies,9072725,R01EB021900,"['Academia', 'Address', 'Affect', 'Algorithms', 'Area', 'Benchmarking', 'Biomedical Technology', 'Capital', 'Cessation of life', 'Chest', 'Chronic Disease', 'Cities', 'Classification', 'Clinic', 'Communicable Diseases', 'Communities', 'Complex', 'Computer Assisted', 'Computer Systems', 'Computer software', 'Computers', 'Data', 'Databases', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Disadvantaged', 'Discipline', 'Engineering', 'Epidemic', 'Evaluation', 'Female', 'Goals', 'Health', 'Health Professional', 'Health Sciences', 'Health Technology', 'Healthcare', 'Healthcare Systems', 'Hispanics', 'Human', 'Image', 'Image Analysis', 'Image Enhancement', 'Imaging Techniques', 'Intervention', 'Learning', 'Lung nodule', 'Machine Learning', 'Medical', 'Minority', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Outcome', 'Patients', 'Peru', 'Population', 'Process', 'Public Health', 'Reader', 'Recruitment Activity', 'Reporting', 'Research', 'Research Infrastructure', 'Resources', 'Running', 'Sensitivity and Specificity', 'Software Tools', 'Solutions', 'Staging', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Thoracic Radiography', 'Time', 'Training', 'Treatment Protocols', 'Tuberculosis', 'Underrepresented Students', 'University Hospitals', 'Vaccines', 'Wireless Technology', 'Woman', 'World Health Organization', 'accurate diagnosis', 'base', 'bioimaging', 'biomedical informatics', 'clinical practice', 'compliance behavior', 'data exchange', 'design', 'digital imaging', 'disease transmission', 'global health', 'handheld mobile device', 'image processing', 'improved', 'mHealth', 'novel', 'open source', 'point of care', 'programs', 'public health relevance', 'reproductive', 'screening', 'social inequality', 'telehealth', 'tool']",NIBIB,UNIVERSITY OF MASSACHUSETTS LOWELL,R01,2015,299984,0.03217961611059152
"Developing and applying information extraction resources and technology to create DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another. Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8866232,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2015,602189,-0.01929103910734604
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics. n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8792208,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2015,248295,0.0017273315205544508
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9126755,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,40625,0.01295834131045745
"In silico identification of phyto-therapies DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits. RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.",In silico identification of phyto-therapies,9117880,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Health', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,BROWN UNIVERSITY,R01,2015,379655,0.01295834131045745
"Large-Scale Semiparametric Graphical Models with Applications to Neuroscience DESCRIPTION: The objective of this proposal is to develop and theoretically evaluate a unified set of statistical, computational, and software tools to address data mining and discovery science challenges in the analysis of existing vast amounts of publicly available neuroimaging data. In particular, we propose to develop scalable and robust semiparametric solutions for high-throughput estimation of resting-state brain connectivity networks, both at the individual and population levels, with the flexibility of incorporating covariate information.  The work will contribute meaningfully to the theory and methods for large-scale semiparametric graphical models and will apply these methods to the largest collections of resting-state fMRI data available. The proposed methods and theory include key directions of research for brain network estimation and mining. First, we pro- pose novel methods for subject-specific network estimation, such as would be needed for biomarker development in functional brain imaging. Secondly, we define and propose to evaluate and implement methods for studying population-level graphs, which study collections of graphs. Thirdly, we propose the use of estimated graphs in predictive modeling. Finally, all of these methods will have complementary software and web services development. Most notably, the idea of population graphs allows for the creation of functional brain network atlases.  In summary, the work of this proposal will result in a unified framework for the analysis of modern neuroimaging data via graphical models. Our methods will further be agnostic to intricacies of the technology, thus making it portable across settings and applicable outside of the field of functional brain imaging. The methods will be carefully evaluated via theory, simulation and data-based application evidence. PUBLIC HEALTH RELEVANCE: Modern neuroimaging data are often Big, Complex, Noisy and Dependent. We propose a systematic attempt on methodological development for the largely unexplored but practically important problem of network estimation and mining based on neuroimaging data. Our proposed work represents a significant step forward over the current methodology and has the potential to be applied to analyze a wide range of scientific problems beyond brain imaging data analysis.",Large-Scale Semiparametric Graphical Models with Applications to Neuroscience,8795225,R01MH102339,"['Address', 'Algorithms', 'Atlases', 'Attention deficit hyperactivity disorder', 'Big Data', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Characteristics', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Development', 'Disease', 'Documentation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Graph', 'Health', 'Heterogeneity', 'Image', 'Individual', 'Internet', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modality', 'Modeling', 'Neurosciences', 'Pattern', 'Population', 'Population Study', 'Process', 'Rest', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Software Tools', 'Solutions', 'Statistical Methods', 'Tail', 'Technology', 'Work', 'abstracting', 'base', 'brain research', 'cloud based', 'data mining', 'flexibility', 'interest', 'neuroimaging', 'novel', 'predictive modeling', 'psychologic', 'simulation', 'theories', 'user friendly software', 'web services']",NIMH,PRINCETON UNIVERSITY,R01,2015,369756,0.004115823965635059
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences. Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8906938,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2015,217378,-0.022652024018908062
"COINSTAC: decentralized, scalable analysis of loosely coupled data ﻿    DESCRIPTION (provided by applicant):     The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway5,10. However, there is a significant gap in existing strategies which focus on anonymized, post-hoc sharing of either 1) full raw or preprocessed data [in the case of open studies] or 2) manually computed summary measures [such as hippocampal volume11, in the case of closed (or not yet shared) studies] which we propose to address. Current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the dat as well as for the individual requesting the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions). This needs to change, so that the scientific community becomes a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see recent overview on this from our group2).    The large amount of existing data requires an approach that can analyze data in a distributed way while also leaving control of the source data with the individual investigator; this motivates  dynamic, decentralized way of approaching large scale analyses. We are proposing a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). The system will provide an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data can be avoided, while the strength of large-scale analyses can be retained. To achieve this, in Aim 1, the uniform data interfaces that we propose will make it easy to share and cooperate. Robust and novel quality assurance and replicability tools will also be incorporated. Collaboration and data sharing will be done through forming temporary (need and project-based) virtual clusters of studies performing automatically generated local computation on their respective data and aggregating statistics in global inference procedures. The communal organization will provide a continuous stream of large scale projects that can be formed and completed without the need of creating new rigid organizations or project-oriented storage vaults. In Aim 2, we develop, evaluate, and incorporate privacy-preserving algorithms to ensure that the data used are not re-identifiable even with multiple re-uses. We also will develop advanced distributed and privacy preserving approaches for several key multivariate families of algorithms (general linear model, matrix factorization [e.g. independent component analysis], classification) to estimate intrinsic networks and perform data fusion. Finally, in Aim 3, we will demonstrate the utility of this approach in a proof of concept study through distributed analyses of substance abuse datasets across national and international venues with multiple imaging modalities.         PUBLIC HEALTH RELEVANCE: Hundreds of millions of dollars have been spent to collect human neuroimaging data for clinical and research purposes, many of which don't have data sharing agreements or collect sensitive data which are not easily shared, such as genetics. Opportunities for large scale aggregated analyses to infer health-relevant facts create new challenges in protecting the privacy of individuals' data. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a good solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we are proposing will capture this 'missing data' and allow for pooling of both open and 'closed' repositories by developing privacy preserving versions of widely-used algorithms and incorporating within an easy-to-use platform which enables distributed computation. In addition, COINSTAC will accelerate research on both open and closed data by offering a distributed computational solution for a large toolkit of widely used algorithms.                ","COINSTAC: decentralized, scalable analysis of loosely coupled data",8975906,R01DA040487,"['AODD relapse', 'Accounting', 'Address', 'Agreement', 'Alcohol or Other Drugs use', 'Algorithms', 'Attention', 'Brain imaging', 'Classification', 'Clinical Research', 'Collaborations', 'Communities', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Data Sources', 'Development', 'Ensure', 'Family', 'Functional Magnetic Resonance Imaging', 'Funding', 'Genetic', 'Genetic Markers', 'Health', 'Hippocampus (Brain)', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Left', 'Letters', 'Linear Models', 'Location', 'Machine Learning', 'Manuals', 'Measures', 'Methods', 'Movement', 'Paper', 'Plant Roots', 'Poaceae', 'Population', 'Privacy', 'Procedures', 'Process', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Site', 'Solutions', 'Stream', 'Substance abuse problem', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'base', 'computing resources', 'cost', 'data sharing', 'distributed data', 'flexibility', 'imaging modality', 'independent component analysis', 'neuroimaging', 'novel', 'peer', 'public health relevance', 'quality assurance', 'repository', 'statistics', 'tool', 'virtual']",NIDA,THE MIND RESEARCH NETWORK,R01,2015,727692,0.022456390414051043
"Statistical methods for large and complex databases of ultra-high-dimensional DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.",Statistical methods for large and complex databases of ultra-high-dimensional,8890255,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Health', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multicenter Studies', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'contrast enhanced', 'data visualization', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2015,347156,-0.004110403771503552
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,8935802,U54EB020405,"['Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cellular Phone', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Outcome', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Science', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visit', 'Walking', 'Work', 'base', 'biomechanical model', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'health data', 'improved', 'industry partner', 'insight', 'models and simulation', 'motor impairment', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'role model', 'sensor', 'social', 'social model', 'tool']",NIBIB,STANFORD UNIVERSITY,U54,2015,401742,0.007216232451062741
"Collaborative Development of Biomedical Ontologies and Terminologies DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient. PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.",Collaborative Development of Biomedical Ontologies and Terminologies,8803385,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Craniofacial Abnormalities', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Health', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial development', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'new technology', 'novel', 'open source', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2015,523965,0.022610822738410575
"Large-scale Automated Synthesis of Functional Neuroimaging Data DESCRIPTION (provided by applicant): The explosive growth of the human neuroimaging literature has led to major advances in understanding of normal and abnormal human brain function, but has also made aggregation and synthesis of neuroimaging findings increasingly difficult. The goal of this project is to develop an automated software platform for large-scale synthesis of human functional neuroimaging studies. Our work builds directly on an existing software platform (NeuroSynth) and involves key extensions and improvements that focus on (i) aggregation, (ii) coding, (iii) synthesis, and (iv) sharing of functional neuroimaging data. In Aim 1, we will use computational linguistics and bioinformatics data mining techniques to develop new algorithms for automatically extracting activation foci and associated metadata from published neuroimaging articles. In Aim 2, we will use topic-modeling techniques such as Latent Dirichlet Analysis in combination with existing cognitive ontologies such as the Cognitive Atlas to develop structured representations of automatically extracted neuroimaging data. In Aim 3, we will improve the meta-analysis and classification capacities of our existing platform by implementing a state-of- the-art hierarchical Bayesian meta-analysis method recently developed by the research team. Finally, in Aim 4, we will develop a state-of-the-art web interface (://neurosynth.org) that supports real-time, in-browser access to the data, results, and tools produced in Aims 1 - 3. Realizing these objectives will introduce powerful new tools for organizing and synthesizing the neuroimaging literature on an unprecedented scale. These tools will be freely and publicly available to anyone with an internet connection, enabling rapid and efficient application to a broad range of clinical and basic research applications. Functional neuroimaging techniques such as fMRI have opened a new frontier in efforts to investigate and understand the neural mechanisms of normal and abnormal cognition. However, the rapidly expanding scope of the literature makes distillation and synthesis of brain imaging findings increasingly challenging. The goal of this project is to develop a new software platform for automated aggregation, synthesis, and sharing of published neuroimaging results, with the potential to advance understanding of mechanisms underlying mental health disorders.",Large-scale Automated Synthesis of Functional Neuroimaging Data,8894083,R01MH096906,"['Algorithms', 'Atlases', 'Basic Science', 'Bayesian Method', 'Bayesian Modeling', 'Bioinformatics', 'Biometry', 'Brain', 'Brain imaging', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Communities', 'Computational Linguistics', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Development', 'Ensure', 'Environment', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Growth', 'High Performance Computing', 'Human', 'Individual', 'Interdisciplinary Study', 'Internet', 'Joints', 'Journals', 'Language', 'Literature', 'Manuals', 'Maps', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Neurosciences', 'Ontology', 'Paper', 'Performance', 'Population', 'Publishing', 'Qualifying', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Specificity', 'Structure', 'Techniques', 'Text', 'Time', 'Training', 'Validation', 'Work', 'awake', 'base', 'cognitive function', 'cognitive process', 'data mining', 'frontier', 'improved', 'information organization', 'interoperability', 'knowledge base', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'open source', 'psychologic', 'theories', 'tool', 'web interface']",NIMH,"UNIVERSITY OF TEXAS, AUSTIN",R01,2015,547000,0.003300126942238836
"Protege: An Ontology-Development Platform for Biomedical Scientists DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Protégé to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Protégé generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Protégé user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries. PUBLIC HEALTH RELEVANCE: Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: An Ontology-Development Platform for Biomedical Scientists,8788417,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Health', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2015,526540,0.028248677935962842
"Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation DESCRIPTION (provided by applicant): Multi-atlas label fusion (MALF) is a powerful new technology that can automatically detect and label anatomical structures in biomedical images. It is arguably the most successful general-purpose automatic image segmentation technique ever developed. Automatic segmentation is in high demand in clinical and research applications of medical imaging, since segmentation forms a crucial step towards extracting quantitative information from imaging data, and since manual and semi-automatic approaches are ill suited for today's increasingly large and complex imaging datasets. Despite a number of papers that demonstrated outstanding performance of MALF methods across a range of biomedical imaging applications, the broader biomedical imaging research community has been slow to adopt this technique. This can be explained by multiple factors, including the technique's high computational demands, lack of a turnkey software implementation, as well as scarcity of validation in clinical imaging datasets and in the presence of extensive pathology. The present application seeks to remove these barriers and to enable a broad range of clinicians and biomedical researchers to take advantage of MALF technology. It builds on our strong track record of innovation in the MALF field, including a novel redundancy-correcting MALF technique that led in segmentation grand challenges in the past two years. Aim 1 seeks to improve the computational performance of MALF by replacing dense deformable image registration, by far the most time consuming component of MALF, with faster and less constrained sparse registration strategies. We hypothesize that this will not only reduce the computational cost of MALF, but will also make it more robust to anatomical variability, in particular enabling its use for tumor and lesion segmentation. Aim 2 proposes algorithmic extensions to MALF that support automatic segmentation of dynamic and multi-modality imaging datasets, which have been largely overlooked in the MALF literature. Aim 3 will develop a turnkey open-source implementation of MALF methodology. Taking advantage of cloud computing technology, this software will allow users with minimal image processing expertise to take full advantage of MALF segmentation on their desktop. Aim 3 will also provide a set of publicly available atlases and the means for users to build new custom atlas sets from their own data. Aim 4 will perform extensive evaluation of the new methods and software in challenging real-world clinical imaging data, including brain and cardiac imaging. As part of this evaluation, we will quantify how well our MALF approach and competing techniques generalize to novel imaging datasets with heterogeneity in acquisition parameters and clinical phenotypes. PUBLIC HEALTH RELEVANCE: This research will make it possible for a wide community of researchers who collect and analyze medical imaging data to take advantage of a new class of computer algorithms that very accurately label and measure anatomical structures and pathological formations in medical images. By offering more accurate image-derived measurements, the project promises to improve the accuracy of diagnosis, reduce the costs of biomedical re- search studies and pharmaceutical trials, and accelerate scientific discovery.",Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation,8887334,R01EB017255,"['Address', 'Adopted', 'Affect', 'Algorithms', 'Atlases', 'Biomedical Research', 'Brain', 'Brain imaging', 'Cardiac', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cloud Computing', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Dementia', 'Diagnostic', 'Evaluation', 'Gold', 'Health', 'Heterogeneity', 'High Performance Computing', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'International', 'Joints', 'Label', 'Lead', 'Learning', 'Lesion', 'Literature', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Medial', 'Medical Imaging', 'Medical Research', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multiple Sclerosis Lesions', 'Myocardium', 'Paper', 'Pathology', 'Patient Care', 'Performance', 'Pharmacologic Substance', 'Public Domains', 'Research', 'Research Infrastructure', 'Research Personnel', 'S-nitro-N-acetylpenicillamine', 'Scheme', 'Services', 'Structure', 'Techniques', 'Technology', 'Temporal Lobe', 'Temporal Lobe Epilepsy', 'Time', 'Training', 'Ultrasonography', 'Uncertainty', 'Validation', 'Work', 'aortic valve', 'base', 'bioimaging', 'cardiovascular visualization', 'clinical application', 'clinical phenotype', 'clinical practice', 'cloud based', 'cohort', 'cost', 'diagnostic accuracy', 'experience', 'image processing', 'image registration', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'interest', 'new technology', 'novel', 'open source', 'outreach', 'research study', 'success', 'targeted imaging', 'tool', 'tumor']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2015,591379,0.00706251108463568
"Pathology Image Informatics Platform for visualization, analysis and management ﻿    DESCRIPTION (provided by applicant): With the advent of whole slide digital scanners, histopathology slides can be digitized into very high-resolution digital images, realizing a new ""big data"" stream that can potentially rival ""omics data"" in size and complexity. Just as with the analysis of high-throughput genetic and expression data, the application of sophisticated image analytic tools and data pipelines can render the often passive data of digital pathology (DP) archives into a powerful source for: (a) rich quantitative insights into cancer biology and (b) companion diagnostic decision support tools for precision medicine. Digital pathology enabled companion diagnostic tests could yield predictions of cancer risk and aggressiveness in a manner similar to molecular diagnostic tests. However, prior to widespread clinical adoption of DP, extensive evaluation of clinical interpretation of DP imaging (DPI) and accompanying decision support tools needs to be undertaken. Wider acceptance of DPI by the cancer community (clinical and research) is hampered by lack of a publicly available, open access image informatics platform for easily viewing, managing, and quantitatively analyzing DPIs. While some commercial platforms exist for viewing and analyzing DPI data, none of these platforms are freely available. Open source image viewing/management platforms that cater to the radiology (e.g. XNAT) and computational biology communities are typically not conducive to handling very large file sizes as encountered with DPI datasets.  This multi-PI U24 proposal seeks to expand on an existing, freely available pathology image viewer (Sedeen Image Viewer) to create a pathology informatics platform (PIIP) for managing, annotating, sharing, and quantitatively analyzing DPI data. Sedeen was designed as a universal platform for DPI (by addressing several proprietary scanner formats and ""big data"" challenges), to provide (1) reliable and useful image annotation tools, and (2) for image registration and analysis of DPI data. Additionally, Sedeen has become an application for cropping large DPIs so that they can be input into programs such as Matlab or ImageJ. Sedeen has been freely available to the public for three years, with over 160 unique users from over 20 countries.  Building on the initial successes of Sedeen and its existing user base, our intent is to massively increase dissemination of DPI and algorithms in the cancer research community and clinical trial efforts, as well as to contribute towards the adoption of a rational and standardized set of DP operational conventions. This unique project will allow end users with different needs and technical backgrounds to seamlessly (a) archive and manage, (b) share, and (c) visualize their DPI data, acquired from different sites, formats, and platforms. The PIIP will provide a unified user interface for third party algorithms (nuclear segmentation, color normalization, biomarker quantification, radiology-pathology fusion) and will allow for algorithmic evaluation upon data arising from a plurality of source sites. By partnering with professional societies, we envision that the PIIP user base will expand to include the oncology, pathology, radiology, and pharmaceutical communities.         PUBLIC HEALTH RELEVANCE: This grant will result in the further development of advanced functionality of the already existing digital pathology image informatics platform (PIIP) with an established user-base for cancer research. Such an enhanced platform will provide the much-needed foundation for advancing (a) routine clinical adoption of digital pathology for primary diagnosis and (b) training and validation of companion diagnostic decision support systems based off histopathology. Thus, the project is aligned with the NCI's goal to foster innovative research strategies and their applications as a basis for ultimately protecting and improving human health.                ","Pathology Image Informatics Platform for visualization, analysis and management",8970326,U24CA199374,"['Accounting', 'Address', 'Adoption', 'Advanced Development', 'Algorithms', 'American', 'Archives', 'Big Data', 'Biological Markers', 'Cancer Biology', 'Cancer Prognosis', 'Clinical', 'Clinical Pathology', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Color', 'Communities', 'Community Clinical Oncology Program', 'Community Trial', 'Complex', 'Computational Biology', 'Computer Vision Systems', 'Computer software', 'Country', 'Data', 'Data Aggregation', 'Data Collection', 'Data Set', 'Data Storage and Retrieval', 'Decision Support Systems', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Ensure', 'Evaluation', 'Fostering', 'Foundations', 'Genetic', 'Goals', 'Grant', 'Health', 'Histopathology', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Informatics', 'Institution', 'International', 'Language', 'Length', 'Letters', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Malignant neoplasm of prostate', 'Medical Imaging', 'Medical Students', 'Molecular Diagnostic Testing', 'Morphology', 'Nuclear', 'Ontology', 'Optics', 'Pathologist', 'Pathology', 'Pharmaceutical Preparations', 'Pharmacologic Substance', 'Protocols documentation', 'Pythons', 'Radiology Specialty', 'Research', 'Resolution', 'Scientist', 'Site', 'Slide', 'Societies', 'Source', 'Stream', 'Training', 'Training and Education', 'Validation', 'anticancer research', 'base', 'biomedical scientist', 'cancer diagnosis', 'cancer imaging', 'cancer risk', 'clinical research site', 'companion diagnostics', 'computer human interaction', 'data exchange', 'data integration', 'data sharing', 'design', 'digital', 'digital imaging', 'high throughput analysis', 'image archival system', 'image registration', 'imaging informatics', 'improved', 'in vivo imaging', 'innovation', 'insight', 'interest', 'malignant breast neoplasm', 'oncology', 'open source', 'photonics', 'precision medicine', 'programs', 'public health relevance', 'quantitative imaging', 'repository', 'research clinical testing', 'success', 'tool', 'tumor', 'user friendly software', 'validation studies']",NCI,CASE WESTERN RESERVE UNIVERSITY,U24,2015,606305,-0.010557693554260415
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8734439,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2014,489755,-0.009298802953753202
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8669161,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2014,212994,-0.01456657100676902
"Summer Institute for Statistics of Big Data     DESCRIPTION:  Funding is sought for the Summer Institute for Statistics of Big Data (SISBID) at the University of Washington. This program will provide workshops on the statistical and computational skills needed to access, process, manage, and analyze large biomedical data sets. It will be co-directed by Ali Shojaie and Daniela Witten, faculty in the Department of Biostatistics at University of Washington.  The SISBID program will consist of five 2.5-day in-person courses, or modules, taught at the University of Washington each July. An individual participant can register for whichever set of modules he or she chooses. The five modules are as follows: (1) Accessing Biomedical Big Data; (2) Data Visualization; (3) Supervised Methods for Statistical Machine Learning; (4) Unsupervised Methods for Statistical Machine Learning; (5) Reproducible Research for Biomedical Big Data. Each module will consist of a combination of formal lectures and hands-on computing labs. Participants will work together in teams in order to apply the skills that they develop in each module to important problems drawn from relevant case studies.  The primary audience for SISBID will consist of biomedical scientists who would like to develop the statistical and computational training needed to make use of Biomedical Big Data. The secondary audience will consist of individuals with stronger statistical or computational backgrounds but little exposure to biology, who will learn how to apply their skills to problems associated with Biomedical Big Data. Participants will include advanced undergraduates, graduate students, post-doctoral fellows, and researchers, and will be drawn from industry, government, and academia. In order to ensure that all participants are able to fully engage in the program, participants will be expected to already have some prior background in R programming and statistical inference, which can be obtained by taking two free online courses before the program begins.  Each of the five modules will be co-taught by two instructors. The ten instructors will be drawn from top universities and research centers across the U.S., such as the University of Washington, Rice University, University of Iowa, Johns Hopkins University, MD Anderson Cancer Research Center, Fred Hutchinson Cancer Research Center, and University of North Carolina. They have been selected based on research expertise and excellence in teaching.  Lecture videos and slides will be made freely available online so that individuals who are unable to attend SISBID in person can still benefit from the program.  This proposal specifically requests funds for 55 student / postdoctoral fellow travel scholarships per year, 130 student / postdoctoral fellow registration scholarships per year, instructor travel and stipends, teaching assistant stipends, and PI salary support.         PUBLIC HEALTH RELEVANCE:   In recent years, the biomedical sciences have been inundated by Big Data, such as DNA sequence data and electronic medical records. In principle, it should be possible to use such data for a variety of tasks, such as predicting an individual's risk of developing diabetes or cancer, and tailoring therapies to an individual should he or she become ill. The Summer Institute for Statistics of Big Data will provide biomedical researchers with the computational and statistical training needed in order to take advantage of Big Data, so that they can more effectively use it to understand human diseases and to improve human health.            ",Summer Institute for Statistics of Big Data,8829422,R25EB020380,"['Academia', 'Area', 'Big Data', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Biometry', 'Cancer Center', 'Case Study', 'Collection', 'Computer software', 'Computerized Medical Record', 'DNA Sequence', 'Data', 'Data Set', 'Diabetes Mellitus', 'Educational process of instructing', 'Educational workshop', 'Ensure', 'Environment', 'Exposure to', 'Faculty', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Government', 'Health', 'Human', 'Hybrids', 'Imagery', 'Individual', 'Industry', 'Institutes', 'Iowa', 'Knowledge', 'Learning', 'Learning Module', 'Machine Learning', 'Malignant Neoplasms', 'NCI Center for Cancer Research', 'North Carolina', 'Participant', 'Persons', 'Postdoctoral Fellow', 'Process', 'Records', 'Research', 'Research Personnel', 'Resources', 'Rice', 'Risk', 'Running', 'Scholarship', 'Science', 'Slide', 'Statistical Computing', 'Statistical Methods', 'Students', 'Training', 'Training Activity', 'Training Programs', 'Travel', 'United States', 'Universities', 'Videotape', 'Wages', 'Washington', 'Work', 'base', 'biomedical scientist', 'graduate student', 'human disease', 'improved', 'instructor', 'lectures', 'member', 'open source', 'programs', 'public health relevance', 'skills', 'statistics', 'teacher', 'web site']",NIBIB,UNIVERSITY OF WASHINGTON,R25,2014,160523,-0.021132182023921198
"GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY     DESCRIPTION:  Many complex diseases such as cancer, cardiovascular disorders, and schizophrenia may be understood as failures in the functioning of nested hierarchies of biomolecular and cellular networks. These nested hierarchies control a range of processes including the differentiation and migration of cells, remodeling of extracellular matrices and tissues, and information encoding in neuronal subsystems. Washington University has established expertise in cutting edge imaging, molecular biology and genomic technologies synergistic with computational approaches such as machine learning and unraveling the principles of hierarchical organization and dynamics of complex systems. This collective expertise is being leveraged to develop new drugs, improve our ability to interpret sophisticated imaging data, understand how populations of neurons act collectively to accomplish complex tasks, and model the onset and progression of complex diseases as dynamical rewiring of hierarchical, multi-scale networks. Biological network analyses provide a rich set of tools for organizing and interpreting the vast quantities of data produced by state-of-the-art experimental protocols. The rapid advancement of computationally intensive research in these areas is outstripping the capabilities of CPU-based high performance computing (HPC) systems. This application would support the acquisition and integration of a large-scale IBM high performance cluster of Graphics Processor Units (GPUs) to be added as an upgrade to the existing IBM-designed Heterogeneous High Performance Computing environment to form a state-of-the-art hybrid computing capability. Such a resource is essential to match the growing need for high performance computing at Washington University and to support state of the art research software applications that are optimized for GPU computing. The acquisition and integration of a high performance GPU cluster will solve critical computing challenges that exist within Washington University's growing NIH research portfolio. The proposed state-of-the-art hybrid GPU/CPU computing capabilities will be deployed within the framework of a stable, productive and rapidly growing resource center. The addition of high-capacity GPU computing capabilities will allow critical calculation to be performed in hours instead of days and enable substantial increases in productivity for existing projects covering a broad range of application areas as well as enabling new research directions.             n/a",GPU COMPUTING RESOURCE TO ENABLE INNOVATION IN IMAGING AND NETWORK BIOLOGY,8640341,S10OD018091,"['Area', 'Biological', 'Biology', 'Cardiovascular Diseases', 'Complex', 'Computer Systems', 'Computer software', 'Data', 'Disease', 'Environment', 'Extracellular Matrix', 'Failure', 'Genomics', 'High Performance Computing', 'Hour', 'Hybrids', 'Image', 'Machine Learning', 'Malignant Neoplasms', 'Modeling', 'Molecular Biology', 'Neurons', 'Performance', 'Pharmaceutical Preparations', 'Population', 'Process', 'Productivity', 'Protocols documentation', 'Research', 'Resources', 'Schizophrenia', 'System', 'Technology', 'Tissues', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'cell motility', 'computing resources', 'design', 'improved', 'innovation', 'tool']",OD,WASHINGTON UNIVERSITY,S10,2014,597700,-0.005146033725915696
"The Center for Predictive Computational Phenotyping-1 Overall     DESCRIPTION (provided by applicant):  The biomedical sciences are being radically transformed by advances in our ability to monitor, record, store and integrate information characterizing human biology and health at scales that range from individual molecules to large populations of subjects. This wealth of information has the potential to substantially advance both our understanding of human biology and our ability to improve human health. Perhaps the most central and general approach for exploiting biomedical data is to use methods from machine learning and statistical modeling to infer predictive models. Such models take as input observable data representing some object of interest, and produce as output a prediction about a particular, unobservable property of the object. This approach has proven to be of high value for a wide range of biomedical tasks, but numerous significant challenges remain to be solved in order for the full potential of predictive modeling to be realized.  To address these challenges, we propose to establish The Center for Predictive Computational Phenotyping (CPCP). Our proposed center will focus on a broad range of problems that can be cast as computational phenotyping. Although some phenotypes are easily measured and interpreted, and are available in an accessible format, a wide range of scientifically and clinically important phenotypes do not satisfy these criteria. In such cases, computational phenotyping methods are required either to (i) extract a relevant  phenotype from a complex data source or collection of heterogeneous data sources, (ii) predict clinically  important phenotypes before they are exhibited, or (iii) do both in the same application.         PUBLIC HEALTH RELEVANCE:  We will develop innovative new approaches and tools that are able to discover, and make crucial inferences with large data sets that include molecular profiles, medical images, electronic health records, population-level data, and various combinations of these and other data types. These approaches will significantly advance the state of the art in wide range of biological and clinical investigations, such as predicting which patients are most at risk for breast cancer, heart attacks and severe blood clots.            ",The Center for Predictive Computational Phenotyping-1 Overall,8774800,U54AI117924,"['Address', 'Arts', 'Biological', 'Blood coagulation', 'Clinical Trials', 'Complex', 'Computational algorithm', 'Computer software', 'Computing Methodologies', 'Data', 'Data Collection', 'Data Set', 'Data Sources', 'Diagnosis', 'Disease', 'Education', 'Electronic Health Record', 'Environment', 'Exhibits', 'General Population', 'Generations', 'Genomics', 'Genotype', 'Greek', 'Health', 'Human', 'Human Biology', 'Individual', 'Knowledge', 'Learning', 'Machine Learning', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular Profiling', 'Monitor', 'Myocardial Infarction', 'Organism', 'Output', 'Patients', 'Phenotype', 'Population', 'Postdoctoral Fellow', 'Property', 'Regulatory Element', 'Research', 'Resources', 'Risk', 'Risk Assessment', 'Sampling', 'Science', 'Scientist', 'Statistical Algorithm', 'Statistical Models', 'Time', 'Training Activity', 'graduate student', 'improved', 'innovation', 'interest', 'malignant breast neoplasm', 'novel strategies', 'outcome forecast', 'predictive modeling', 'public health relevance', 'success', 'tool', 'treatment planning']",NIAID,UNIVERSITY OF WISCONSIN-MADISON,U54,2014,73173,-0.0019104992430428723
"ENIGMA Center for Worldwide Medicine, Imaging & Genomics     DESCRIPTION (provided by applicant): The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort bringing together 287 scientists and all their vast biomedical datasets, to work on 9 major human brain diseases: schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images, genomes, connectomes and biomarkers on an unprecedented scale, with new kinds of computation for integration, clustering, and learning from complex biodata types. ENIGMA, founded in 2009, performed the largest brain imaging studies in history (N>26,000 subjects; Stein +207 authors, Nature Genetics, 2012) screening genomes and images at 125 institutions in 20 countries. Responding to the BD2K RFA, ENIGMA'S Working Groups target key programmatic goals of BD2K  funders across the NIH, including NIMH, NIBIB, NICHD, NIA, NINDS, NIDA, NIAAA, NHGRI and FIC. ENIGMA creates novel computational algorithms and a new model for Consortium Science to revolutionize the way Big Data is handled, shared and optimized. We unleash the power of sparse machine learning, and high dimensional combinatorics, to cluster and inter-relate genomes, connectomes, and multimodal brain images to discover diagnostic and prognostic markers. The sheer computational power and unprecedented collaboration advances distributed computation on Big Data leveraging US and non-US infrastructure, talents and data. Our projects will better identify factors that resist and promote brain disease, that help diagnosis and prognosis, and identify new mechanisms and drug targets. Our Data Science Research Cores create new algorithms to handle Big Data from (1) Imaging Genomics, (2) Connectomics, and (3) Machine Learning & Clinical Prediction. Led by world leaders in the field who developed major software packages (e.g., Jieping  Ye/SLEP), we prioritize trillions of computations for gene-image clustering, distributed multi-task machine  learning, and new approaches to screen brain connections based on the Partition Problem in mathematics.  Our ENIGMA Training Program offers a world class Summer School coordinated with other BD2K Centers, worldwide scientific exchanges. Challenge-based Workshops and hackathons to stimulate innovation, and Web Portals to disseminate tools and engage scientists in Big Data science.         PUBLIC HEALTH RELEVANCE: The ENIGMA Center for Worldwide Medicine, Imaging and Genomics is an unprecedented global effort uniting 287 scientists from 125 institutions and all their vast biomedical data, to work on 9 major human brain diseases:  schizophrenia, bipolar disorder, major depression, ADHD, OCD, autism, 22q deletion syndrome, HIV/AIDS and addictions. ENIGMA integrates images from multiple modalities, genomes, connectomes and biomarkers on an unimaginable scale, with new computations to integrate, cluster, and learn from complex biodata types.            ","ENIGMA Center for Worldwide Medicine, Imaging & Genomics",8774373,U54EB020403,"['Acquired Immunodeficiency Syndrome', 'Algorithms', 'Attention deficit hyperactivity disorder', 'Autistic Disorder', 'Big Data', 'Biological Markers', 'Bipolar Disorder', 'Brain', 'Brain Diseases', 'Brain imaging', 'Clinical', 'Collaborations', 'Combinatorics', 'Complex', 'Computational algorithm', 'Computer software', 'Country', 'Data', 'Data Set', 'Diagnosis', 'Diagnostic', 'Disease', 'Drug Targeting', 'Educational workshop', 'Genes', 'Genetic', 'Genome', 'Genomics', 'Goals', 'HIV', 'Human', 'Image', 'Institution', 'Internet', 'Joints', 'Learning', 'Machine Learning', 'Major Depressive Disorder', 'Mathematics', 'Medicine', 'Modality', 'Modeling', 'National Human Genome Research Institute', 'National Institute of Biomedical Imaging and Bioengineering', 'National Institute of Child Health and Human Development', 'National Institute of Drug Abuse', 'National Institute of Mental Health', 'National Institute of Neurological Disorders and Stroke', 'National Institute on Alcohol Abuse and Alcoholism', 'Nature', 'Obsessive-Compulsive Disorder', 'Prognostic Marker', 'Recording of previous events', 'Research', 'Research Infrastructure', 'Schizophrenia', 'Schools', 'Science', 'Scientist', 'Talents', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Work', 'addiction', 'base', 'chromosome 22q deletion syndrome', 'computer science', 'innovation', 'multidisciplinary', 'multitask', 'neuroimaging', 'novel', 'novel strategies', 'outcome forecast', 'public health relevance', 'screening', 'success', 'tool', 'working group']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,U54,2014,2087641,0.00191475177066529
"Developing Cloud-based tools for Big Neural Data     DESCRIPTION (provided by applicant): Big data has the potential to dramatically advance the electrophysiology biodata sciences in similar ways that it has transformed Genetics. Differences between these two areas dictate separate approaches to apply Big Data tools, and methods in order to provide successful assets to the research community. For one, neural datasets are very heterogeneous by nature. The data is difficult to interpret without knowing specifics about the data acquisition protocol, the experimental paradigm and the physiological state of the recorded subject. Many neural datasets are complemented with complex meta-data sets, which should be an integral component in any effort to integrate and share these data with other researchers. The goal of this project is to develop novel, generalizable Big Data tools to facilitate cloud-base analysis of complex multi-scale neural data. Epilepsy research will be used as a specific use case to guide the development of the tools. A cohort of established senior investigators performing epilepsy research will use and validate these tools in their laboratories. Epilepsy research is currently limited by its narrow focus on single models (animal or human) in individual centers and laboratories. Just as Genetics was revolutionized through Big Data techniques, so too can Epilepsy research be transformed through novel approaches to standardize, share, and mine data across groups of investigators. Over the past several years I have co-developed a NINDS funded cloud-based data platform, ://ieeg.org, giving me a central role in developing Big Data solutions for neural data, such as customized data sharing, large-scale cloud-based data analysis, and search and interrogation techniques for complex data and metadata. My scientific objectives for this project are: (1) to develop generalizable tools to curate, analyze, and interrogate multi-scale neural data, and (2) to create a platform that will galvanize a research community focused on sharing data, and methods to advance Big Data research in the basic and translational neurosciences. Equally important to this proposal, I present a training plan to prepare me for an academic career focused on Big Data in the neurosciences. This plan supplements my background in bioengineering and statistical modeling of neural data with broader data-science expertise in data integration and machine learning, and deeper domain knowledge of the clinical neurosciences. I have assembled a group of collaborators, basic investigators and clinician scientists, who will use the tools developed in this project to analyze and validate their data and methods. I will use the results of this project as the foundation for a R01 Grant application, in which I will expand the developed platform and tools to target other research domains (TBI, Emergency Care, Cardiac), as well as integrate other data-modalities such as Imaging, and Genomics. OMB No. 0925-0001/0002 (Rev. 08/12 Approved Through 8/31/2015) Page Continuation Format Page         PUBLIC HEALTH RELEVANCE: The goal of this proposal is to advance Big Data research in the neurosciences by developing tools and techniques to interrogate electrophysiology data sets from animal models of human neurological disorders. Development of these tools requires close collaboration between domain experts in Neuroscience, Machine Learning, Statistics and Computer Science. When developed, this platform and these tools will allow investigators to share, collaborate, annotate, standardize and analyze large, complex, multiscale data sets that are a crucial first step in advancing this field.            ",Developing Cloud-based tools for Big Neural Data,8830141,K01ES025436,"['Animal Model', 'Applications Grants', 'Area', 'Big Data', 'Biomedical Engineering', 'Cardiac', 'Clinical', 'Collaborations', 'Communities', 'Complement', 'Complex', 'Computational Technique', 'Data', 'Data Analyses', 'Data Provenance', 'Data Set', 'Electrophysiology (science)', 'Emergency Care', 'Epilepsy', 'Evaluation', 'Feedback', 'Fostering', 'Foundations', 'Funding', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Image', 'Incentives', 'Individual', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Machine Learning', 'Metadata', 'Methods', 'Mining', 'Modality', 'National Institute of Neurological Disorders and Stroke', 'Nature', 'Neurosciences', 'Organism', 'Performance', 'Physiological', 'Process', 'Protocols documentation', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Series', 'Solutions', 'Standardization', 'Statistical Models', 'Techniques', 'Time', 'Training', 'base', 'career', 'cloud based', 'cohort', 'comparative', 'computer science', 'data acquisition', 'data integration', 'data management', 'data mining', 'data sharing', 'improved', 'nervous system disorder', 'novel', 'novel strategies', 'relating to nervous system', 'statistics', 'tool', 'tool development', 'translational neuroscience']",NIEHS,UNIVERSITY OF PENNSYLVANIA,K01,2014,192201,-0.008361971229818311
"Protect Privacy of Healthcare Data in the Cloud     DESCRIPTION (provided by applicant): Cloud computing is gain popularity due to its cost-effective storage and computation. There are few studies on how to leverage cloud computing resources to facilitate healthcare research in a privacy preserving manner. This project proposes an advanced framework that combines rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment. Comparing to traditional centralized data anonymization, we are facing major challenges such as lack of global knowledge and the difficulty to enforce consistency. We adopt differential privacy as our privacy criteria and will leverage homomorphic encryption and Yao's garbled circuit protocol to build secure yet scalable information exchange to overcome the barrier.             Project narrative Sustainability and privacy are critical concerns in handling large and growing healthcare data. New challenges emerge as new paradigms like cloud computing become popular for cost-effective storage and computation. This project will develop an advanced framework to combine rigorous privacy protection and encryption techniques to facilitate healthcare data sharing in the cloud environment.",Protect Privacy of Healthcare Data in the Cloud,8810023,R21LM012060,"['Adopted', 'Algorithms', 'Cloud Computing', 'Communities', 'Data', 'Data Analyses', 'Data Set', 'Environment', 'Goals', 'Health Services Research', 'Healthcare', 'Individual', 'Institution', 'Intuition', 'Knowledge', 'Machine Learning', 'Modeling', 'Privacy', 'Protocols documentation', 'Provider', 'Records', 'Research Infrastructure', 'Research Personnel', 'Secure', 'Security', 'Services', 'Societies', 'Techniques', 'Technology', 'Trust', 'Work', 'base', 'computing resources', 'cost', 'cost effective', 'data sharing', 'encryption', 'light weight', 'novel', 'predictive modeling', 'research study', 'tool']",NLM,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R21,2014,183155,0.007421434757105086
"Developing and applying information extraction resources and technology to create     DESCRIPTION (provided by applicant): Building on 8 years of highly productive work in technology development that included the creation of the Colorado Richly Annotated Full Text corpus (CRAFT), we hypothesize that text mining resources and methods are approaching the level of maturity required to productively process a significant proportion of the full text biomedical literature to create a well-represented formal knowledge base of molecular biology. We propose a detailed, integrated plan to achieve this long-standing goal. Success in this effort will make possible a transformative new way for the biomedical research community to identify access and integrate existing knowledge, breaking down disciplinary boundaries and other silos that have kept scientists from fully exploiting relevant prior results in their research.      Our successes in the prior funding period broadened the applicability of biomedical concept identification systems to a much wider set of tasks, demonstrating the ability to target multiple community-curated ontologies in text mining, and generate scientifically significant insights from the results. The proposed work would take advantage of the resources we produced to transcend several of the limitations of previous efforts. We propose innovative new approaches to formal knowledge representation and to characterizing relationships between textual elements and semantic content. We will design, implement and evaluate computational systems that have the potential to transform enormous text collections into semantically rich, logic-based, standards-compliant, formal representations of biomedical knowledge with clearly identified provenance. The resulting representations will express complex assertions about a very wide range of entities, processes, qualities, and, most importantly, their specific relationships with one another.              Program Director/Principal Investigator (Last, First, Middle): Hunter, Lawrence E. Project narrative  This project will affect public health by increasing the access of physicians, researchers, and the general public to highly targeted information from published research and electronic health records. PHS 398/2590 (Rev. 06/09) Page Continuation Format Page",Developing and applying information extraction resources and technology to create,8694375,R01LM008111,"['Adrenergic beta-Antagonists', 'Affect', 'Biomedical Research', 'Collection', 'Colorado', 'Communities', 'Complex', 'Data', 'Electronic Health Record', 'Elements', 'Funding', 'General Population', 'Goals', 'Gold', 'Guidelines', 'Heart failure', 'Knowledge', 'Linguistics', 'Literature', 'Logic', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Ontology', 'Output', 'Pattern', 'Performance', 'Physicians', 'Principal Investigator', 'Process', 'Public Health', 'Publishing', 'Research', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'System', 'Techniques', 'Technology', 'Text', 'Transcend', 'Work', 'base', 'design', 'improved', 'information organization', 'innovation', 'insight', 'knowledge base', 'novel strategies', 'programs', 'success', 'syntax', 'technology development', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2014,746561,-0.01929103910734604
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci     DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics.              n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8631080,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2014,250003,0.0017273315205544508
"In silico identification of phyto-therapies     DESCRIPTION (provided by applicant): Plants have been acknowledged as forming the basis of medicines dating back to the most ancient civilizations. To complement synthetic drug discovery processes, there remains a significant opportunity for identifying potential new therapies from plant-based sources (""phyto-therapies""). Current approaches used for the discovery of potential phyto-therapies are laborious, time-consuming, and mostly manual. The increased availability of ethnobotanical and biomedical knowledge in digital formats suggests that there may be the potential to leverage automated techniques to facilitate the phyto-therapy discovery process. The long-term goal of this initiative is thus to develop a semantically integrated framework that could be used to identify and validate potential phyto-therapies embedded within ethnobotanical and biomedical knowledge sources, and thus encourage the conservation of this knowledge and biodiversity. The overall project is built around three major aims, which are to: (1) develop a standards-driven gold standard that can be used for benchmarking automated phyto-therapy identification approaches; (2) develop an automated approach to identify potential phyto-therapies from digitized biodiversity literature (Biodiversity Heritage Library), biomedical literature citations (MEDLINE) or digital full-text (PubMed Central), genomic (GenBank), clinical trial (ClinicalTrials.gov), and chemical (PubChem) resources; and (3) leverage vector space modeling techniques to predict the relevance of potential phyto-therapies. The success of this endeavor will set the stage for the translation of a growing, but currently disjointed, evidence-base of medicinal plant knowledge into tools for the elucidation of potential phyto-therapies. Furthermore, through achieving these aims, this project will also establish a first-of- its-kind in silico platform that could be extended to identify additional therapeutics from a broad spectrum of biodiversity sources. The core aspects of this project will build on experience with developing computational techniques to bridge biodiversity and biomedical knowledge, including those that have been pioneered by the research team.      This project will bring together biomedical informatics, library science, and ethnobotany experience and expertise from two institutions: the University of Vermont and The New York Botanical Garden. The multi- institutional and multi-PI aspects of this project support the feasibility of the proposed project aims and will furthermore enable the load-balancing of essential tasks such that they may meet the proposed milestones set for each aim. To this end, the success of the proposed endeavor will be built on a foundation of experiences in gathering ethnobotanical knowledge, analyzing and linking biodiversity and biomedical knowledge sources, and developing approaches for systematically annotating corpora for subsequent purposes in support of natural language processing and data mining pursuits.                RELEVANCE TO PUBLIC HEALTH  The identification of potential therapies is a significant area of research with direct public health implications. As such, the integration of knowledge from traditionally disjoint knowledge sources may offer a more holistic view of the ethnobotanical and biomedical research knowledge that can support the development of new disease treatment regimens.  ",In silico identification of phyto-therapies,8749705,R01LM011963,"['Address', 'Affect', 'Archives', 'Area', 'Automated Annotation', 'Back', 'Benchmarking', 'Biodiversity', 'Biological Factors', 'Biomedical Research', 'Books', 'Botanicals', 'Chemicals', 'Civilization', 'Clinical Trials', 'Complement', 'Computational Technique', 'Computer Simulation', 'Data', 'Data Set', 'Development', 'Disease', 'Equilibrium', 'Ethnobotany', 'Evaluation', 'Expert Opinion', 'Foundations', 'Future', 'Genbank', 'Genomics', 'Goals', 'Gold', 'HIV', 'Hepatitis', 'Individual', 'Institution', 'Island', 'Knowledge', 'Libraries', 'Library Science', 'Link', 'Literature', 'MEDLINE', 'Manuals', 'Medicinal Plants', 'Medicine', 'Methods', 'Modeling', 'Names', 'Natural Language Processing', 'New York', 'Ontology', 'Peer Review', 'Performance', 'Plants', 'Primary Health Care', 'Process', 'PubChem', 'PubMed', 'Public Health', 'Publishing', 'Relative (related person)', 'Research', 'Resources', 'Review Literature', 'Samoan', 'Source', 'Space Models', 'Staging', 'Surveys', 'System', 'T-Lymphocyte', 'Techniques', 'Text', 'Therapeutic', 'Therapeutic Uses', 'Time', 'Toxic effect', 'Translations', 'Treatment Protocols', 'Trees', 'Universities', 'Vermont', 'base', 'biomedical informatics', 'clinical application', 'computer infrastructure', 'data mining', 'digital', 'drug candidate', 'drug discovery', 'evidence base', 'experience', 'indexing', 'literature citation', 'meetings', 'prostratin', 'success', 'synthetic drug', 'tool', 'vector']",NLM,UNIVERSITY OF VERMONT & ST AGRIC COLLEGE,R01,2014,389869,0.01295834131045745
"Large-Scale Semiparametric Graphical Models with Applications to Neuroscience     DESCRIPTION: The objective of this proposal is to develop and theoretically evaluate a unified set of statistical, computational, and software tools to address data mining and discovery science challenges in the analysis of existing vast amounts of publicly available neuroimaging data. In particular, we propose to develop scalable and robust semiparametric solutions for high-throughput estimation of resting-state brain connectivity networks, both at the individual and population levels, with the flexibility of incorporating covariate information.  The work will contribute meaningfully to the theory and methods for large-scale semiparametric graphical models and will apply these methods to the largest collections of resting-state fMRI data available. The proposed methods and theory include key directions of research for brain network estimation and mining. First, we pro- pose novel methods for subject-specific network estimation, such as would be needed for biomarker development in functional brain imaging. Secondly, we define and propose to evaluate and implement methods for studying population-level graphs, which study collections of graphs. Thirdly, we propose the use of estimated graphs in predictive modeling. Finally, all of these methods will have complementary software and web services development. Most notably, the idea of population graphs allows for the creation of functional brain network atlases.  In summary, the work of this proposal will result in a unified framework for the analysis of modern neuroimaging data via graphical models. Our methods will further be agnostic to intricacies of the technology, thus making it portable across settings and applicable outside of the field of functional brain imaging. The methods will be carefully evaluated via theory, simulation and data-based application evidence.         PUBLIC HEALTH RELEVANCE: Modern neuroimaging data are often Big, Complex, Noisy and Dependent. We propose a systematic attempt on methodological development for the largely unexplored but practically important problem of network estimation and mining based on neuroimaging data. Our proposed work represents a significant step forward over the current methodology and has the potential to be applied to analyze a wide range of scientific problems beyond brain imaging data analysis.                ",Large-Scale Semiparametric Graphical Models with Applications to Neuroscience,8611397,R01MH102339,"['Address', 'Algorithms', 'Atlases', 'Attention deficit hyperactivity disorder', 'Big Data', 'Biological Markers', 'Brain', 'Brain Diseases', 'Brain imaging', 'Characteristics', 'Collection', 'Complex', 'Computational algorithm', 'Computer software', 'Data', 'Data Aggregation', 'Data Analyses', 'Data Set', 'Databases', 'Dependence', 'Development', 'Disease', 'Documentation', 'Exhibits', 'Functional Magnetic Resonance Imaging', 'Graph', 'Heterogeneity', 'Image', 'Individual', 'Internet', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modality', 'Modeling', 'Neurosciences', 'Pattern', 'Population', 'Population Study', 'Process', 'Rest', 'Sampling', 'Science', 'Signal Transduction', 'Site', 'Software Tools', 'Solutions', 'Statistical Methods', 'Tail', 'Technology', 'Work', 'abstracting', 'base', 'brain research', 'cloud based', 'data mining', 'flexibility', 'interest', 'neuroimaging', 'novel', 'predictive modeling', 'psychologic', 'public health relevance', 'simulation', 'theories', 'user friendly software', 'web services']",NIMH,PRINCETON UNIVERSITY,R01,2014,378911,0.004115823965635059
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.             Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8727093,R00LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,NORTHWESTERN UNIVERSITY AT CHICAGO,R00,2014,223898,-0.022652024018908062
"Statistical methods for large and complex databases of ultra-high-dimensional     DESCRIPTION: Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences.         PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8738735,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2014,343683,-0.004110403771503552
"Mobility Data Integration to Insight     DESCRIPTION (provided by applicant): Mobility is essential for human health. Regular physical activity helps prevent heart disease and stroke, relieves symptoms of depression, and promotes weight loss. Unfortunately, many conditions, such as cerebral palsy, osteoarthritis, and obesity, limit mobility at an enormous personal and societal cost. While vast amounts of data are available from hundreds of research labs and millions of smartphones, there is a dearth of methods for analyzing this massive, heterogeneous dataset.  We propose to establish the National Center for Mobility Data Integration to Insight (the Mobilize Center) to overcome the data science challenges facing mobility big data and biomedical big data in general. Our preliminary work identified four bottlenecks in data science, which drive four Data Science Research Cores.  The Cores include Biomechanical Modeling, Statistical Learning, Behavioral and Social Modeling, and Integrative Modeling and Prediction. Our Cores will produce novel methods to integrate diverse modeling modalities and gain insight from noisy, sparse, heterogeneous, and time-varying big data. Our data-sharing consortia, with clinical, research, and industry partners, will provide mobility data for over ten million people.  Three Driving Biomedical Problems will focus and validate our data science research.  The Mobilize Center will disseminate our novel data science tools to thousands of researchers and create a sustainable data-sharing consortium. We will train tens of thousands of scientists to use data science methods in biomedicine through our in-person and online educational programs. We will establish a cohesive, vibrant, and sustainable National Center through the leadership of an experienced executive team and will help unify the BD2K consortia through our Biomedical Computation Review publication and the Simtk.org resource portal.  The Mobilize Center will lay the groundwork for the next generation of data science systems and revolutionize diagnosis and treatment for millions of people affected by limited mobility.         PUBLIC HEALTH RELEVANCE:  Regular physical activity is essential for human health, yet a broad range of conditions impair mobility. This project will transform human movement research by developing tools for data analysis and creating software that will advance research to prevent, diagnose, and reduce impairments that limit human movement.            ",Mobility Data Integration to Insight,8775015,U54EB020405,"['Affect', 'Area', 'Automobile Driving', 'Behavioral', 'Behavioral Model', 'Big Data', 'Biomechanics', 'Biomedical Computing', 'Biomedical Research', 'Body Weight decreased', 'Cerebral Palsy', 'Child', 'Classification', 'Clinical Research', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Degenerative polyarthritis', 'Diabetes Mellitus', 'Diagnosis', 'Educational workshop', 'Elderly', 'Ethics', 'Exercise', 'Fellowship', 'Fostering', 'Gait', 'Health', 'Heart Diseases', 'Human', 'Impairment', 'Individual', 'Injury', 'Joints', 'Leadership', 'Limb structure', 'Machine Learning', 'Medical center', 'Methods', 'Mission', 'Modality', 'Modeling', 'Monitor', 'Movement', 'NCI Scholars Program', 'Nature', 'Obesity', 'Operative Surgical Procedures', 'Outcome', 'Overweight', 'Pathology', 'Persons', 'Physical activity', 'Prevention', 'Problem Solving', 'Public Health', 'Publications', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Running', 'Science', 'Scientist', 'Stroke', 'System', 'Techniques', 'Testing', 'Time', 'Training', 'Visit', 'Walking', 'Work', 'base', 'clinical decision-making', 'cognitive function', 'cohesion', 'cost', 'data integration', 'data modeling', 'data sharing', 'depressive symptoms', 'experience', 'flexibility', 'improved', 'industry partner', 'insight', 'models and simulation', 'next generation', 'novel', 'novel strategies', 'prevent', 'programs', 'public health relevance', 'role model', 'sensor', 'social', 'social model', 'tool']",NIBIB,STANFORD UNIVERSITY,U54,2014,209258,0.007216232451062741
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8628132,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'FaceBase', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2014,525880,0.022610822738410575
"Large-scale Automated Synthesis of Functional Neuroimaging Data     DESCRIPTION (provided by applicant): The explosive growth of the human neuroimaging literature has led to major advances in understanding of normal and abnormal human brain function, but has also made aggregation and synthesis of neuroimaging findings increasingly difficult. The goal of this project is to develop an automated software platform for large-scale synthesis of human functional neuroimaging studies. Our work builds directly on an existing software platform (NeuroSynth) and involves key extensions and improvements that focus on (i) aggregation, (ii) coding, (iii) synthesis, and (iv) sharing of functional neuroimaging data. In Aim 1, we will use computational linguistics and bioinformatics data mining techniques to develop new algorithms for automatically extracting activation foci and associated metadata from published neuroimaging articles. In Aim 2, we will use topic-modeling techniques such as Latent Dirichlet Analysis in combination with existing cognitive ontologies such as the Cognitive Atlas to develop structured representations of automatically extracted neuroimaging data. In Aim 3, we will improve the meta-analysis and classification capacities of our existing platform by implementing a state-of- the-art hierarchical Bayesian meta-analysis method recently developed by the research team. Finally, in Aim 4, we will develop a state-of-the-art web interface (://neurosynth.org) that supports real-time, in-browser access to the data, results, and tools produced in Aims 1 - 3. Realizing these objectives will introduce powerful new tools for organizing and synthesizing the neuroimaging literature on an unprecedented scale. These tools will be freely and publicly available to anyone with an internet connection, enabling rapid and efficient application to a broad range of clinical and basic research applications.          Functional neuroimaging techniques such as fMRI have opened a new frontier in efforts to investigate and understand the neural mechanisms of normal and abnormal cognition. However, the rapidly expanding scope of the literature makes distillation and synthesis of brain imaging findings increasingly challenging. The goal of this project is to develop a new software platform for automated aggregation, synthesis, and sharing of published neuroimaging results, with the potential to advance understanding of mechanisms underlying mental health disorders.                ",Large-scale Automated Synthesis of Functional Neuroimaging Data,8672688,R01MH096906,"['Algorithms', 'Atlases', 'Basic Science', 'Bayesian Method', 'Bayesian Modeling', 'Bioinformatics', 'Biometry', 'Brain', 'Brain imaging', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Communities', 'Computational Linguistics', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Development', 'Ensure', 'Environment', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Growth', 'High Performance Computing', 'Human', 'Individual', 'Interdisciplinary Study', 'Internet', 'Joints', 'Journals', 'Language', 'Literature', 'Manuals', 'Maps', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Neurosciences', 'Ontology', 'Paper', 'Performance', 'Population', 'Process', 'Publishing', 'Qualifying', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Specificity', 'Structure', 'Techniques', 'Text', 'Time', 'Training', 'Validation', 'Work', 'awake', 'base', 'cognitive function', 'data mining', 'frontier', 'improved', 'information organization', 'interoperability', 'knowledge base', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'open source', 'psychologic', 'theories', 'tool', 'web interface']",NIMH,"UNIVERSITY OF TEXAS, AUSTIN",R01,2014,603330,0.003300126942238836
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8597446,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2014,533554,0.028248677935962842
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8737919,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2014,2941548,0.03973768830881846
"Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation DESCRIPTION (provided by applicant): Multi-atlas label fusion (MALF) is a powerful new technology that can automatically detect and label anatomical structures in biomedical images. It is arguably the most successful general-purpose automatic image segmentation technique ever developed. Automatic segmentation is in high demand in clinical and research applications of medical imaging, since segmentation forms a crucial step towards extracting quantitative information from imaging data, and since manual and semi-automatic approaches are ill suited for today's increasingly large and complex imaging datasets. Despite a number of papers that demonstrated outstanding performance of MALF methods across a range of biomedical imaging applications, the broader biomedical imaging research community has been slow to adopt this technique. This can be explained by multiple factors, including the technique's high computational demands, lack of a turnkey software implementation, as well as scarcity of validation in clinical imaging datasets and in the presence of extensive pathology. The present application seeks to remove these barriers and to enable a broad range of clinicians and biomedical researchers to take advantage of MALF technology. It builds on our strong track record of innovation in the MALF field, including a novel redundancy-correcting MALF technique that led in segmentation grand challenges in the past two years. Aim 1 seeks to improve the computational performance of MALF by replacing dense deformable image registration, by far the most time consuming component of MALF, with faster and less constrained sparse registration strategies. We hypothesize that this will not only reduce the computational cost of MALF, but will also make it more robust to anatomical variability, in particular enabling its use for tumor and lesion segmentation. Aim 2 proposes algorithmic extensions to MALF that support automatic segmentation of dynamic and multi-modality imaging datasets, which have been largely overlooked in the MALF literature. Aim 3 will develop a turnkey open-source implementation of MALF methodology. Taking advantage of cloud computing technology, this software will allow users with minimal image processing expertise to take full advantage of MALF segmentation on their desktop. Aim 3 will also provide a set of publicly available atlases and the means for users to build new custom atlas sets from their own data. Aim 4 will perform extensive evaluation of the new methods and software in challenging real-world clinical imaging data, including brain and cardiac imaging. As part of this evaluation, we will quantify how well our MALF approach and competing techniques generalize to novel imaging datasets with heterogeneity in acquisition parameters and clinical phenotypes. PUBLIC HEALTH RELEVANCE: This research will make it possible for a wide community of researchers who collect and analyze medical imaging data to take advantage of a new class of computer algorithms that very accurately label and measure anatomical structures and pathological formations in medical images. By offering more accurate image-derived measurements, the project promises to improve the accuracy of diagnosis, reduce the costs of biomedical re- search studies and pharmaceutical trials, and accelerate scientific discovery.",Adaptive Large-Scale Framework for Automatic Biomedical Image Segmentation,8761531,R01EB017255,"['Address', 'Adopted', 'Affect', 'Algorithms', 'Atlases', 'Biomedical Research', 'Brain', 'Build-it', 'Cardiac', 'Clinical', 'Clinical Data', 'Clinical Research', 'Cloud Computing', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computer software', 'Consensus', 'Custom', 'Data', 'Data Set', 'Dementia', 'Diagnostic', 'Evaluation', 'Gold', 'Health', 'Heterogeneity', 'High Performance Computing', 'Hippocampus (Brain)', 'Image', 'Image Analysis', 'International', 'Joints', 'Label', 'Lead', 'Learning', 'Lesion', 'Literature', 'Magnetic Resonance Imaging', 'Manuals', 'Measurement', 'Measures', 'Medial', 'Medical Imaging', 'Medical Research', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Multiple Sclerosis Lesions', 'Myocardium', 'Paper', 'Pathology', 'Patient Care', 'Performance', 'Pharmacologic Substance', 'Public Domains', 'Research', 'Research Infrastructure', 'Research Personnel', 'S-nitro-N-acetylpenicillamine', 'Scheme', 'Services', 'Structure', 'Techniques', 'Technology', 'Temporal Lobe', 'Temporal Lobe Epilepsy', 'Time', 'Training', 'Ultrasonography', 'Uncertainty', 'Validation', 'Work', 'aortic valve', 'base', 'bioimaging', 'clinical application', 'clinical phenotype', 'clinical practice', 'cloud based', 'cohort', 'cost', 'diagnostic accuracy', 'experience', 'image processing', 'image registration', 'imaging Segmentation', 'imaging modality', 'improved', 'innovation', 'interest', 'new technology', 'novel', 'open source', 'outreach', 'research study', 'success', 'tool', 'tumor']",NIBIB,UNIVERSITY OF PENNSYLVANIA,R01,2014,611476,0.00706251108463568
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8474789,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2013,486484,-0.009298802953753202
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8484438,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'screening', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2013,202118,-0.01456657100676902
"Brain Science Computer Cluster     DESCRIPTION (provided by applicant): Computational requirements of contemporary brain science research often exceed financial and resource management limits of individual investigator laboratories. Many contemporary neuroscience research projects require analysis of large data sets with advanced statistical methods and anatomical reconstruction techniques. These methods require high speed computational and graphics engines operating in a multiple processor environments equipped with large capacity, high-speed storage devices. A limitation in the Brown brain science effort at understanding neural processing is the lack of a readily accessible high-speed computational resource. A central computational resource based on a unified cluster of contemporary Linux CPUs and GPUs will serve the computational needs of a core group of brain science investigators at Brown without compromising individual access common to stand-alone workstations. The requested computer cluster has system software that automatically balances CPU and GPU usage, thereby ensuring maximum access to the computational resource for all users. Intensive 3D graphics are off-loaded either to GPUs or to client workstations, thereby further reducing the central computational load. Commercial or open-source software with an open operating environment will be used for analysis using standard and novel statistical and machine learning approaches to assess significance of large data sets. This proposal details the architecture and benefits of a contemporary computational resource for the major and minor users, and more generally the Brown brain science community. The resource was designed to fill immediate and near-term computational and storage needs of a core group of Brown brain scientists. The system can be readily expansion as needs, either computational, storage, or new users, arise. Expansion of the existing core investigators group can occur easily since the computational power or storage capacity of the system can be readily enhanced at relatively low cost. The flexible nature of the system will serve a variety of research needs of the Brown brain science community. The computational resource is expected to bring together researchers at Brown working on the common problem of neural processing.             n/a",Brain Science Computer Cluster,8447697,S10OD016366,"['Architecture', 'Brain', 'Client', 'Communities', 'Computer software', 'Data Set', 'Devices', 'Ensure', 'Environment', 'Equilibrium', 'Individual', 'Laboratories', 'Linux', 'Machine Learning', 'Methods', 'Minor', 'Nature', 'Neurosciences Research', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Speed', 'Statistical Methods', 'System', 'Techniques', 'Work', 'base', 'computer cluster', 'computing resources', 'cost', 'design', 'flexibility', 'novel', 'open source', 'reconstruction', 'relating to nervous system', 'software systems']",OD,BROWN UNIVERSITY,S10,2013,599598,0.004571518213533501
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8426190,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'stress disorder', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2013,315672,-0.007726280609322553
"BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci     DESCRIPTION (provided by applicant): Ideally, as neuroscientists collect terabytes of image stacks, the data are automatically processed for open access and analysis. Yet, while several labs around the world are collecting data at unprecedented rates- up to terabytes per day-the computational technologies that facilitate streaming data-intensive computing remain absent. Also deploying data-intensive compute clusters is beyond the means and abilities of most experimental labs. This project will extend, develop, and deploy such technologies. To demonstrate these tools, we will utilize them in support of the ongoing mouse brain architecture (MBA) project, which already has amassed over 0.5 petabytes (PBs) of image data. The main computational challenges posed by these datasets are ones of scale. The tasks that follow remain relatively stereotyped across acquisition modalities. Until now, labs collecting data on this scale have been almost entirely isolated, left to ""reinvent the wheel"" for each of these problems. Moreover, the extant solutions are insufficient for a number of reasons: they often include numerous excel spreadsheets that rely on manual data entry, they lack scalable scientific database backends, and they run on ad hoc clusters not specifically designed for the computational tasks at hand. We aim to augment the current state of the art by implementing the following technological advancements into the MBA project pipeline: (1) Data Management will consist of a unified system that automatically captures metadata, launches processing pipelines, and provides quality control feedback in minutes instead of hours. (2) Data Processing tasks will run algorithms ""out-of-core"", appropriate for their computational requirements, including registration, alignment, and semantic segmentation of cell bodies and processes. (3) Data Storage will automatically build databases for storing multimodal image data and extracted annotations learned from the machine vision algorithms. These databases will be spatially co-registered and stored on an optimized heterogeneous compute cluster. (4) Data Access will be automatically available to everyone-including all the image data and data derived products-via Web-services, including 3D viewing, downloading, and further processing. (5) Data Analytics will extend random graph models suitable for multiscale circuit graphs. RELEVANCE (See instructions): Nervous system disorders are responsible for approximately 30% of the total burden of illness in the United States. Whole brain neuroanatomy-available from massive neuroscientific image stacks-is widely believed to be a key missing link in our ability to prevent and treat such illnesses. Thus, this project aims to close this gap via the development and application of BIGDATA tools for management, storage, access, and analytics.              n/a",BIGDATA: Small DCM: ESCA DA Computational infrastructure for massive neurosci,8599834,R01DA036400,"['Algorithms', 'Architecture', 'Brain', 'Cell physiology', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Databases', 'Development', 'Feedback', 'Graph', 'Hand', 'Hour', 'Image', 'Instruction', 'Left', 'Link', 'Machine Learning', 'Manuals', 'Metadata', 'Modality', 'Modeling', 'Multimodal Imaging', 'Mus', 'Neuroanatomy', 'Process', 'Quality Control', 'Running', 'Semantics', 'Solutions', 'Stereotyping', 'Stream', 'System', 'Technology', 'United States', 'Vision', 'burden of illness', 'cluster computing', 'computer infrastructure', 'computerized data processing', 'data management', 'design', 'nervous system disorder', 'neuronal cell body', 'prevent', 'tool', 'web services']",NIDA,COLD SPRING HARBOR LABORATORY,R01,2013,249999,0.0017273315205544508
"Statistical methods for large and complex databases of ultra-high-dimensional  Abstract Medical imaging is a cornerstone of basic science and clinical practice. To discover new mechanisms and markers of disease and their crucial implications for clinical practice, large multi-center imaging studies are acquiring terabytes of complex multi-modality imaging data cross-sectionally and longitudinally over decades. The statistical analysis of data from such studies is challenging due to the complex structure of the imaging data acquired and the ultra-high dimensionality. Furthermore, the heterogeneity of anatomy, pathology, and imaging protocols causes instability and failure of many current state-of-the-art image analysis methods. This grant proposes statistical frameworks for studying populations through biomedical imaging, scalable and robust methods for the identification and accurate quantification of pathology, and analytic tools for the cross-sectional and longitudinal examination of etiology and disease progression. These techniques will be applied to address key goals of the motivating large and multi- center studies of multiple sclerosis and Alzheimer's disease conducted at Johns Hopkins Hospital, the National Institute of Neurological Disorders and Stroke, and across the globe. The project will create methods for uncovering and quantifying brain lesion pathology, incidence, and trajectory. Methods developed under this grant will be targeted towards these neuroimaging goals, but will form the basis for statistical image analysis methods applicable broadly in the biomedical sciences. PUBLIC HEALTH RELEVANCE: This project involves the development of statistical frameworks and methods for the analysis of complex ultra-high-dimensional biomedical imaging. Methods developed are applied to study the clinical management and etiology of multiple sclerosis and Alzheimer's disease longitudinally and cross-sectionally.                ",Statistical methods for large and complex databases of ultra-high-dimensional,8614974,R01NS085211,"['Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Applications Grants', 'Area', 'Attention deficit hyperactivity disorder', 'Basic Science', 'Behavior', 'Brain', 'Brain Pathology', 'Brain imaging', 'Clinical Management', 'Complex', 'Computer software', 'Computing Methodologies', 'Contrast Media', 'Data', 'Data Analyses', 'Databases', 'Development', 'Disease Marker', 'Disease Progression', 'Etiology', 'Failure', 'Goals', 'Grant', 'Heterogeneity', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Incidence', 'Journals', 'Lesion', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Medical Imaging', 'Methodology', 'Methods', 'Modeling', 'Multiple Sclerosis', 'National Institute of Neurological Disorders and Stroke', 'Pathology', 'Population Study', 'Positioning Attribute', 'Protocols documentation', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Sampling', 'Scheme', 'Science', 'Site', 'Solutions', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Models', 'Structure', 'Techniques', 'Technology', 'United States National Institutes of Health', 'Visualization software', 'Work', 'base', 'bioimaging', 'clinical practice', 'design', 'falls', 'imaging Segmentation', 'imaging modality', 'member', 'neuroimaging', 'next generation', 'open source', 'public health relevance', 'skills', 'tool', 'white matter']",NINDS,UNIVERSITY OF PENNSYLVANIA,R01,2013,373406,-0.004110403771503552
"Transcriptomics of Tuberculosis Latency and Reactivation in Primates DESCRIPTION (provided by applicant): Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, in a quiescent state. These bacilli are able to reactivate and cause pulmonary TB, when the immune system is compromised. Hence, a complete understanding of TB latency and reactivation is required for the effective control of TB. The research models and tools necessary to perform these studies are now available. Nonhuman Primates (NHPs) are excellent models of TB, especially to study the progression of experimental infection to latency, and to study the pathology and biology of granulomatous lesions - the hallmarks of TB infections. We have established a model of human TB, by exposing NHPs to true Mtb aerosols. While many research groups focus on the bacterial factors of latency and reactivation, we would like to leverage our highly tractable model to identify host signatures and mediators of this process. We show that pro-inflammatory immune signaling pathways, initially induced in NHP TB lesions, are overwhelmingly silenced over the course of next several weeks. This transcriptional reprogramming could be a host response to changes in bacterial replication and physiology. Further, these responses could reflect the efforts of the pathogen to prevent excessive immunopathology during the infection of lungs. The central hypothesis of our proposal is that host granuloma responses can be used to predict latent and reactivation TB. We propose to perform a systematic study of the ""transcriptome"" and the ""miRNAome"" of NHP lung lesions. Temporal profiles will be obtained from NHPs infected with a low-dose of Mtb aerosols, accurately modeling long-term latent infection. Profiles will also be obtained from NHPs in which latent TB is reactivated by simian AIDS. These system-wide profiles, in conjugation with the clinical, microbiological and immunological data obtained from infected NHPs will generate statistical learning algorithms and mixed effects computational models of latent and reactivation TB. The relevance of some of the most informative set of genetic predictors available from the data collected will be tested back in both the NHP model, as well as in human patients. The expression profiles of CCL24, CCL25 and CCL27 show negative correlation with all other chemokine ligands and receptors in primate TB granulomas. The expression of these three chemokines is significantly increased in late, rather than early lesions. We hypothesize that these chemokines are important for the long-term maintenance of primate lesions harboring latent Mtb bacilli. The expression of LAG3 was induced more than 40-fold in early primate lesions relative to late ones. LAG3 is a novel marker of Treg cells. We hypothesize that LAG3 is responsible for negatively regulating protective immune responses generated by effector T cells in primate TB lesions. The expression of ""latency"" specific genes CCL24/25/27 and the ""active-TB: specific gene LAG3 will be silenced in NHPs using a novel lipidated-siRNA nanoparticle based approach. The progression of latent disease and its immunological and molecular correlates will then be studied in these animals. Finally, the expression of an immune response to these and other ""latency"" and ""reactivation""- specific profiles will be determined in human patients of latent and active TB, as well as TB/AIDS co- infected patients. These systems-biology studies will likely exponentially enhance our understanding of TB latency and reactivation in a host that mimics both TB and AIDS in the closest possible manner to humans. Eventually, these advances may empower clinicians better to detect and treat latent TB. PUBLIC HEALTH RELEVANCE: Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised. Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation. Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced in NHP TB lesions, but silence over the course of several weeks. This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model. Towards this, we have assembled a highly diverse and collaborative team including microbiologists, aerobiologists, bioinformaticians, mathematicians/statisticians, veterinarians, veterinary pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.",Transcriptomics of Tuberculosis Latency and Reactivation in Primates,8528702,R01HL106790,"['Acquired Immunodeficiency Syndrome', 'Aerosols', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Automobile Driving', 'Bacillus (bacterium)', 'Back', 'Biological', 'Biological Process', 'Biology', 'Biopsy', 'CCL24 gene', 'CCL25 gene', 'CCL27 gene', 'Caring', 'Cells', 'Clinical', 'Communicable Diseases', 'Complex', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Dose', 'Emergency Situation', 'Equilibrium', 'Excision', 'Exhibits', 'Focus Groups', 'Gene Expression Profile', 'Gene Silencing', 'Genes', 'Genetic', 'Genetic Models', 'Granuloma', 'Granulomatous', 'Growth', 'HIV', 'Health', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Infection', 'Inflammatory', 'Lesion', 'Life', 'Ligands', 'Lung', 'Macaca mulatta', 'Machine Learning', 'Maintenance', 'Mediator of activation protein', 'Metadata', 'Methodology', 'MicroRNAs', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'Organogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physiology', 'Primates', 'Process', 'Proteins', 'Pulmonary Tuberculosis', 'RNA Interference', 'Regulatory T-Lymphocyte', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'SIV', 'Sampling', 'Signal Pathway', 'Simian Acquired Immunodeficiency Syndrome', 'Small Interfering RNA', 'Statistical Models', 'System', 'Systems Biology', 'T-Lymphocyte', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transcript', 'Tuberculosis', 'Vaccines', 'Veterinarians', 'base', 'chemokine', 'combat', 'empowered', 'genome-wide', 'immunopathology', 'in vivo', 'innovation', 'latent infection', 'nanoparticle', 'nonhuman primate', 'novel', 'novel marker', 'pathogen', 'prevent', 'pulmonary granuloma', 'reactivation from latency', 'receptor', 'respiratory', 'response', 'tool', 'transcriptomics', 'tuberculosis granuloma']",NHLBI,TULANE UNIVERSITY OF LOUISIANA,R01,2013,696467,0.011845740895850668
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral No abstract available  Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8528719,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Chips', 'Genes', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'genome analysis', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2013,526738,-0.02515013012071489
"Large-scale Automated Synthesis of Functional Neuroimaging Data     DESCRIPTION (provided by applicant): The explosive growth of the human neuroimaging literature has led to major advances in understanding of normal and abnormal human brain function, but has also made aggregation and synthesis of neuroimaging findings increasingly difficult. The goal of this project is to develop an automated software platform for large-scale synthesis of human functional neuroimaging studies. Our work builds directly on an existing software platform (NeuroSynth) and involves key extensions and improvements that focus on (i) aggregation, (ii) coding, (iii) synthesis, and (iv) sharing of functional neuroimaging data. In Aim 1, we will use computational linguistics and bioinformatics data mining techniques to develop new algorithms for automatically extracting activation foci and associated metadata from published neuroimaging articles. In Aim 2, we will use topic-modeling techniques such as Latent Dirichlet Analysis in combination with existing cognitive ontologies such as the Cognitive Atlas to develop structured representations of automatically extracted neuroimaging data. In Aim 3, we will improve the meta-analysis and classification capacities of our existing platform by implementing a state-of- the-art hierarchical Bayesian meta-analysis method recently developed by the research team. Finally, in Aim 4, we will develop a state-of-the-art web interface (://neurosynth.org) that supports real-time, in-browser access to the data, results, and tools produced in Aims 1 - 3. Realizing these objectives will introduce powerful new tools for organizing and synthesizing the neuroimaging literature on an unprecedented scale. These tools will be freely and publicly available to anyone with an internet connection, enabling rapid and efficient application to a broad range of clinical and basic research applications.          Functional neuroimaging techniques such as fMRI have opened a new frontier in efforts to investigate and understand the neural mechanisms of normal and abnormal cognition. However, the rapidly expanding scope of the literature makes distillation and synthesis of brain imaging findings increasingly challenging. The goal of this project is to develop a new software platform for automated aggregation, synthesis, and sharing of published neuroimaging results, with the potential to advance understanding of mechanisms underlying mental health disorders.                ",Large-scale Automated Synthesis of Functional Neuroimaging Data,8523981,R01MH096906,"['Algorithms', 'Atlases', 'Basic Science', 'Bioinformatics', 'Biometry', 'Brain', 'Brain imaging', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Development', 'Ensure', 'Environment', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Growth', 'High Performance Computing', 'Human', 'Individual', 'Interdisciplinary Study', 'Internet', 'Joints', 'Journals', 'Language', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Neurosciences', 'Ontology', 'Paper', 'Performance', 'Population', 'Process', 'Publishing', 'Qualifying', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Specificity', 'Structure', 'Techniques', 'Text', 'Time', 'Training', 'Validation', 'Work', 'awake', 'base', 'cognitive function', 'data mining', 'frontier', 'improved', 'information organization', 'interoperability', 'knowledge base', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'open source', 'psychologic', 'theories', 'tool', 'web interface']",NIMH,UNIVERSITY OF COLORADO,R01,2013,565156,0.003300126942238836
"Collaborative Development of Biomedical Ontologies and Terminologies     DESCRIPTION (provided by applicant): The construction of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process  natural language, and to build systems for decision support has set many communities  of biomedical investigators to work building large ontologies.  We developed and evaluated the Collaborative Prot¿g¿ system in the first phase of our research project. This software system has become an indispensable open-source resource for an international community of scientists who develop ontologies in a cooperative, distributed manner. In this competing renewal proposal, we describe novel data-driven methods and tools that promise to make collaborative ontology design both more streamlined and more principled. Our goal is to create a more empirical basis for ontology engineering, and to develop methods whereby the ontology-engineering enterprise both can profit from data regarding the underlying processes and those processes in turn can generate increasing amounts of data to inform future ontology-engineering activities.  Our research plan entails three specific aims. First, we will enable ontology developers to apply ontology-design patterns (ODPs) to their ontologies, and we will measure the way in which these patterns alter the ontology-engineering process. Second, we will analyze the vast amounts of log data that we collect from users of Collaborative Prot¿g¿ to understand the patterns of ontology development. We will use these patterns to recommend to developers areas of ontologies that may need their attention, facilitating the process of reaching consensus and making collaborative ontology engineering more efficient. Finally, we will use the extensive data collected by our group and others to understand how scientists reuse terms from various ontologies and we will use these emerging patterns to facilitate term reuse. Each of these analyses not only will increase our understanding of collaboration in scientific modeling, but also will lead to new technology within our Collaborative Prot¿g¿ suite that will improve the ontology-development process and make collaboration among biomedical scientists more efficient.         PUBLIC HEALTH RELEVANCE: Collaborative Prot�g� is a software system that helps a burgeoning user community to cooperate in developing ontologies that enhance biomedical research and improve patient care. Collaborative Prot�g� supports scientists, clinician researchers, and workers in informatics to build ontologies to solve problems in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision support. The proposed research will develop data-driven methods to identify patterns in design, development, and use of ontologies, and will apply these methods to help us to build new technology that both facilitates the ontology-development process and makes ontology design more principled.            ",Collaborative Development of Biomedical Ontologies and Terminologies,8504843,R01GM086587,"['Address', 'Applications Grants', 'Area', 'Attention', 'Biomedical Research', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Consensus', 'Data', 'Data Analyses', 'Data Set', 'Decision Support Systems', 'Development', 'Engineering', 'Future', 'Generations', 'Genes', 'Goals', 'Human', 'Informatics', 'Information Retrieval', 'International', 'International Classification of Diseases', 'Knowledge', 'Lead', 'Learning', 'Maintenance', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Morphologic artifacts', 'NCI Thesaurus', 'National Cancer Institute', 'Natural Language Processing', 'Ontology', 'Parasites', 'Patient Care', 'Pattern', 'Phase', 'Problem Solving', 'Process', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Scientist', 'Software Design', 'Software Engineering', 'Specialist', 'System', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Traditional Medicine', 'Work', 'base', 'biomedical ontology', 'biomedical resource', 'biomedical scientist', 'craniofacial', 'data integration', 'design', 'experience', 'improved', 'interoperability', 'malformation', 'new technology', 'novel', 'open source', 'public health relevance', 'repository', 'software systems', 'tool', 'tool development']",NIGMS,STANFORD UNIVERSITY,R01,2013,527736,0.022610822738410575
"Protege: An Ontology-Development Platform for Biomedical Scientists     DESCRIPTION (provided by applicant): The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in  biomedicine.  Ontologies help both humans and computers to manage burgeoning numbers of data.  The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Prot¿g¿ system has become an indispensable open-source resource for an enormous internationa community of scientists-supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Prot¿g¿ users has grown from 3,500 in 2002 to more than 195,000 users as of this writing. To date, however, the use of ontologies in biomedicine has been limited by the complexity of the ontology-development tools, which often make ontologies inaccessible to many biomedical scientists.  In this proposal, we will develop new methods and tools that will significantly lower the barrier of entry for ontology development, expanding Prot¿g¿ to provide intuitive and user-friendly ontology-acquisition methods throughout the ontology lifecycle.  Our plan entails five specific aims.  First, we will develop methods that enable initial specification of ontology terms in an informal manner, using lists and diagrams.  Scientists will be able to start modeling their domain without having to think in terms of formal ontological distinctions. Second, we will provide intuitive, easy-to-use tools for ontology specification that will aid developers as they start to formalize their models.  Third, we will track the requirements that an ontology must address and develop novel  methods  for  evaluating  ontology  coverage  based  on  these  requirements.  Fourth, for ontologies that inherently have complex internal structure that cannot be represented fully using only simple ontology constructs, we will develop methods that will create templates covering regular structures in the ontology. Scientists will then be able to fill out forms based o these templates, with Prot¿g¿ generating the corresponding logical structure in the background.  Fifth, we will continue to expand and support the thriving Prot¿g¿ user community, as it expands to include the biomedical scientists who will now be able to build the ontologies to support their data-driven research and discoveries.          PUBLIC HEALTH RELEVANCE: Prot�g� is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care.  Prot�g� supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Prot�g� resource provides critical semantic- technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.            ",Protege: An Ontology-Development Platform for Biomedical Scientists,8438361,R01GM103316,"['Address', 'Adoption', 'Applications Grants', 'Area', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Clinical', 'Communities', 'Complex', 'Computer software', 'Computerized Patient Records', 'Computers', 'Custom', 'Data', 'Data Set', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Electronics', 'Engineering', 'Evolution', 'Feedback', 'Foundations', 'Funding', 'Grant', 'Hand', 'Home environment', 'Human', 'Indium', 'Informatics', 'Information Retrieval', 'Information Systems', 'International', 'Knowledge', 'Knowledge Discovery', 'Laboratories', 'Letters', 'Libraries', 'Mails', 'Maintenance', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Patient Care', 'Pattern', 'Publications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Scientist', 'Semantics', 'Source', 'Structure', 'Support System', 'System', 'Technology', 'Terminology', 'To specify', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Update', 'Work', 'Writing', 'base', 'biomedical ontology', 'biomedical scientist', 'data integration', 'design', 'improved', 'innovation', 'knowledge base', 'next generation', 'novel', 'open source', 'public health relevance', 'research and development', 'software development', 'software systems', 'success', 'tool', 'tool development', 'user-friendly']",NIGMS,STANFORD UNIVERSITY,R01,2013,533554,0.028248677935962842
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541872,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2013,872488,0.03973768830881846
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8545224,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,4024095,0.002513241226822428
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known  as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network  (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5  years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases  Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and  procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on  32 current studies, contingent upon the successful re-competition of their associated clinical research  consortia, addition of new studies reflecting the growth of the network, accommodation of federated  databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and  registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per  year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical  trials. We will continue development of new technologies to support scalability and generalizability and tools  for cross-disease data mining. Our international clinical information network is secure providing coordinated  data management services for collection, storage and analysis of diverse data types from multiple diseases  and geographically disparate locations and a portal for the general public and larger community of clinical  investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8734648,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2013,304154,0.002513241226822428
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.       This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8309015,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2012,179306,-0.009298802953753202
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence  Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.  Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8471822,R00LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R00,2012,224100,-0.01456657100676902
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems. This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8701603,R01GM095476,[' '],NIGMS,UNIV OF MASSACHUSETTS MED SCH WORCESTER,R01,2012,332000,-0.009298802953753202
"Machine learning methods to increase genomic accessibility by next-gen sequencing     DESCRIPTION (provided by applicant): DNA sequencing has become an indispensable tool in many areas of biology and medicine. Recent techno- logical breakthroughs in next-generation sequencing (NGS) have made it possible to sequence billions of bases quickly and cheaply. A number of NGS-based tools have been created, including ChIP-seq, RNA-seq, Methyl- seq and exon/whole-genome sequencing, enabling a fundamentally new way of studying diseases, genomes and epigenomes. The widespread use of NGS-based methods calls for better and more efficient tools for the analysis and interpretation of the NGS high-throughput data. Although a number of computational tools have been devel- oped, they are insufficient in mapping and studying genome features located within repeat, duplicated and other so-called unmappable regions of genomes. In this project, computational algorithms and software that expand genomic accessibility of NGS to these previously understudied regions will be developed.  The algorithms will begin with a new way of mapping raw reads from NGS to the reference genome, followed by a machine learning method to resolve ambiguously mapped reads, and will be integrated into a comprehen- sive analysis pipeline for ChIP-seq. More specifically, the three aims of the research are to develop: (1) Data structures and efficient algorithms for read mapping to rapidly identify all mapping locations. Unlike existing methods, the focus of this research is to rapidly identify all candidate locations of each read, instead of one or only a few locations. (2) Machine learning algorithms for read analysis to resolve ambiguously mapped reads for both ChIP-seq analysis and genetic variation detection. This work will develop probabilistic models to resolve ambiguously mapped reads by pooling information from the entire collection of reads. (3) A comprehensive ChIP- seq analysis pipeline to systematically study genomic features located within unmappable regions of genomes. These algorithms will be tested and refined using both publicly available data and data from established wet-lab collaborators.  In addition to discovering new genomic features located within repeat, duplicated or other previously unac- cessible regions, this work will provide the NGS community with (a) a faster and more accurate tool for mapping short sequence reads, (b) a general methodology for expanding genomic accessibility of NGS, and (c) a versatile, modular, open-source toolbox of algorithms for NGS data analysis, (d) a comprehensive analysis of protein-DNA interactions in repeat regions in all publicly available ChIP-seq datasets.  This work is a close collaboration between computer scientists and web-lab biologists who are developing NGS assays to study biomedical problems. In particular, we will collaborate with Timothy Osborne of Sanford- Burnham Medical Research Institute to study regulators involved in cholesterol and fatty acid metabolism, with Kyoko Yokomori of UC Irvine to study Cohesin, Nipbl and their roles in Cornelia de Lange syndrome, and Ken Cho of UC Irvine to study the roles of FoxH1 and Schnurri in development and growth control.        PUBLIC HEALTH RELEVANCE: DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.              DNA-sequencing has become an indispensable tool for basic biomedical research as well as for discovering new treatments and helping biomedical researchers understand disease mechanisms. Next-generation sequencing, which enables rapid generation of billions of bases at relatively low cost, poses a significant computational challenge on how to analyze the large amount of sequence data efficiently and accurately. The goal of this research is to develop open-source software to improve both the efficiency and accuracy of the next-generation sequencing analysis tools, and thereby allowing biomedical researchers to take full advantage of next-generation sequencing to study biology and disease.            ",Machine learning methods to increase genomic accessibility by next-gen sequencing,8350385,R01HG006870,"['Algorithms', 'Anus', 'Area', 'Base Sequence', 'Binding', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Research', 'Bruck-de Lange syndrome', 'ChIP-seq', 'Cholesterol', 'Chromatin', 'Collaborations', 'Collection', 'Communities', 'Computational algorithm', 'Computer software', 'Computers', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Analyses', 'Data Set', 'Detection', 'Disease', 'Exons', 'Facioscapulohumeral', 'Foundations', 'Generations', 'Genetic Variation', 'Genome', 'Genomics', 'Goals', 'Growth and Development function', 'Internet', 'Location', 'Machine Learning', 'Maps', 'Medical Research', 'Medicine', 'Methodology', 'Methods', 'Muscular Dystrophies', 'Procedures', 'Publishing', 'RNA', 'Reading', 'Research', 'Research Institute', 'Research Personnel', 'Role', 'Scientist', 'Sequence Analysis', 'Software Engineering', 'Speed', 'Statistical Models', 'Structure', 'Testing', 'Uncertainty', 'Work', 'base', 'cohesin', 'computerized tools', 'cost', 'fatty acid metabolism', 'functional genomics', 'genome sequencing', 'genome-wide', 'improved', 'insertion/deletion mutation', 'next generation', 'novel', 'open source', 'tool', 'transcription factor', 'xenopus development']",NHGRI,UNIVERSITY OF CALIFORNIA-IRVINE,R01,2012,220000,-0.02858403918854628
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8515555,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2012,290837,-0.009952817373637863
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8309419,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,596909,-0.01454513477207519
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8318224,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2012,572436,-0.01191674862862718
"Informatics Infrastructure for vector-based neuroanatomical atlases  Abstract The adage: 'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study. Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science. This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system. This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale). These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles). A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available. This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner. We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure. As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature. We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register. The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).  Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8238371,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'abstracting', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2012,326844,-0.007726280609322553
"Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs     DESCRIPTION (provided by applicant):         Research plan: The use of clinical knowledge systems such as UpToDate that provide reliable information at the point of care has been shown to improve patient safety and decision-making. With similar content to UpToDate, Mayo Clinic's AskMayoExpert (AME) is an online knowledge system that primarily contains over 5000 (and increasing) specialist-vetted answers to FAQs for point of care use. However, because of the overabundance of clinical resources and guidelines, adding new answers manually to AME and ensuring that it is consistent with evidence is time consuming. This problem is also faced with other systems such as UpToDate. This career grant proposes to investigate the feasibility of using a novel text mining based informatics approach to semi-automate the management of a clinical knowledge system, using AME as the test bed. Although the methods will be applicable to any clinical knowledge system and any topic, they will be evaluated using two important test topics from cardiology (which has the biggest focus in AME) - atrial fibrillation (a topic exhaustively covered in AME) and congestive heart failure (a topic less covered, but is an increasingly complex vast field with knowledge from huge literature). While the existing content of AME is private, the methods and the code we develop to assist in generating the content will be released open-source as part of the Open Health Natural Language Processing (OHNLP) consortium in UIMA framework. Career plan: As most communication of information in clinical practice and biomedical research occurs through the medium of text, the development of methods to render this text computer-interpretable is a prerequisite to the use of this information to improve quality of care and support scientific discovery. The PI's long-term career goal is to become a leader in biomedical informatics (informatics applied to biomedical data), with focus on textual data such as scientific papers and clinical notes. He has BS in Computer Science, PhD in Biomedical Informatics and over a dozen of peer-reviewed publications in biomedical text mining. His career goal is to advance diverse methods and applications of text mining across biomedical informatics (BMI). He will focus on: a) discovering information needs and gaps that can be filled, b) adapting and extending existing text mining algorithms, and c) validating the utility of the applications in the biomedical environment. Rationale: Making the transition from a mentored researcher to an independent researcher requires three main facets of career growth: a) developing a working familiarity with clinical information systems and medical terminologies; b) understanding the information needs of clinicians; and c) training in clinical research. The proposal will translate the PI's knowledgeof the text mining methods to practical experience in an operation clinical environment. Courses listed in the ""Career Development/Training Activities"" will educate him more about the environment and train him on clinical research. He will continue sharpening his informatics expertise by attending scientific conferences.              Project Narrative Medical errors are one of the leading causes of death in the United States. It has been observed that point of care access to relevant clinical knowledge support decision making and decreases medical errors, thereby improving patient safety and healthcare costs. The proposed research aims to empower physicians specialized in the area (specialists) in quickly gathering evidence from literature or finding citations supporting or qualifying their expert opinion. It will also generate the answers and suggest updates to the existing answers for their perusal.",Improving the Efficiency and Efficacy in Authoring Essential Clinical FAQs,8442618,K99LM011389,"['Address', 'Algorithms', 'Area', 'Atrial Fibrillation', 'Beds', 'Biomedical Research', 'Calculi', 'Cardiology', 'Cause of Death', 'Clinic', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Code', 'Communication', 'Complex', 'Computers', 'Congestive Heart Failure', 'Cross-Over Studies', 'Data', 'Decision Making', 'Doctor of Philosophy', 'Electronic Health Record', 'Ensure', 'Environment', 'Expert Opinion', 'Familiarity', 'Frequencies', 'Future', 'General Practitioners', 'Goals', 'Grant', 'Growth', 'Guidelines', 'Health', 'Health Care Costs', 'Health Services Accessibility', 'Healthcare', 'Human', 'Informatics', 'Information Retrieval Systems', 'Journals', 'Knowledge', 'Language', 'Link', 'Literature', 'Medical', 'Medical Errors', 'Mentors', 'Methods', 'Natural Language Processing', 'Nurses', 'Paper', 'Peer Review', 'Physicians', 'Publications', 'Publishing', 'Qualifying', 'Quality of Care', 'Randomized', 'Research', 'Research Personnel', 'Resources', 'Semantics', 'Source', 'Specialist', 'System', 'Terminology', 'Testing', 'Text', 'Time', 'Training', 'Training Activity', 'Translating', 'United States', 'Update', 'Validation', 'Vocabulary', 'Work', 'Workload', 'Writing', 'base', 'biomedical informatics', 'career', 'career development', 'clinical decision-making', 'clinical practice', 'computer science', 'empowered', 'evidence base', 'experience', 'improved', 'information gathering', 'medical information system', 'method development', 'novel', 'open source', 'operation', 'patient safety', 'point of care', 'statistics', 'symposium', 'text searching', 'usability']",NLM,MAYO CLINIC ROCHESTER,K99,2012,96552,-0.022652024018908062
"Transcriptomics of Tuberculosis Latency and Reactivation in Primates DESCRIPTION (provided by applicant): Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, in a quiescent state. These bacilli are able to reactivate and cause pulmonary TB, when the immune system is compromised. Hence, a complete understanding of TB latency and reactivation is required for the effective control of TB. The research models and tools necessary to perform these studies are now available. Nonhuman Primates (NHPs) are excellent models of TB, especially to study the progression of experimental infection to latency, and to study the pathology and biology of granulomatous lesions - the hallmarks of TB infections. We have established a model of human TB, by exposing NHPs to true Mtb aerosols. While many research groups focus on the bacterial factors of latency and reactivation, we would like to leverage our highly tractable model to identify host signatures and mediators of this process. We show that pro-inflammatory immune signaling pathways, initially induced in NHP TB lesions, are overwhelmingly silenced over the course of next several weeks. This transcriptional reprogramming could be a host response to changes in bacterial replication and physiology. Further, these responses could reflect the efforts of the pathogen to prevent excessive immunopathology during the infection of lungs. The central hypothesis of our proposal is that host granuloma responses can be used to predict latent and reactivation TB. We propose to perform a systematic study of the ""transcriptome"" and the ""miRNAome"" of NHP lung lesions. Temporal profiles will be obtained from NHPs infected with a low-dose of Mtb aerosols, accurately modeling long-term latent infection. Profiles will also be obtained from NHPs in which latent TB is reactivated by simian AIDS. These system-wide profiles, in conjugation with the clinical, microbiological and immunological data obtained from infected NHPs will generate statistical learning algorithms and mixed effects computational models of latent and reactivation TB. The relevance of some of the most informative set of genetic predictors available from the data collected will be tested back in both the NHP model, as well as in human patients. The expression profiles of CCL24, CCL25 and CCL27 show negative correlation with all other chemokine ligands and receptors in primate TB granulomas. The expression of these three chemokines is significantly increased in late, rather than early lesions. We hypothesize that these chemokines are important for the long-term maintenance of primate lesions harboring latent Mtb bacilli. The expression of LAG3 was induced more than 40-fold in early primate lesions relative to late ones. LAG3 is a novel marker of Treg cells. We hypothesize that LAG3 is responsible for negatively regulating protective immune responses generated by effector T cells in primate TB lesions. The expression of ""latency"" specific genes CCL24/25/27 and the ""active-TB: specific gene LAG3 will be silenced in NHPs using a novel lipidated-siRNA nanoparticle based approach. The progression of latent disease and its immunological and molecular correlates will then be studied in these animals. Finally, the expression of an immune response to these and other ""latency"" and ""reactivation""- specific profiles will be determined in human patients of latent and active TB, as well as TB/AIDS co- infected patients. These systems-biology studies will likely exponentially enhance our understanding of TB latency and reactivation in a host that mimics both TB and AIDS in the closest possible manner to humans. Eventually, these advances may empower clinicians better to detect and treat latent TB. PUBLIC HEALTH RELEVANCE: Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised. Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation. Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced in NHP TB lesions, but silence over the course of several weeks. This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model. Towards this, we have assembled a highly diverse and collaborative team including microbiologists, aerobiologists, bioinformaticians, mathematicians/statisticians, veterinarians, veterinary pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.",Transcriptomics of Tuberculosis Latency and Reactivation in Primates,8548633,R01HL106790,"['Acquired Immunodeficiency Syndrome', 'Aerosols', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Automobile Driving', 'Bacillus (bacterium)', 'Back', 'Biological', 'Biological Process', 'Biology', 'Biopsy', 'CCL24 gene', 'CCL25 gene', 'CCL27 gene', 'Caring', 'Cells', 'Clinical', 'Communicable Diseases', 'Complex', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Dose', 'Emergency Situation', 'Equilibrium', 'Excision', 'Exhibits', 'Focus Groups', 'Gene Expression Profile', 'Gene Silencing', 'Genes', 'Genetic', 'Genetic Models', 'Granuloma', 'Granulomatous', 'Growth', 'HIV', 'Health', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Infection', 'Inflammatory', 'Lesion', 'Life', 'Ligands', 'Lung', 'Macaca mulatta', 'Machine Learning', 'Maintenance', 'Mediator of activation protein', 'Metadata', 'Methodology', 'MicroRNAs', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'Organogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physiology', 'Primates', 'Process', 'Proteins', 'Pulmonary Tuberculosis', 'RNA Interference', 'Regulatory T-Lymphocyte', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'SIV', 'Sampling', 'Signal Pathway', 'Simian Acquired Immunodeficiency Syndrome', 'Small Interfering RNA', 'Statistical Models', 'System', 'Systems Biology', 'T-Lymphocyte', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transcript', 'Tuberculosis', 'Vaccines', 'Veterinarians', 'base', 'chemokine', 'combat', 'empowered', 'genome-wide', 'immunopathology', 'in vivo', 'innovation', 'latent infection', 'nanoparticle', 'nonhuman primate', 'novel', 'novel marker', 'pathogen', 'prevent', 'pulmonary granuloma', 'reactivation from latency', 'receptor', 'respiratory', 'response', 'tool', 'transcriptomics', 'tuberculosis granuloma']",NHLBI,TULANE UNIVERSITY OF LOUISIANA,R01,2012,83778,0.011845740895850668
"Transcriptomics of Tuberculosis Latency and Reactivation in Primates DESCRIPTION (provided by applicant): Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, in a quiescent state. These bacilli are able to reactivate and cause pulmonary TB, when the immune system is compromised. Hence, a complete understanding of TB latency and reactivation is required for the effective control of TB. The research models and tools necessary to perform these studies are now available. Nonhuman Primates (NHPs) are excellent models of TB, especially to study the progression of experimental infection to latency, and to study the pathology and biology of granulomatous lesions - the hallmarks of TB infections. We have established a model of human TB, by exposing NHPs to true Mtb aerosols. While many research groups focus on the bacterial factors of latency and reactivation, we would like to leverage our highly tractable model to identify host signatures and mediators of this process. We show that pro-inflammatory immune signaling pathways, initially induced in NHP TB lesions, are overwhelmingly silenced over the course of next several weeks. This transcriptional reprogramming could be a host response to changes in bacterial replication and physiology. Further, these responses could reflect the efforts of the pathogen to prevent excessive immunopathology during the infection of lungs. The central hypothesis of our proposal is that host granuloma responses can be used to predict latent and reactivation TB. We propose to perform a systematic study of the ""transcriptome"" and the ""miRNAome"" of NHP lung lesions. Temporal profiles will be obtained from NHPs infected with a low-dose of Mtb aerosols, accurately modeling long-term latent infection. Profiles will also be obtained from NHPs in which latent TB is reactivated by simian AIDS. These system-wide profiles, in conjugation with the clinical, microbiological and immunological data obtained from infected NHPs will generate statistical learning algorithms and mixed effects computational models of latent and reactivation TB. The relevance of some of the most informative set of genetic predictors available from the data collected will be tested back in both the NHP model, as well as in human patients. The expression profiles of CCL24, CCL25 and CCL27 show negative correlation with all other chemokine ligands and receptors in primate TB granulomas. The expression of these three chemokines is significantly increased in late, rather than early lesions. We hypothesize that these chemokines are important for the long-term maintenance of primate lesions harboring latent Mtb bacilli. The expression of LAG3 was induced more than 40-fold in early primate lesions relative to late ones. LAG3 is a novel marker of Treg cells. We hypothesize that LAG3 is responsible for negatively regulating protective immune responses generated by effector T cells in primate TB lesions. The expression of ""latency"" specific genes CCL24/25/27 and the ""active-TB: specific gene LAG3 will be silenced in NHPs using a novel lipidated-siRNA nanoparticle based approach. The progression of latent disease and its immunological and molecular correlates will then be studied in these animals. Finally, the expression of an immune response to these and other ""latency"" and ""reactivation""- specific profiles will be determined in human patients of latent and active TB, as well as TB/AIDS co- infected patients. These systems-biology studies will likely exponentially enhance our understanding of TB latency and reactivation in a host that mimics both TB and AIDS in the closest possible manner to humans. Eventually, these advances may empower clinicians better to detect and treat latent TB. PUBLIC HEALTH RELEVANCE: Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised. Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation. Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced in NHP TB lesions, but silence over the course of several weeks. This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model. Towards this, we have assembled a highly diverse and collaborative team including microbiologists, aerobiologists, bioinformaticians, mathematicians/statisticians, veterinarians, veterinary pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.",Transcriptomics of Tuberculosis Latency and Reactivation in Primates,8322168,R01HL106790,"['Acquired Immunodeficiency Syndrome', 'Aerosols', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Automobile Driving', 'Bacillus (bacterium)', 'Back', 'Biological', 'Biological Process', 'Biology', 'Biopsy', 'CCL24 gene', 'CCL25 gene', 'CCL27 gene', 'Caring', 'Cells', 'Clinical', 'Communicable Diseases', 'Complex', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Dose', 'Emergency Situation', 'Equilibrium', 'Excision', 'Exhibits', 'Focus Groups', 'Gene Expression Profile', 'Gene Silencing', 'Genes', 'Genetic', 'Genetic Models', 'Granuloma', 'Granulomatous', 'Growth', 'HIV', 'Health', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Infection', 'Inflammatory', 'Lesion', 'Life', 'Ligands', 'Lung', 'Macaca mulatta', 'Machine Learning', 'Maintenance', 'Mediator of activation protein', 'Metadata', 'Methodology', 'MicroRNAs', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'Organogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physiology', 'Primates', 'Process', 'Proteins', 'Pulmonary Tuberculosis', 'RNA Interference', 'Regulatory T-Lymphocyte', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'SIV', 'Sampling', 'Signal Pathway', 'Simian Acquired Immunodeficiency Syndrome', 'Small Interfering RNA', 'Statistical Models', 'System', 'Systems Biology', 'T-Lymphocyte', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transcript', 'Tuberculosis', 'Vaccines', 'Veterinarians', 'base', 'chemokine', 'combat', 'empowered', 'genome-wide', 'immunopathology', 'in vivo', 'innovation', 'latent infection', 'nanoparticle', 'nonhuman primate', 'novel', 'novel marker', 'pathogen', 'prevent', 'pulmonary granuloma', 'reactivation from latency', 'receptor', 'respiratory', 'response', 'tool', 'transcriptomics', 'tuberculosis granuloma']",NHLBI,TULANE UNIVERSITY OF LOUISIANA,R01,2012,751864,0.011845740895850668
"Large-scale Automated Synthesis of Functional Neuroimaging Data     DESCRIPTION (provided by applicant): The explosive growth of the human neuroimaging literature has led to major advances in understanding of normal and abnormal human brain function, but has also made aggregation and synthesis of neuroimaging findings increasingly difficult. The goal of this project is to develop an automated software platform for large-scale synthesis of human functional neuroimaging studies. Our work builds directly on an existing software platform (NeuroSynth) and involves key extensions and improvements that focus on (i) aggregation, (ii) coding, (iii) synthesis, and (iv) sharing of functional neuroimaging data. In Aim 1, we will use computational linguistics and bioinformatics data mining techniques to develop new algorithms for automatically extracting activation foci and associated metadata from published neuroimaging articles. In Aim 2, we will use topic-modeling techniques such as Latent Dirichlet Analysis in combination with existing cognitive ontologies such as the Cognitive Atlas to develop structured representations of automatically extracted neuroimaging data. In Aim 3, we will improve the meta-analysis and classification capacities of our existing platform by implementing a state-of- the-art hierarchical Bayesian meta-analysis method recently developed by the research team. Finally, in Aim 4, we will develop a state-of-the-art web interface (://neurosynth.org) that supports real-time, in-browser access to the data, results, and tools produced in Aims 1 - 3. Realizing these objectives will introduce powerful new tools for organizing and synthesizing the neuroimaging literature on an unprecedented scale. These tools will be freely and publicly available to anyone with an internet connection, enabling rapid and efficient application to a broad range of clinical and basic research applications.        PUBLIC HEALTH RELEVANCE: Functional neuroimaging techniques such as fMRI have opened a new frontier in efforts to investigate and understand the neural mechanisms of normal and abnormal cognition. However, the rapidly expanding scope of the literature makes distillation and synthesis of brain imaging findings increasingly challenging. The goal of this project is to develop a new software platform for automated aggregation, synthesis, and sharing of published neuroimaging results, with the potential to advance understanding of mechanisms underlying mental health disorders.                  Functional neuroimaging techniques such as fMRI have opened a new frontier in efforts to investigate and understand the neural mechanisms of normal and abnormal cognition. However, the rapidly expanding scope of the literature makes distillation and synthesis of brain imaging findings increasingly challenging. The goal of this project is to develop a new software platform for automated aggregation, synthesis, and sharing of published neuroimaging results, with the potential to advance understanding of mechanisms underlying mental health disorders.                ",Large-scale Automated Synthesis of Functional Neuroimaging Data,8397498,R01MH096906,"['Algorithms', 'Atlases', 'Basic Science', 'Bioinformatics', 'Biometry', 'Brain', 'Brain imaging', 'Classification', 'Clinical', 'Clinical Research', 'Code', 'Cognition', 'Cognitive', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Databases', 'Development', 'Ensure', 'Environment', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Goals', 'Growth', 'High Performance Computing', 'Human', 'Individual', 'Interdisciplinary Study', 'Internet', 'Joints', 'Journals', 'Language', 'Linguistics', 'Literature', 'Manuals', 'Maps', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Nature', 'Neurosciences', 'Ontology', 'Paper', 'Performance', 'Population', 'Process', 'Publishing', 'Qualifying', 'Research', 'Research Personnel', 'Resources', 'Sample Size', 'Specificity', 'Structure', 'Techniques', 'Text', 'Time', 'Training', 'Validation', 'Work', 'awake', 'base', 'cognitive function', 'data mining', 'frontier', 'improved', 'information organization', 'interoperability', 'knowledge base', 'neuroimaging', 'neuroinformatics', 'neuromechanism', 'open source', 'psychologic', 'theories', 'tool', 'web interface']",NIMH,UNIVERSITY OF COLORADO,R01,2012,723109,-0.0012644628670110047
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8242742,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2012,392767,0.02470344118796245
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8330927,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,1821611,0.03973768830881846
"National Center for Biomedical Ontology    DESCRIPTION (PROVIDED BY APPLICANT): We propose to continue the National Center for Biomedical Ontology (NCBO), which develops tools and methods for assimilating, archiving, accessing, and applying machine-processable representations of biomedical domain objects, processes, and relations to assist in the management, integration, visualization, analysis, and interpretation of the huge, distributed data sets that are now the hallmark of biomedical research and clinical care. Our center is truly national in scope, with participation of leading scientific groups at Stanford, Mayo Clinic, University at Buffalo, and the University of Victoria. Our objectives are defined by the following six Cores: (1) the development of enhanced computational methods for management of ontologies and controlled terminologies using current Web standards; integration of ontology authoring, publishing, and peer review; creation of a comprehensive ontology-based index of publicly available data resources; development of new analytic methods to summarize and profile biomedical data; (2) the promotion of Driving Biological Projects that can stimulate our research by suggesting new requirements and offering new test beds for deployment-initially involving the Cardiovascular Research Grid, the Rat Genome Database, the caNanoLab nanoparticle database, and the i2b2 National Center for Biomedical Computing, and later engaging the WHO's development of lCD-11, studies performed by ArrayExpress, and projects that will be selected via open requests for applications; (3) the maintenance of a computational infrastructure to support our research, development, and dissemination activities; provision of user support to the growing number of researchers and clinicians who use our   technologies; (4) the training of the next generation of scientists in biomedical ontology; (5) a comprehensive set of dissemination activities, that include workshops, tutorials. Web-based seminars, and a major international conference; and (6) outstanding project administration conducted by a dedicated and talented management team. The NCBO will accelerate the transition of biomedicine into the world of e-science, facilitate the creation of a National Health Information Infrastructure, and extend a network of collaboration through its interactions with other NCBCs, with other research consortia, and with the biomedical community at large.    RELEVANCE (See instructions):  The NCBO supports a burgeoning user community that is using ontologies to enhance biomedical research and to improve patient care. It supports bench scientists, clinician researchers, and workers in informatics in data annnotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. It is a primary source of semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information svstems.",National Center for Biomedical Ontology,8541935,U54HG004028,"['Adoption', 'Archives', 'Automobile Driving', 'Beds', 'Biological', 'Biological Sciences', 'Biology', 'Biomedical Computing', 'Biomedical Research', 'Buffaloes', 'Cardiovascular system', 'Clinic', 'Clinical', 'Collaborations', 'Communities', 'Computerized Patient Records', 'Computers', 'Computing Methodologies', 'DNA Microarray Chip', 'DNA Sequence', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Discipline', 'Educational workshop', 'Electronics', 'Evaluation', 'Event', 'Evidence Based Medicine', 'Evolution', 'Feedback', 'Generations', 'Genes', 'Goals', 'Government', 'Growth', 'Health', 'Healthcare', 'Home environment', 'Imagery', 'Informatics', 'Information Retrieval', 'Information Technology', 'Instruction', 'Interest Group', 'International', 'International Classification of Diseases', 'Internet', 'Knowledge', 'Language', 'Life', 'Link', 'Maintenance', 'Medicine', 'Methods', 'NIH Program Announcements', 'National Cancer Institute', 'Natural Language Processing', 'Neurosciences', 'North America', 'Online Systems', 'Ontology', 'Participant', 'Patient Care', 'Patients', 'Peer Review', 'Process', 'Property', 'Publishing', 'Publishing Peer Reviews', 'Recommendation', 'Request for Applications', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Services', 'Shapes', 'Societies', 'Solutions', 'Source', 'System', 'Systematized Nomenclature of Medicine', 'Technology', 'Terminology', 'Testing', 'Thesauri', 'Time', 'Training', 'United States National Institutes of Health', 'Universities', 'Vendor', 'Vocabulary', 'Work', 'base', 'biomedical ontology', 'clinical care', 'comparative effectiveness', 'computer based Semantic Analysis', 'computer infrastructure', 'data integration', 'design', 'distributed data', 'e-science', 'genome database', 'health information technology', 'improved', 'indexing', 'interest', 'interoperability', 'knowledge base', 'nanoparticle', 'new technology', 'next generation', 'novel', 'novel strategies', 'open source', 'rat genome', 'research and development', 'research study', 'response', 'symposium', 'text searching', 'tool']",NHGRI,STANFORD UNIVERSITY,U54,2012,100000,0.03973768830881846
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8330246,U54NS064808,"['Access to Information', 'Accountability', 'Address', 'Adherence', 'Administrator', 'Adverse event', 'Agreement', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Biological Markers', 'Biological Neural Networks', 'Bite', 'Businesses', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Classification', 'Clinic Visits', 'Clinical', 'Clinical Data', 'Clinical Investigator', 'Clinical Management', 'Clinical Protocols', 'Clinical Research', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials', 'Clinical Trials Cooperative Group', 'Clinical Trials Data Monitoring Committees', 'Code', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Communication', 'Communities', 'Computer Architectures', 'Computer Security', 'Computer software', 'Computers', 'Confidentiality', 'Consent Forms', 'Cost Savings', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Base Management', 'Data Collection', 'Data Element', 'Data Protection', 'Data Quality', 'Data Reporting', 'Data Security', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Directories', 'Disasters', 'Disease', 'Documentation', 'Drops', 'Drug Monitoring', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'General Population', 'Generations', 'Generic Drugs', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Graph', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Human Resources', 'Image', 'Individual', 'Industry', 'Informatics', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Institutional Review Boards', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Measures', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Metric', 'Modeling', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Participant', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patient Self-Report', 'Patients', 'Performance', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Population', 'Population Study', 'Positron-Emission Tomography', 'Prevention strategy', 'Principal Investigator', 'Printing', 'Privacy', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Proteomics', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Rare Diseases', 'Reader', 'Recording of previous events', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Regulation', 'Relative (related person)', 'Reporting', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Subjects', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Sampling', 'Scanning', 'Schedule', 'Scientist', 'Secure', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Structure', 'Support Groups', 'Support System', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'clinical research site', 'cluster computing', 'computer science', 'computerized data processing', 'data acquisition', 'data management', 'data mining', 'data modeling', 'data sharing', 'database design', 'database structure', 'demographics', 'design', 'distributed data', 'electronic data', 'eligible participant', 'experience', 'federal policy', 'federated computing', 'firewall', 'follow-up', 'forest', 'graphical user interface', 'improved', 'information display', 'interest', 'meetings', 'member', 'new technology', 'novel strategies', 'operation', 'optical character recognition', 'patient advocacy group', 'patient registry', 'predictive modeling', 'professor', 'programs', 'prospective', 'protocol development', 'quality assurance', 'radiologist', 'remediation', 'repository', 'research study', 'response', 'sample collection', 'software development', 'statistics', 'success', 'symposium', 'technology development', 'therapeutic development', 'tool', 'trafficking', 'user-friendly', 'vector', 'volunteer', 'web interface', 'web page', 'web services', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2012,2827142,0.002513241226822428
"Exploring Natural Language Processing, Image Processing, Machine Learning, and Us    DESCRIPTION (provided by applicant): Most biomedical text mining systems target only text information and do not provide intelligent access to other important data such as Figures. More than any other documentation, figures usually represent the ""evidence"" of discovery in the biomedical literature. Full-text biomedical articles nearly always incorporate images that are the crucial content of biomedical knowledge discovery. Biomedical scientists need to access images to validate research facts and to formulate or to test novel research hypotheses. Evaluation has shown that textual statements reported in the literature are frequently noisy (i.e., contain ""false facts""). Capturing images that are essentially experimental ""evidence"" to support the textual ""fact"" will benefit biomedical information systems, databases, and biomedical scientists. We are developing a biomedical literature figure search engine BioFigureSearch. We develop innovative algorithms and models in natural language processing, image processing, machine learning and user interfacing. The deliverables will be novel biomedical natural language figure processing (bNLfP) algorithms and iBioFigureSearch allowing biomedical scientists to access figure data effectively, and open-source tools that will enhance biomedical information retrieval, summarization, and question answering. The bNLfP algorithms we will be developing can be applied or integrated into other biomedical text-mining systems.      PUBLIC HEALTH RELEVANCE: This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.          This project proposes innovative algorithms and models in natural language processing, image processing, machine learning, and user interfacing, to return figures in response to biomedical queries. It is anticipated that the algorithms, models, and tools developed will significantly enhance biomedical scientists' access to figures reported in literature, and thereby expedite biomedical knowledge discovery.","Exploring Natural Language Processing, Image Processing, Machine Learning, and Us",8106768,R01GM095476,"['Address', 'Algorithms', 'Automobile Driving', 'Biomedical Computing', 'Cognitive', 'Collaborations', 'Collection', 'Comprehension', 'Computer Simulation', 'Data', 'Databases', 'Diagnostic', 'Discipline', 'Disease', 'Documentation', 'Evaluation', 'Genomics', 'Human', 'Hybrids', 'Image', 'Information Retrieval', 'Information Retrieval Systems', 'Knowledge', 'Knowledge Discovery', 'Libraries', 'Licensing', 'Literature', 'Machine Learning', 'Measures', 'Medicine', 'Methods', 'Modeling', 'Natural Language Processing', 'Process', 'Prognostic Marker', 'Proteins', 'PubMed', 'Publications', 'Publishing', 'Reading', 'Reporting', 'Research', 'Research Personnel', 'Retrieval', 'Semantics', 'System', 'T-Cell Receptor-Rearrangement Excision DNA Circles', 'Techniques', 'Testing', 'Text', 'Validation', 'abstracting', 'base', 'biomedical information system', 'biomedical scientist', 'design', 'genome-wide', 'image processing', 'improved', 'innovation', 'medical schools', 'natural language', 'novel', 'open source', 'response', 'text searching', 'tool']",NIGMS,UNIVERSITY OF WISCONSIN MILWAUKEE,R01,2011,500273,-0.007398647709106807
"Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence    DESCRIPTION (provided by applicant): Translation of biomedical research into practice depends in part on the production of quality systematic reviews that synthesize available evidence. Unfortunately, about 20% of reviews are never completed. Of those that reach fruition, the average time to completion may be 2.4 years, with a reported maximum of 9 years. A major bottleneck occurs when teammates screen studies. In the first step, they independently identify provisionally eligible studies by reading the same set of perhaps thousands of titles and abstracts. To date, researchers have used supervised machine learning (ML) methods in an attempt to automate identification of eligible randomized controlled trials (RCTs). However, finding nonrandomized (NR) studies for inclusion in systematic reviews has yet to be addressed. This is an important problem because RCTs may be unlikely or even unethical for some research questions. Hypotheses. It is broadly hypothesized that (a) methods based on natural language processing and ML can be used to automatically identify topically relevant studies with a mix of NR designs eligible for inclusion in systematic reviews; and (b) machine performance can consistently reach current human standards with respect to identifying eligible studies. Aims. This research has three aims: (1) Compare the language that biomedical researchers use to describe their NR study designs with existing relevant vocabularies. Develop complementary terminologies for overlooked NR study designs to improve coverage of important vocabularies. Develop and validate a standalone terminology to support librarians who add free-text terms to expert searches. (2) Develop and compare procedures based on natural language processing and supervised ML methods to identify provisionally eligible NR studies that are topically relevant from a set of citations, including titles, abstracts, and metadata. Use terms for NR study designs to improve classification. (3) Generalize procedures developed under Aims 1 and 2 to select topically relevant studies with a mix of designs for provisional inclusion in several types of systematic reviews. Use contextual information in segments of full texts tagged for location to enrich feature vectors. Methods. Reference standards will be built from studies in published Cochrane reviews. Features will be extracted from citations and regions of full texts. Additionally, feature vectors will be enriched with terms for designs that researchers use in combination with terms extracted from major vocabularies. Model performance will be compared with respect to several measures, including mean recall and precision, for 10-fold cross-validations and validations on held-out test sets. Significance. The proposed research is significant because it will help support translation of biomedical research to improve human health. Moreover, developing procedures to identify NR studies is essential for the expeditious translation of a very large body of research.           Translation of biomedical research helps to improve public health by delivering the best available evidence to clinicians. This process depends in part on the production of systematic reviews of research. Computerized procedures will be developed to reduce the labor associated with screening nonrandomized studies for inclusion in reviews.",Screening Nonrandomized Studies for Inclusion in Systematic Reviews of Evidence,8190163,K99LM010943,"['Address', 'Biomedical Research', 'Classification', 'Health', 'Human', 'Language', 'Librarians', 'Location', 'Machine Learning', 'Measures', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Performance', 'Procedures', 'Process', 'Production', 'Public Health', 'Publishing', 'Randomized Controlled Trials', 'Reading', 'Reference Standards', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Screening procedure', 'Terminology', 'Testing', 'Text', 'Time', 'Translations', 'Validation', 'Vocabulary', 'abstracting', 'base', 'computerized', 'design', 'improved', 'research to practice', 'systematic review', 'vector']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2011,89802,-0.015483525202992513
"CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING    DESCRIPTION (provided by applicant): Case Studies in Bayesian Statistics and Machine Learning I continues in the tradition of the Case Studies in Bayesian Statistics series. The original series of workshops were held in odd years at Carnegie Mellon University in the early fall. The first edition of the new workshop will be held at Carnegie Mellon University on October 14-15, 2011. The highest level goal of the workshop series is to generate and present successful solutions to difficult substantive problems in a wide variety of areas. The specific objectives of the workshop are to 1. Present and discuss solutions to challenging scientific problems that illustrate the potential for statistical machine learning approaches in substantive research; 2. Present an opportunity for statisticians and computer scientists to present applications-oriented research  that changes the way that data are analyzed in scientific fields; 3. Stimulate discussion of the challenges of the analysis of high-dimensional and complex datasets in a scientifically useful manner; 4. Encourage young researchers, including graduate students, to present their applied work; 5. Provide a small meeting atmosphere to facilitate the interaction of young researchers with senior colleagues; 6. Expose young researchers to important challenges and opportunities in collaborative research; 7. Include as participants women, under-represented minorities and persons with disabilities who might benefit from the small workshop environment; 8. Encourage dissemination of the findings presented at the workshop via well-documented and peer- reviewed journal articles.      PUBLIC HEALTH RELEVANCE: Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.           Bayesian and statistical machine learning approaches are essential for the analysis of data in the health sciences, particularly in complex diseases like cancer. The proposed workshop will highlight interesting applications of Bayesian and statistical machine learning, particularly in bioinformatics and imaging, which are relevant to cancer research and provide a venue for important collaboration amongst junior and senior researchers in statistics, computer science, and other disciplines.         ",CASE STUDIES IN BAYESIAN STATISTICS AND MACHINE LEARNING,8203089,R13CA144626,"['Area', 'Bioinformatics', 'Case Study', 'Collaborations', 'Communities', 'Complex', 'Computers', 'Data Analyses', 'Data Set', 'Disabled Persons', 'Discipline', 'Disease', 'Educational workshop', 'Environment', 'Fostering', 'Goals', 'Hand', 'Health Sciences', 'Image', 'Institutes', 'Machine Learning', 'Malignant Neoplasms', 'Methodology', 'Methods', 'National Human Genome Research Institute', 'Participant', 'Peer Review', 'Research', 'Research Personnel', 'Scientist', 'Series', 'Solutions', 'Underrepresented Minority', 'Universities', 'Woman', 'Work', 'anticancer research', 'computer science', 'data modeling', 'falls', 'graduate student', 'interest', 'journal article', 'meetings', 'peer', 'planetary Atmosphere', 'statistics', 'symposium']",NCI,CARNEGIE-MELLON UNIVERSITY,R13,2011,7500,-0.020866342653213767
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,8062287,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Health Sciences', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Schools', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2011,140381,-0.010132273182725056
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,8075593,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2011,274386,0.0022368166289779964
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.           Project Narrative The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.",Scalable Learning with Ensemble Techniques and Parallel Computing,8045486,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Biological Sciences', 'Biomedical Research', 'Classification', 'Communication', 'Communities', 'Community Financing', 'Companions', 'Complex', 'Computer software', 'Consult', 'Crowding', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Health', 'Imagery', 'Knowledge', 'Knowledge Discovery', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Validation', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'new technology', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2011,374673,0.019184560404420743
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,8034342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Health', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2011,332732,-0.009952817373637863
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,8117587,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,597135,-0.01454513477207519
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,8055907,R01LM009731,"['Address', 'Algorithms', 'Area', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'International', 'Investigation', 'Joints', 'Knowledge', 'Learning', 'Link', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Population', 'Prevention', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'disorder prevention', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2011,325956,0.025234755989556216
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,8139258,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2011,513952,-0.01191674862862718
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,8044673,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health', 'Health Care Costs', 'Image', 'Informatics', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Source', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Traumatic Stress Disorders', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'research study', 'text searching', 'tool', 'vector', 'web services']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,324859,-0.0012054624015386895
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,8055527,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'Traumatic Stress Disorders', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2011,248610,-0.011751589450473564
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,8138486,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2011,266112,-0.008392538545642286
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,8143297,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Area', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'animal tissue', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2011,132786,-0.0026117046836027632
"Transcriptomics of Tuberculosis Latency and Reactivation in Primates    DESCRIPTION (provided by applicant):  Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, in a quiescent state. These bacilli are able to reactivate and cause pulmonary TB, when the immune system is compromised. Hence, a complete understanding of TB latency and reactivation is required for the effective control of TB. The research models and tools necessary to perform these studies are now available.   Nonhuman Primates (NHPs) are excellent models of TB, especially to study the progression of experimental infection to latency, and to study the pathology and biology of granulomatous lesions - the hallmarks of TB infections. We have established a model of human TB, by exposing NHPs to true Mtb aerosols. While many research groups focus on the bacterial factors of latency and reactivation, we would like to leverage our highly tractable model to identify host signatures and mediators of this process.    We  show  that  pro-inflammatory  immune  signaling  pathways,  initially  induced  in  NHP  TB  lesions,  are  overwhelmingly  silenced  over  the  course  of  next  several  weeks.  This transcriptional reprogramming could be a host response to changes in bacterial replication and physiology. Further, these  responses  could  reflect  the  efforts  of  the  pathogen  to  prevent  excessive  immunopathology  during  the  infection of lungs. The central hypothesis of our proposal is that host granuloma responses can be used to predict latent and reactivation TB. We propose to perform a systematic study of the ""transcriptome"" and the ""miRNAome"" of NHP lung lesions. Temporal profiles will be obtained from NHPs infected with a low-dose of Mtb aerosols, accurately modeling long-term latent infection.  Profiles will also be obtained from NHPs in which latent TB is reactivated by simian AIDS. These system-wide profiles, in conjugation  with  the  clinical,  microbiological  and  immunological  data  obtained  from  infected  NHPs  will  generate  statistical learning algorithms and mixed effects computational models of latent and reactivation TB.    The relevance of some of the most informative set of genetic predictors available from the data collected will be tested back in both the NHP model, as well as in human patients. The expression profiles of CCL24, CCL25 and CCL27 show negative correlation with all other chemokine ligands and receptors in primate TB granulomas. The expression of these three chemokines is significantly increased in late, rather than early lesions. We hypothesize that these chemokines are important for the long-term maintenance of primate lesions harboring latent Mtb bacilli. The expression of LAG3 was induced more than 40-fold in early primate lesions relative to late ones. LAG3 is a novel marker of Treg cells. We hypothesize that LAG3 is responsible for negatively regulating protective immune responses generated by effector T cells in primate TB lesions.  The expression of ""latency"" specific genes CCL24/25/27 and the ""active-TB: specific  gene  LAG3  will  be  silenced  in  NHPs  using  a  novel  lipidated-siRNA  nanoparticle  based  approach.  The progression of latent disease and its immunological and molecular correlates will then be studied in these animals. Finally, the expression of an immune response to these and other ""latency"" and ""reactivation""- specific  profiles  will  be  determined  in  human  patients  of  latent  and  active  TB,  as  well  as  TB/AIDS  co- infected patients. These systems-biology studies will likely exponentially enhance our understanding of TB latency and reactivation in a host that mimics both TB and AIDS in the closest possible manner to humans.  Eventually, these advances may empower clinicians better to detect and treat latent TB.       PUBLIC HEALTH RELEVANCE:  Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised.  Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation.     Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced  in  NHP  TB  lesions,  but  silence  over  the  course  of  several  weeks.  This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model.  Towards  this,  we  have  assembled  a  highly  diverse  and  collaborative  team  including  microbiologists,  aerobiologists,  bioinformaticians,  mathematicians/statisticians,  veterinarians,  veterinary  pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.              PUBLIC HEALTH RELEVANCE: Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised. Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation. Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced in NHP TB lesions, but silence over the course of several weeks. This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model. Towards this, we have assembled a highly diverse and collaborative team including microbiologists, aerobiologists, bioinformaticians, mathematicians/statisticians, veterinarians, veterinary pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.",Transcriptomics of Tuberculosis Latency and Reactivation in Primates,8145569,R01HL106790,"['Acquired Immunodeficiency Syndrome', 'Aerosols', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Automobile Driving', 'Bacillus (bacterium)', 'Back', 'Biological', 'Biological Process', 'Biology', 'Biopsy', 'CCL24 gene', 'CCL25 gene', 'CCL27 gene', 'Caring', 'Cells', 'Clinical', 'Communicable Diseases', 'Complex', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Dose', 'Emergency Situation', 'Equilibrium', 'Excision', 'Exhibits', 'Focus Groups', 'Gene Expression Profile', 'Gene Silencing', 'Genes', 'Genetic', 'Genetic Models', 'Granuloma', 'Granulomatous', 'Growth', 'HIV', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Infection', 'Inflammatory', 'Lesion', 'Life', 'Ligands', 'Lung', 'Macaca mulatta', 'Machine Learning', 'Maintenance', 'Mediator of activation protein', 'Metadata', 'Methodology', 'MicroRNAs', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'Organogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physiology', 'Primates', 'Process', 'Proteins', 'Pulmonary Tuberculosis', 'RNA Interference', 'Regulatory T-Lymphocyte', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'SIV', 'Sampling', 'Signal Pathway', 'Simian Acquired Immunodeficiency Syndrome', 'Small Interfering RNA', 'Statistical Models', 'System', 'Systems Biology', 'T-Lymphocyte', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transcript', 'Tuberculosis', 'Vaccines', 'Veterinarians', 'base', 'chemokine', 'combat', 'empowered', 'genome-wide', 'immunopathology', 'in vivo', 'innovation', 'latent infection', 'nanoparticle', 'nonhuman primate', 'novel', 'novel marker', 'pathogen', 'prevent', 'public health relevance', 'pulmonary granuloma', 'reactivation from latency', 'receptor', 'respiratory', 'response', 'tool', 'transcriptomics', 'tuberculosis granuloma']",NHLBI,TULANE UNIVERSITY OF LOUISIANA,R01,2011,756737,0.01167552062327812
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,8039246,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Health', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Systems Development', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2011,526649,0.02470344118796245
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7802898,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Comorbidity', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2010,136911,-0.010132273182725056
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7851323,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2010,278345,0.0022368166289779964
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,8013208,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Performance', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Randomized', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Voting', 'Work', 'base', 'computer cluster', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'public health relevance', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSILICOS,R44,2010,376899,0.019300539949593423
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7881671,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2010,264640,-0.0055122548994077985
"Textpresso information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso information retrieval and extraction system for biological literature,7772342,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2010,326303,-0.009952817373637863
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7908806,R01LM008111,"['Address', 'Biomedical Research', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,618469,-0.01454513477207519
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7805478,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'global health', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'relational database', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2010,339537,0.025234755989556216
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7935408,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2010,515594,-0.01191674862862718
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7754089,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2010,2037327,0.000984925043692754
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7846105,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,326254,-0.0012054624015386895
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7799875,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2010,250650,-0.011751589450473564
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7929664,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2010,277200,-0.008392538545642286
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7941984,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'phase 1 study', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2010,303186,-0.0026117046836027632
"Transcriptomics of Tuberculosis Latency and Reactivation in Primates    DESCRIPTION (provided by applicant):  Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, in a quiescent state. These bacilli are able to reactivate and cause pulmonary TB, when the immune system is compromised. Hence, a complete understanding of TB latency and reactivation is required for the effective control of TB. The research models and tools necessary to perform these studies are now available.   Nonhuman Primates (NHPs) are excellent models of TB, especially to study the progression of experimental infection to latency, and to study the pathology and biology of granulomatous lesions - the hallmarks of TB infections. We have established a model of human TB, by exposing NHPs to true Mtb aerosols. While many research groups focus on the bacterial factors of latency and reactivation, we would like to leverage our highly tractable model to identify host signatures and mediators of this process.    We  show  that  pro-inflammatory  immune  signaling  pathways,  initially  induced  in  NHP  TB  lesions,  are  overwhelmingly  silenced  over  the  course  of  next  several  weeks.  This transcriptional reprogramming could be a host response to changes in bacterial replication and physiology. Further, these  responses  could  reflect  the  efforts  of  the  pathogen  to  prevent  excessive  immunopathology  during  the  infection of lungs. The central hypothesis of our proposal is that host granuloma responses can be used to predict latent and reactivation TB. We propose to perform a systematic study of the ""transcriptome"" and the ""miRNAome"" of NHP lung lesions. Temporal profiles will be obtained from NHPs infected with a low-dose of Mtb aerosols, accurately modeling long-term latent infection.  Profiles will also be obtained from NHPs in which latent TB is reactivated by simian AIDS. These system-wide profiles, in conjugation  with  the  clinical,  microbiological  and  immunological  data  obtained  from  infected  NHPs  will  generate  statistical learning algorithms and mixed effects computational models of latent and reactivation TB.    The relevance of some of the most informative set of genetic predictors available from the data collected will be tested back in both the NHP model, as well as in human patients. The expression profiles of CCL24, CCL25 and CCL27 show negative correlation with all other chemokine ligands and receptors in primate TB granulomas. The expression of these three chemokines is significantly increased in late, rather than early lesions. We hypothesize that these chemokines are important for the long-term maintenance of primate lesions harboring latent Mtb bacilli. The expression of LAG3 was induced more than 40-fold in early primate lesions relative to late ones. LAG3 is a novel marker of Treg cells. We hypothesize that LAG3 is responsible for negatively regulating protective immune responses generated by effector T cells in primate TB lesions.  The expression of ""latency"" specific genes CCL24/25/27 and the ""active-TB: specific  gene  LAG3  will  be  silenced  in  NHPs  using  a  novel  lipidated-siRNA  nanoparticle  based  approach.  The progression of latent disease and its immunological and molecular correlates will then be studied in these animals. Finally, the expression of an immune response to these and other ""latency"" and ""reactivation""- specific  profiles  will  be  determined  in  human  patients  of  latent  and  active  TB,  as  well  as  TB/AIDS  co- infected patients. These systems-biology studies will likely exponentially enhance our understanding of TB latency and reactivation in a host that mimics both TB and AIDS in the closest possible manner to humans.  Eventually, these advances may empower clinicians better to detect and treat latent TB.       PUBLIC HEALTH RELEVANCE:  Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues, reactivating when the immune system is compromised.  Hence, effective and long-term control of TB requires a better understanding of TB latency and reactivation.     Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced  in  NHP  TB  lesions,  but  silence  over  the  course  of  several  weeks.  This could be a response to the progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems biology approach to study the host granulomatous response to Mtb latency and reactivation, using our NHP model.  Towards  this,  we  have  assembled  a  highly  diverse  and  collaborative  team  including  microbiologists,  aerobiologists,  bioinformaticians,  mathematicians/statisticians,  veterinarians,  veterinary  pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.               NARRATIVE    Tuberculosis (TB), caused by Mycobacterium tuberculosis (Mtb), is a global infectious disease emergency. A  major hurdle in combating TB is the fact Mtb is able to persist for long periods of time in host tissues,  reactivating  when  the  immune  system  is  compromised.  Hence,  effective  and  long-term  control  of  TB  requires a better understanding of TB latency and reactivation.     Nonhuman Primates (NHPs) are excellent models of TB. We have recently established a model of human  TB, by exposing NHPs to Mtb aerosols. Pro-inflammatory immune signaling pathways are initially induced  in  NHP  TB  lesions,  but  silence  over  the  course  of  several  weeks.  This  could  be  a  response  to  the  progression of Mtb to a latent phase of growth within these NHP granulomas. We will employ a systems  biology approach to study the host granulomatous response to Mtb latency and reactivation, using our  NHP  model.  Towards  this,  we  have  assembled  a  highly  diverse  and  collaborative  team  including  microbiologists,  aerobiologists,  bioinformaticians,  mathematicians/statisticians,  veterinarians,  veterinary  pathologists, as well as infectious disease and critical respiratory care clinicians/clinician-researchers.    ",Transcriptomics of Tuberculosis Latency and Reactivation in Primates,8052291,R01HL106790,"['Acquired Immunodeficiency Syndrome', 'Aerosols', 'Algorithms', 'Animal Model', 'Animals', 'Area', 'Automobile Driving', 'Bacillus (bacterium)', 'Back', 'Biological', 'Biological Process', 'Biology', 'Biopsy', 'CCL24 gene', 'CCL25 gene', 'CCL27 gene', 'Caring', 'Cells', 'Clinical', 'Communicable Diseases', 'Complex', 'Computer Simulation', 'Data', 'Development', 'Disease', 'Dose', 'Emergency Situation', 'Equilibrium', 'Excision', 'Exhibits', 'Focus Groups', 'Gene Expression Profile', 'Gene Silencing', 'Genes', 'Genetic', 'Genetic Models', 'Granuloma', 'Granulomatous', 'Growth', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunology', 'Infection', 'Inflammatory', 'Lesion', 'Life', 'Ligands', 'Lung', 'Macaca mulatta', 'Machine Learning', 'Maintenance', 'Mediator of activation protein', 'Metadata', 'Methodology', 'MicroRNAs', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'Organogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Phase', 'Physiology', 'Primates', 'Process', 'Proteins', 'Pulmonary Tuberculosis', 'RNA Interference', 'Regulatory T-Lymphocyte', 'Relative (related person)', 'Research', 'Research Personnel', 'Risk', 'SIV', 'Sampling', 'Signal Pathway', 'Simian Acquired Immunodeficiency Syndrome', 'Small Interfering RNA', 'Statistical Models', 'System', 'Systems Biology', 'T-Lymphocyte', 'Testing', 'Therapeutic', 'Time', 'Tissues', 'Transcript', 'Tuberculosis', 'Vaccines', 'Veterinarians', 'base', 'chemokine', 'combat', 'empowered', 'genome-wide', 'immunopathology', 'in vivo', 'innovation', 'latent infection', 'nanoparticle', 'nonhuman primate', 'novel', 'novel marker', 'pathogen', 'prevent', 'public health relevance', 'pulmonary granuloma', 'reactivation from latency', 'receptor', 'respiratory', 'response', 'tool', 'transcriptomics', 'tuberculosis granuloma']",NHLBI,TULANE UNIVERSITY OF LOUISIANA,R01,2010,790087,0.01167552062327812
"Enhancing 3dsvm to improve its interoperability and dissemination    DESCRIPTION (provided by applicant): This research plan outlines crucial software enhancements to a program called 3dsvm, which is a command line program and graphical user interface (gui) plugin for AFNI (Cox, 1996). 3dsvm performs support vector machine (SVM) analysis on fMRI data, which constitutes one important approach to performing multivariate supervised learning of neuroimaging data. 3dsvm originally provided the ability to analyze fMRI data as described in (LaConte et al., 2005). Since its first distribution as a part of AFNI, it has been steadily extended to provide new functionality including regression and non-linear kernels, as well as multiclass classification capabilities. In addition to its integration into AFNI, features that make 3dsvm particularly well suited for fMRI analysis are that it is easy to spatially mask voxels (to include/exclude them in the SVM analysis) as well as to flexibly select subsets of a dataset to use as training or testing samples. It has been used to generate results for our own work and for collaborative efforts and has been cited as a resource by others (Mur et al. 2009; Hanke et al. 2009). Despite many positive aspects of 3dsvm, the priorities of PAR-07-417 address a genuine need that this software project has - the ability to focus on improvements that will increase its dissemination and interoperability. A major motivation for PAR-07-417 is to facilitate the improved interface, characterization, and documentation to enhance the extent of sharing and to provide the groundwork for future extensions. Our aims are well aligned with this program announcement. Further, there is a growing need in the neuroimaging community for tools such as 3dsvm. Since 3dsvm is not a new project, is tightly integrated into the software environment of AFNI, and can be further integrated to enable better functionality to support needs as diverse as NIfTI format capabilities to rtFMRI, this proposed project will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.      PUBLIC HEALTH RELEVANCE: This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.           NARRATIVE This proposal focuses on improving, characterizing, and documenting an existing neuroinformatics software tool. The project described will help to further the NIH Blueprint for Neuroscience Research by supporting its need for wide-spread adoption of high-quality neuroimaging tools.",Enhancing 3dsvm to improve its interoperability and dissemination,8278135,R03EB012464,[' '],NIBIB,VIRGINIA POLYTECHNIC INST AND ST UNIV,R03,2010,156500,0.004228752563336715
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,8076789,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2010,956625,0.028233042646701848
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7774343,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'conflict resolution', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2010,525262,0.02470344118796245
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7945368,R01HG004836,"['Anatomy', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Communities', 'Competence', 'Complex', 'Computer software', 'Consultations', 'Controlled Vocabulary', 'Dana-Farber Cancer Institute', 'Data', 'Databases', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Fostering', 'Future', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'In Vitro', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methodology', 'Methods', 'Molecular', 'National Cancer Institute', 'Nature', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'Validation', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'design', 'empowered', 'gene function', 'graphical user interface', 'information organization', 'instrument', 'interoperability', 'novel', 'open source', 'repository', 'research study', 'response', 'scale up', 'sound', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2010,428079,0.02278272899691479
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7928868,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2010,605009,0.0017697180575018147
"Data Management and Coordinating Center (DMCC) This application seeks funding for the Data Management and Coordinating Center (DMCC) (formerly known as Data Technology Coordinating Center, DTCC) for the Rare Diseases Clinical Research Network (RDCRN). The applicant, Dr. Krischer, has served as the Principal Investigator for the DTCC for the last 5 years and seeks to renew the cooperative agreement for the DMCC which supports the Rare Diseases Clinical Research Network (RDCRN). The DMCC propose to extend the systems, processes, and procedures developed successfully over the last grant cycle to accommodate the 3000 subjects enrolled on 32 current studies, contingent upon the successful re-competition of their associated clinical research consortia, addition of new studies reflecting the growth of the network, accommodation of federated databases, work with consortia that have pre-existing infrastructure (registries, patient databases, etc.) and registries, provide a user friendly website for web-based recruitment which receives over 3.4 million hits per year at present and a 4000+ member contact registry enhanced for subjects seeking enrollment on clinical trials. We will continue development of new technologies to support scalability and generalizability and tools for cross-disease data mining. Our international clinical information network is secure providing coordinated data management services for collection, storage and analysis of diverse data types from multiple diseases and geographically disparate locations and a portal for the general public and larger community of clinical investigators. The proposed DMCC will facilitate clinical research in rare diseases by providing a test-bed for distributed  clinical data management that incorporates novel approaches and technologies for data management, data  mining, and data sharing across rare diseases, data types, and platforms; and access to information related  to rare diseases for basic and clinical researchers, academic and practicing physicians, patients, and the lay public.",Data Management and Coordinating Center (DMCC),8150047,U54NS064808,"['Address', 'Adherence', 'Algorithms', 'Architecture', 'Archives', 'Area', 'Automatic Data Processing', 'Beds', 'Biological', 'Bite', 'Cancer Patient', 'Case Report Form', 'Cellular Phone', 'Characteristics', 'Clinical', 'Clinical Data', 'Clinical Management', 'Clinical Research Associate', 'Clinical Research Protocols', 'Clinical Trials Cooperative Group', 'Collaborations', 'Collection', 'Committee Members', 'Common Data Element', 'Common Terminology Criteria for Adverse Events', 'Computer software', 'Custom', 'Cystic Fibrosis', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Quality', 'Data Reporting', 'Data Set', 'Databases', 'Decision Trees', 'Descriptor', 'Development', 'Diagnosis', 'Diagnostic radiologic examination', 'Disasters', 'Disease', 'Electronic Mail', 'Electronics', 'Eligibility Determination', 'Engineering', 'Enrollment', 'Ensure', 'Environment', 'Epidemiology', 'Etiology', 'Evaluation', 'Event', 'Exclusion Criteria', 'Expert Systems', 'Extensible Markup Language', 'Faculty', 'Family history of', 'Feedback', 'Flare', 'Foundations', 'Freezing', 'Frequencies', 'Funding', 'Future', 'Genetic', 'Genetic Transcription', 'Genus - Lotus', 'Grant', 'Grouping', 'Growth', 'Guidelines', 'Hand', 'Health', 'Image', 'Individual', 'Industry', 'Information Networks', 'Information Systems', 'Informed Consent', 'Institution', 'Insulin-Dependent Diabetes Mellitus', 'International', 'Internet', 'Interview', 'Label', 'Laboratories', 'Laboratory Research', 'Language', 'Laws', 'Lead', 'Learning', 'Letters', 'Libraries', 'Life', 'Link', 'Location', 'Logic', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mails', 'Manuals', 'Maps', 'Mechanics', 'Medical', 'Medical History', 'Methodology', 'Methods', 'Monitor', 'Monitoring Clinical Trials', 'Nature', 'Neurofibromatoses', 'Notification', 'Online Systems', 'Optics', 'Outcome Study', 'Pamphlets', 'Paralysed', 'Pathologic', 'Pathology', 'Patient Outcomes Assessments', 'Patients', 'Persons', 'Pharmacy facility', 'Phase', 'Physical environment', 'Physicians', 'Pilot Projects', 'Policies', 'Positron-Emission Tomography', 'Principal Investigator', 'Printing', 'Procedures', 'Process', 'Production', 'Programming Languages', 'Protocol Compliance', 'Protocols documentation', 'Publications', 'Published Directory', 'Publishing', 'Qualifying', 'Quality Control', 'Quality of life', 'Radiology Specialty', 'Randomized', 'Reader', 'Records', 'Recovery', 'Recruitment Activity', 'Registries', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Resolution', 'Resources', 'Risk', 'Role', 'SNOMED Clinical Terms', 'Scanning', 'Scientist', 'Security', 'Selection Bias', 'Self Assessment', 'Services', 'Side', 'Single-Gene Defect', 'Site', 'Site Visit', 'Source', 'Specific qualifier value', 'Specimen', 'Stream', 'Support Groups', 'System', 'Techniques', 'Technology', 'Test Result', 'Testing', 'Text', 'Time', 'Training', 'Translations', 'Trees', 'U-Series Cooperative Agreements', 'United States National Institutes of Health', 'Update', 'Validation', 'Variant', 'Videoconferences', 'Videoconferencing', 'Visit', 'Visual', 'Voice', 'Work', 'X-Ray Computed Tomography', 'base', 'computerized data processing', 'data management', 'data mining', 'electronic data', 'follow-up', 'improved', 'interest', 'meetings', 'patient advocacy group', 'programs', 'quality assurance', 'radiologist', 'sample collection', 'statistics', 'tool', 'web site', 'working group']",NINDS,UNIVERSITY OF SOUTH FLORIDA,U54,2010,959195,0.002513241226822428
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7910730,P41HG004059,[' '],NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2010,1093220,0.00043955184949191425
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7597080,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,138161,-0.010132273182725056
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7920591,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'clinical practice', 'computer science', 'cost effectiveness', 'data management', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'patient population', 'primary outcome', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2009,54000,-0.010132273182725056
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7656692,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2009,282304,0.0022368166289779964
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7651469,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'genome database', 'improved', 'interest', 'natural language', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2009,273993,-0.01717961573509041
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7651387,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'genome-wide', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2009,267801,-0.0055122548994077985
"Textpresso: information retrieval and extraction system for biological literature    DESCRIPTION (provided by applicant): We developed an information retrieval and extraction system that processes the full text of biological papers. The system, called Textpresso, separates text into sentences, labels words and phrases according to an ontology (an organized lexicon), and allows queries to be performed on a database of labeled sentences. The current ontology comprises approximately one hundred categories of terms, such as ""gene"", ""regulation"", ""human disease"", ""brain area"" etc., and also contains main Gene Ontology (GO) categories. Extraction of particular biological facts, such as gene-gene interactions, or the curation of GO cellular components, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences. Search engine for four literatures, C. elegans, Drosophila, Arabidopsis and Neuroscience have been established by us, and nine systems for other literatures have been developed by other groups around the world. The system will be further developed in many aspects. In collaboration with the respective model organism databases, we will set up literature search engine for zebrafish, rat and Dictyostelium and consider systems for important diseases such as cancer, Alzheimer's and AIDS. We will improve the quality of searchable full text by carrying super- and subscripts as well as special character information, and recognizing subsections of a paper. Website and system enhancement will include synonym searches, better website customization features (""myTextpresso""), browsing and searching a paper taxonomy, implementation of batch queries and notification of search result changes due to corpus changes. We will offer webservices for Textpresso and maintain a public subversion system for the software. Named entity recognition algorithms will be implemented to find new terms for the ontology from full text. We will work on the problem of high specificity of terms in the lexica, which reduces recall, and enable searches for GO annotations. Strategies for (semi-) automated literature curation include installing a paper triage system and first pass curation to identify where in a paper which relevant data types can be found. Automated curation tasks include producing connections between a paper and a biological entity such as gene. We will develop learning algorithms that discover new categories and lexica in text. We will improve our curation strategy of developing specialized curation categories that are used to retrieve specific data, and develop corresponding curator interfaces to automate the processing pipeline from full text to database. We will research and implement new, more semantically oriented ways of searching by combining latent semantic indexing with new similarity measures. Machine learning algorithms for classifying sentences and extracting information will be implemented using hidden Markov models. A new approach of finding categories and lexica using graph theory will be investigated. PUBLIC HEALTH RELEVANCE: Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.           Narrative Biomedical researchers need to read or skim many thousands of scientific articles each year, more than is humanly possible. This project will extend and improve an automatic system, Textpresso, that finds relevant sentences within millions of sentences that likely contain crucial information. Textpresso also extracts some types of information automatically, making it possible to have organized databases of important information.",Textpresso: information retrieval and extraction system for biological literature,7583249,R01HG004090,"['Access to Information', 'Acquired Immunodeficiency Syndrome', 'Address', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Arabidopsis', 'Area', 'Biological', 'Biological Models', 'Biological Sciences', 'Biological databases', 'Body of uterus', 'Brain', 'Caenorhabditis elegans', 'Categories', 'Cells', 'Classification', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Databases', 'Development', 'Dictyostelium', 'Disease', 'Drosophila genus', 'Feedback', 'Figs - dietary', 'Gene Expression', 'Gene Expression Regulation', 'Gene Proteins', 'Genes', 'Genome', 'Gold', 'Graph', 'Individual', 'Information Retrieval', 'Label', 'Learning', 'Literature', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Measures', 'Methods', 'Names', 'Natural Language Processing', 'Neurosciences', 'Notification', 'Ontology', 'Organism', 'Paper', 'Process', 'Rattus', 'Reading', 'Research', 'Research Personnel', 'Retrieval', 'Scientist', 'Semantics', 'Site', 'Software Tools', 'Specificity', 'Speed', 'System', 'Taxonomy', 'Testing', 'Text', 'Training', 'Triage', 'Work', 'Writing', 'Zebrafish', 'base', 'biological systems', 'gene function', 'gene interaction', 'genome sequencing', 'human disease', 'improved', 'indexing', 'markov model', 'model organisms databases', 'novel strategies', 'phrases', 'public health relevance', 'software systems', 'text searching', 'theories', 'tool', 'web interface', 'web site']",NHGRI,CALIFORNIA INSTITUTE OF TECHNOLOGY,R01,2009,320000,-0.009952817373637863
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7668609,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Ontology', 'Phenotype', 'Readiness', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,613451,-0.01454513477207519
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7848604,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170861,0.025234755989556216
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7901729,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,170789,0.025234755989556216
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7612766,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'public health research', 'success', 'theories', 'tool', 'transmission process', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2009,342967,0.025234755989556216
"Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral    DESCRIPTION (provided by applicant):       Recent developments in text mining research, and in scientific publication, have brought us to the moment when the long-standing potential of natural language processing technology to benefit biomedical researchers may finally be realized. Technological advances, recent results in computational linguistics, maturation of biomedical ontology, and the advent of resources such as PubMedCentral have set the stage for an attempt at an integrated computational analysis of a large proportion of the full text biomedical literature. Such an analysis has the potential to dramatically extend the way that biomedical researchers can effectively use the scientific literature, particularly in the analysis of genome-scale datasets, broadly accelerating and increasing the efficiency of scientific discovery. We hypothesize that it is now possible to extract a wide variety of ontologically-grounded entities and relationships by processing the entire PubMedCentral document collection accurately and with good coverage, to use this extracted information to produce new genres of scientifically valuable tools and analysis techniques, and to demonstrate its utility in the analysis of genome-scale data. The challenges that we plan to overcome range from fundamental linguistic issues (e.g. cross- document coreference resolution) to high-performance computing (e.g. scaling up integrated processing to include millions of complex documents), to fielding practical systems that can exploit enormous knowledge-bases to accelerate the analysis of very large molecular data sets.              Project narrative Enormous amounts of biomedical information are now available in the PubMedCentral database, but computers cannot work with it because it is in the form of human-language text and humans can't read it all due to its large volume. The goal of this project is to harvest large amounts of that information automatically, making it available to humans in summarized form and to computers in computer-readable form.",Biomedical Language Processing Writ Large:  Scaling to all of PubMedCentral,7781934,R01LM009254,"['Biological', 'Collection', 'Complex', 'Computer Analysis', 'Computers', 'Data', 'Data Set', 'Databases', 'Development', 'Disease', 'Evaluation Research', 'Funding', 'Gene Expression', 'Genes', 'Genome', 'Goals', 'Harvest', 'Health', 'High Performance Computing', 'Human', 'Imagery', 'Journals', 'Knowledge', 'Language', 'Linguistics', 'Literature', 'Methods', 'Molecular', 'Natural Language Processing', 'Nature', 'Pharmaceutical Preparations', 'Process', 'Publications', 'Reading', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Staging', 'System', 'Techniques', 'Technology', 'Text', 'Work', 'biomedical ontology', 'clinically relevant', 'information organization', 'knowledge base', 'language processing', 'scale up', 'text searching', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2009,505564,-0.01191674862862718
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7582301,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool', 'working group']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2009,2057843,0.000984925043692754
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7669241,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,829379,0.00043955184949191425
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7921192,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Reader', 'Request for Proposals', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computer infrastructure', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'meetings', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web site', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2009,250001,0.00043955184949191425
"Accurate detection of chromosomal abnormalities with multi-color image processing    DESCRIPTION (provided by applicant): The combination of high resolution assays in genomics with microscopic imaging has been used for the detection of complex chromosomal rearrangements, a significant but difficult problem in prenatal and postnatal diagnosis, birth defect detection and cancer research. As a recently developed molecular cytogenetic technique, multiplex fluorescence in situ hybridization (M-FISH) imaging has provided rapid and high resolution detection of chromosomal abnormalities associated with cancer and genetic disorders. However, the technique is currently limited to research use and only serves as an adjunct tool to the G-banding based monochromatic chromosomal karyotyping in a clinical laboratory. A primary barrier of the technique is the lower classification accuracy when classifying chromosomes from multi-color microscopic imaging data. Therefore, the goal of this R15 project is to develop innovative multi-spectral image processing and machine learning techniques for M-FISH image analysis so that chromosomal rearrangement detection can be made more reproducible, robust, and faster, thereby significantly increasing the ability and efficacy of this newly developed cellular imaging technique. Our proposed approaches such as multiscale feature extraction, nonlinear manifold analysis and adaptive fuzzy clustering are able to target specific features of multi-spectral imaging data, promising a significant improvement over the current techniques. In order to validate the technique and bring it into clinical use, we will partner with a clinical geneticist, Dr. Merlin Butler, and a cytogeneticist, Dr. Diane Persons both at Kansas University Medical Center. In addition, we will collaborate with an industrial scientist, Dr. Kenneth Castleman, who is the pioneer in developing and commercializing cytogenetic imaging products. Through our interdisciplinary research and collaboration, we will accomplish the following specific aims: 1) develop image normalization approaches to improve the acquisition of multi-color FISH images; 2) develop multiscale dimension analysis to extract features from multi-color images; 3) design adaptive fuzzy clustering and incorporate contextual information to improve the pixel-wise classification of chromosomes; and 4) validate computational approaches with clinical testing in collaboration with medical and industrial partners. This research project will also enhance our research infrastructure in biomedical image informatics and provide undergraduate and graduate students opportunities to touch the frontier of molecular and cellular imaging by participating in the proposed research activities. PUBLIC HEALTH RELEVANCE: Unraveling complex rearrangements using cytogenetic approaches such as M-FISH imaging has been extremely useful in prenatal, postnatal and cancer diagnoses. Our proposed approaches have the potential to significantly improve the reliability of the newly developed M-FISH imaging technique, making it feasible for clinical use. This will in turn benefit the health of human beings. Furthermore, the developed computational techniques can be applicable to a wide range of multi-color bio-imaging problems, thereby having a broad impact on the biomedical community.           Project Narrative Unraveling complex rearrangements using cytogenetic approaches such as M-FISH imaging has been extremely useful in prenatal, postnatal and cancer diagnoses. Our proposed approaches have the potential to significantly improve the reliability of the newly developed M-FISH imaging technique, making it feasible for clinical use. This will in turn benefit the health of human beings. Furthermore, the developed computational techniques can be applicable to a wide range of multi-color bio-imaging problems, thereby having a broad impact on the biomedical community.",Accurate detection of chromosomal abnormalities with multi-color image processing,7727717,R15GM088802,"['Academic Medical Centers', 'Academic Research Enhancement Awards', 'Address', 'Algorithms', 'Biological Assay', 'Cells', 'Chromosomal Rearrangement', 'Chromosome abnormality', 'Chromosomes', 'Chromosomes, Human, Pair 4', 'Cities', 'Classification', 'Clinical', 'Clinical Data', 'Collaborations', 'Color', 'Communities', 'Complex', 'Computational Technique', 'Congenital Abnormality', 'Cytogenetic Analysis', 'Cytogenetics', 'DNA Sequence Rearrangement', 'Data', 'Databases', 'Detection', 'Development', 'Diagnosis', 'Dimensions', 'Engineering', 'Environment', 'Figs - dietary', 'Fluorescent in Situ Hybridization', 'G-Banding', 'Genetic', 'Genomics', 'Goals', 'Health Benefit', 'Hereditary Disease', 'Human', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Imaging problem', 'Interdisciplinary Study', 'Kansas', 'Karyotype', 'Karyotype determination procedure', 'Laboratories', 'Learning', 'Machine Learning', 'Masks', 'Medical', 'Methods', 'Microscopic', 'Missouri', 'Molecular Probes', 'Neurofibromin 2', 'Patients', 'Persons', 'Research', 'Research Activity', 'Research Infrastructure', 'Research Project Grants', 'Resolution', 'Resources', 'Schools', 'Scientist', 'Signal Transduction', 'Spectral Karyotyping', 'Techniques', 'Technology', 'Time', 'Touch sensation', 'Training', 'United States National Institutes of Health', 'Universities', 'anticancer research', 'assay development', 'base', 'bioimaging', 'cancer diagnosis', 'cancer genetics', 'cellular imaging', 'clinical application', 'clinical effect', 'design', 'experience', 'frontier', 'graduate student', 'high throughput screening', 'image processing', 'imaging informatics', 'improved', 'innovation', 'leukemia', 'meetings', 'molecular/cellular imaging', 'multidisciplinary', 'novel', 'postnatal', 'prenatal', 'prevent', 'programs', 'public health relevance', 'research clinical testing', 'response', 'tool']",NIGMS,UNIVERSITY OF MISSOURI KANSAS CITY,R15,2009,229519,-0.03347816906972955
"Informatics Infrastructure for vector-based neuroanatomical atlases    DESCRIPTION (provided by applicant):  The adage:  'all data is spatial' is especially pertinent in the field of neuroscience, since neuronal data must be indexed by the neuroanatomical location of phenomena or entities under study.  Brain atlases are very widely used as laboratory tools, being some of the most highly cited publications in science.  This proposal seeks to use brain atlases as a method for indexing data from the literature in a neuroinformatics system.  This data includes both textual and graphical information, ranging from highly detailed maps constructed from vector- based spatial primitives, histological photographs and drawings, to textual reports of experimental findings in the literature (which we will analyze on a large scale).  These data represent a significant scientific investment that are currently locked away in previously published journal articles (and the detailed data-sets and drawings from researchers that were used to write the articles).  A prototype that summarizes the last fifteen years' output of one of the world's most prominent neuroanatomy laboratories is immediately available.  This project will develop a collaborative environment to enable neuroscientists to use these valuable maps within the community as a whole by contributing data to a shared system with an open-source system (called 'NeuARt II') that permits querying, overlaying, viewing and annotating such data in an integrated manner.  We will also use Natural Language Processing (NLP) techniques to index neuroanatomical references in large numbers of journal articles to be accessible within our infrastructure.  As an initial text corpus, we over 110,000 documents taken from last 35 years of published information from the primary neuroanatomical literature.  We will construct a neuroanatomical interface that conceptually resembles the 'Google Maps' system, permitting users to use an intuitive spatial interface to browse large amounts of biomedical data in spatial register.  The proposed infrastructure will also provide new opportunities to compare and synthesize the anatomical components of neuroscience data multiple modalities (physiological, behavioral, clinical, genetic, molecular, etc.).PUBLIC HEALTH RELEVANCE:  Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject.  Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source:  Anxiety Disorders Association of America).  A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.        Narrative Given the scope of the use of neuroanatomical maps from the rat and mouse in the study of neurobiological disease, we expect our research impact scientists working in almost all subfields of the subject. Our collaborators who form the early adopters of this approach are neuroendocrinology researchers studying the mechanisms of stress and anxiety disorders which are estimated to affect 19.1 million people in the USA, costing $42 billion in health care costs per year (source: Anxiety Disorders Association of America). A better understanding of the neuronal mechanisms underlying these disorders could lead to new, effective therapies and treatments.",Informatics Infrastructure for vector-based neuroanatomical atlases,7582189,R01MH079068,"['Accounting', 'Address', 'Affect', 'Americas', 'Anxiety Disorders', 'Atlases', 'Behavioral', 'Body of uterus', 'Brain', 'Chromosome Mapping', 'Clinical', 'Communities', 'Community Outreach', 'Data', 'Data Set', 'Databases', 'Devices', 'Disease', 'Engineering', 'Environment', 'Fluoro-Gold', 'Harvest', 'Health Care Costs', 'Image', 'Informatics', 'Internet', 'Investments', 'Laboratories', 'Lead', 'Lesion', 'Literature', 'Location', 'Maps', 'Methods', 'Modality', 'Molecular Genetics', 'Mus', 'Names', 'Natural Language Processing', 'Neuroanatomy', 'Neurobiology', 'Neuroendocrinology', 'Neurons', 'Neurosciences', 'Online Systems', 'Output', 'Paper', 'Physiological', 'Publications', 'Publishing', 'Rattus', 'Reporting', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Source', 'Stress', 'Structure', 'System', 'Techniques', 'Text', 'Tissues', 'Work', 'Writing', 'base', 'biomedical ontology', 'cost', 'effective therapy', 'indexing', 'journal article', 'neuroinformatics', 'novel', 'open source', 'prototype', 'public health relevance', 'research study', 'text searching', 'tool', 'vector']",NIMH,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,312453,-0.0012054624015386895
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7591237,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Funding', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Language', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Ontology', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'graduate student', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2009,272818,-0.011751589450473564
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7693803,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'biomedical ontology', 'genome-wide', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2009,280000,-0.008392538545642286
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of  computer scientists, software engineers, and medical investigators who develop computational tools for the  analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure  and environment for the development of computational algorithms and open source technologies, and then  oversee the training and dissemination of these tools to the medical research community. This world-class  software and development environment serves as a foundation for accelerating the development and  deployment of computational tools that are readily accessible to the medical research community. The team  combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state  of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed,  open-source environment) to enable computational examination of both basic neurosience and neurologicat  disorders. In developing this infrastructure resource, the team will significantly expand upon proven open  systems technology and platforms. The driving biological projects will come initially from the study of  schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open  systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures  and connectivity patterns in the brain, derangements of which have long been thought to play a role in the  etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range  of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially  including microscopic, genomic, and other image data. It will apply to image data from individual  )atients,and to studies executed across large poplulations. The data will be taken from subjects across a  Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7915998,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Genomics', 'Goals', 'Healthcare', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Time', 'Tissues', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2009,3720664,0.018068477087959858
"A Fully Automatic System For Verified Computerized Stereoanalysis    DESCRIPTION (provided by applicant):  A Fully Automatic System For Verified Computerized Stereoanalysis SUMMARY The requirement for a trained user to interact with tissue and images is a long-standing impediment to higher throughput analysis of biological microstructures using unbiased stereology, the state-of-the-art method for accurate quantification of biological structure. Phase 1 studies addressed this limitation with Verified Computerized Stereoanalysis (VCS), an innovative approach for automatic stereological analysis that improves throughput efficiency by 6-9 fold compared to conventional computerized stereology. Work in Phase 2 integrated VCS into the Stereologer"", an integrated hardware-software-microscopy system for stereological analysis of tissue sections and stored images. Validation studies of first-order stereological parameters. i.e., volume, surface area, length, number, confirmed that the color-based detection methods in the VCS approach achieve accurate results for automatic stereological analysis of high S:N biological microstructures. These studies indicate that fully automatic stereological analysis of tissue sections and stored images can be realized by elimination of two remaining barriers, which will be addressed in this Phase II Continuation Competing Renewal. In Aim 1, applications for feature extraction and microstructure classification, developed in part with funding from the Office of Naval Research, will be integrated into the VCS program. The new application (VCS II) will use these approaches to automatically detect and classify polymorphic microstructures of biological interest using a range of feature calculations, including size, color, border, shape, and texture, with support from active learning and Support Vector Machines. Work in Aim 2 will eliminate physical handling of glass slides during computerized stereology studies by equipping the Stereologer system with automatic slide loading/unloading technology controlled by the Stereologer system. This technology will approximately double the throughput efficiency of the current VCS program and support ""human-in-the-loop"" interaction for sample microstructures on the border between two or more adjacent classes. The studies in Aim 3 will rigorously test the hypothesis that fully automatic VCS can quantify first- and second-order stereological parameters, without a loss of accuracy compared to the current gold-standard - non-automatic computerized stereology, e.g., manual Stereologer. If these studies validate the accuracy of VCS II, then commercialization of the fully automatic program will facilitate the throughout efficiency for testing scientific hypotheses in a wide variety of biomedical research projects; reduce labor costs for computerized stereology studies; hasten the growth of our understanding of biological processes that underlie health, longevity, and disease; and accelerate the development of novel approaches for the therapeutic management of human disease. Solid evidence that the SRC and its strategic partners can effectively commercialize this technology is demonstrated by their worldwide sales and support of the Stereologer system for the past 13 years. Key personnel and participating institutions: 7 Peter R. Mouton, Ph.D. (PI), Stereology Resource Center, Chester, MD. 7 Dmitry Goldgof, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Larry Hall, Ph.D., University of South Florida Coll. Engineering, Tampa, Fl. 7 Joel Durgavich, MS, Systems Planning and Analysis, Alexandria, VA. 7 Kurt Kramer, MS, Computer Programmer, University of South Florida, Coll. Engineering, Tampa, Fl. 7 Michael E. Calhoun, Ph.D., Sinq Systems, Columbia, MD        PUBLIC HEALTH RELEVANCE: Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.           PROJECT NARRATIVE Many fields of scientific research require a trained expert to make tedious and repetitive measurements of microscopic changes in animal and human tissues. This project will produce a computer program that performs these measurements with equal accuracy to a trained expert, but with dramatic savings in time and costs. Allowing scientists to complete more research in less time will accelerate our understanding of the factors that promote health and longevity, and hasten progress toward the development of new treatments for human diseases.",A Fully Automatic System For Verified Computerized Stereoanalysis,7804332,R44MH076541,"['Active Learning', 'Address', 'Algorithms', 'Animals', 'Area', 'Arts', 'Biological', 'Biological Process', 'Biomedical Research', 'Blood capillaries', 'Cell Volumes', 'Classification', 'Color', 'Communities', 'Computer software', 'Computer-Assisted Image Analysis', 'Computers', 'Databases', 'Detection', 'Development', 'Disease', 'Doctor of Philosophy', 'Educational workshop', 'Engineering', 'Florida', 'Funding', 'Glass', 'Goals', 'Gold', 'Grant', 'Growth', 'Health', 'Histology', 'Human', 'Human Resources', 'Image', 'Institution', 'International', 'Length', 'Libraries', 'Longevity', 'Machine Learning', 'Manuals', 'Marketing', 'Measurement', 'Methodology', 'Methods', 'Microscopic', 'Microscopy', 'Noise', 'Performance', 'Phase', 'Phase I Clinical Trials', 'Probability', 'Research', 'Research Project Grants', 'Resources', 'Sales', 'Sampling', 'Savings', 'Scientist', 'Shapes', 'Signal Transduction', 'Slide', 'Small Business Innovation Research Grant', 'Solid', 'Staining method', 'Stains', 'Structure', 'Surface', 'System', 'Technology', 'Testing', 'Texture', 'Therapeutic', 'Time', 'Tissue Stains', 'Tissues', 'Training', 'United States National Institutes of Health', 'Universities', 'Update', 'Vendor', 'Work', 'base', 'capillary', 'commercialization', 'computer program', 'computerized', 'cost', 'digital imaging', 'high throughput analysis', 'human disease', 'human tissue', 'improved', 'innovation', 'interest', 'novel strategies', 'novel therapeutic intervention', 'programs', 'public health relevance', 'validation studies', 'vector']",NIMH,"STEREOLOGY RESOURCE CENTER, INC.",R44,2009,363247,-0.0026117046836027632
"Onto-BioThesaurus: ontological representation of gene/protein names for biomedica    DESCRIPTION (provided by applicant):       The long-term goal of our research is to develop resources and tools for knowledge retrieval management in the biomedical domain. As the pace of biomedical research accelerates, researchers become more and more dependent on computers to manage the explosive amount of biomedical information being published. The high quality of many databases is guaranteed by database curators who extract and synthesize information stored in literature or other databases. It is important to accurately recognize biomedical entity names in text and map the identified names to corresponding records in biomedical databases. Usually, a biomedical database provides a list of names either entered by curators or extracted from other databases. Those names could be used to retrieve records from databases or map names to database records by NLP systems. However, there are several characteristics associated with biomedical entity names, namely: synonymy (i.e., different names refer to the same database entry), ambiguity (i.e., one name is associated with different entries), and novelty (i.e., names or entities are not present in databases or knowledge bases) which make the task of retrieving database records using names and the task of associating names in text to database records very daunting. Additionally, biomedical entities can appear in text as short forms (SFs) abbreviated from their long forms (LFs). The prevalent use of SFs representing biomedical entities is another challenge faced by end users and NLP applications because of the high ambiguity of SFs.       Recently, ontology-based knowledge management is becoming increasingly popular since ontologies provide formal, machine-processable, and human-interpretable representations of the biomedical entities and their relations. We hypothesize that biomedical ontologies can be used to reduce the difficulty associated with retrieving records using names or mapping names in text to database records. Specific aims and the corresponding hypotheses are: i) develop onto-BioThesaurus by enriching BioThesaurus with gene/protein-related ontologies (Hypothesis: aligning gene/protein names to gene/protein-related ontologies can reduce the complexity associated with gene/protein names); ii) harvest synonyms for gene/protein classes and entities from online resources and text (Hypothesis: harvesting synonyms especially gene/protein SFs is critical since SFs are frequently used to represent gene/protein entities); iii) build a web user interface for gene/protein names and entries search and query through ontology-enabled onto-BioThesaurus (Hypothesis: enhancing BioThesaurus with gene/protein-related ontologies would enable us to build heuristic rules to enable machine reasoning); and iv) evaluate and distribute research methods/outcome (Hypothesis: evaluating and distributing research methods/outcome are critical to advance both basic and applied biomedical science.            The proposed research is critical for biomedical knowledge retrieval and management. It serves as one of the foundation for storing, retrieving, and extracting knowledge and information in the biomedical domain. Additionally, the proposed research will benefit biomedical researchers and general community for understanding and managing biomedical text through web interfaces and automated systems.",Onto-BioThesaurus: ontological representation of gene/protein names for biomedica,7654995,R01LM009959,"['Abbreviations', 'Biomedical Research', 'Characteristics', 'Communities', 'Computers', 'Databases', 'Expert Opinion', 'Foundations', 'Gene Proteins', 'Genes', 'Goals', 'Harvest', 'Human', 'Information Resources', 'Information Resources Management', 'Internet', 'Investigation', 'Knowledge', 'Literature', 'Manuals', 'Maps', 'Names', 'Natural Language Processing', 'Online Systems', 'Ontology', 'Outcome', 'Peer Review', 'Process', 'Proteins', 'Publishing', 'Records', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Retrieval', 'Review Literature', 'Science', 'Services', 'System', 'Techniques', 'Terminology', 'Text', 'Thesauri', 'Time', 'acronyms', 'base', 'biomedical ontology', 'heuristics', 'knowledge base', 'tool', 'web interface', 'web site']",NLM,GEORGETOWN UNIVERSITY,R01,2009,608650,-0.05242941001178496
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7660538,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical ontology', 'biomedical scientist', 'design', 'information organization', 'innovation', 'knowledge base', 'meetings', 'member', 'next generation', 'open source', 'repository', 'research and development', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2009,688362,0.028233042646701848
"Automated Integration of Biomedical Knowledge Today, ontologies are critical instruments for biomedical investigators, especially in those areas, such as cancer research, that require the command of a vast amount of information and a systemic approach to the design and interpretation of experiments. In fact, ontologies are proliferating in all areas of biomedical research, offering both challenges and opportunities. One of the principal challenges of this field stems from the fact that ontologies are developed in isolation, rendering it impossible to move, for instance, from genes to organisms, to diseases, to drugs. The National Center for Biomedical Ontology (NCBO) represents a fundamental endeavor in the collection, coordination and distribution of biomedical ontologies and offers an unparalleled opportunity to combine these biomedical ontologies into a single search space where genetic, anatomic, molecular and pharmacological information can be seamlessly explored and exploited as a holistic representation of biomedical knowledge. Unfortunately, ontology integration using standard means of manual curation is a labor intensive task, unable to scale up and keep up with the current growth rate of biomedical ontologies. We have developed a systematic framework for automated ontology engineering based on information theory, and we have successfully applied it to the analysis and engineering of Gene Ontology (GO), the development gene and protein databases, and the identification of peripheral biomarkers of disease progression and drug response. This project brings together a unique group of competences, ranging from ontology engineering, statistical signal processing, bioinformatics, cancer research, and clinical pharmacogenomics, to develop a principled method, grounded on the mathematics of information theory, to automatically combine and integrate biomedical ontologies and implement it as part of the NCBO architecture Ontologies are critical instruments for biomedical investigators especially in those areas, such as cancer research, that require a vast amount of information and a systemic approach to the design and interpretation of their experiments. In collaboration with the National Center for Biomedical Ontology (NCBO), this project will develop a principled method, grounded on the mathematics of information theory, to automatically combine biomedical ontologies. As a result, this project will integrate biomedical knowledge along dimensions that are today isolated and, in so doing, it will empower investigators with a new holistic understanding of disease, it will fast track the clinical  translation of biological discoveries, and it will change the approach to discovery, especially for those diseases that, like cancer, require a systemic view of their biological mechanisms.",Automated Integration of Biomedical Knowledge,7558468,R01HG004836,"['Anatomy', 'Architecture', 'Area', 'Artificial Intelligence', 'Bioinformatics', 'Biological', 'Biological Markers', 'Biomedical Research', 'Classification', 'Clinical', 'Collaborations', 'Collection', 'Colorectal Cancer', 'Competence', 'Complex', 'Development', 'Dimensions', 'Disease', 'Disease Progression', 'Engineered Gene', 'Engineering', 'Gene Proteins', 'Genes', 'Genetic', 'Goals', 'Growth', 'Human', 'Information Theory', 'Internet', 'Java', 'Knowledge', 'Link', 'Literature', 'Malignant Neoplasms', 'Manuals', 'Maps', 'Mathematics', 'Methods', 'Molecular', 'Ontology', 'Organism', 'Peripheral', 'Pharmaceutical Preparations', 'Pharmacogenomics', 'Proliferating', 'Protein Databases', 'Research Infrastructure', 'Research Personnel', 'Services', 'Side', 'Structure', 'Testing', 'Text', 'Tissues', 'Translations', 'anticancer research', 'base', 'biomedical ontology', 'computer based Semantic Analysis', 'computerized data processing', 'design', 'empowered', 'graphical user interface', 'insight', 'instrument', 'open source', 'programs', 'repository', 'research study', 'response', 'scale up', 'statistics', 'stem']",NHGRI,BRIGHAM AND WOMEN'S HOSPITAL,R01,2009,428078,0.02278272899691479
"Collaborative Development of Biomedical Ontologies and Terminologies    DESCRIPTION (provided by applicant): The development of ontologies that define entities and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage the burgeoning data that are pervasive in biology and medicine. The need to annotate, retrieve, and integrate high-throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. To date, these groups of ontology developers have been limited by the lack of methods and tools that facilitate distributed, collaborative engineering of large-scale ontologies and vocabularies. In this proposal, we outline three specific aims. First, we will explore basic computational methods that are essential for collaborative ontology engineering. We will investigate methods for representing diverse collaborative workflows, information about changes and concept history, trust, and provenance, and for recording decision making and design rationale. Empirical analysis of existing ontology-development projects will inform our construction of models for collaborative development workflows that will guide the processes of authoring, reviewing, and curating biomedical ontologies. Second, we will use the results from our first specific aim to build cProtigi, a set of robust, customizable, interactive tools to support distributed users in their collaborative work to build and edit terminologies and ontologies. Third, we will evaluate our work in the context of real-world, large-scale ontology-engineering projects, including the autism ontology of the National Database for Autism Research; the 11th revision of the WHO's International Classification of Diseases; the Ontology for Biomedical Investigations, under development by a wide range of NIH-supported researchers; and BiomedGT, under development by NCI. It is no longer feasible to imagine that investigators can create biomedical ontologies working independently. The collaborative methods that we will study and the tools that we will build will lead to expanded opportunities to support the diverse data- and knowledge-intensive activities that pervade BISTI, the CTSAs, the NCBCs, and myriad biomedical initiatives that require robust, scaleable ontologies. PUBLIC HEALTH RELEVANCE: The knowledge-based nature of modern medicine requires the use of ontologies and terminologies to process and integrate data. Ontology development itself becomes a collaborative process, with members of the larger research community contributing to and commenting on emerging ontologies. We plan to extend the Protigi ontology editor-the most widely used ontology editor today, with almost 100,000 registered users-to support collaborative development of ontologies and to evaluate the new tools by deploying them at the World Health Organization for the development of ICD-11 and in other settings.             n/a",Collaborative Development of Biomedical Ontologies and Terminologies,7565504,R01GM086587,"['Adopted', 'Autistic Disorder', 'Beds', 'Bioinformatics', 'Biology', 'Collaborations', 'Communities', 'Computer Systems Development', 'Computers', 'Computing Methodologies', 'Conflict (Psychology)', 'Consensus', 'Data', 'Data Set', 'Databases', 'Decision Making', 'Decision Support Systems', 'Development', 'Engineering', 'Evaluation', 'Generic Drugs', 'Goals', 'Human', 'Industry', 'Institutes', 'International Classification of Diseases', 'Internet', 'Investigation', 'Knowledge', 'Lead', 'Life', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Modeling', 'Modern Medicine', 'NCI Thesaurus', 'NIH Program Announcements', 'Natural Language Processing', 'Nature', 'Online Systems', 'Ontology', 'Outsourcing', 'Process', 'Program Development', 'Published Comment', 'Recording of previous events', 'Research', 'Research Personnel', 'Resolution', 'Scientist', 'Source', 'Staging', 'Terminology', 'Testing', 'Trust', 'United States National Institutes of Health', 'Vocabulary', 'Work', 'World Health Organization', 'biomedical ontology', 'biomedical scientist', 'cancer Biomedical Informatics Grid', 'design', 'experience', 'flexibility', 'forging', 'knowledge base', 'member', 'open source', 'programs', 'public health relevance', 'research study', 'response', 'tool', 'usability']",NIGMS,STANFORD UNIVERSITY,R01,2009,529858,0.02470344118796245
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7684604,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'biomedical ontology', 'computer science', 'computerized tools', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'natural language', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2009,639134,0.0017697180575018147
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7498449,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2008,153203,0.00232009535554097
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7364629,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2008,134890,-0.010132273182725056
"ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS    DESCRIPTION (provided by applicant):  We propose development of an adaptive, personalizable, information management tool, which can be configured and trained by an individual biologist to most effectively exploit the particular knowledge bases and document collections that are most useful for him or her. The proposed tool represents a novel approach for monitoring scientific progress in biology, which has become a formidable task. We will exploit recent advances in machine learning and database systems to develop a useful approximation to a personalized biological knowledge base f.i.i.e., single information resource that would include all the knowledge sources on which a biologist relies. More specifically, we propose a scheme for loosely integrating both structured information and unstructured text, and then querying the integrated information using easily-formulated similarity queries. The system will also learn from every episode in which a biologist seeks information. The research team on this project includes a computer scientist and two biologists. The proposed work will make systems for monitoring scientific progress in biology more effective. This will make biologists, clinicians and medical researchers better able to track advances in the biomedical literature that are relevant to their work.          n/a",ADAPTIVE PERSONALIZED INFORMATION MANAGEMENT FOR BIOLOGISTS,7432910,R01GM081293,"['Address', 'Biological', 'Biological Phenomena', 'Biology', 'Collection', 'Communities', 'Computers', 'Data Sources', 'Databases', 'Development', 'Eukaryota', 'Eukaryotic Cell', 'Genes', 'Goals', 'Grant', 'Individual', 'Information Management', 'Information Resources', 'Knowledge', 'Learning', 'Literature', 'Machine Learning', 'Medical', 'Metric', 'Monitor', 'Output', 'Persons', 'Process', 'Proteins', 'Publications', 'Research', 'Research Personnel', 'Ribosomes', 'Role', 'Scheme', 'Scientist', 'Solutions', 'Source', 'Staging', 'Structure', 'Surface', 'System', 'Techniques', 'Technology', 'Text', 'Time', 'Training', 'Work', 'base', 'design', 'desire', 'experience', 'knowledge base', 'man', 'novel strategies', 'programs', 'tool']",NIGMS,CARNEGIE-MELLON UNIVERSITY,R01,2008,273906,0.0022368166289779964
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7433144,R44GM083965,"['Adoption', 'Algorithms', 'Architecture', 'Arts', 'Biological Sciences', 'Biomedical Research', 'Cations', 'Class', 'Classification', 'Communication', 'Communities', 'Companions', 'Complex', 'Computer software', 'Computers', 'Consult', 'Data', 'Data Set', 'Databases', 'Detection', 'Diagnosis', 'Disease', 'Effectiveness', 'Emerging Technologies', 'Ensure', 'Fostering', 'Foundations', 'Future', 'Generations', 'Goals', 'Graph', 'Grouping', 'Imagery', 'Knowledge', 'Language', 'Learning', 'Libraries', 'Machine Learning', 'Memory', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Numbers', 'Performance', 'Personal Satisfaction', 'Phase', 'Prevention', 'Problem Solving', 'Program Development', 'Public Health', 'Randomized', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Running', 'Scheme', 'Simulate', 'Software Design', 'Software Tools', 'Speed', 'Structure', 'Techniques', 'Technology', 'Testing', 'Today', 'Training', 'Voting', 'Work', 'base', 'computerized tools', 'data mining', 'design', 'forest', 'improved', 'innovation', 'next generation', 'parallel computing', 'programs', 'prototype', 'research and development', 'response', 'software development', 'sound', 'statistics', 'theories', 'tool']",NIGMS,INSIGHTFUL CORPORATION,R44,2008,25548,0.019300539949593423
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7465580,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2008,273993,-0.01717961573509041
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7468497,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2008,268274,-0.0055122548994077985
"Technology Development for a MolBio Knowledge-base    DESCRIPTION (provided by applicant):       In the three years since the original proposal was submitted, the claims we made about the impending readiness of knowledge-based approaches and natural language processing to address pressing problems of information overload in molecular biology have been resoundingly confirmed, and such methods have become increasingly accepted within the computational bioscience and systems biology communities. We are now well into the era of broad use of semantic representation technology to support biomedical research, and at the cusp of the use of biomedical natural language processing software to create the enormous number of necessary formal representations automatically from biomedical texts. The results of the work during the last funding period have not only contributed    innovative and significant new methods, but have helped us identify a set of specific research issues we claim are now the rate-limiting factors in building an extensive, high-quality computational knowledge-base of molecular biology. The aims of this competitive renewal are to address those factors, making it possible to scale our impressive results on intentionally narrow applications to much   larger (and more significant) tasks, specifically: (1) to create an enriched, relationally decomposed set of conceptual frames, hewing closely to multiple, community curated ontologies; (2) develop language  processing tools capable of recognizing and populating instances of those conceptual frames, and (3) develop systems for integrating and using diverse knowledge from multiple sources to generate scientific insights, focusing on the analysis of sets of dozens to hundreds of genes produced by diverse high-throughput methodologies. An innovative aspect of this proposal is the creation and application of novel, insight-based extrinsic evaluation techniques for such systems.          n/a",Technology Development for a MolBio Knowledge-base,7474790,R01LM008111,"['Address', 'Biomedical Research', 'Body of uterus', 'Budgets', 'Chemicals', 'Communities', 'Computer software', 'Data', 'Data Set', 'Evaluation', 'Funding', 'Genes', 'Goals', 'Human', 'Information Resources', 'Knowledge', 'Linguistics', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Numbers', 'Ontology', 'Phenotype', 'Rate', 'Readiness', 'Representations, Knowledge (Computer)', 'Research', 'Semantics', 'Source', 'Structure', 'System', 'Systems Biology', 'Techniques', 'Technology', 'Text', 'Work', 'base', 'cell type', 'computer based Semantic Analysis', 'concept', 'high throughput analysis', 'improved', 'information organization', 'innovation', 'insight', 'interest', 'knowledge base', 'language processing', 'new technology', 'novel', 'technology development', 'tool']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2008,614419,-0.01454513477207519
"Discovering hidden groups across tuberculosis patient and pathogen genotype data    DESCRIPTION (provided by applicant):       The principal objective of this project is to develop methods that combine pathogen genotyping and patient epidemiology data that can be used in the control, understanding, and tracking of infectious diseases. This work focuses on the modeling of large international collections of patient epidemiology and strain data for the Mycobacterium tuberculosis complex (MTC), the causative agent of tuberculosis disease (TB), because of the urgent global need and the unique data availability due to the National TB genotyping program. Specifically, the project addresses the following problem: given MTC DNA fingerprinting and TB patient data being accumulated nationally and internationally, identify hidden groups capturing MTC genetic families and TB epidemiology using machine learning, and use these hidden groups to address problems in the control, understanding, prevention, and treatment of tuberculosis at city, state, national, and international levels. To address this objective, we identify several aims. The first aim is to gather and merge large databases of MTC patient-isolate genotypes as well as associated patient information from the New York City, New York State, United States, and the rest of the world. The second aim is to identify MTC strain families based on multiple genotype methods using graphical models constrained to reflect background knowledge. The third aim is to identify hidden host-pathogen groups within TB patient demographics and MTC genotypes using a combination of probabilistic graphical models and deterministic multi-way tensor analysis methods designed to capture the temporal dynamics of TB. The fourth aim answers public health questions posed by TB experts by transforming the questions into quantifiable metrics applied to the hidden groups. The hidden group models and metrics will be embedded in analysis methods, and then evaluated by TB experts. The proposed models and analysis methods will capture and share knowledge embedded in large TB patient and MTC genotyping databases without necessarily sharing the actual data.          n/a",Discovering hidden groups across tuberculosis patient and pathogen genotype data,7354450,R01LM009731,"['Address', 'Age', 'Algorithms', 'Area', 'Biology', 'Boxing', 'Centers for Disease Control and Prevention (U.S.)', 'Cities', 'Class', 'Collection', 'Communicable Diseases', 'Complex', 'Country', 'DNA Fingerprinting', 'DNA Insertion Elements', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnosis', 'Disease', 'Disease Outbreaks', 'Epidemiology', 'Epidemiology, Other', 'Exercise', 'Family', 'Fingerprint', 'Gender', 'Genetic Variation', 'Genomics', 'Genotype', 'Goals', 'Guadeloupe', 'Health', 'Healthcare', 'Individual', 'Infectious Disease Epidemiology', 'Institutes', 'International', 'Investigation', 'Joints', 'Knowledge', 'Label', 'Learning', 'Link', 'Literature', 'Location', 'Machine Learning', 'Methods', 'Metric', 'Modeling', 'Molecular', 'Molecular Epidemiology', 'Mycobacterium tuberculosis', 'Nature', 'New York', 'New York City', 'Patients', 'Pattern', 'Phylogeny', 'Population', 'Prevention', 'Principal Investigator', 'Property', 'Protocols documentation', 'Public Health', 'Research Institute', 'Research Personnel', 'Rest', 'Restriction fragment length polymorphism', 'Single Nucleotide Polymorphism', 'Social Network', 'Source', 'Stream', 'Structure', 'Time', 'Translating', 'Trees', 'Tuberculosis', 'United States', 'Visual', 'Work', 'base', 'demographics', 'design', 'disorder control', 'family genetics', 'fight against', 'genetic analysis', 'genetic variant', 'improved', 'mycobacterial', 'novel', 'pathogen', 'patient privacy', 'programs', 'prototype', 'success', 'theories', 'tool', 'transmission process', 'transposon/insertion element', 'trend', 'tuberculosis treatment']",NLM,RENSSELAER POLYTECHNIC INSTITUTE,R01,2008,342967,0.025234755989556216
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7367958,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2008,2037396,0.000984925043692754
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7371085,K12MH069281,"['Anatomy', 'Area', 'Artificial Intelligence', 'Base of the Brain', 'Biomedical Engineering', 'Biomedical Informatics Research Network', 'Brain', 'Brain imaging', 'Caliber', 'Class', 'Clinical', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Development', 'Discipline', 'Educational Activities', 'Exposure to', 'Foundations', 'Functional disorder', 'Future', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Laboratories', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Monitor', 'Neurosciences', 'Numbers', 'Operative Surgical Procedures', 'Physicians', 'Procedures', 'Program Development', 'Psyche structure', 'Range', 'Rate', 'Recruitment Activity', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'SECTM1 gene', 'Science', 'Scientist', 'Surface', 'Techniques', 'Technology', 'Training', 'Training Programs', 'Woman', 'base', 'bioimaging', 'biomedical informatics', 'brain morphology', 'career', 'cationic antimicrobial protein CAP 37', 'clinical application', 'cognitive neuroscience', 'computer science', 'imaging informatics', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'neuroinformatics', 'post-doctoral training', 'programs', 'research and development', 'response', 'skills']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2008,462488,-0.012754187466548104
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7459910,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2008,118789,-0.01998057877219589
"BioScholar: a Biomedical Knowledge Engineering framework based on the published l    DESCRIPTION (provided by applicant): Studying the primary research literature is a universal, primary activity for biomedical scientists. It underlies scientists' understanding of their subject and strengthens their capability to plan, execute, and interpret experiments. This proposal is concerned with the maintenance and continued development of software that supports scientists in their scholarly work. Our goal is to develop a knowledge engineering platform (called `BioScholar') to permit a single graduate student or postdoctoral worker to design, build, curate, and maintain a Knowledge Base (KB) for the literature of interest to a specific laboratory. This continues a previous software development project that was funded by the National Library of Medicine (LM 07061). We will continue to maintain the software using modern software engineering tools and approaches, whilst making it fully interoperable with a widely used ontology engineering platform (Protege /OWL). We will also develop the systems' existing capabilities to assist scientists with management of bibliographic data (citation information and full-text PDF articles). We will further develop tools to allow researchers to annotate PDF files with highlights, simple comments and with structured data. We will then use this annotation framework to drive the process of constructing knowledge bases using Protege/OWL (a widely used ontology editor). We will then incorporate Information Extraction (IE) techniques from modern Natural Language Processing (NLP) to improve the efficiency of this curation process. The NLP methods we use are based on the Conditional Random Fields (CRF) model which is considered state-of-the-art amongst NLP researchers. Finally, the most research-oriented component of this proposal is the development of a new methodology for knowledge representation and reasoning in biomedicine based on experimental design, involving experimental controls, independent and dependent variables, statistical significance and correlation between variables. This representation will be (a) understandable to experimental scientists, (b) lightweight, (c) versatile, and (d) capable of supporting inference between experiments. During the course of this project, we will build a KB for the world-leading neuroendocrinology laboratory of Prof. Alan Watts at University Southern California. Prof. Watts' work is concerned with the study of catecholaminergic control of the stress response, drawing on research from a large number of different fields (anatomy, physiology, molecular biology, etc.). After developing this KB, we will test its validity using subjective methods (questionnaires and interviews), and objective experiments (`mock exams' to see if students' performance with test questions based on comprehension of the primary literature). We will release all findings and tools to the biomedical community as research papers and open-source software. Narrative This project will help biomedical scientists manage, understand and communicate the complex information they must learn from scientific papers in multiple biomedical disciplines. As a demonstration of this work, we will build a comprehensive summary of research underlying brain circuits involved in stress. Stress and anxiety disorders are estimated to affect 19.1 million people in the USA, costing $42 billion in health costs per year (source: Anxiety Disorders Association of America).          n/a",BioScholar: a Biomedical Knowledge Engineering framework based on the published l,7426246,R01GM083871,"['Address', 'Affect', 'Americas', 'Anatomy', 'Anxiety Disorders', 'Architecture', 'Arts', 'Biological Sciences', 'Brain', 'California', 'Cataloging', 'Catalogs', 'Communities', 'Complex', 'Comprehension', 'Computer software', 'Computing Methodologies', 'Data', 'Depth', 'Development', 'Digital Libraries', 'Discipline', 'Electronics', 'Engineering', 'Experimental Designs', 'Facility Construction Funding Category', 'Funding', 'GDF15 gene', 'Goals', 'Guidelines', 'Health Care Costs', 'Individual', 'Information Retrieval', 'Interview', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Learning', 'Literature', 'Logic', 'Maintenance', 'Methodology', 'Methods', 'Modeling', 'Molecular Biology', 'Natural Language Processing', 'Neuroendocrinology', 'Numbers', 'Ontology', 'PLAB Protein', 'Paper', 'Performance', 'Physiology', 'Process', 'Protocols documentation', 'Proxy', 'PubMed', 'Published Comment', 'Publishing', 'Questionnaires', 'Reading', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Review Literature', 'Scientist', 'Software Engineering', 'Solutions', 'Source', 'Stress', 'Strigiformes', 'Structure', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Text', 'Time', 'United States National Library of Medicine', 'Universities', 'Work', 'base', 'biological adaptation to stress', 'biomedical scientist', 'computer based Semantic Analysis', 'cost', 'design', 'design and construction', 'improved', 'information organization', 'interest', 'knowledge base', 'novel strategies', 'open source', 'repository', 'research study', 'software development', 'statistics', 'text searching', 'tool', 'tool development']",NIGMS,UNIVERSITY OF SOUTHERN CALIFORNIA,R01,2008,266213,-0.011751589450473564
"Integrating Microarray and Proteomic Data by Ontology-based Annotation    DESCRIPTION (provided by applicant):       With the completion of the Human Genome Project, there is a need to translate genome-era discoveries into clinical utility. One difficulty in making bench-to-bedside translations with gene-expression and proteomic data is our current inability to relate these findings with each other and with clinical measurements. A translational researcher studying a particular biological process using microarrays or proteomics will want to gather as many relevant publicly-available data sets as possible, to compare findings. Translational investigators wanting to relate clinical or chemical data with multiple genomic or proteomic measurements will want to find and join related data sets. Unfortunately, finding and joining relevant data sets is particularly challenging today, as the useful annotations of this data are still represented only by unstructured free-text, limiting its secondary use. A question we have sought to answer is whether prior investments in biomedical ontologies can provide leverage in determining the context of genomic data in an automated manner, thereby enabling integration of gene expression and proteomic data and the secondary use of genomic data in multiple fields of research beyond those for which the data sets were originally targeted. The three specific aims to address this question are to (1) develop tools that comprehensively map contextual annotations to the largest biomedical ontology, the Unified Medical Language System (UMLS), built and supported by the National Library of Medicine, validate, and disseminate the mappings, (2) execute a four-pronged strategy to evaluate experiment-concept mappings, and (3) apply experiment-context mappings to find and integrate data within and across microarray and proteomics repositories. To keep these tools relevant to biomedical investigators, we have included three Driving Biological Projects (DBPs), in the domains of breast cancer, organ transplantation, and T-cell biology. To accomplish these DBPs, our tools and mappings will be used to find and join experimental data within and across microarray and proteomic repositories. Having DBPs to address will focus our development on a set of scalable tools that can access and analyze experimental data covering a large variety of diseases. Through our advisory committee of world-renowned NIH-funded investigators, we will ensure that our findings will have broad applicability and are useful to a wide variety of biomedical researchers.          n/a",Integrating Microarray and Proteomic Data by Ontology-based Annotation,7467204,R01LM009719,"['Address', 'Advisory Committees', 'Automobile Driving', 'Biological', 'Biological Process', 'Cells', 'Cellular biology', 'Chemicals', 'Classification', 'Clinical', 'Computer software', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Ensure', 'Funding', 'Gene Expression', 'Genetic Transcription', 'Genome', 'Genomics', 'Growth', 'Head', 'Human Genome Project', 'Improve Access', 'International', 'Investments', 'Machine Learning', 'Manuals', 'Maps', 'Measurement', 'Methods', 'Molecular Biology', 'Nature', 'Online Systems', 'Ontology', 'Organ Transplantation', 'Personal Satisfaction', 'Phenotype', 'Play', 'Process', 'Proteomics', 'Publications', 'Research', 'Research Personnel', 'Role', 'Sampling', 'Scientist', 'Sensitivity and Specificity', 'Specificity', 'System', 'T-Lymphocyte', 'Text', 'Time', 'Today', 'Translating', 'Translations', 'Transplantation', 'Unified Medical Language System', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Writing', 'base', 'bench to bedside', 'biomedical informatics', 'concept', 'improved', 'malignant breast neoplasm', 'repository', 'research study', 'text searching', 'tool', 'translational medicine']",NLM,STANFORD UNIVERSITY,R01,2008,280000,-0.008392538545642286
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688808,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,100000,0.018068477087959858
"National Alliance-Medical Imaging Computing (NAMIC)(RMI) The National Alliance for Medical Imaging Computing (NAMIC) is a multiinstitutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state of the art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neurosience and neurologicat disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual )atients,and to studies executed across large poplulations. The data will be taken from subjects across a Nide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs. n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7688368,U54EB005149,"['Affect', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Area', 'Arts', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Class', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computational algorithm', 'Computer Vision Systems', 'Computers', 'Computing Methodologies', 'Data', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Elements', 'Environment', 'Etiology', 'Foundations', 'Functional disorder', 'Future', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Imagery', 'Imaging Techniques', 'Individual', 'Life', 'Link', 'Localized', 'Measures', 'Medical', 'Medical Imaging', 'Medical Research', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Morphology', 'Neurons', 'Operative Surgical Procedures', 'Organ', 'Patients', 'Pattern', 'Physiological', 'Play', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Purpose', 'Range', 'Research', 'Research Infrastructure', 'Research Personnel', 'Resources', 'Role', 'Sampling', 'Schizophrenia', 'Science', 'Scientist', 'Services', 'Software Engineering', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Textiles', 'Thinking', 'Time', 'Tissues', 'Today', 'Training', 'Visible Radiation', 'Vision', 'Vision research', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'disability', 'egg', 'insight', 'mathematical model', 'neuroimaging', 'open source', 'programs', 'receptor', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,50000,0.018068477087959858
"Scalable Learning with Ensemble Techniques and Parallel Computing    DESCRIPTION (provided by applicant): The ability to conduct basic and applied biomedical research is becoming increasingly dependent on data produced by new and emerging technologies. This data has an unprecedented amount of detail and volume. Researchers are therefore dependent on computing and computational tools to be able to visualize, analyze, model, and interpret these large and complex sets of data. Tools for disease detection, diagnosis, treatment, and prevention are common goals of many, if not all, biomedical research programs. Sound analytical and statistical theory and methodology for class pre- diction and class discovery lay the foundation for building these tools, of which the machine learning techniques of classification (supervised learning) and clustering (unsupervised learning) are crucial. Our goal is to produce software for analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies. Ensemble techniques are recent advances in machine learning theory and methodology leading to great improvements in accuracy and stability in data set analysis and interpretation. The results from a committee of primary machine learners (classifiers or clusterers) that have been trained on different instance or feature subsets are combined through techniques such as voting. The high prediction accuracy of classifier ensembles (such as boosting, bagging, and random forests) has generated much excitement in the statistics and machine learning communities. Recent research extends the ensemble methodology to clustering, where class information is unavailable, also yielding superior performance in terms of accuracy and stability. In theory, most ensemble techniques are inherently parallel. However, existing implementations are generally serial and assume the data set is memory resident. Therefore current software will not scale to the large data sets produced in today's biomedical research. We propose to take two approaches to scale ensemble techniques to large data sets: data partitioning approaches and parallel computing. The focus of Phase I will be to prototype scalable classifier ensembles using parallel architectures. We intend to: establish the parallel computing infrastructures; produce a preliminary architecture and software design; investigate a wide range of ensemble generation schemes using data partitioning strategies; and implement scalable bagging and random forests based on the preliminary design. The focus of Phase II will be to complete the software architecture and implement the scalable classifier ensembles and scalable clusterer ensembles within this framework. We intend to: complete research and development of classifier ensembles; extend the classification framework to clusterer ensembles; research and develop a unified interface for building ensembles with differing generation mechanisms and combination strategies; and evaluate the effectiveness of the software on simulated and real data. PUBLIC HEALTH RELEVANCE: The common goals to many, if not all, biomedical research programs are the development of tools for disease detection, diagnosis, treatment, and prevention. These programs often rely on new types of data that have an unprecedented amount of detail and volume. Our goal is to produce software for the analysis and interpretation of large data sets using ensemble machine learning techniques and parallel computing technologies to enable researchers who are dependent on computational tools to have the ability to visualize, analyze, model, and interpret these large and complex sets of data.          n/a",Scalable Learning with Ensemble Techniques and Parallel Computing,7748401,R44GM083965,"['Learning', 'Techniques', 'parallel computing']",NIGMS,INSILICOS,R44,2008,143361,0.019300539949593423
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.       n/a",Bioconductor: an open computing resource for genomics,7495201,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CANCER RESEARCH CENTER,P41,2008,805222,-0.006228980017508683
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7502636,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2008,640921,0.0017697180575018147
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7479786,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2008,3534631,0.0190469494647772
"Morphometry Biomedical Informatics Research Network    DESCRIPTION (provided by applicant):     Technological advances in imaging have revolutionized the biomedical investigation of illness. The tremendous potential that this methodology brings to advancing diagnostic and prognostic capabilities and in treatment of illnesses has as yet remained largely an unfulfilled promise. This potential has been limited by a number of technological impediments that could be in large part overcome by the availability of a federated imaging database and the attendant infrastructure. Specifically, the ability to conduct clinical imaging studies across multiple sites, to analyze imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well characterized image and clinical data would have a demonstrable and positive impact on progress in this field. The Morphometry BIRN (mBIRN), established in October 2001, has made substantial progress in the development of this national infrastructure to develop a data and computational network based on a federated data acquisition and database across seven sites in the service of facilitating multi-site neuroanatomic analysis. Standardized structural MRI image acquisition protocols have been developed and implemented that demonstrably reduce initial sources of inter-site variance. Data structure, transmission, storage and querying aspects of the federated database have been implemented. In this continuation of the mBIRN efforts, we propose three broad areas of work:   1) continuing structural MRI acquisition optimization, calibration and validation to include T2 and DTI; 2) translation of site specific state-of-the-art image analysis, visualization and machine learning technologies to work in the federated, multi-site BIRN environment; and 3) extension of data management and database query capabilities to include additional imaging modalities, clinical disorders and individualized human genetic covariates. These broad areas of work will come together in through key collaborations that will ensure utilization promotion by facilitating data entry into the federated database and creation of database incentive functionality. Our participating sites include MGH (PI), BWH, UCI, Duke, UCLA, UCSD, John Hopkins, and newly added Washington University and MIT. We have made a concerted effort to bridge the gap that can exist between biomedical and computational sciences by recruiting to our group leaders in both of these domains. Our efforts will be coordinated with those of the entire BIRN consortium in order to insure that acquisition and database functionality, and application-based disorder queries are interoperable across sites and designed to advance the capabilities to further knowledge and understanding of health and disease.         n/a",Morphometry Biomedical Informatics Research Network,7467385,U24RR021382,[' '],NCRR,MASSACHUSETTS GENERAL HOSPITAL,U24,2008,5140823,0.0035888697672531097
"Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi    DESCRIPTION (provided by applicant): The study of complex biological systems increasingly depends on vast amounts of dynamic information from diverse sources. The scientific analysis of the parasite Trypanosoma cruzi (T.cruzi), the principal causative agent of human Chagas disease, is the driving biological application of this proposal. Approximately 18 million people, predominantly in Latin America, are infected with the T.cruzi parasite. As many as 40 percent of these are predicted eventually to suffer from Chagas disease, which is the leading cause of heart disease and sudden death in middle-aged adults in the region. Research on T. cruzi is therefore an important human disease related effort. It has reached a critical juncture with the quantities of experimental data being generated by labs around the world, due in large part to the publication of the T.cruzi genome in 2005. Although this research has the potential to improve human health significantly, the data being generated exist in independent heterogeneous databases with poor integration and accessibility. The scientific objectives of this research proposal are to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for T.cruzi. This is in collaboration with the National Center for Biomedical Ontologies (NCBO) and will leverage its resources to achieve the objectives of this proposal as well as effectively to disseminate results to the broader life science community, including researchers in human pathogens. The PSE allows the dynamic integration of local and public data to answer biological questions at multiple levels of granularity. The PSE will utilize state-of- the-art semantic technologies for effective querying of multiple databases and, just as important, feature an intuitive and comprehensive set of interfaces for usability and easy adoption by biologists. Included in the multimodal datasets will be the genomic data and the associated bioinformatics predictions, functional information from metabolic pathways, experimental data from mass spectrometry and microarray experiments, and textual information from Pubmed. Researchers will be able to use and contribute to a rigorously curated T.cruzi knowledge base that will make it reusable and extensible. The resources developed as part of this proposal will be also useful to researchers in T.cruzi related kinetoplastids, Trypanosoma brucei and Leishmania major (among other pathogenic organisms), which use similar research protocols and face similar informatics challenges. PUBLIC HEALTH RELEVANCE: The scientific objective of this proposal is to develop and deploy a novel ontology-driven semantic problem-solving environment (PSE) for Trypanosoma cruzi, a parasite that infects approximately 18 million people, predominantly in Latin America. As many as 40 percent of those infected are predicted to eventually suffer from Chagas disease, the leading cause of heart disease and sudden death in middle-aged adults in the region. Facilitating T.cruzi research through the PSE, with the aim of identifying vaccine, diagnostic, and therapeutic targets, is an important human disease related endeavor.          n/a",Semantics and Services enabled Problem Solving Environment for Trypanosoma cruzi,7428761,R01HL087795,"['Acquired Immunodeficiency Syndrome', 'Address', 'Adherence', 'Adopted', 'Adoption', 'Adult', 'Algorithms', 'Anatomy', 'Animal Model', 'Anti-Retroviral Agents', 'Architecture', 'Archives', 'Area', 'Arts', 'Automobile Driving', 'Beds', 'Behavior', 'Bioinformatics', 'Biological', 'Biological Sciences', 'Biomedical Computing', 'Biomedical Research', 'Body of uterus', 'Buffaloes', 'California', 'Caring', 'Chagas Disease', 'Childhood', 'Chronic', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Complex', 'Computer Systems Development', 'Computer software', 'Computers', 'Controlled Vocabulary', 'DNA', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Development', 'Diagnostic', 'Disease', 'Doctor of Medicine', 'Doctor of Philosophy', 'Doctor of Public Health', 'Drops', 'Drosophila genus', 'Educational Activities', 'Educational workshop', 'Electronics', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evolution', 'Face', 'Feedback', 'Foundations', 'Future', 'Gene Mutation', 'Generations', 'Generic Drugs', 'Genes', 'Genetic', 'Genetic Variation', 'Genome', 'Genomics', 'Geographic Locations', 'Goals', 'HIV', 'HIV Infections', 'Health', 'Heart Diseases', 'Homologous Gene', 'Human', 'Human Resources', 'Imagery', 'Immunologic Deficiency Syndromes', 'Immunology', 'Individual', 'Infection', 'Informatics', 'Information Management', 'Information Resources', 'Information Services', 'Information Technology', 'International', 'Internet', 'Interruption', 'Knowledge', 'Laboratories', 'Laboratory Organism', 'Language', 'Latin America', 'Lead', 'Learning', 'Leishmania major', 'Libraries', 'Link', 'Manuals', 'Maps', 'Mass Spectrum Analysis', 'Medical Informatics', 'Medicine', 'Metabolic Pathway', 'Metadata', 'Methodology', 'Methods', 'Mind', 'Mining', 'Modeling', 'Mutation', 'Natural Language Processing', 'Nature', 'Online Mendelian Inheritance In Man', 'Online Systems', 'Ontology', 'Operative Surgical Procedures', 'Oregon', 'Organism', 'Orthologous Gene', 'Outcome', 'Parasites', 'Pathogenesis', 'Patients', 'Peer Review', 'Pharmaceutical Preparations', 'Phase', 'Phenotype', 'Philosophy', 'Physiology', 'Prevention strategy', 'Principal Investigator', 'Problem Solving', 'Process', 'Proteomics', 'Protocols documentation', 'PubMed', 'Public Health', 'Publications', 'Publishing', 'Purpose', 'Randomized Clinical Trials', 'Range', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Resources', 'San Francisco', 'Science', 'Scientist', 'Semantics', 'Services', 'Site', 'Software Tools', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Structure', 'Study models', 'Sudden Death', 'Sum', 'System', 'TAF8 gene', 'Talents', 'Techniques', 'Technology', 'Terminology', 'Testing', 'Thinking', 'Training', 'Treatment Protocols', 'Trypanosoma brucei brucei', 'Trypanosoma cruzi', 'USA Georgia', 'United States National Institutes of Health', 'United States National Library of Medicine', 'Universities', 'Update', 'Vaccines', 'Vertical Disease Transmission', 'Victoria Austrailia', 'Virtual Library', 'Virus', 'Western Asia Georgia', 'Work', 'Zebrafish', 'abstracting', 'base', 'biocomputing', 'biomedical scientist', 'college', 'computer based Semantic Analysis', 'computer science', 'concept', 'data integration', 'design', 'desire', 'fundamental research', 'human disease', 'improved', 'indexing', 'innovative technologies', 'knowledge base', 'member', 'metabolomics', 'middle age', 'novel', 'novel strategies', 'open source', 'outreach', 'pandemic disease', 'pathogen', 'prevent', 'programs', 'protein protein interaction', 'repository', 'research and development', 'research study', 'syntax', 'theories', 'therapeutic target', 'tool', 'usability']",NHLBI,WRIGHT STATE UNIVERSITY,R01,2008,393930,-0.01085511831140666
"Semantic and Machine Learning Methods for Mining Connections in the UMLS    DESCRIPTION (provided by applicant):       The Unified Medical Language System (UMLS) is an invaluable resource for the biomedical community.   One of the intended uses of the UMLS Metathesaurus is to support the translation of terms from a source terminology into terms in a target terminology. It is evident from the research literature on the UMLS that users generally need to perform more broader types of ""translations"" that involve finding terms with closest meaning to source term (mapping), finding terms that are related to source term and can serve as proxy for various functions (e.g. information retrieval, knowledge discovery) or finding target terms that satisfy some structural or semantic constraint (e.g. information theoretic distance). The methods for finding such ""translations"" or connections between terms in Meta (other than the case of one-to-one synonymy) are not at all clear. Previous attempts to exploit such connections have depended on either manual selection of relevant connections, or problem-specific algorithms that use expert knowledge about the relative suitability of various inter-concept relationships. We believe that machine learning techniques offer automated, generalizable approaches that are appropriate for use with the UMLS, given the large set of potential connections and the need for a problem-independent approach. We hypothesize that learning strategies that exploit the relational features, scale free properties and probabilistic dependencies of connections in the UMLS will identify meaningful inter-term relationships and that a combined approach will perform better across different problem domains when compared to any of the approaches in isolation. We will evaluate the proposed learning algorithms with training connections from a variety of problem domains in biomedicine. We will disseminate the successful algorithms via the UMLS Knowledge Source API toolkit for mining and visualizing the connections. We believe that the UMLS provides a unique fertile ground to develop novel semantic relational mining methods and advance our understanding of mining large biomedical concept graphs.             n/a",Semantic and Machine Learning Methods for Mining Connections in the UMLS,7299922,R21LM009638,"['Algorithms', 'Communities', 'Complex', 'Data', 'Data Set', 'Dependency', 'Disease', 'Graph', 'Healthcare', 'Information Retrieval', 'Knowledge', 'Language', 'Learning', 'Literature', 'Machine Learning', 'Manuals', 'Maps', 'Medical', 'Methods', 'Mining', 'Nature', 'Ontology', 'Organism', 'Pathway interactions', 'Property', 'Proxy', 'Relative (related person)', 'Research', 'Resources', 'Retrieval', 'Sampling', 'Semantics', 'Social Network', 'Solutions', 'Source', 'System', 'Techniques', 'Terminology', 'Training', 'Translating', 'Translations', 'Unified Medical Language System', 'Work', 'base', 'biomedical resource', 'concept', 'interest', 'metathesaurus', 'microbial alkaline proteinase inhibitor', 'novel', 'success']",NLM,COLUMBIA UNIVERSITY HEALTH SCIENCES,R21,2007,181125,0.00232009535554097
"Intelligent Control Approach to Anemia Management    DESCRIPTION (provided by applicant):   Management of anemia due to end-stage renal disease is a multifactorial decision process involving administration of recombinant human erythropoietin (rHuEPO) and iron, as well as assessment of other factors influencing the progress of the disease. This application aims at improving the cost-effectiveness of this process through the use of state-of-the-art numerical tools from control engineering and machine learning. The specific aims are the collection of anemia management data and development of new guidelines for period of measuring hemoglobin levels if necessary, development of individualized, computer-assisted approach to rHuEPO dosing based on modern control engineering and machine learning approach, evaluation of the developed tools through numeric simulation and assessment of the potential improvements in therapy and projected savings in rHuEPO utilization. The final aim is to provide a physical implementation and to perform a clinical evaluation of the developed methodology. The applicant, Dr. Adam E. Gaweda, is an Instructor of Medicine in the Department of Medicine, Division of Nephrology at the University of Louisville. His original training is in the field of electrical engineering (M.Eng.) and computer science (Ph.D.). The applicant plans to develop as an independent and well established researcher in the field of biomedical engineering with focus on translation of state-of-the-art technology to heath care. To achieve this goal the applicant will enroll into the Clinical Research, Epidemiology and Statistics Training (CREST) Program at the University of Louisville, School of Public Health and Information Sciences       n/a",Intelligent Control Approach to Anemia Management,7209599,K25DK072085,"['Adverse event', 'Algorithms', 'Anemia', 'Anti-Arrhythmia Agents', 'Artificial Intelligence', 'Arts', 'Biological Models', 'Biological Neural Networks', 'Biomedical Engineering', 'Blood', 'Blood Vessels', 'Caring', 'Chronic', 'Clinical', 'Clinical Research', 'Collection', 'Complex', 'Computer Assisted', 'Computer Simulation', 'Computer software', 'Computing Methodologies', 'Controlled Clinical Trials', 'Data', 'Decision Support Systems', 'Development', 'Dialysis procedure', 'Disease', 'Doctor of Philosophy', 'Dose', 'Drug Prescriptions', 'Effectiveness', 'Electrical Engineering', 'End stage renal failure', 'Engineering', 'Enrollment', 'Epidemiology', 'Erythropoietin', 'Evaluation', 'Feedback', 'Frequencies', 'Funding', 'Goals', 'Guidelines', 'Hemodialysis', 'Hemoglobin', 'Hemoglobin concentration result', 'Human', 'Individual', 'Information Sciences', 'Insulin', 'Iron', 'Machine Learning', 'Measures', 'Medicine', 'Methodology', 'Methods', 'Morbidity - disease rate', 'Nephrology', 'Outcome', 'Patient Monitoring', 'Patients', 'Pharmaceutical Preparations', 'Physical Dialysis', 'Physiological', 'Population', 'Process', 'Protocols documentation', 'Public Health Schools', 'Range', 'Research', 'Research Personnel', 'Safety', 'Sampling', 'Savings', 'Standards of Weights and Measures', 'Techniques', 'Technology', 'Testing', 'Training', 'Training Programs', 'Translations', 'Universities', 'Veterans', 'Waran', 'Work', 'base', 'computer science', 'cost effectiveness', 'data management', 'desire', 'experience', 'improved', 'insight', 'instructor', 'mathematical algorithm', 'novel', 'prescription document', 'prescription procedure', 'programs', 'recombinant human erythropoietin', 'research clinical testing', 'response', 'simulation', 'statistics', 'theories', 'tool']",NIDDK,UNIVERSITY OF LOUISVILLE,K25,2007,112534,-0.010132273182725056
"Adaptive Information Monitoring and Extraction    DESCRIPTION (provided by applicant):       It is now widely recognized that there is a great need for more powerful automated methods to assist biomedical scientists in filtering, querying, and extracting information from the scientific literature. Building on our past research accomplishments in biomedical text mining, we plan to develop new algorithms and software systems that will significantly improve the ability of biomedical researchers to exploit the scientific literature. In particular, we plan to develop, evaluate and field systems that (1) aid in annotating high-throughput experiments by extracting and organizing information from text sources, and (2) assist genome database curators by identifying relevant articles and predicting appropriate ontology codes for specific query genes and proteins. In support of these systems, we plan to develop novel machine-learning based text-mining algorithms for training on coarsely labeled data, and inducing models of relationships among specific types of entities expressed in natural language.          n/a",Adaptive Information Monitoring and Extraction,7264196,R01LM007050,"['Address', 'Algorithms', 'Arabidopsis', 'Arts', 'Class', 'Code', 'Collection', 'Computing Methodologies', 'Data', 'Databases', 'Gene Proteins', 'Genes', 'Human', 'Internet', 'Label', 'Language', 'Learning', 'Literature', 'MEDLINE', 'Machine Learning', 'Methods', 'Modeling', 'Monitor', 'Mus', 'Names', 'Ontology', 'Proteins', 'RGD (sequence)', 'Rattus', 'Research', 'Research Personnel', 'Research Proposals', 'Resolution', 'Source', 'Support System', 'System', 'Technology', 'Testing', 'Text', 'To specify', 'Training', 'Work', 'Yeasts', 'abstracting', 'base', 'biomedical scientist', 'design', 'desire', 'genome database', 'improved', 'interest', 'novel', 'research study', 'software systems', 'text searching']",NLM,UNIVERSITY OF WISCONSIN-MADISON,R01,2007,279300,-0.01717961573509041
"New Machine Learning Methods for Biomedical Data    DESCRIPTION (provided by applicant):  In the past few years, we have witnessed a dramatic increase of the amount of data available to biomedical research. An example is the recent advances of high-throughput biotechnologies, making it possible to access genome-wide gene expressions. To address biomedical issues at molecular levels, extraction of the relevant information from massive data of complex structures is essential. This calls for advanced mechanisms for statistical prediction and inference, especially in genomic discovery and prediction, where statistical uncertainty involved in a discovery process is high. The proposed approach focuses on the development of mixture model-based and large margin approaches in semisupervised and unsupervised learning, motivated from biomedical studies in gene discovery and prediction. In particular, we propose to investigate how to improve accuracy and efficiency of mixture model-based and large margin learning systems in generalization. In addition, we will develop innovative methods taking the structure of sparseness and the grouping effect into account to battle the curse of dimensionality, and blend them with the new learning tools. A number of technical issues will be investigated, including: a) developing model selection criteria and performing automatic feature selection, especially when the number of features greatly exceeds that of samples; b) developing large margin approaches for multi-class learning, with most effort towards sparse as well as structured learning; c) implementing efficient computation for real-time applications, and d) analyzing two biological datasets for i) gene function discovery and prediction for E. coli, and ii) new class discovery and prediction for BOEC samples; e) developing public-domain software. Furthermore, computational strategies will be explored based on global optimization techniques, particularly convex programming and difference convex programming.           n/a",New Machine Learning Methods for Biomedical Data,7299383,R01GM081535,"['Accounting', 'Address', 'Algorithms', 'Area', 'Arts', 'Biological', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Blood', 'Blood Cells', 'Class', 'Code', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Condition', 'Consult', 'DNA Sequence', 'DNA-Protein Interaction', 'Data', 'Data Set', 'Development', 'Dimensions', 'Documentation', 'Endothelial Cells', 'Escherichia coli', 'Gene Cluster', 'Gene Expression', 'Genome', 'Genomics', 'Goals', 'Grouping', 'Human', 'Knowledge', 'Lead', 'Learning', 'Machine Learning', 'Malignant Neoplasms', 'Medical', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Nonparametric Statistics', 'Numbers', 'Outcome', 'Pan Genus', 'Performance', 'Process', 'Property', 'Public Domains', 'Research', 'Research Project Grants', 'Sample Size', 'Sampling', 'Selection Criteria', 'Standards of Weights and Measures', 'Structure', 'System', 'Techniques', 'Testing', 'Thinking', 'Time', 'Uncertainty', 'base', 'computerized tools', 'concept', 'cost', 'design', 'disorder subtype', 'gene discovery', 'gene function', 'genome sequencing', 'improved', 'information classification', 'innovation', 'insight', 'interest', 'novel', 'novel strategies', 'programs', 'protein protein interaction', 'research study', 'software development', 'statistics', 'tool']",NIGMS,UNIVERSITY OF MINNESOTA,R01,2007,266852,-0.0055122548994077985
"The CardioVascular Research Grid    DESCRIPTION (provided by applicant):       We are proposing to establish the Cardiovascular Research Grid (CVRG). The CVRG will provide the national cardiovascular research community a collaborative environment for discovering, representing, federating, sharing and analyzing multi-scale cardiovascular data, thus enabling interdisciplinary research directed at identifying features in these data that are predictive of disease risk, treatment and outcome. In this proposal, we present a plan for development of the CVRG. Goals are: To develop the Cardiovascular Data Repository (CDR). The CDR will be a software package that can be downloaded and installed locally. It will provide the grid-enabled software components needed to manage transcriptional, proteomic, imaging and electrophysiological (referred to as ""multi-scale"") cardiovascular data. It will include the software components needed for linking CDR nodes together to extend the CVRG To make available, through community access to and use of the CVRG, anonymized cardiovascular data sets supporting collaborative cardiovascular research on a national and international scale To develop Application Programming Interfaces (APIs) by which new grid-enabled software components, such as data analysis tools and databases, may be deployed on the CVRG To: a) develop novel algorithms for parametric characterization of differences in ventricular shape and motion in health versus disease using MR and CT imaging data; b) develop robust, readily interpretable statistical learning methods for discovering features in multi-scale cardiovascular data that are predictive of disease risk, treatment and outcome; and c) deploy these algorithms on the CVRG via researcher-friendly web-portals for use by the cardiovascular research community To set in place effective Resource administrative policies for managing project development, for assuring broad dissemination and support of all Resource software and to establish CVRG Working Groups as a means for interacting with and responding to the data management and analysis needs of the cardiovascular research community and for growing the set of research organizations managing nodes of the CVRG. (End of Abstract).          n/a",The CardioVascular Research Grid,7246847,R24HL085343,"['Algorithms', 'Cardiovascular system', 'Communities', 'Computer software', 'Data', 'Data Analyses', 'Data Set', 'Databases', 'Development', 'Development Plans', 'Disease', 'Environment', 'Goals', 'Health', 'Image', 'Interdisciplinary Study', 'International', 'Internet', 'Link', 'Machine Learning', 'Methods', 'Motion', 'Policies', 'Proteomics', 'Research', 'Research Personnel', 'Resources', 'Shapes', 'Treatment outcome', 'Ventricular', 'Work', 'abstracting', 'data management', 'disorder risk', 'novel', 'programs', 'tool']",NHLBI,JOHNS HOPKINS UNIVERSITY,R24,2007,2183988,0.000984925043692754
"Bioconductor: an open computing resource for genomics    DESCRIPTION (provided by applicant): The Bioconductor project provides an open resource for the development and distribution of innovative reliable software for computational biology and bioinformatics. The range of available software is broad and rapidly growing as are both the user community and the developer community. The project maintains a web portal for delivering software and documentation to end users as well as an active mailing list. Additional services for developers include a software archive, mailing list and assistance and advice program development and design      We propose an active development strategy designed to meet new challenges while simultaneously providing user and developer support for existing tools and methods. In particular we emphasize a design strategy that accommodates the imperfect, yet evolving nature of biological knowledge and the relatively rapid development of new experimental technologies. Software solutions must be able to rapidly adapt and to facilitate new problems when they arise.      CRITQUE 1:      The Bioconductor project began in 2001. In 2002 it was awarded a BISTI grant for three years 2003-2006). During this time the project has expanded and provided support for a world wide community of researchers. This is a proposal for continued development for Bioconductor, which is a set of statistical programs which are specifically tailored to the computatational biology community. Bioconductor is composed of over 130 R packages that have been contributed by a large number of developers. The software packages range from state of the art statistical methods which typically are used in microarray analysis, to annotation tools, to plotting functions, GUIs, to sequence alignment and data management packages. Contributions to and usage of Bioconductor is growing rapidly and the applicants are requesting support to continue its development as well as general logistical support for software distribution and quality assurance. The proposal includes a research component for Bioconductor which will involve the development of analysis techniques. This will include optimization of the R statistical analyses, statistical processing of Affymetrix data, analysis of SNP data, improved standards, data storage, retreivals from NCBI, sequence management, machine learning, web services and distributed computing.      SCIENTIFIC MERIT   The applicants address many issues that are crucial to the success of a large open source project with multiple contributors. Examples of training, scientific publication, documentation and resource development run throughout the proposal. Many tangible examples were given on the usage of the system by the scientific community.        EXPERIMENTAL DESIGN   This is a description of their management workflow for the project which does a good job of demonstrating the technical excellence brought to the project by this group. 1) Build annotation packages every three months, Integrate changes in annotation source data structure into annotation package building code. 2) Maintain project website, mailing lists, source control archive. Organize web resources for short course and conferences. 3) Improve existing software. 4) Sustain automated nightly builds. Work with developers whose packages fail to pass QA. 5) Resolve cross-platform issues. 6) Review new submissions. Answer questions on the mailing lists. 7) Use software engineering best practices. Develop unit testing strategies. Design appropriate classes and methods for new data types. Refactor existing code for better interoperability and extensibility. 8) Develop and organize training materials and documentation.      Extensive detail on testing, build procedures, interoperability, quality assurance and project management is given elsewhere in the document. They clearly have dealt with many issues necessary for a project of this size. They state that one of the biggest cost items is support of this package to run on multiple platforms. They point out that many contributors focus on a single platform, much of their work is track down cross-platform bugs. This is time well-spent, given the platforms used are in sync with the needs of the greater bioinformatics community.        ORIGINALITY   While a high degree of originality is not a particularly critical element of open source software development project, there are certainly areas in the proposal that are unique. Most importantly, it is safe to say that there is not another project which has this blend of statistical analysis systems specifically tailored to a important research bioinformatics area that can be deployed on a number of different computer environments.      INVESTIGATOR AND CO-INVESTIGATORS   Dr. Gentleman is the founder and leader of the Bioconductor project. Dr. Gentlemen was an Associate Professor in the Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana Farber Cancer Institute. In 2004 he became Program Head, Computational Biology, at the Fred Hutchinson Cancer Research Center in Seattle. He has on the order of ten publications relating to Bioconductor or related statistical analysis. He implemented the original versions of the R programming language jointly with another co-founder. He is PI or Investigator of a number of research grants, at least two are directly related to this work. He and other members of the proposal have taught a number of courses and given lectures on Bioconductor, the amount of these courses certainly indicate significant dedication to the project.  A review of the PI and Co-PI activities related to this project are shown on Table 3 on page 42 of the application. The roles and time allocations assigned to each participant appear to be reasonable.  Dr. Gentleman will serve as project leader and will manage the programmers, coordinating the project, and investigating new computational methods and approaches.  Dr. Vincent Carey, as co Principal Investigator has 20% time allocated for the project.  In 2005 he became Associate Professor of Medicine (Biostatistics). Carey is a senior member of the Bioconductor development core. He will improve interoperability to allow Bioconductor reuse of external modules in Java, Perl and other languages as well as strengthen interfaces between high throughput experimental workflows and machine learning tools, and ontology capture.  An administrative assistant will assist Dr. Carey with administrative requirements, including call coordination, manuscript preparation and distribution, scheduling and budget management.  Dr. Rafael Irizarry as co-PI will spend 30% effort on the project.  Dr. Irizarry has four years experience developing methods for microarray data analysis and in the Department of Biostatistics serving as faculty liaison to the Johns Hopkins Medical Institution's Microarray Core.  He will supervize all efforts to support preprocessing on all platforms and support for microarray related consortiums such as the ERCC, GEO, and ArrayExpress.      Programmers will be responsible for the project website, managing email lists, maintaining training materials, upgrading software, refactoring and other code enhancements, managing the svn archive, and Bioconductor releases. They will handle checking all submitted packages, developing unit tests, and simplifying downloads, nightly build procedures, cross-platform issues, data technologies as well as integrating resources found in other languages (e.g. large C libraries of routines for string handling, machine learning and so on). Programmers have familiarity with R packages and systems for database management and for parallel and distributed computing. They will be responsible for managing the annotation data including package building and liaising with organism specific and other data providers.      SIGNIFICANCE   Given the scope of the proposal, and the size of the Bioconductor project in general the request for the above resources is appropriate. There is an excellent mix of grounded project management along with development of newer state of the art techniques that will benifit many members of the bioinformatics community. There is a high probability that funding this project will help to maintain and advance this important community resource.      ENVIRONMENT   The computer infrastructure, and the local departments of the PI and Co-PIs, as well as the work with the larger scientific community are all excellent environments to support this project.      IN SUMMARY   This is a terrific resource.  It is a well managed large open source project with very well crafted QA testing, documentation and training.  Continuation of this is a three year project. Beyond that period, a statement of long term stated goals is needed. The PI should articulate the strategic goals, as well as their research motivation and translate that into an action plan. They should also use that context to describe how they would go about choosing packages that are put into the Bioconductor system; Table 3 only listed the names of the packages made by the applicants, it could have gone further to give the reader more information for choosing packages.  A simple example would have been if they stated in the document: ""Given our assessment of the microarray state of the art, we ultimately aim to overlay annotation data, ontological information, and other forms of meta data onto a statistical framework for expression data."" The resulting research plan would then justify a five year project, but it was not strong enough in this application.       It should be noted that many of the benificiaries to this system are not just users that download the system.  In many cases a centralized informatics service downloads their system and then performs analysis for other members of the campus or the wider www community. While that type of ""success measure"" is hard to assess, more effort in this area in subsequent proposals would be helpful.           n/a",Bioconductor: an open computing resource for genomics,7293650,P41HG004059,"['Address', 'Archives', 'Area', 'Arts', 'Award', 'Bioconductor', 'Bioinformatics', 'Biological', 'Biology', 'Biometry', 'Budgets', 'Building Codes', 'Class', 'Code', 'Communities', 'Complex', 'Computational Biology', 'Computer Simulation', 'Computer software', 'Computers', 'Computing Methodologies', 'Dana-Farber Cancer Institute', 'Data', 'Data Analyses', 'Data Sources', 'Data Storage and Retrieval', 'Database Management Systems', 'Dedications', 'Development', 'Discipline', 'Documentation', 'Educational process of instructing', 'Electronic Mail', 'Elements', 'Environment', 'Evolution', 'Experimental Designs', 'Faculty', 'Familiarity', 'FarGo', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Genomics', 'Goals', 'Grant', 'Head', 'Human Genome', 'Human Resources', 'Individual', 'Informatics', 'Institution', 'Internet', 'Investigation', 'Java', 'Knowledge', 'Language', 'Libraries', 'Machine Learning', 'Mails', 'Manuscripts', 'Measures', 'Medical', 'Medicine', 'Methodology', 'Methods', 'Microarray Analysis', 'Motivation', 'Names', 'Nature', 'Numbers', 'Occupations', 'Ontology', 'Operative Surgical Procedures', 'Organism', 'Participant', 'Policies', 'Preparation', 'Principal Investigator', 'Probability', 'Procedures', 'Process', 'Program Development', 'Programming Languages', 'Provider', 'Public Health Schools', 'Publications', 'Range', 'Reader', 'Request for Proposals', 'Research', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Role', 'Running', 'Schedule', 'Scientist', 'Sequence Alignment', 'Services', 'Software Design', 'Software Engineering', 'Solutions', 'Source', 'Standards of Weights and Measures', 'Statistical Methods', 'System', 'Systems Analysis', 'Techniques', 'Technology', 'Testing', 'Time', 'Training', 'Translating', 'Work', 'cluster computing', 'computing resources', 'cost', 'cost effective', 'data management', 'data structure', 'design', 'experience', 'falls', 'improved', 'innovation', 'interoperability', 'lectures', 'member', 'model development', 'open source', 'originality', 'professor', 'programs', 'quality assurance', 'research study', 'size', 'software development', 'success', 'symposium', 'tool', 'tool development', 'web-accessible']",NHGRI,FRED HUTCHINSON CAN RES CTR,P41,2007,796910,0.00043955184949191425
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7189144,K12MH069281,"['Anatomy', 'Area', 'Artificial Intelligence', 'Base of the Brain', 'Biomedical Engineering', 'Biomedical Informatics Research Network', 'Brain', 'Brain imaging', 'Caliber', 'Class', 'Clinical', 'Clinical Research', 'Communities', 'Competence', 'Complex', 'Development', 'Discipline', 'Educational Activities', 'Exposure to', 'Foundations', 'Functional disorder', 'Future', 'Goals', 'Grant', 'Health', 'Health Sciences', 'Hospitals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Laboratories', 'Mentored Research Scientist Development Award', 'Mentors', 'Methodology', 'Monitor', 'Neurosciences', 'Numbers', 'Operative Surgical Procedures', 'Physicians', 'Procedures', 'Program Development', 'Psyche structure', 'Range', 'Rate', 'Recruitment Activity', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'SECTM1 gene', 'Science', 'Scientist', 'Surface', 'Techniques', 'Technology', 'Training', 'Training Programs', 'Woman', 'base', 'bioimaging', 'biomedical informatics', 'brain morphology', 'career', 'cationic antimicrobial protein CAP 37', 'clinical application', 'cognitive neuroscience', 'computer science', 'imaging informatics', 'multidisciplinary', 'nervous system disorder', 'neuroimaging', 'neuroinformatics', 'post-doctoral training', 'programs', 'research and development', 'response', 'skills']",NIMH,MASSACHUSETTS GENERAL HOSP,K12,2007,450967,-0.012754187466548104
"Interactive Learning Modules for Writing Grant Proposals DESCRIPTION (provided by applicant):  The overall objective of this application is to continue our challenge to empower faculty at institutions with high minority enrollment to develop and submit competitive research proposals. Building on our past experience, the competing renewal has three facets which will take place concurrently: 1) up-dating current modules (14) and the development, testing, and evaluation of two new internet course modules; 2) recruitment and training of participants through 5 workshops at remote sites and subsequent participation in the web-based course; 3) continuing evaluation of the training modules for the purpose of technological and content versions. Significant changes from the original program include moving the pre-course conference off-site to maximize the number of faculty participating from contiguous institutions and the introduction of machine language technology to aid in the editing and packaging of participants' grant writing efforts. At the conclusion of the initiative, each participant should be both motivated and empowered to submit a competitive proposal. Thus, our continuing partnership with NIGMS should improve the skills and abilities of researchers/grant writers at minority institutions, increase the number of minorities engaged in biomedical research, and strengthen minority institution's overall research environment. n/a",Interactive Learning Modules for Writing Grant Proposals,7252088,U13GM058252,"['Advisory Committees', 'Applications Grants', 'Biomedical Research', 'Computer Retrieval of Information on Scientific Projects Database', 'Computer software', 'Databases', 'Development', 'Educational workshop', 'Enrollment', 'Environment', 'Evaluation', 'Faculty', 'Feedback', 'Funding', 'Future', 'Goals', 'Grant', 'Grant Review Process', 'Institution', 'Internet', 'Language', 'Learning', 'Machine Learning', 'Mentors', 'Minority', 'National Institute of General Medical Sciences', 'Numbers', 'Online Systems', 'Participant', 'Peer Review', 'Progress Reports', 'Purpose', 'Research', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Rest', 'Role', 'Scientist', 'Services', 'Site', 'Study Section', 'System', 'Technology', 'Time', 'Training', 'Training Programs', 'Underrepresented Minority', 'United States National Institutes of Health', 'Universities', 'Work', 'Writing', 'base', 'evaluation/testing', 'experience', 'follow-up', 'impression', 'improved', 'member', 'novel strategies', 'programs', 'skills', 'symposium', 'training project']",NIGMS,UNIVERSITY OF KENTUCKY,U13,2007,276104,-0.01998057877219589
The RPI Exploratory Center for Cheminformatics (RMI) No abstract available n/a,The RPI Exploratory Center for Cheminformatics (RMI),7472067,P20HG003899,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2007,364010,-0.0018167612812546772
Carolina Exploratory Center for Cheminformatics Research No abstract available n/a,Carolina Exploratory Center for Cheminformatics Research,7472715,P20HG003898,"['Address', 'Algorithms', 'Applications Grants', 'Area', 'Belief', 'Bioinformatics', 'Biotechnology', 'Categories', 'Cell Nucleus', 'Chemicals', 'Chemistry', 'Class', 'Classification', 'Communities', 'Computer software', 'Computers', 'Data', 'Data Set', 'Decision Making', 'Descriptor', 'Development', 'Educational workshop', 'Effectiveness', 'Environment', 'Evaluation', 'Faculty', 'Funding', 'Generations', 'Generic Drugs', 'Grant', 'Hybrids', 'Industry', 'Institution', 'Interdisciplinary Study', 'Knowledge', 'Laboratories', 'Location', 'Machine Learning', 'Methodology', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Molecular Bank', 'Molecular Medicine', 'Molecular Structure', 'Online Systems', 'Pattern Recognition', 'Pharmacotherapy', 'Pilot Projects', 'Preparation', 'Process', 'Property', 'Purpose', 'Range', 'Relative (related person)', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Science', 'Scientist', 'Screening procedure', 'Seeds', 'Site', 'Specialist', 'Structure', 'Students', 'Sum', 'TNFRSF5 gene', 'Techniques', 'Testing', 'Travel', 'Validation', 'Vision', 'Work', 'base', 'chemical property', 'cheminformatics', 'computer center', 'computer science', 'concept', 'data mining', 'design', 'in vitro Assay', 'interdisciplinary approach', 'knowledge base', 'model development', 'organizational structure', 'predictive modeling', 'symposium', 'tool', 'training project']",NHGRI,UNIV OF NORTH CAROLINA CHAPEL HILL,P20,2007,373960,-0.0018167612812546772
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methodswithin the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data,  earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user  nterface to support access to information resources for enhanced decision-making by practitioners. The ong-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3  Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technologywork; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C). n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7284424,P01HK000027,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2007,1889501,0.005430199168079639
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7475421,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,160000,0.028233042646701848
"A Resource for Biomedical Ontologies and Knowledge Bases    DESCRIPTION (provided by applicant):       For more than two decades, our laboratory has been studying technology to develop, manage, and use formal descriptions of biomedical concepts. The result of this work is Protege, a workbench that allows users to edit and apply controlled terminologies, ontologies, and knowledge bases to a wide range of information-management problems. To date, more than 50,000 people have registered as users of the system. Many diverse projects in biomedicine-supported by nearly every institute and center at NIH-have become critically dependent on this software and the knowledge-engineering principles that it supports. This P41 competing renewal application seeks to continue support for Protege, as a biomedical informatics resource that will benefit the system's entire user community.      We propose technology research and development to expand the capabilities of the Protege system to meet the current and anticipated needs of the user community. We will re-engineer Protege with a service-oriented architecture that can adapt to the requirements of new ontology languages, large ontology repositories, and cutting-edge ontology-management-services, such as reasoning, alignment, and evolution. We will create support for collaborative ontology development, in the context of both large, centralized projects and open, decentralized efforts. We also will develop advanced support for using ontologies in application software development and as integral parts of software systems.      As a biomedical informatics resource, we will expand our collaborative research projects with other Prot¿g¿ users. We will provide service to the Protege user community through enhanced technical support, user documentation, tutorials, and workshops. These activities will serve to disseminate information about the resource and will aid research and development in many aspects of biomedical informatics both in the United States and internationally.          n/a",A Resource for Biomedical Ontologies and Knowledge Bases,7248464,P41LM007885,"['Address', 'Adopted', 'Anatomy', 'Applications Grants', 'Architecture', 'Area', 'Biomedical Computing', 'Biomedical Technology', 'Class', 'Clinical', 'Code', 'Communities', 'Complex', 'Computer software', 'Data Set', 'Development', 'Documentation', 'Educational workshop', 'Electronics', 'Engineering', 'Ensure', 'Environment', 'Evolution', 'Facility Construction Funding Category', 'Foundations', 'Funding', 'Generic Drugs', 'Genes', 'Goals', 'Grant', 'Guidelines', 'Information Management', 'Institutes', 'International', 'Knowledge', 'Laboratories', 'Language', 'Mails', 'Maintenance', 'Modeling', 'Natural Language Processing', 'Numbers', 'Ontology', 'Participant', 'Process', 'Published Comment', 'Range', 'Representations, Knowledge (Computer)', 'Research', 'Research Infrastructure', 'Research Project Grants', 'Resources', 'Semantics', 'Services', 'Software Engineering', 'Strigiformes', 'System', 'Technology', 'Terminology', 'Time', 'Training', 'United States', 'United States National Institutes of Health', 'Work', 'Writing', 'base', 'biomedical informatics', 'biomedical resource', 'concept', 'design', 'information organization', 'innovation', 'knowledge base', 'member', 'next generation', 'open source', 'repository', 'research and development', 'size', 'software development', 'software systems', 'symposium', 'tool']",NLM,STANFORD UNIVERSITY,P41,2007,693808,0.028233042646701848
"Ontologies and Biomedical Language Processing    DESCRIPTION (provided by applicant): We hypothesize that there are significant synergies between the applications of biomedical ontologies and of biomedical language processing (BLP) which can be used to improve the quality and scope of both activities. A growing body of work suggests such synergies might exist, but there has yet to be a systematic exploration of their potential. We propose to carry out a focused effort to explore both the potential for, and obstacles to, the mutual application of biomedical ontologies and biomedical language processing. To provide immediate biological relevance to our work, we propose to focus on the topics of autoimmune and pulmonary disease. We group our proposed explorations into three specific aims: (1) Create novel tools and approaches for the application and maintenance of biomedical ontologies, based on an assessment of the processes and tools used for the ontological annotation of textual corpora in the biomedical language processing community. Particularly, we will focus on the creation of new methods for effective search through large ontologies, compositional approaches to annotation, effective capture of the evidence underlying annotations, and the use of automated suggestions for manual confirmation. (2) Evaluate the utility of BLP tools and techniques when applied to terms and definitions of biomedical ontologies, both to enrich and interconnect orthogonal ontologies, and to provide quality assurance and quality control mechanisms. Particularly, we propose to develop and evaluate methods for connecting terms within and across ontologies, for assessing completeness of an ontology against the literature, and for implementing automatically executable measures of ontology quality. (3) Compare the differences between annotations produced by manual procedures and those produced by automated BLP methods for completeness and correctness. Based on the resulting data, produce guidelines for the optimal interplay between manual and automatic procedures for producing broad, accurate and useful knowledge-bases. Because ontologies are the central organizing tool of the model organism databases, improvements in their quality and in the ease and efficiency of their use will have a major effect on the model organism databases, speed the translational process generally, and create a potentially large public health impact.          n/a",Ontologies and Biomedical Language Processing,7364235,R01GM083649,"['Address', 'Area', 'Autoimmune Diseases', 'Autoimmune Process', 'Autoimmunity', 'Automated Annotation', 'Biological', 'Biomedical Computing', 'Body of uterus', 'Classification', 'Collaborations', 'Communities', 'Complex', 'Data', 'Development', 'Elements', 'Ensure', 'Environment', 'Genes', 'Guidelines', 'Human', 'Immunology', 'Insulin-Dependent Diabetes Mellitus', 'Language', 'Linguistics', 'Literature', 'Lung diseases', 'Maintenance', 'Manuals', 'Maps', 'Measures', 'Medicine', 'Metaphor', 'Methods', 'Mission', 'Modeling', 'Molecular', 'Motivation', 'Natural Language Processing', 'Numbers', 'Ontology', 'Peer Review', 'Procedures', 'Process', 'Process Assessment', 'Production', 'Psyche structure', 'Public Health', 'Publications', 'Pulmonary Hypertension', 'Quality Control', 'Recording of previous events', 'Representations, Knowledge (Computer)', 'Research', 'Research Personnel', 'Rheumatoid Arthritis', 'Role', 'Semantics', 'Speed', 'Suggestion', 'Techniques', 'Technology', 'Text', 'Ursidae Family', 'Validation', 'Work', 'base', 'computer science', 'computerized tools', 'concept', 'cost', 'design', 'experience', 'gene function', 'improved', 'information organization', 'innovation', 'knowledge base', 'language processing', 'model organisms databases', 'novel', 'quality assurance', 'tool']",NIGMS,UNIVERSITY OF COLORADO DENVER,R01,2007,631600,0.0017697180575018147
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7494181,U54EB005149,"['Address', 'Affect', 'Alcohols', 'Algorithms', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Anisotropy', 'Appearance', 'Area', 'Atlases', 'Automobile Driving', 'Behavioral Research', 'Biological', 'Biology', 'Biomedical Computing', 'Brain', 'Budgets', 'Cells', 'Characteristics', 'Clinical', 'Clinical Data', 'Cognitive', 'Collaborations', 'Collection', 'Commit', 'Complex', 'Computational algorithm', 'Computer software', 'Computer-Assisted Image Analysis', 'Computing Methodologies', 'Coupling', 'Data', 'Data Correlations', 'Data Sources', 'Development', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Electroencephalography', 'Elements', 'Ensure', 'Epilepsy', 'Feedback', 'Fiber', 'Fostering', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Functional disorder', 'Future', 'Genetic', 'Genomics', 'Goals', 'Healthcare', 'Heart', 'Hemoglobin', 'Hippocampus (Brain)', 'Histocompatibility Testing', 'Image', 'Image Analysis', 'Image-Guided Surgery', 'Imagery', 'Imaging Techniques', 'Individual', 'Knowledge', 'Life', 'Link', 'Localized', 'Location', 'Magnetic Resonance Imaging', 'Measurement', 'Measures', 'Medical', 'Medical Imaging', 'Metabolic', 'Metabolism', 'Methodology', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modification', 'Morphology', 'Multiple Sclerosis', 'Neurons', 'Neurosciences', 'Operative Surgical Procedures', 'Organ', 'Other Imaging Modalities', 'Output', 'Participant', 'Patients', 'Pattern', 'Performance', 'Physiological', 'Polishes', 'Population', 'Positron-Emission Tomography', 'Process', 'Property', 'Range', 'Recording of previous events', 'Relative (related person)', 'Research', 'Research Infrastructure', 'Research Personnel', 'Role', 'Sampling', 'Scanning', 'Schizophrenia', 'Science', 'Services', 'Shapes', 'Software Engineering', 'Source', 'Specific qualifier value', 'Staging', 'Statistical Distributions', 'Structure', 'Syndrome', 'System', 'Systems Analysis', 'Techniques', 'Testing', 'Textiles', 'Time', 'Tissues', 'Today', 'USA Georgia', 'Utah', 'Visible Radiation', 'Vision', 'Western Asia Georgia', 'Work', 'base', 'bioimaging', 'computerized tools', 'cost', 'design', 'disability', 'experience', 'image registration', 'insight', 'interest', 'mathematical model', 'neuroimaging', 'novel', 'prenatal', 'research study', 'response', 'shape analysis', 'software development', 'tool', 'vector', 'vision development', 'water diffusion', 'white matter']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,87500,0.0190469494647772
"National Alliance for Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (NAMIC)(RMI),7271955,U54EB005149,[' '],NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2007,3888915,0.0190469494647772
"Morphometry Biomedical Informatics Research Network    DESCRIPTION (provided by applicant):     Technological advances in imaging have revolutionized the biomedical investigation of illness. The tremendous potential that this methodology brings to advancing diagnostic and prognostic capabilities and in treatment of illnesses has as yet remained largely an unfulfilled promise. This potential has been limited by a number of technological impediments that could be in large part overcome by the availability of a federated imaging database and the attendant infrastructure. Specifically, the ability to conduct clinical imaging studies across multiple sites, to analyze imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well characterized image and clinical data would have a demonstrable and positive impact on progress in this field. The Morphometry BIRN (mBIRN), established in October 2001, has made substantial progress in the development of this national infrastructure to develop a data and computational network based on a federated data acquisition and database across seven sites in the service of facilitating multi-site neuroanatomic analysis. Standardized structural MRI image acquisition protocols have been developed and implemented that demonstrably reduce initial sources of inter-site variance. Data structure, transmission, storage and querying aspects of the federated database have been implemented. In this continuation of the mBIRN efforts, we propose three broad areas of work:   1) continuing structural MRI acquisition optimization, calibration and validation to include T2 and DTI; 2) translation of site specific state-of-the-art image analysis, visualization and machine learning technologies to work in the federated, multi-site BIRN environment; and 3) extension of data management and database query capabilities to include additional imaging modalities, clinical disorders and individualized human genetic covariates. These broad areas of work will come together in through key collaborations that will ensure utilization promotion by facilitating data entry into the federated database and creation of database incentive functionality. Our participating sites include MGH (PI), BWH, UCI, Duke, UCLA, UCSD, John Hopkins, and newly added Washington University and MIT. We have made a concerted effort to bridge the gap that can exist between biomedical and computational sciences by recruiting to our group leaders in both of these domains. Our efforts will be coordinated with those of the entire BIRN consortium in order to insure that acquisition and database functionality, and application-based disorder queries are interoperable across sites and designed to advance the capabilities to further knowledge and understanding of health and disease.         n/a",Morphometry Biomedical Informatics Research Network,7253351,U24RR021382,[' '],NCRR,MASSACHUSETTS GENERAL HOSP,U24,2007,5123449,0.0035888697672531097
"AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM DESCRIPTION (provided by applicant):     The proposed training program in Informatics for Global Health will consist of several activities carried out over a five year period 1) two short courses for 30-50 participants in basic informatics to be held in Peru 2) short term skills based training in informatics in Seattle with emphasis on genomics and resource access skills building 3) long term training in Informatics at the post doctoral or Masters of Science level through the Biomedical and Health Informatics program at the School of Medicine. Six scholars will be trained in addition to the short course participants. Four scholars now engaged in research will receive focused, skills building technical training and six scholars will be recruited and enrolled for longer term (2-3 year) post doctoral or Masters of Science training at the University of Washington. It is envisioned that this training program will foster the development of the capacity of UPCH to continue its role as a leading biomedical research institution and also to create a home to health informatics research activity in Peru. n/a",AMAUTA HEALTH INFORMATICS RESEARCH AND TRAINING PROGRAM,7249492,D43TW007551,"['1 year old', 'AIDS prevention', 'AIDS/HIV problem', 'Academic Detailing', 'Academic Medical Centers', 'Achievement', 'Acquired Immunodeficiency Syndrome', 'Acyclovir', 'Address', 'Admission activity', 'Adopted', 'Adult', 'Advisory Committees', 'Aeromonas', 'Affect', 'Age', 'Age Reporting', 'Agreement', 'AlamarBlue', 'Algorithms', 'Alleles', 'Am 80', 'Amauta', 'American', 'Americas', 'Amino Acid Sequence', 'Aminoglycosides', 'Anal Sex', 'Anogenital venereal warts', 'Anti-Retroviral Agents', 'Antibiotic susceptibility', 'Antigens', 'Antimicrobial susceptibility', 'Antitubercular Agents', 'Appendix', 'Appointment', 'Area', 'Artificial Intelligence', 'Arts', 'Asia', 'Aspirate substance', 'Australia', 'Automation', 'Award', 'Bacillus (bacterium)', 'Back', 'Bacteria', 'Bacterial Infections', 'Bacterial Vaginosis', 'Baseline Surveys', 'Basic Science', 'Behavior', 'Behavior Control', 'Behavior Therapy', 'Behavioral Research', 'Biochemical', 'Bioinformatics', 'Biological', 'Biological Assay', 'Biology', 'Biomedical Engineering', 'Biomedical Research', 'Biometry', 'Biotechnology', 'Bioterrorism', 'Birth', 'Bisexual', 'Blinded', 'Blood Tests', 'Bolivia', 'Books', 'Businesses', 'CD4 Lymphocyte Count', 'Calmette-Guerin Bacillus', 'Campylobacter', 'Canada', 'Candida', 'Caring', 'Case-Control Studies', 'Cataloging', 'Catalogs', 'Categories', 'Cause of Death', 'Cell physiology', 'Cells', 'Centers for Disease Control and Prevention (U.S.)', 'Certification', 'Cervical', 'Characteristics', 'Chest', 'Child', 'Child health care', 'Childhood', 'Chlamydia', 'Chlamydia Infections', 'Chromosome Mapping', 'Chronic', 'Ciprofloxacin', 'Cities', 'Class', 'Classification', 'Client', 'Climate', 'Clinic', 'Clinical', 'Clinical Informatics', 'Clinical Medicine', 'Clinical Research', 'Clinical Trials', 'Clinical assessments', 'Clinical trial protocol document', 'Code', 'Cohort Studies', 'Collaborations', 'Collection', 'Color', 'Commit', 'Committee Membership', 'Communicable Diseases', 'Communication', 'Communities', 'Community Medicine', 'Competence', 'Complex', 'Computer Assisted', 'Computer software', 'Computerized Medical Record', 'Computers', 'Congenital Syphilis', 'Consultations', 'Contact Tracing', 'Costa Rica', 'Coughing', 'Counseling', 'Country', 'County', 'Coupled', 'Critiques', 'Cross-Sectional Studies', 'Cryptosporidium', 'DNA', 'DNA Microarray Chip', 'DNA Microarray format', 'DNA amplification', 'Daily', 'Data', 'Data Collection', 'Data Sources', 'Databases', 'Decision Analysis', 'Decision Making', 'Decision Support Systems', 'Dental Schools', 'Depth', 'Detection', 'Developed Countries', 'Developing Countries', 'Development', 'Devices', 'Diagnosis', 'Diagnostic', 'Diagnostic radiologic examination', 'Diagnostic tests', 'Diarrhea', 'Dimensions', 'Discipline', 'Discipline of Nursing', 'Disease', 'Disease remission', 'Distance Education', 'Distance Learning', 'Doctor of Philosophy', 'Dose', 'Drug Formulations', 'Drug usage', 'Early Diagnosis', 'Ecology', 'Economics', 'Education', 'Educational Curriculum', 'Educational Intervention', 'Educational Status', 'Educational process of instructing', 'Effectiveness', 'Electronic Mail', 'Electronics', 'Elements', 'Emergency Situation', 'Engineering', 'English Language', 'Enrollment', 'Ensure', 'Environment', 'Environmental Health', 'Epidemiologist', 'Epidemiology', 'Ethambutol', 'Ethics', 'Etiology', 'Evaluation', 'Event', 'Evolution', 'Expert Systems', 'Exposure to', 'Face', 'Faculty', 'Familiarity', 'Family', 'Feedback', 'Fellowship', 'Fellowship Program', 'Female', 'Female Condoms', 'Fertility Rates', 'Fever', 'Figs - dietary', 'Financial Support', 'Floor', 'Focus Groups', 'Fostering', 'Foundations', 'Fred Hutchinson Cancer Research Center', 'Funding', 'Future', 'Gene Conversion', 'Gene Expression', 'General Population', 'Generations', 'Genes', 'Genetic', 'Genetic Markers', 'Genetic Recombination', 'Genetic screening method', 'Genome', 'Genomics', 'Genus Mycobacterium', 'Geographic Information Systems', 'Geography', 'Giardia lamblia', 'Globus Pallidus', 'Glues', 'Goals', 'Gonorrhea', 'Government', 'Grant', 'Grenada', 'Guanosine Diphosphate', 'Guidelines', 'HIV', 'HIV Infections', 'HIV Seropositivity', 'HIV prevention trials network', 'HIV vaccine', 'HIV-1', 'HIV-2 vaccine', 'Hand', 'Head', 'Health', 'Health Information System', 'Health Personnel', 'Health Policy', 'Health Professional', 'Health Science Library', 'Health Sciences', 'Health Services', 'Health Services Research', 'Health Status', 'Health care facility', 'Health education', 'Healthcare', 'Heterogeneity', 'Heterosexuals', 'High Prevalence', 'Hispanics', 'Home Page', 'Home environment', 'Hospital Administration', 'Hospital Information Systems', 'Hospitalization', 'Hospitals', 'Hour', 'Housing', 'Human', 'Human Herpesvirus 2', 'Human Papilloma Virus Vaccine', 'Human Papillomavirus', 'Human Resources', 'Human immunodeficiency virus test', 'Human papillomavirus 16', 'Hybrids', 'Hyphae', 'IL2 gene', 'Image', 'Immersion Investigative Technique', 'Immune', 'Incidence', 'Individual', 'Induration', 'Infant', 'Infection', 'Infection Control', 'Informatics', 'Information Centers', 'Information Management', 'Information Networks', 'Information Resources', 'Information Sciences', 'Information Systems', 'Institutes', 'Institution', 'Instruction', 'Interdisciplinary Study', 'Interleukin-2', 'Internal Medicine', 'International', 'International AIDS', 'International Classification of Diseases', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Intervention', 'Intervention Trial', 'Interview', 'Isoniazid resistance', 'Japan', 'Job Description', 'Journals', 'Kenya', 'Knowledge', 'Knowledge acquisition', 'Laboratories', 'Laboratory Finding', 'Laboratory Research', 'Lamivudine/Zidovudine', 'Language', 'Latin America', 'Laws', 'Lead', 'Leadership', 'Learning', 'Left', 'Leishmaniasis', 'Lesion', 'Librarians', 'Libraries', 'Library Science', 'Licensing', 'Life', 'Life Expectancy', 'Light', 'Link', 'Linux', 'Literature', 'Local Area Networks', 'Location', 'London', 'Low income', 'Lung diseases', 'MEDLINE', 'Machine Learning', 'Malaria', 'Manuscripts', 'Maps', 'Marketing', 'Master of Science', 'Master&apos', 's Degree', 'Measures', 'Mediating', 'Mediation', 'Medical', 'Medical Education', 'Medical Informatics', 'Medical Libraries', 'Medical Research', 'Medical Students', 'Medical Surveillance', 'Medical Technology', 'Medical center', 'Medicine', 'Mentors', 'Mentorship', 'Methods', 'Metronidazole', 'Microbiology', 'Mining', 'Minority', 'Mission', 'Modeling', 'Modification', 'Molecular', 'Monitor', 'Montenegro', 'Mothers', 'Motivation', 'Multi-Drug Resistance', 'Multidrug-Resistant Tuberculosis', 'Mutation', 'Mycobacterium tuberculosis', 'N.I.H. Research Support', 'Names', 'Nature', 'Needs Assessment', 'Neighborhoods', 'Neisseria', 'Neonatology', 'Nested Case-Control Study', 'Nested PCR', 'Network-based', 'Nevirapine', 'Nuclear Energy', 'Nucleic Acids', 'Numbers', 'Nurses', 'Nursing Faculty', 'Nursing Schools', 'Occupational', 'Occupations', 'Online Systems', 'Ontology', 'Operating System', 'Oral', 'Outcome', 'Outcome Measure', 'Oxidation-Reduction', 'Oxidative Stress', 'Pacific Northwest', 'Pan American Health Organization', 'Paper', 'Parasites', 'Parasitic infection', 'Parasitology', 'Participant', 'Pathogenesis', 'Pathologist', 'Pathology', 'Pathway interactions', 'Patient Education', 'Patient currently pregnant', 'Patients', 'Pattern', 'Pediatrics', 'Peer Review', 'Pelvic Examination', 'Pelvic Inflammatory Disease', 'Peptide Sequence Determination', 'Performance', 'Perinatal', 'Peripheral Blood Mononuclear Cell', 'Peroxidase', 'Peroxidases', 'Personal Satisfaction', 'Persons', 'Peru', 'Pharmaceutical Preparations', 'Pharmacists', 'Pharmacologic Substance', 'Pharmacy Schools', 'Pharmacy facility', 'Phase', 'Phenotype', 'Philosophy', 'Physicians', 'Physics', 'Pilot Projects', 'Placebos', 'Placement', 'Plasma', 'Play', 'Pliability', 'Policies', 'Polymerase Chain Reaction', 'Population', 'Positioning Attribute', 'Postdoctoral Fellow', 'Practice Management', 'Pre-Post Tests', 'Predictive Value', 'Predisposition', 'Pregnancy', 'Pregnant Women', 'Prenatal care', 'Preparation', 'Prevalence', 'Prevention', 'Prevention strategy', 'Preventive', 'Primary Health Care', 'Principal Investigator', 'Printing', 'Private Practice', 'Prize', 'Problem Solving', 'Procedures', 'Process', 'Production', 'Professional counselor', 'Program Development', 'Program Evaluation', 'Programmed Learning', 'Progress Reports', 'Prophylactic treatment', 'Proteins', 'Proteomics', 'Protocols documentation', 'Protozoa', 'Provider', 'Province', 'Psychological reinforcement', 'Public Health', 'Public Health Administration', 'Public Health Education', 'Public Health Informatics', 'Public Health Practice', 'Public Health Schools', 'Public Hospitals', 'Public Policy', 'Publications', 'Publishing', 'Pulmonary Tuberculosis', 'Purpose', 'Pyrazinamide', 'Qualifying', 'Qualitative Methods', 'Questionnaires', 'Radiation', 'Radiation Oncology', 'Radiation therapy', 'Randomized', 'Randomized Controlled Clinical Trials', 'Randomized Controlled Trials', 'Range', 'Rate', 'Reaction', 'Readiness', 'Recommendation', 'Recording of previous events', 'Recruitment Activity', 'Recurrence', 'Regulation', 'Regulatory Element', 'Relative (related person)', 'Reporting', 'Representations, Knowledge (Computer)', 'Reproductive Tract Infections', 'Research', 'Research Activity', 'Research Design', 'Research Ethics Committees', 'Research Infrastructure', 'Research Institute', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Research Proposals', 'Research Training', 'Resistance', 'Resources', 'Restriction fragment length polymorphism', 'Retrieval', 'Review Literature', 'Rheumatology', 'Rifampin', 'Risk', 'Risk Behaviors', 'Risk Factors', 'Robotics', 'Role', 'Rotavirus', 'Route', 'Running', 'Rural', 'Rural Health', 'Rural Population', 'Safe Sex', 'Salmonella', 'Sampling', 'Scholarship', 'School Nursing', 'Schools', 'Science', 'Scientist', 'Score', 'Screening procedure', 'Second Pregnancy Trimester', 'Secure', 'Security', 'Seeds', 'Senegal', 'Sentinel', 'Sentinel Surveillance', 'Sequence Alignment', 'Sequence Analysis', 'Series', 'Seroepidemiologic Studies', 'Serological', 'Seroprevalences', 'Services', 'Sex Behavior', 'Sexual Partners', 'Sexually Transmitted Diseases', 'Side', 'Signs and Symptoms', 'Simulate', 'Singapore', 'Sister', 'Site', 'Social Development', 'Social Psychology', 'Software Engineering', 'Software Tools', 'Solid', 'Solutions', 'Source', 'Specificity', 'Specimen', 'Spectinomycin', 'Speed', 'Sputum', 'Staining method', 'Stains', 'Standards of Weights and Measures', 'Stomach', 'Streptomycin', 'Structure', 'Students', 'Supplementation', 'Support System', 'Surrogate Markers', 'Surveillance Program', 'Surveys', 'Swab', 'Switzerland', 'Symptoms', 'Syndrome', 'Syphilis', 'System', 'T-Lymphocyte', 'TAF8 gene', 'Taiwan', 'Tanzania', 'Teaching Materials', 'Techniques', 'Technology', 'Teleconferences', 'Telemedicine', 'Television', 'Test Result', 'Testing', 'Textbooks', 'Therapeutic', 'Thinking', 'Time', 'TimeLine', 'Title', 'Touch sensation', 'Trainers Training', 'Training', 'Training Activity', 'Training Programs', 'Training and Infrastructure', 'Treatment Efficacy', 'Treponema pallidum', 'Triad Acrylic Resin', 'Trichomonas Infections', 'Tropical Disease', 'Trust', 'Tuberculin', 'Tuberculosis', 'U-Series Cooperative Agreements', 'USAID', 'Ulcer', 'Underemployment', 'United States', 'United States Dept. of Health and Human Services', 'United States National Institutes of Health', 'Universities', 'Unmarried', 'Update', 'Urban Population', 'Urethritis', 'Vaccination', 'Vaccines', 'Vagina', 'Variant', 'Venezuela', 'Vertebral column', 'Vibrio', 'Videoconferences', 'Videoconferencing', 'Virulence', 'Virulence Factors', 'Vision', 'Visit', 'Visual', 'Voice', 'Walking', 'Washington', 'Week', 'Western Europe', 'Wing', 'Wolves', 'Woman', 'Work', 'Workplace', 'World Health Organization', 'Writing', 'Zidovudine', 'Zinc', 'abstracting', 'base', 'behavior change', 'biomedical informatics', 'blood filter', 'cancer therapy', 'career', 'case control', 'case-based', 'catalase', 'clinically relevant', 'cohort', 'college', 'computer center', 'computer science', 'computing resources', 'concept', 'condoms', 'contextual factors', 'coping', 'cost', 'cytokine', 'data integration', 'data management', 'data mining', 'database design', 'day', 'design', 'desire', 'digital', 'epidemiology study', 'evaluation/testing', 'expectation', 'experience', 'falls', 'fetal', 'fly', 'follow-up', 'forging', 'genital herpes', 'genome sequencing', 'global environment', 'health administration', 'health application', 'high risk behavior', 'human subject', 'image processing', 'imaging informatics', 'immunopathology', 'improved', 'informatics training', 'information gathering', 'information organization', 'innovation', 'insight', 'instrumentation', 'interest', 'international center', 'isoniazid', 'knowledge base', 'laboratory facility', 'lectures', 'macrophage', 'male', 'mathematical model', 'medical schools', 'member', 'men', 'microbial', 'mortality', 'mouse Gdi2 protein', 'mycobacterial', 'neglect', 'nevirapine resistance', 'new growth', 'next generation', 'older patient', 'older women', 'oncology', 'open source', 'outreach', 'pathogen', 'pediatric AIDS', 'pediatrician', 'peer', 'point of care', 'prenatal', 'prevent', 'professor', 'programs', 'prospective', 'prototype', 'recombinase', 'rectal', 'research and development', 'research study', 'response', 'satisfaction', 'sex', 'size', 'skills', 'skills training', 'software development', 'sound', 'statistics', 'success', 'symposium', 'syndromic surveillance', 'teacher', 'tool', 'tool development', 'trafficking', 'transcription factor', 'transmission process', 'treatment planning', 'trend', 'tumor', 'web based interface', 'wide area network', 'willingness', 'young adult']",FIC,UNIVERSITY OF WASHINGTON,D43,2007,73250,-0.015884564705179134
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,7122136,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2006,39750,-0.005094489234762806
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,7109207,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2006,649537,0.01607507612894991
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,7069599,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2006,398762,0.00676469700373887
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,7056141,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2006,439784,-0.012754187466548104
"The RPI Exploratory Center for Cheminformatics (RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics (RMI),7125575,P20HG003899,"['Internet', 'NIH Roadmap Initiative tag', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2006,377226,-0.013097868367528196
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,7284550,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2006,498959,-0.02698850285660688
"24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS    DESCRIPTION (provided by applicant):    This conference grant (R13) application requests funds to partially cover the cost of planning, organizing, publicizing and hosting the 24th Annual Symposium on Nonhuman Primate Models for AIDS. The symposium will be held October 4-7, 2006, at the Omni Hotel at CNN Center in downtown Atlanta, Georgia, and will be hosted by the Yerkes National Primate Research Center, Emory University. This meeting is the premier forum for the presentation and exchange of the most recent scientific advances in AIDS research utilizing the nonhuman primate model. The latest findings in primate pathogenesis, immunology, genomics, virology, vaccines and therapeutics will be presented. It is anticipated more than 300 scientists from the United States and other countries will attend. The symposium will encompass five half-day scientific sessions and an evening poster session. The scientific sessions will be: Virology, Pathogenesis, Immunology, Vaccines and Therapeutics/Genomics. Each session will have an invited Chair, a scientific leader in the field, who will give a 30-minute state-of-the-field presentation to open the session, and a Co-Chair from the Scientific Committee, who will moderate the session and entertain questions. In addition, there will be an invited keynote speaker and a banquet speaker, who will address scientific approaches and concerns regarding the global AIDS crisis and related issues of public health. A Scientific Program Committee consisting of eight-ten members drawn from the Yerkes/Emory community and other institutions will review abstracts and assign oral or poster presentations for each of the scientific sessions. Committee members will include leaders in the field from a variety of scientific disciplines. Criteria for selection of oral presentations will include relevance of the topic as well as originality and quality of the information contained in the abstract. Those giving talks will be invited to submit their presentations in manuscript form for publication in the Journal of Medical Primatology. A poster session will include meritorious presentations that cannot be accommodated in one of the platform sessions. A local Organizing Committee will handle arrangements and logistics for the symposium. Feedback from the participants will be obtained through written questionnaires or oral comments to members of the organizing committee. This format has been successfully followed using NCRR support for the previous Annual symposium.           n/a",24th ANNUAL SYMPOSIUM ON NONHUMAN PRIMATE MODELS FOR AIDS,7114527,R13RR022961,"['AIDS', 'Primates', 'disease /disorder model', 'meeting /conference /symposium', 'travel']",NCRR,EMORY UNIVERSITY,R13,2006,63089,0.0025968479396396744
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),7104243,U54EB005149,"['NIH Roadmap Initiative tag', 'bioimaging /biomedical imaging', 'bioinformatics', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2006,3809481,0.0190469494647772
"Morphometry Biomedical Informatics Research Network    DESCRIPTION (provided by applicant):     Technological advances in imaging have revolutionized the biomedical investigation of illness. The tremendous potential that this methodology brings to advancing diagnostic and prognostic capabilities and in treatment of illnesses has as yet remained largely an unfulfilled promise. This potential has been limited by a number of technological impediments that could be in large part overcome by the availability of a federated imaging database and the attendant infrastructure. Specifically, the ability to conduct clinical imaging studies across multiple sites, to analyze imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well characterized image and clinical data would have a demonstrable and positive impact on progress in this field. The Morphometry BIRN (mBIRN), established in October 2001, has made substantial progress in the development of this national infrastructure to develop a data and computational network based on a federated data acquisition and database across seven sites in the service of facilitating multi-site neuroanatomic analysis. Standardized structural MRI image acquisition protocols have been developed and implemented that demonstrably reduce initial sources of inter-site variance. Data structure, transmission, storage and querying aspects of the federated database have been implemented. In this continuation of the mBIRN efforts, we propose three broad areas of work:   1) continuing structural MRI acquisition optimization, calibration and validation to include T2 and DTI; 2) translation of site specific state-of-the-art image analysis, visualization and machine learning technologies to work in the federated, multi-site BIRN environment; and 3) extension of data management and database query capabilities to include additional imaging modalities, clinical disorders and individualized human genetic covariates. These broad areas of work will come together in through key collaborations that will ensure utilization promotion by facilitating data entry into the federated database and creation of database incentive functionality. Our participating sites include MGH (PI), BWH, UCI, Duke, UCLA, UCSD, John Hopkins, and newly added Washington University and MIT. We have made a concerted effort to bridge the gap that can exist between biomedical and computational sciences by recruiting to our group leaders in both of these domains. Our efforts will be coordinated with those of the entire BIRN consortium in order to insure that acquisition and database functionality, and application-based disorder queries are interoperable across sites and designed to advance the capabilities to further knowledge and understanding of health and disease.         n/a",Morphometry Biomedical Informatics Research Network,7078667,U24RR021382,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data management', 'human subject', 'imaging /visualization /scanning', 'information systems', 'magnetic resonance imaging', 'method development', 'molecular biology information system', 'morphometry', 'neurogenetics', 'neuroimaging', 'neuropsychology']",NCRR,MASSACHUSETTS GENERAL HOSPITAL,U24,2006,5010926,0.0035888697672531097
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,7033080,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2006,133459,0.014816599489689865
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6929696,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2005,206378,0.02210514456412942
"Information Integration of Heterogeneous Data Sources    DESCRIPTION (provided by applicant): The overall goal of this proposal is to develop an information integration architecture and associated tools to support rapid integration of data and knowledge from distributed heterogeneous data sources. The architecture aims to play a significant role in extracting coherent knowledge bases for biomedical research and improving the accuracy, completeness and quality of the extracted knowledge. Towards achieving these goals, the proposed scalable architecture includes new innovative generalized integration algorithms and tools for the generation of mediators to capture the functional behavior of data sources, semantic representation of data sources to support automated generation of integration agents, and optimization of integrated data queries. The information integration architecture keeps pace with the evolving Internet-based XML electronic data interchange, semantic web services, and web services discovery standards. Thus, leveraging the Internet technologies and standards for the purpose of providing lasting state-of-the-art solutions for information integration. In addition, the proposed architecture is inherently scalable in terms of the number of data sources that can be integrated, the number of users of the integrated system, and the range of biomedical problems that can be tackled. During phase I of the project, prototypes of the proposed integration algorithms and tools will be developed as proofs of concept and to form the foundation for evaluation and pilot testing of the proposed integration mechanisms, using private and public data sources, in terms of scalability and integration capabilities.         n/a",Information Integration of Heterogeneous Data Sources,6881960,R43RR018667,"['artificial intelligence', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'information retrieval', 'information system analysis', 'information systems', 'mathematics']",NCRR,"INFOTECH SOFT, INC.",R43,2005,260661,-0.005916921490249287
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6955060,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2005,39150,-0.005094489234762806
"Shifting Conceptions of Human Identity DESCRIPTION (provided by applicant):  . One of the most important questions raised by the ongoing achievements of the Human Genome Project is how this new biological knowledge - and the powers it confers - will affect our identity and self-understanding as human beings. This book project focuses on one key aspect of this complex issue: exploring the extent to which human identity can be reconciled with deliberate design or partial redesign. The author proposes to shed new light on this question by comparing the debates surrounding two areas of scientific innovation that are not normally associated with each other, but that are in fact deeply related: the enterprise of human genetic intervention and the enterprise of building intelligent machines. Both these enterprises entail ""pushing the limits"" of traditional concepts of what it means to be human; and both ultimately confront their makers with the same core ""family"" of questions: What are the defining features of human personhood? To what extent can those features be modified or extended, before human personhood begins to break down? Can some (or all) of those features find embodiment in an entity other than a human being? These kinds of questions are no longer the sole province of science fiction writers, but have been taken up with increasing seriousness by mainstream scientists and technologists, as well as by a wide array of ""science watchers"" in academia, legislative circles, and the news media.   . Through documentary research and interviews, this project aims to deepen our understanding of the history and sociology of the debates surrounding these powerful new technologies, electro-mechanical and biological, that are perceived as destabilizing human identity. The intended audience for the book is a broad one: scientists and technological practitioners interested in the social and cultural reception of their research; legislators and other policymakers with a stake in the governance of science; general educated readers who are concerned about the role of science and technology in shaping our collective future. n/a",Shifting Conceptions of Human Identity,6915830,R03HG003298,"['adult human (21+)', 'artificial intelligence', 'behavioral /social science research tag', 'biotechnology', 'books', 'clinical research', 'ethics', 'genetic manipulation', 'history of life science', 'human subject', 'identity', 'interview', 'robotics', 'self concept', 'sociology /anthropology']",NHGRI,VANDERBILT UNIVERSITY,R03,2005,75833,-0.03725858623742013
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,6953037,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2005,647432,0.01607507612894991
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,7000653,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,SPELMAN COLLEGE,R33,2005,260030,-0.00530383738290215
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6896406,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2005,403171,0.00676469700373887
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6931476,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2005,392500,-0.019675970625768823
"The RPI Exploratory Center for Cheminformatics(RMI) The purpose of this Exploratory Center for Cheminformatics Research (ECCR) P20 planning grant is to develop a mechanism for bringing together and stimulating collaborative pilot projects among a constantly-evolving nucleus of experts in Cheminformatics-related fields ranging from methods of encoding and capturing molecular information, to machine learning and data mining techniques, to predictive model development, validation, interpretation and utilization. In addition to these research efforts, the Center will bring together a set of domain specialists and application scientists who will serve as both data generators and end users of the knowledge provided by the molecular property models and modeling methods developed during the course of the grant. This group will also test the new Cheminformatics software that will constitute a tangible, deliverable product from this work. Ten application project modules that exemplify possible interactions between various groups and areas of expertise within the Center are presented as part of this proposal. The unifying vision behind the proposed Center is that much of what is done in each of the subdisciplines represented here can be expressed in a Cheminformatics context: The many diverse project areas can be grouped into one or more overlapping categories: ""Data Generators"" (those who use either theoretical or experimental methods for creating or extracting knowledge), ""Machine Learning and Datamining"" groups (who perform model validation, feature selection, pattern recognition, generation of potentials of mean force and knowledge-based potential work), as well as ""Property-Prediction"" groups (who perform chemically-aware model building, molecular property descriptor generation, Quantitative Structure-Property Relationship modeling, validation, and interpretation), and ""Application"" groups who utilize the information made available using the new tools and methods that are developed as part of the Center. It is our strong belief that these areas of expertise can be brought together within this Planning Grant proposal to generate something larger than the sum of the parts. The Exploratory Center will seed new interdisciplinary projects and train graduate students in these areas.   Relevance: Advances in the generation, mining and analysis of chemical information is crucial to the development of new drug therapies, and to modern methods of bioinformatics and molecular medicine. n/a",The RPI Exploratory Center for Cheminformatics(RMI),7032113,P20HG003899,"['Internet', 'bioinformatics', 'chemical models', 'cheminformatics', 'computer program /software', 'computers', 'data collection methodology /evaluation', 'data management', 'information retrieval', 'interdisciplinary collaboration', 'model design /development', 'molecular biology']",NHGRI,RENSSELAER POLYTECHNIC INSTITUTE,P20,2005,375639,-0.013097868367528196
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,6870217,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2005,428924,-0.012754187466548104
"Biomedical Ontology and Tools for Database Curation DESCRIPTION (provided by applicant): This proposal describes a new tool for text data mining-a biomedical language ontology and integrated natural-language-processing methods. Our long-term goal is to provide resources for biomedical knowledge discovery from text. Our immediate goal is to provide a knowledge discovery tool for the curation of organism databases such as the Genome Database (SGD). The proposed research not only serves the research needs of the SGD community, it also helps the broader biomedical community exploit the strengths of the comparative approach to biological research. The hypothesis of this proposal is that knowledge discovery from biomedical text requires a knowledge base that integrates both genomic and linguistic information. This hypothesis is based on two observations: (a) the language of biomedicine, like all natural language, is complex in structure and morphology (the basic units of meaning) and poses problems of synonymy (several terms having the same meaning), polysemy (a term having more than one meaning), hypernymy (one term being more general than another), hyponymy (one term being more specific than another), denotation (what a term refers to in contrast to what it means), and denotation and description (different ways of referring to the same thing); and (b) important biomedical knowledge sources, such as the Gene Ontology (GO), are expressed in natural language. The specific aims of the proposed project are to: 1. Extend an existing biomedical language ontology to include genomic and linguistic data from SGD; 2. Use this ontology to discover, in full-text articles made available by SGD, information about the molecular function of yeast gene products that can be inferred from direct experimental assays; 3. Evaluate the effectiveness of the new tool and methods by comparing its results to those of the SGD curators for gene products that have GO functional annotations with evidence code IDA (Inferred from Direct Assay). n/a",Biomedical Ontology and Tools for Database Curation,6885487,R43HG003600,"['computer program /software', 'computer system design /evaluation', 'fungal genetics', 'information retrieval', 'information system analysis', 'molecular biology information system', 'yeasts']",NHGRI,"CONVERSPEECH, LLC",R43,2005,99250,-0.030922199633822537
"CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS    DESCRIPTION (provided by applicant): The University of Washington proposes to establish the Center of Excellence in Public Health Informatics: Improving the Public's Health through Information Integration. Partners include the Washington Department of Health, Kitsap County Health District, the Public Health Informatics Institute, and Inland Northwest Health Services. This Center will focus on three research topics: Project 1 (Surveillance Integration and Decision Support) will develop public health surveillance methods within the emerging health information infrastructure. We will: 1) develop methods by which regional health information organizations can enhance public health surveillance; 2) develop and evaluate a probabilistic decision support system classifier for disease surveillance; and 3) investigate the usability of a web survey-assessment system for population tracking and disease reporting. Project 2 (Customizable Knowledge Management Repository System for Prevention: Design, Development, and Evaluation) will develop an interactive digital knowledge management system to support the collection, management, and retrieval of public health documents, data, earning objects, and tools. The focus will be the development of tools, including concept mapping services that will provide rapid access to answers from a variety of key resources, including the ""gray literature"". The system will focus on the application of natural language processing and information visualization techniques. Components will include a knowledge repository system, integrative web services and a role-based user interface to support access to information resources for enhanced decision-making by practitioners. The long-term goal is to create an environment in which practitioners can pose questions in ""plain English"" and receive answers to their questions rather than simply a list of possible places to look for answers. Project 3 Supporting Integration: Work Process, Change Management and System Modeling) will: 1) refine and validate an integrated model of public health information technology work; 2) provide a Change Management Toolkit to support public health agencies in making changes to current practice called for by the integrated model; and 3) build a Virtual Public Health Information Technology Environment to serve as a testbed and to explore informatics challenges. These projects are supported by three cores: Administration Core (Core A), Epidemiology and Biostatistics Science Core (Core B), and Technology and Design Science Core (Core C).             n/a",CENTER OF EXCELLENCE IN PUBLIC HEALTH INFORMATICS,7084856,P01CD000261,[' '],ODCDC,UNIVERSITY OF WASHINGTON,P01,2005,1270432,0.005355921310513026
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6848697,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2005,1446246,-0.02698850285660688
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6943136,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2005,467823,-0.025760518494331322
"National Alliance-Medical Imaging Computing (NAMIC)(RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance-Medical Imaging Computing (NAMIC)(RMI),6950028,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2005,3800000,0.0190469494647772
"Morphometry Biomedical Informatics Research Network    DESCRIPTION (provided by applicant):     Technological advances in imaging have revolutionized the biomedical investigation of illness. The tremendous potential that this methodology brings to advancing diagnostic and prognostic capabilities and in treatment of illnesses has as yet remained largely an unfulfilled promise. This potential has been limited by a number of technological impediments that could be in large part overcome by the availability of a federated imaging database and the attendant infrastructure. Specifically, the ability to conduct clinical imaging studies across multiple sites, to analyze imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well characterized image and clinical data would have a demonstrable and positive impact on progress in this field. The Morphometry BIRN (mBIRN), established in October 2001, has made substantial progress in the development of this national infrastructure to develop a data and computational network based on a federated data acquisition and database across seven sites in the service of facilitating multi-site neuroanatomic analysis. Standardized structural MRI image acquisition protocols have been developed and implemented that demonstrably reduce initial sources of inter-site variance. Data structure, transmission, storage and querying aspects of the federated database have been implemented. In this continuation of the mBIRN efforts, we propose three broad areas of work:   1) continuing structural MRI acquisition optimization, calibration and validation to include T2 and DTI; 2) translation of site specific state-of-the-art image analysis, visualization and machine learning technologies to work in the federated, multi-site BIRN environment; and 3) extension of data management and database query capabilities to include additional imaging modalities, clinical disorders and individualized human genetic covariates. These broad areas of work will come together in through key collaborations that will ensure utilization promotion by facilitating data entry into the federated database and creation of database incentive functionality. Our participating sites include MGH (PI), BWH, UCI, Duke, UCLA, UCSD, John Hopkins, and newly added Washington University and MIT. We have made a concerted effort to bridge the gap that can exist between biomedical and computational sciences by recruiting to our group leaders in both of these domains. Our efforts will be coordinated with those of the entire BIRN consortium in order to insure that acquisition and database functionality, and application-based disorder queries are interoperable across sites and designed to advance the capabilities to further knowledge and understanding of health and disease.         n/a",Morphometry Biomedical Informatics Research Network,6952714,U24RR021382,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data management', 'human subject', 'imaging /visualization /scanning', 'information systems', 'magnetic resonance imaging', 'method development', 'molecular biology information system', 'morphometry', 'neurogenetics', 'neuroimaging', 'neuropsychology']",NCRR,MASSACHUSETTS GENERAL HOSPITAL,U24,2005,5013536,0.0035888697672531097
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6865478,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2005,162750,0.014816599489689865
"Nurse Practitioner Access to Genetics Health Literature    DESCRIPTION (provided by applicant): This training proposal describes a research plan which tests the application of a computer science solution to a clinical problem encountered by nurse practitioners (NPs). As our understanding of genetics health increases, NPs will need to provide care and create health promotion regimens mindful of each client's genetic profile. Access to the rapidly developing genetics health literature is critical for this practice model, but may be difficult, because existing search methods lead to many irrelevant results, and may retrieve the proper result only when precise keywords are used. The NP searching for information that would guide her practice may be forced to make a decision with less than complete or current information. This proposal outlines three studies, which investigate how the semantic web concepts of linking related ideas and terms can improve NPs' access to the genetics health literature. Study 1 explores NPs' genetics health information needs. Study 2 tests the applicability of existing ontologies (terminologies coupled with machine-readable statements about the meanings and relationships of the terms) to nursing. Study 3 tests a prototype intelligent agent employing ontology to retrieve literature relevant to NPs' genetics health information needs.         n/a",Nurse Practitioner Access to Genetics Health Literature,6837265,F37LM008636,"['artificial intelligence', 'clinical research', 'genetics', 'human subject', 'informatics', 'information retrieval', 'nurse practitioners', 'patient care management', 'predoctoral investigator', 'publications', 'semantics', 'young adult human (21-34)']",NLM,UNIVERSITY OF WISCONSIN MADISON,F37,2004,38596,-0.005094489234762806
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6783420,R33GM066400,"['artificial intelligence', 'automated data processing', 'biomedical resource', 'computer data analysis', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data management', 'information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2004,200515,0.02210514456412942
"Collaborative Brain Mapping: Tools for Sharing    DESCRIPTION (provided by applicant): Sharing data between research centers is increasingly important for contemporary brain imaging studies because they involve large numbers of subjects and complex analysis protocols that require highly specialized expertise. Our long-term objective is to facilitate brain-imaging research by enabling remote researchers to pool data between institutions and to analyze data using the appropriate algorithms executing on distributed resources. There are a number of difficult data management and technology challenges that have limited the success of data sharing environments. Rather than attempt to develop a comprehensive and general solution, we propose to develop a set of open, interoperable, and portable software tools that address critical issues currently limiting efforts to share and analyze brain-imaging data. Building upon years of providing brain-mapping expertise to collaborators, we propose to solve problems that we repeatedly encounter and that currently limit progress in brain imaging research. We propose to develop validated tools that enable collaborators to remotely access a variety of data analysis methods and databases. We will create web-based tools to perform multi-institutional studies, and provide access to complex data processing protocols executing on distributed computing resources. There are three specific aims. 1) Enable the web based acquisition and management of data utilizing an access control system that includes consideration of subject consent limits and investigator imposed conditions to facilitate data pooling for multi-institutional studies. This system will convert data files between different formats and schemas so that data can be used consistently between analysis programs and databases. It will also anonymize images and metadata according to institution-specific protocols. 2) Develop a system that is aware of data type and provenance so that it may act intelligently to arbitrate between different analysis programs. This system will capture the expertise of experienced lab personnel in the usage of various tools and assist new users in designing appropriate analytic strategies. 3) Create meta-algorithms that improve the robustness of techniques for neuroimaging analysis by intelligently combining the results from multiple algorithms. The proposed approach will provide a set of tools that address significant problems in data sharing and utilization. The resulting information technology will be scalable and applicable to other scientific data sharing problems.         n/a",Collaborative Brain Mapping: Tools for Sharing,6802141,R01MH071940,"['Internet', 'artificial intelligence', 'bioimaging /biomedical imaging', 'brain imaging /visualization /scanning', 'brain mapping', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'confidentiality', 'data management', 'information dissemination', 'information systems', 'mathematics']",NIMH,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2004,668796,0.01607507612894991
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6697243,R33EY013688,"['artificial intelligence', 'clinical research', 'computer human interaction', 'computer program /software', 'computer system design /evaluation', 'data collection methodology /evaluation', 'disease /disorder classification', 'disease /disorder etiology', 'human subject', 'informatics', 'interdisciplinary collaboration', 'macular degeneration', 'mathematics', 'pathologic process', 'phenotype']",NEI,UNIVERSITY OF IOWA,R33,2004,252586,-0.00530383738290215
"Automated Knowledge Extraction for Biomedical Literature DESCRIPTION (provided by applicant):     It is becoming increasingly difficult for biologists to keep pace with information being published within their own fields, let alone biology as a whole. The ability to rapidly access specific and current biomedical information as well as to quickly gain an overview of current knowledge in a given field is becoming more difficult while at the same time more important. Traditional methods of keeping up with advances are therefore becoming inadequate.      Here we propose to continue to develop our Medstract Project to apply recent advances in the computational analysis of text to organize and structure the biological literature. The Medstract project will reduce the time required for biomedical researchers to find information of interest and should facilitate the development of new research insights.  This project is the result of a unique collaboration between a computational linguistics lab at Brandeis University and a molecular biology lab at Tufts University School of Medicine. Previously we have developed an extensive set of tools for analyzing and processing biomedical text. We have used these tools to develop databases of biomedical acronyms, inhibitors, regulators, and interactors from Medline abstracts and have made these available on the web. These resources are currently used by hundreds of investigators every day. In addition we have generated and made available gold standard markup files for several biological terms and relations for use as testing standards by other groups developing knowledge extraction engines for the biomedical domain.      Here we propose to extend and enhance our current Medstract databases as well to generate new databases using the tools that we have developed. New databases will include protein modifications, domains and motifs, and tissue and cellular localization information. In addition, we will use the bio-relation databases as the foundation for constructing a system allowing point-to-point regulatory pathway identification. We will enhance the robustness of these databases by utilizing algorithms that we have developed for rerendering the semantic ontologies for the biomedical lexicon.  Furthermore, by applying coreference resolution algorithms to the text, we will improve precision and recall of knowledge extraction for populating the database n/a",Automated Knowledge Extraction for Biomedical Literature,6774132,R01LM006649,"['Internet', 'abstracting', 'artificial intelligence', 'computer assisted instruction', 'computer assisted sequence analysis', 'computer system design /evaluation', 'educational resource design /development', 'informatics', 'information retrieval', 'information system analysis', 'information systems', 'molecular biology information system', 'nucleic acid sequence', 'protein sequence', 'publications', 'semantics', 'syntax', 'vocabulary development for information system']",NLM,BRANDEIS UNIVERSITY,R01,2004,411436,0.00676469700373887
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6787778,P20GM067650,"['animal tissue', 'artificial intelligence', 'bioinformatics', 'computational biology', 'computer data analysis', 'computer human interaction', 'computer simulation', 'computer system design /evaluation', 'data management', 'disease /disorder etiology', 'epidemiology', 'functional /structural genomics', 'gene expression', 'human subject', 'interdisciplinary collaboration', 'mathematical model', 'pharmacokinetics', 'science education', 'statistics /biometry', 'technology /technique development', 'therapy', 'training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2004,392500,-0.019675970625768823
"Neuroimaging Neuroinformatics Training Program    DESCRIPTION (provided by applicant): This proposal is in response to PAR-03-034 ""Neuroinformatics Institutional Mentored Research Scientist Development Award (K12)."" The overarching goal of this application is to provide an excellent postdoctoral training program in neuroimaging neuroinformatics that capitalizes on the many strengths of the existing neuroscientists, informatics and imaging resources that our combined resources represent. Our proposed Neuroimaging Neuroinformatics Training Program (NNTP) is based upon a number of important strategic alliances. The first cornerstone of this effort is the existing HBP grants held by Dr. Anders Dale (R01 NS39581: Cortical-Surface-Based Brain Imaging) and Dr. David Kennedy (R01 NS34189: Anatomic Morphologic Analysis of MR Brain Images). These efforts span a wealth of technological developments, research and clinical application areas in the rapidly developing area of quantitative morphometric image analysis. A second and vital cornerstone is our association with the Harvard-MIT Division of Health Sciences and Technology (HST) Biomedical Informatics Program. This existing pre- and post-graduate academic program, within a world class biomedical engineering department, is an ideal setting for the development of a coordinated training effort in Neuroinformatics. The established track record in training skilled scientists in areas of informatics will prove invaluable in this new initiative. The third cornerstone is the combined clinical research opportunities afforded by the Harvard-wide biomedical imaging resources. These include the MGH/MIT/HST Athinoula A. Martinos Center for Biomedical Imaging, the Harvard Neuroimaging Center, the Surgical Planning Lab at Brigham and Women's Hospital, the Brain Morphology BIRN (Biomedical Informatics Research Network) and the MIT Artificial Intelligence Laboratory. Together, these active and vibrant programs provide for the best possible training opportunities in imaging science, computer science, clinical application areas, and cognitive neuroscience. A substantial and successful pool of internationally renowned mentors have agreed to participate in this program, and the combined resources provide the best possible exposure to all neuroimaging procedures and insure the capability to draw the highest caliber trainees. A plan for recruiting, selecting and monitoring trainees is proposed. This program will be an asset to the Neuroinformatics initiatives of the Human Brain Project by helping to prepare future scientists with advanced neuroinformatics skills         n/a",Neuroimaging Neuroinformatics Training Program,6700018,K12MH069281,"['bioimaging /biomedical imaging', 'bioinformatics', 'brain imaging /visualization /scanning', 'brain morphology', 'career', 'image processing', 'morphometry', 'neuroimaging', 'neurosciences', 'training']",NIMH,MASSACHUSETTS GENERAL HOSPITAL,K12,2004,311472,-0.012754187466548104
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6795323,U01AA013524,"['alcoholic beverage consumption', 'alcoholism /alcohol abuse information system', 'bioinformatics', 'biomedical facility', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data collection methodology /evaluation', 'electrophysiology', 'neuroanatomy', 'neurochemistry', 'neurophysiology', 'neuroregulation', 'neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2004,754129,-0.025760518494331322
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6699330,P01CA017094,"['antineoplastics', 'biomedical facility', 'chemosensitizing agent', 'clinical research', 'drug design /synthesis /production', 'drug resistance', 'drug screening /evaluation', 'neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2004,1405759,-0.02698850285660688
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6777517,R43EB000608,"['archives', 'clinical research', 'computer data analysis', 'computer system design /evaluation', 'confidentiality', 'data management', 'health care policy', 'health related legal', 'human data', 'imaging /visualization /scanning', 'information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2004,149200,-0.004079021053517123
"National Alliance for Medical Imaging Computing (RMI)    DESCRIPTION (provided by applicant):   The National Alliance for Medical Imaging Computing (NAMIC) is a multi-institutional, interdisciplinary team of computer scientists, software engineers, and medical investigators who develop computational tools for the analysis and visualization of medical image data. The purpose of the center is to provide the infrastructure and environment for the development of computational algorithms and open source technologies, and then oversee the training and dissemination of these tools to the medical research community. This world-class software and development environment serves as a foundation for accelerating the development and deployment of computational tools that are readily accessible to the medical research community. The team combines cutting-edge computer vision research (to create medical imaging analysis algorithms) with state-of-the-art software engineering techniques (based on ""extreme"" programming techniques in a distributed, open-source environment) to enable computational examination of both basic neuroscience and neurological disorders. In developing this infrastructure resource, the team will significantly expand upon proven open systems technology and platforms. The driving biological projects will come initially from the study of schizophrenia, but the methods will be applicable to many other diseases. The computational tools and open systems technologies and platforms developed by NAMIC will initially be used to study anatomical structures and connectivity patterns in the brain, derangements of which have long been thought to play a role in the etiology of schizophrenia. The overall analysis will occur at a range of scales, and will occur across a range of modalities including diffusion MRI, quantitative EGG, and metabolic and receptor PET, but potentially including microscopic, genomic, and other image data. It will apply to image data from individual patients, and to studies executed across large populations. The data will be taken from subjects across a wide range of time scales and ultimately apply to a broad range of diseases in a broad range of organs.             n/a",National Alliance for Medical Imaging Computing (RMI),6847712,U54EB005149,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational neuroscience', 'computer system design /evaluation', 'cooperative study']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,U54,2004,100000,0.0190469494647772
"Morphometry Biomedical Informatics Research Network    DESCRIPTION (provided by applicant):     Technological advances in imaging have revolutionized the biomedical investigation of illness. The tremendous potential that this methodology brings to advancing diagnostic and prognostic capabilities and in treatment of illnesses has as yet remained largely an unfulfilled promise. This potential has been limited by a number of technological impediments that could be in large part overcome by the availability of a federated imaging database and the attendant infrastructure. Specifically, the ability to conduct clinical imaging studies across multiple sites, to analyze imaging data with the most powerful software regardless of development site, and to test new hypotheses on large collections of subjects with well characterized image and clinical data would have a demonstrable and positive impact on progress in this field. The Morphometry BIRN (mBIRN), established in October 2001, has made substantial progress in the development of this national infrastructure to develop a data and computational network based on a federated data acquisition and database across seven sites in the service of facilitating multi-site neuroanatomic analysis. Standardized structural MRI image acquisition protocols have been developed and implemented that demonstrably reduce initial sources of inter-site variance. Data structure, transmission, storage and querying aspects of the federated database have been implemented. In this continuation of the mBIRN efforts, we propose three broad areas of work:   1) continuing structural MRI acquisition optimization, calibration and validation to include T2 and DTI; 2) translation of site specific state-of-the-art image analysis, visualization and machine learning technologies to work in the federated, multi-site BIRN environment; and 3) extension of data management and database query capabilities to include additional imaging modalities, clinical disorders and individualized human genetic covariates. These broad areas of work will come together in through key collaborations that will ensure utilization promotion by facilitating data entry into the federated database and creation of database incentive functionality. Our participating sites include MGH (PI), BWH, UCI, Duke, UCLA, UCSD, John Hopkins, and newly added Washington University and MIT. We have made a concerted effort to bridge the gap that can exist between biomedical and computational sciences by recruiting to our group leaders in both of these domains. Our efforts will be coordinated with those of the entire BIRN consortium in order to insure that acquisition and database functionality, and application-based disorder queries are interoperable across sites and designed to advance the capabilities to further knowledge and understanding of health and disease.         n/a",Morphometry Biomedical Informatics Research Network,6909355,U24RR021382,"['bioimaging /biomedical imaging', 'bioinformatics', 'clinical research', 'computational biology', 'computer data analysis', 'computer program /software', 'computer system design /evaluation', 'cooperative study', 'data management', 'human subject', 'imaging /visualization /scanning', 'information systems', 'magnetic resonance imaging', 'method development', 'molecular biology information system', 'morphometry', 'neurogenetics', 'neuroimaging', 'neuropsychology']",NCRR,MASSACHUSETTS GENERAL HOSPITAL,U24,2004,3821536,0.0035888697672531097
"Understanding Figures & Captions for Location Proteomics    DESCRIPTION (provided by applicant):     This proposal is for mentored training in the molecular biosciences of an established computer scientist. The training plan includes basic and advanced course work in modern biology, interactions with biological research groups, attendance at seminars and conferences, and laboratory training. Mentoring on the culture and practices of biomedical research will be provided by the sponsor. The training institution has a longstanding tradition of interdisciplinary research and specific expertise in cutting edge proteomics methods. The candidate will be fully committed to a combination of training and research. The research plan is based on the critical need to organize and summarize the knowledge in the vast biomedical literature. Curated databases are expensive to create and maintain; do not estimate confidence of assertions; and do not allow for divergence of opinions. Information extraction (IE) methods can be used to partially overcome these limitations by automatically extracting certain types of information from biomedical text.       In most genres of scientific publication, the most important results in a paper are illustrated in non-textual forms, such as images and graphs. The broad thesis underlying our proposed research is that one can provide better access to the information in online scientific publications by extracting information jointly from figure images and their accompanying captions. With the exception of certain previous work by the Murphy group, previous biomedical IE systems have not attempted to extract information from image data, only text.      This proposal addresses these issues in the specific context of fluorescence microscope images depicting the subcellular localization of proteins. This goal is consonant with a major focus of current biomedical research: the identification of expressed genes and the description of the proteins they encode. Motivated by recent large-scale projects which major focus of current biomedical research is the identification of expressed genes and the description (or annotation) of the proteins they encode, the Murphy group has developed automated systems for recognizing subcellular structures in 2D and 3D images. Automated image analysis techniques have also been applied to images harvested from online biomedical journal articles. This system will be extended to create a robust, comprehensive toolset for extracting, verifying and querying biologically relevant information from the text and images found in online journals. Based on this toolkit, a set of tools will be developed for aiding researchers to identify and locate information found in online journals. Upon completion of the proposed training, the candidate will be well placed to take a leadership position in machine learning applications to the range of experimental methods used in biomedical research.               n/a",Understanding Figures & Captions for Location Proteomics,6709988,K25DA017357,"['bioengineering /biomedical engineering', 'bioinformatics', 'computational biology', 'computer program /software', 'computer system design /evaluation', 'fluorescence microscopy', 'gene expression', 'image processing', 'online computer', 'protein localization', 'proteomics', 'publications', 'training']",NIDA,CARNEGIE-MELLON UNIVERSITY,K25,2004,161668,0.014816599489689865
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6658916,R33GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R33,2003,200824,0.02210514456412942
"MicroSeer, Analysis software for microscopy imagery  DESCRIPTION (provided by applicant): This project will create new pattern recognition software to improve the analysis and interpretation of in vivo  biomedical imagery. Currently, researchers can get remarkably detailed images of living cells and their  constituent proteins using molecular genetic and microscopy-based approaches in conjunction with  sophisticated microscopy hardware. Available image analysis techniques and software, however, lag behind  the power of this new imaging equipment to visualize the microscopic world.  This phase I SBIR project will apply existing technology in spatial analysis of satellite image data to microscopy data, create new statistical techniques specific to the study of spatial association of proteins in  cells, and create software that implements these statistics for use in the analysis of spatial association timeslice in vivo biomedical imagery.   n/a","MicroSeer, Analysis software for microscopy imagery",6581125,R43EB000575,"['artificial intelligence', ' bioimaging /biomedical imaging', ' computer data analysis', ' computer graphics /printing', ' computer program /software', ' computer system design /evaluation', ' image processing', ' statistics /biometry', ' structural biology']",NIBIB,BIOMEDWARE,R43,2003,177261,-0.010480784575423943
"Multi-Agent Collaboration for AMD Subtype Classification  DESCRIPTION (provided by applicant): Age-related macular degeneration (AMD) is the most common cause of blindness in developed countries and as such represents a very significant public health problem a number of specific genes, and the discovery, characterization, and eventual therapeutic control of these genes represent major goals of the vision research community. Although the strategies for gene discovery have become very powerful in recent years, there remains a major obstacle to the discovery of genes that underlie common, late-onset diseases like AMD. That obstacle is that clinicians cannot reliably sort patients with different molecular subtypes of late-onset disease into sufficiently homogeneous groups. The purpose of this project is to use the power of multi-agent systems computer technology in a novel way to aid clinicians in the collaborative development of a robust classification system based upon the ophthalmoscopic features of AMD. The result of this project will contribute to an NIH's Innovations in Biomedical Information and Science and Technology Program goal of speeding the progress of biomedical research through the development tools for electronic collaboration that will have impact on broader areas of biomedical research.   We hypothesize that a multi-agent approach to this problem will result in a classification system with greater reproducibility and discriminative power than a system developed by clinicians without such computer assistance. The availability of populations of AMD patients with lower molecular complexity will significantly increase the power of statistical techniques for AMD gene discovery. In addition to this immediate and specific benefit, the strategies we will develop during this project for objectively interfacing medical experts with each other as well as with computers will have applications in the search for other late-onset disease genes as well as in the development of multi-center and multidisciplinary clinical trials of new therapeutic approaches. The proposed system, the Intelligent Distributed Ontology Consensus system (IDOCS) goes beyond conventional groupware by addressing drawbacks to direct, synchronous interaction by providing an autonomously coordinated, asynchronous interaction and collaboration platform among clinicians through their representative intelligent agents. IDOCS will provide a generic meta-data infrastructure using XMLJRDF to make it easily configurable for other diseases.   n/a",Multi-Agent Collaboration for AMD Subtype Classification,6549357,R33EY013688,"['artificial intelligence', ' clinical research', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data collection methodology /evaluation', ' disease /disorder classification', ' disease /disorder etiology', ' human subject', ' informatics', ' interdisciplinary collaboration', ' macular degeneration', ' mathematics', ' pathologic process', ' phenotype']",NEI,UNIVERSITY OF IOWA,R33,2003,259228,-0.00530383738290215
"Computational Approaches to Disease Causes and Treatment DESCRIPTION (provided by applicant): The State University of New York at Buffalo has assembled a multi-disciplinary team of investigators to plan and establish a National Program of Excellence in Biomedical Computing. The overall theme of the center is ""Novel Data Mining Algorithms for Applications in Genomics"" with a focus on the development of novel techniques for storing, managing, analyzing, modeling and visualizing multi-dimensional data sets. We intend to provide the expertise and infrastructure that will merge the research activities of computational and biomedical scientists. The focus of the proposed research is the study of common diseases, such as cancer, multiple sclerosis and coronary artery disease in which the underlying causes are multi-factorial. In this new paradigm, we will use advanced computational techniques and approaches to convert raw genomic data into knowledge that will advance the understanding of these common diseases and potentially identify new modalities of treatment. The Center will play a critical role in fostering multidisciplinary collaborations between faculty from the Departments of Computer Science and Engineering, Biology, Chemistry, Pharmaceutical Science and various departments in the School of Medicine and Biomedical Sciences. By co-locating biomedical and computer scientists, common understanding of research approaches will result in the development of computational tools that will meet the real-life needs of the biomedical researchers to help advance their projects. The Center will provide a broad range of educational and training activities for individuals who wish to pursue a career focusing on computational biology and bioinformatics. The focus of the education program will be the interdisciplinary training of computer science and engineering students who wish to pursue research in functional genomics and other biomedical areas, and the cross training of biomedically oriented students in topics with more of a computing orientation. We have identified three development projects that provide unique scientific opportunities to integrate the expertise of mathematicians, statisticians, and computer scientists with medical scientists, and to investigate novel computational approaches. These computational related projects are: 1. Data integration and data mining of clinical data and genomic data to advance clinical and epidemiological genetics as well as drug effect studies; 2. Pharmacodynamic analysis of drug-responsive gene expression changes; and 3. Chemi-genetic approaches to mapping regulatory pathways. These research projects will be supported by three core resources: genomics core, computational core, and clinical core. The common nature of these applications is that they all generate multidimensional data sets with numerical, functional or symbolic attributes. The management, retrieval and visualization of these data sets and analyses is likely to prove to be a rate limiting factor for new biomedical discoveries and the development of techniques for the effective analyses of genomic datasets is a critical step for the medical applications of bioinformatics. n/a",Computational Approaches to Disease Causes and Treatment,6690235,P20GM067650,"['animal tissue', ' artificial intelligence', ' computer data analysis', ' computer human interaction', ' computer simulation', ' computer system design /evaluation', ' data management', ' disease /disorder etiology', ' epidemiology', ' functional /structural genomics', ' gene expression', ' human subject', ' informatics', ' interdisciplinary collaboration', ' mathematical model', ' pharmacokinetics', ' science education', ' statistics /biometry', ' technology /technique development', ' therapy', ' training']",NIGMS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,P20,2003,392500,-0.019675970625768823
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6626535,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2003,1357166,-0.02698850285660688
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6647589,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2003,729100,-0.025760518494331322
"Access to distributed de-identified imaging data DESCRIPTION (provided by applicant):    The widespread adoption of picture archiving and communications systems (PACS) in radiology and the implementation and deployment of the DICOM communication standard represent an opportunity to link multiple PACS at multiple sites into a distributed data warehouse of great potential utility for investigators in oncology research and epidemiology. Where the federal HIPAA privacy regulations have largely been seen as an emerging impediment to oncology research from the creation, management and use of cancer registries to large-scale retrospective studies addressing rarer forms of neoplasia, in fact the digital nature of PACS-based imaging data lends itself to automated de-identification that could transform multiple distributed clinical information systems into a readily accessible treasure trove of research data that falls within the ""safe harbor"" provisions of HIPAA's privacy regulations. Our firm has developed a platform, originally intended for clinical use, to securely link multiple PACS and RIS from multiple vendors beneath a web interface giving users transparent access to a ""virtual archive"" spanning an arbitrary number of institutions. In this Phase I SBIR application, we propose to explore the feasibility of extending our system to grant researchers access to large volumes of dynamically de-identified imaging data while surmounting each of the major criticisms of the viability of such data for research purposes. We propose developing an open web-services architecture that will enable straightforward integration with any other information system and propose a design that adheres to existing industry standards while laying the groundwork for compliance with future standards and informatics initiatives. This study will also involve examining the regulation of re-identification through the use of threshold cryptography, as well as the feasibility of a probabilistic sampling search engine intended to prevent unauthorized identification of patients through multiple intersecting queries on narrowing criteria, while still permitting researchers to choose the appropriate resolving power of the engine to suit a particular investigation. These studies will include benchmarking the performance of these dynamic processes, quantifying the load they place on live clinical information systems, and optimizing the design to minimize such impact. Should feasibility be demonstrated, Phase II would involve a proof-of-concept demonstration across multiple academic medical institutions as well as steps to prepare for commercialization including indexing studies based on structured reporting and natural language processing, content-based information retrieval, refinement and usability testing of the web interfaces, and extension of the system to permit IRB-approved research on individually-identifiable data. Commercialization is expected as subscription service not unlike current bioinformatics databases, granting investigators access to a large-scale, globally distributed data warehouse comprised of participating PACS-enabled medical centers. n/a",Access to distributed de-identified imaging data,6694270,R43EB000608,"['archives', ' clinical research', ' computer data analysis', ' computer system design /evaluation', ' data management', ' health care policy', ' health related legal', ' human data', ' imaging /visualization /scanning', ' information systems']",NIBIB,"HX TECHNOLOGIES, INC.",R43,2003,250800,-0.004079021053517123
"Visual Data Extraction and Conversion Programming Tool  DESCRIPTION (provided by applicant): In recent decades, biomedical researchers are facing a new challenge that grows exponentially. The challenge is how to handle the large volume of biological data automatically generated by various whole-cell study methods such as genomics, microarrays, and proteomics. These new methods provide enormous opportunities for rapid advances in biomedical research and medicine because they allow scientists to study living beings in a global scale with greater speed. However, analyzing the data generated by these new methods can be a daunting task and often requires the development of specialized data extraction and conversion computer programs. Because only a few scientists are well trained both in life sciences and computer science, there exists a bottleneck between the great research opportunities these volume data can provide us, and the actual advances scientists can achieve from using them.   In this project, we propose to develop an auto-programming tool for biomedical scientists to help them handle the large amount of data in their research. This tool will observe the visual extraction and conversion of sample data by users via a graphical user interlace, i.e., through the point, click and drag operations familiar to most computer users. After that, it will be able to automatically generate computer programs that can carry out the same data extraction and conversion tasks for its users, on any new data. That is to say, by seeing a few examples of a user's data extraction and conversion needs, this tool can automatically turn that into computer solutions. Using this tool will be easy and will not require any sophisticated computer science training because it does the programming job for its users automatically.   This tool can have the broadest applicability in all biomedical research areas where textual format data are generated and processed with computational technologies. Therefore, this tool will provide great enabling power to biomedical scientists to help them make rapid advances in biomedical research and medicine.   n/a",Visual Data Extraction and Conversion Programming Tool,6549345,R21GM066400,"['artificial intelligence', ' automated data processing', ' biomedical resource', ' computer data analysis', ' computer human interaction', ' computer program /software', ' computer system design /evaluation', ' data management', ' information retrieval']",NIGMS,IOWA STATE UNIVERSITY,R21,2002,99510,0.02210514456412942
"Deployment Framework for Medical Imaging Applications DESCRIPTION (provided by applicant): There are many reasons for the relatively slow proliferation of advanced medical image processing methods but a significant reason is the present paradigm for providing access: most applications are still tied to proprietary software and hardware environments that carry significant up-front costs. The ultimate intent of this work is leverage commodity computing technologies to develop an open, extensible framework for deploying medical image processing applications in the heterogeneous, networked computing environment of today. The framework will provide clinicians and researchers access to state-of-the-art image processing applications regardless of their particular computing platform or locally available computing resources connecting them with federated database resources, with high-end computing resources, or even with their colleagues in a peer-to-peer computing environment. The aims for Phase I of this project are: (1) Demonstrate that the framework provides access to image processing applications to an extent that is largely independent of local computing resources. (2) Demonstrate that the framework is general in that the same components can be reused for deploying a wide variety of medical imaging applications. (3) Demonstrate that the framework is customizable both by third-party developers and by end-users allowing power-users to both create and deploy new applications. Work in Phase II will extend the framework and develop two-demonstration applications--computer aided diagnosis (CAD) for mammography and multimodality image fusion. The ultimate goal is to obtain key partnerships and the private equity investment necessary for commercialization, which will proceed by launching revenue-generating versions of the CAD and image fusion applications. n/a",Deployment Framework for Medical Imaging Applications,6494576,R44EB000149,"['artificial intelligence', ' bioimaging /biomedical imaging', ' clinical research', ' computed axial tomography', ' computer assisted diagnosis', ' computer human interaction', ' computer network', ' computer program /software', ' computer system design /evaluation', ' human data', ' image processing', ' mammography', ' mathematics', ' positron emission tomography', ' telemedicine']",NIBIB,"FRONTIER MEDICAL, LLC",R44,2002,143577,-0.022264742886181892
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6533705,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2002,688297,-0.025760518494331322
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6489015,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2002,1329187,-0.02698850285660688
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6391275,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2001,215487,0.0004573306199887692
"Markov Chain Monte Carlo and Exact Logistic Regression   DESCRIPTION (provided by applicant): Logistic regression is a very popular           model for the analysis of binary data with widespread applicability in the           physical, behavioral and biomedical sciences. Parameter inference for this           model is usually based on maximizing the unconditional likelihood function.          However unconditional maximum likelihood inference can produce inconsistent          point estimates, inaccurate p-values and inaccurate confidence intervals for         small or unbalanced data sets and for data sets with a large number of               parameters relative to the number of observations. Sometimes the method fails        entirely as no estimates can be found that maximize the unconditional                likelihood function. A methodologically sound alternative approach that has          none of the aforementioned drawbacks is the exact conditional approach in which      one generates the permutation distributions of the sufficient statistics for         the parameters of interest conditional on fixing the sufficient statistics of        the remaining nuisance parameters at their observed values. The major stumbling      block to this approach is the heavy computational burden it imposes. Monte           Carlo methods attempt to overcome this problem by sampling from the reference        set of possible permutations instead of enumerating them all. Two competing          Monte Carlo methods are network based sampling and Markov Chain Monte Carlo          (MCMC) sampling. Network sampling suffers from memory limitations while MCMC         sampling can produce incorrect results if the Markov chain is not ergodic or if      the process is not in the steady state. We propose a novel approach which            combines the network and MCMC sampling, draws upon the strengths of each of          them and overcomes their individual limitations. We propose to implement this        hybrid network-MCMC method in our LogXact software and as an external procedure      in the SAS system.                                                                   PROPOSED COMMERCIAL APPLICATION:  There is great demand for logistic regression software that can handle small, sparse or  unbalanced data sets by exact methods.  Our LogXact package is the only software that  can provide exact inference for data sets which are not ""toy problems"".  Yet even  LogXact quickly breaks down on moderate sized problems.  The new generation of hybrid  network-MCMC algorithms will handle substantially larger problems that nevertheless need  exact inference.  The commercial potential is considerable since such data sets are common  in scientific studies.                                                                                      n/a",Markov Chain Monte Carlo and Exact Logistic Regression,6404971,R43CA093112,"['artificial intelligence', ' computer data analysis', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' mathematics', ' statistics /biometry']",NCI,CYTEL SOFTWARE CORPORATION,R43,2001,113111,0.0014307786412317218
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6392266,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2001,232139,-0.01643773302061975
"Integrated Neuroinformatics Resource for Alcoholism (IN* DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",Integrated Neuroinformatics Resource for Alcoholism (IN*,6449653,U01AA013524,"['alcoholic beverage consumption', ' alcoholism /alcohol abuse information system', ' biomedical facility', ' computer data analysis', ' computer program /software', ' computer system design /evaluation', ' cooperative study', ' data collection methodology /evaluation', ' electrophysiology', ' neuroanatomy', ' neurochemistry', ' neurophysiology', ' neuroregulation', ' neurosciences']",NIAAA,UNIVERSITY OF COLORADO DENVER,U01,2001,658874,-0.025760518494331322
"NEW DRUG TARGETS FOR APOPTOSIS DESCRIPTION (provided by applicant): The aim of this proposal is to establish an Integrated Neuroinformatics Resource on Alcoholism (INRA) as the informatics core component of the Integrative Neuroscience Initiative on Alcoholism Consortium (INIA). The overall goal of the INRA will be to create an integrated, multiresolution repository of neuroscience data, ranging from molecules to behavior for collaborative research on alcoholism. As the neuroinformatics core of the INIAC, the INRA will enable the integration of all data generated by all components of the INIAC. Furthermore, it will support synthesis of new knowledge through computational neurobiology tools for exploratory analysis including visualization, data mining and simulation. The INRA will represent a synthesis of emerging approaches in bioinformatics and existing methods of neuroinformatics to provide the INIAC a versatile toolbox of computational methods for elucidating the effects of alcohol on the nervous system. The specific aims of the INRA will be: (i) implementation of an informatics infrastructure for integrating complex neuroscience data, from molecules to behavior, generated by the consortium and relevant data available in the public domain; (ii) development of an integrated secure web-based environment so that consortium members can interactively visualize, search and update the integrated neuroscience knowledge; and (iii) development of data mining tools, including biomolecular sequence analysis, gene expression array analysis, characterization of Biochemical pathways, and natural language processing to support hypothesis generation and testing regarding ethanol Consumption and neuroadaptation to alcohol. We will also collaborate with related neuroscience projects to utilize existing resources for brain atlases, neuronal circuits and neuronal properties. The INRA will The made available to the INIAC through a Neb-based system through interactive graphical user interfaces that will seamlessly integrate tools for data entry, modification, search, retrieval and mining. The core of the INRA will be based on robust knowledge management methods and tools that will Effectively integrate disparate forms of neuroscience data and make it amenable to complex inferences. Our proposed strategy ensures that the informatics resource is: (i) flexible and scalable to address the evolving needs of the INIAC, and (ii) highly intuitive and user-friendly to ensure optimal utilization by the INIAC members. The proposed INRA is a novel system for Collaborative research in neuroscience and alcoholism which will be developed by an interdisciplinary team of experts in Bioinformatics, computational biology, neuroscience and alcoholism research. We believe the INRA will greatly enhance the Dace of discovery in the area of ethanol consumption and neuroadaptation to alcohol within the INIAC as well as the general research community. n/a",NEW DRUG TARGETS FOR APOPTOSIS,6230917,P01CA017094,"['antineoplastics', ' biomedical facility', ' chemosensitizing agent', ' drug design /synthesis /production', ' drug resistance', ' drug screening /evaluation', ' neoplasm /cancer chemotherapy']",NCI,UNIVERSITY OF ARIZONA,P01,2001,1291932,-0.02698850285660688
"DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS Our long-term goal is to assist biomedical scientists by extracting and          codifying new knowledge from large biomedical databases routinely by             computer. As large collections of data become more readily accessibly,           the opportunities for discovering new information increase. We propose           here to work toward this goal by extending our prior research on machine         learning in two important directions: (1) codification of disparate              pieces of knowledge into a coherent model (model building), and (2)              discovery of new information in medical databases (data mining).                                                                                                  Machine learning programs find classification rules (or decision trees           or networks) that separate members of a target class from other                  individuals. They have emphasized predictive accuracy, with some                 attention to tradeoffs between accuracy and cost of errors or between            accuracy and simplicity. We propose a framework in which these, and              other, tradeoffs are explicit and the criteria by which tradeoffs are            made are available for modification. We also include semantic                    considerations among the criteria to control the internal coherence of           models.                                                                                                                                                           ""Data mining"" is a recently-coined term for using computers to explore           large databases, with a goal of discovering new relationships but                usually with no specific target defined at the outset. In addition to            accuracy, simplicity, coherence, and cost, a program that purports to            discover new relationships must be able to assess novelty. We propose to         measure the extent to which proposed relationships are novel by                  comparing them against existing knowledge in the domain of discourse,            and to look for unusual rules (and other relations) that would be very           interesting if true.                                                                                                                                              The computer program we are primarily building on, RL, is a knowledge-           based learning program that learns classification rules from a                   collection of data. RL has been demonstrated to be flexible enough to            allow guidance from prior knowledge, and powerful enough to learn                publishable information for scientists working in several different              domains. Both parts of the research will requires extending the RL               system in new ways detailed in the research plan, which are consistent           with the overall design philosophy of the present system. We will                primarily work with data already collected on pneumonia patients with            with which we have considerable. We will test the generality of the              criteria used to evaluate models and discoveries with a Baynesian Net            learning. We will test the generality of the generality  of the criteria         used to evaluate models and discoveries with Bayesian Net learning               system, K2.                                                                       n/a",DATA MINING AND MODEL BUILDING IN MEDICAL INFORMATICS,6185231,R01LM006759,"['artificial intelligence', ' classification', ' computer assisted instruction', ' computer simulation', ' computer system design /evaluation', ' human data', ' informatics', ' information retrieval', ' model design /development', ' pneumonia']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,R01,2000,213046,0.0004573306199887692
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6181086,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,WASHINGTON UNIVERSITY,R01,2000,44870,-0.0019258718481706305
"COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES DESCRIPTION:  Computing with biochemical reactions is increasingly important     in studying genomes, assessing toxicity, and developing therapeutics.  There     are several important information sources, but their data are rudimentary        and often inaccurate.  Incorporation of biochemical information into             databases is extremely slow compared to that of sequence and structural          information, and will lag further as large-scale surveys of gene expression      and other reactions accelerate over the next few years.  Mechanisms for          review exist, but are manual, paper-dependent, and can be delayed for a year     or more.                                                                                                                                                          As curators and coordinators of biochemical information sources, the             applicants share a number of problems in the collection and review of            information.  Moreover, they are mutually dependent for the means to do so:      compound information is critical in checking reaction data, reaction             information is needed to spot errors in compound information, and the            automatic verification algorithms for either are closely related and need        both.  The applicants, therefore, propose to build a curatorial exchange for     the deposit and review of biochemical information by the scientific              community.  The applicants' goal is to demonstrate a system that will            encourage the mandating of deposit while ensuring that the information is of     the highest quality.                                                                                                                                              The role of the exchange is to receive deposits, check and classify their        biochemical information automatically, forward them to panels of human           reviewers for vetting, and publish the information by release to the             participating data sources--all over the World-Wide Web. It will track the       origin and status of deposits and reviews, serve computations for the            relevant pattern matching and simulation, and maintain an archival copy of       data.  The databases remain independent, and separately provide additional       information.  Algorithm development and testing depends on an adequate           information infrastructure, so the applicants will complete a basic data set     of compounds and reactions.  They will use this experience to develop a more     comprehensive domain model that better captures modern biochemistry, and         implement it for deposit and review.  Since the basic data and algorithms        will be valuable to the community at large, they plan to serve these to the      World-Wide Web. the exchange and its underlying data form the infrastructure     necessary for sustainable, cost-effective development of biochemical             informatics resources for biomedical research.                                    n/a",COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES,6495949,R01GM056529,"['artificial intelligence', ' biochemistry', ' chemical information system', ' chemical reaction', ' chemical structure', ' computer program /software', ' computer system design /evaluation', ' technology /technique development']",NIGMS,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2000,286664,-0.0019258718481706305
"PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS DESCRIPTION (Taken from application abstract):  Over the last decade             computational modeling has become central to neurobiology.  While much of        this work has focused on cellular and sub-cellular processes, the last few       years have seen increasing interest in systems level models and in               integrative accounts that span data from the subcellular to behavioral           levels.  Our proposal, in summary, is to extend existing work in parallel        discrete event simulation (PDES) and integrate it with existing work on          compartmental modeling environments, to produce a software environment which     has comprehensive support for modeling large scale, highly structured            networks of biophysically realistic cells; and which can efficiently exploit     the full range of parallel platforms, including the largest parallel             supercomputers, for simulation of these network models, which integrate          information about the nervous system from sub-cellular to the whole-brain        level.  Because of the scale of the models needed at this level of               integration, advanced parallel computing is required.  The critical              technical insight upon which this work rests is that neuronal modeling at        the systems level can often be reduced to a form of discrete event               simulation in which single cells are node functions and voltage spikes are       events.                                                                                                                                                           Three neuroscience modeling projects, will mold, test, and utilize these new     capabilities in investigations of system-level models of the nervous system      which integrate behavioral, anatomical and physiological data on a scale         that exceeds current simulation capabilities.  In collaboration with             computer scientists at Pittsburgh Supercomputing Center and UCLA,                neuroscientists at University of Virginia, the Born-Bunge Foundation,            Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS        packages, these tools will be developed and made available to the                neuroscience community.  The software development aims include 1)                investigation of a portable, PDES system capable of running efficiently on       diverse parallel platforms, 2) development of interfaces to the PDES for         NEURON and GENESIS allowing models developed in those packages to be scaled      up, 3) investigation of a network specification language for neuronal            models, and associated a visualization interface, to facilitate                  investigation of systems-level models, 4) sufficiently robust and                well-documented software for download and installation at other sites.  The      three neuroscience projects will guide development of the software tools and     use the tools for investigation of large-scale models of cerebellum,             hippocampus and thalamocortical circuits.                                         n/a",PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS,6186179,R01MH057358,"['artificial intelligence', ' bioimaging /biomedical imaging', ' biomedical automation', ' biotechnology', ' cerebellar cortex', ' computational neuroscience', ' computer network', ' computer program /software', ' computer simulation', ' computer system design /evaluation', ' hippocampus', ' mathematical model', ' neural information processing', ' neurotransmitters', ' parallel processing', ' supercomputer', ' thalamocortical tract', ' vocabulary development for information system']",NIMH,CARNEGIE-MELLON UNIVERSITY,R01,2000,234591,-0.01643773302061975
"Transfer learning to improve the re-usability of computable biomedical knowledge Candidate: With my multidisciplinary background in Artificial Intelligence (PhD), Public Health Informatics (MS), Epidemiology and Health Statistics (MS), and Preventive Medicine (Bachelor of Medicine), my career goal is to become an independent investigator working at the intersection of Artificial Intelligence and Biomedicine, with a particular emphasis initially in machine learning and public health. Training plan: My K99/R00 training plan emphasizes machine learning, deep learning and scientific communication skills (presentation, writing articles, and grant applications), which will complement my current strengths in artificial intelligence, statistics, medicine and public health. I have a very strong mentoring team. My mentors, Drs. Michael Becich (primary), Gregory Cooper, Heng Huang, and Michael Wagner, all of whom are experienced with research and professional career development. Research plan: The research goal of my proposed K99/R00 grant is to increase the re-use of computable biomedical knowledge, which is knowledge represented in computer-interpretable formalisms such as Bayesian networks and neural networks. I refer to such representations as models. Although models can be re-used in toto in another setting, there may be loss of performance or, even more problematically, fundamental mismatches between the data required by the model and the data available in the new setting making their re-use impossible. The field of transfer learning develops algorithms for transferring knowledge from one setting to another. Transfer learning, a sub-area of machine learning, explicitly distinguishes between a source setting, which has the model that we would like to re-use, and a target setting, which has data insufficient for deriving a model from data and therefore needs to re-use a model from a source setting. I propose to develop and evaluate several Bayesian Network Transfer Learning (BN- TL) algorithms and a Convolutional Neural Network Transfer Learning algorithm. My specific research aims are to: (1) further develop and evaluate BN-TL for sharing computable knowledge across healthcare settings; (2) develop and evaluate BN-TL for updating computable knowledge over time; and (3) develop and evaluate a deep transfer learning algorithm that combines knowledge in heterogeneous scenarios. I will do this research on models that are used to automatically detect cases of infectious disease such as influenza. Impact: The proposed research takes advantage of large datasets that I previously developed; therefore I expect to quickly have results with immediate implications for how case detection models are shared from a region that is initially experiencing an epidemic to another location that wishes to have optimal case-detection capability as early as possible. More generally, it will bring insight into machine learning enhanced biomedical knowledge sharing and updating. This training grant will prepare me to work independently and lead efforts to develop computational solutions to meet biomedical needs in future R01 projects. Transfer learning to improve the re-usability of computable biomedical knowledge Narrative Re-using computable biomedical knowledge in the form of a mathematical model in a new setting is challenging because the new setting may not have data needed as inputs to the model. This project will develop and evaluate transfer learning algorithms, which are computer programs that adapt a model to a new setting by removing and adding local variables to it. The developed methods for re-using models are expected to benefit the public’s health by: (1) improving case detection during epidemics by enabling re-use of automatic case detectors developed in the earliest affected regions with other regions, and, more generally, (2) increasing the impact of NIH’s investment in machine learning by enabling machine-learned models to be used in more institutions and locations.",Transfer learning to improve the re-usability of computable biomedical knowledge,9952803,K99LM013383,"['Affect', 'Algorithms', 'Applications Grants', 'Area', 'Artificial Intelligence', 'Bayesian Method', 'Bayesian Modeling', 'Bayesian Network', 'Big Data', 'Clinical', 'Communicable Diseases', 'Communication', 'Complement', 'Computerized Medical Record', 'Computers', 'Data', 'Detection', 'Development', 'Diagnosis', 'Disease', 'Doctor of Philosophy', 'Epidemic', 'Epidemiology', 'Future', 'Goals', 'Grant', 'Health', 'Healthcare Systems', 'Heterogeneity', 'Influenza', 'Institution', 'Investigation', 'Investments', 'Knowledge', 'Lead', 'Location', 'Lung diseases', 'Machine Learning', 'Medical center', 'Medicine', 'Mentors', 'Methods', 'Modeling', 'Natural Language Processing', 'Parainfluenza', 'Patients', 'Performance', 'Play', 'Preventive Medicine', 'Process', 'Psychological Transfer', 'Public Health', 'Public Health Informatics', 'Research', 'Research Personnel', 'Role', 'Semantics', 'Societies', 'Source', 'Testing', 'Time', 'Training', 'Twin Multiple Birth', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'Update', 'Utah', 'Work', 'Writing', 'base', 'career', 'career development', 'computer program', 'convolutional neural network', 'deep learning', 'deep neural network', 'detector', 'experience', 'health care settings', 'improved', 'insight', 'large datasets', 'learning algorithm', 'mathematical model', 'multidisciplinary', 'neural network', 'skills', 'statistics', 'usability']",NLM,UNIVERSITY OF PITTSBURGH AT PITTSBURGH,K99,2020,92359,0.002654478962035135
"Interpretable Deep Learning Algorithms for Pathology Image Analysis Interpretable Deep Learning Algorithms for Pathology Image Analysis Abstract The microscopic examination of stained tissue is a fundamental component of biomedical research and for the understanding of biological processes of disease which leads to improved diagnosis, prognosis and therapeutic response prediction. Ranging from cancer diagnosis to heart rejection and forensics the subjective interpretation of histopathology sections forms the basis of clinical decision making and research outcomes. However, it has been shown that such subjective interpretation of pathology slides suffers from large interobserver and intraobserver variability. Recent advances in computer vision and deep learning has enabled the objective and automated analysis of images. These methods have been applied with success to histology images which have demonstrated potential for development of objective image interpretation paradigms. However, significant algorithmic challenges remain to be addressed before such objective analysis of histology images can be used by clinicians and researchers. Leveraging extensive experience in developing and decimating research software based on deep learning the PI will pioneer novel algorithmic approaches to address these challenges including but not limited to: (1) training data-efficient and interpretable deep learning models with gigapixel size microscopy images for classification and segmentation using weakly supervised labels (2) fundamental redesign of data fusion paradigms for integrating information from microscopy images and molecular profiles (from multi-omics data) for improved diagnostic and prognostic determinations (3) developing visualization and interpretation software for researchers and clinical workflows to improve clinical and research validation and reproducability. The system will be designed in a modular, user-friendly manner and will be open-source, available through GitHub as universal plug-and-play modules ready to be adapted to various clinical and research applications. We will also develop a web resource with pretrained models for various organs, disease states and subtypes these will be accompanied with detailed manuals so researchers can apply deep learning to their specific research problems. Overall, the laboratory’s research will yield high impact discoveries from pathology image analysis, and its software will enable many other NIH funded laboratories to do the same, across various biomedical disciplines. Project Narrative The microscopic examination of stained tissue is a fundamental component of biomedical research, disease diagnosis, prognosis and therapeutic response prediction. However, the subjective interpretation of histology sections is subject to large interobserver and interobserver variability. This project focuses on developing artificial intelligence algorithms for the objective and automated analysis of whole histology slides leading to the development of an easy-to-use open source software package for biomedical researchers.",Interpretable Deep Learning Algorithms for Pathology Image Analysis,10029418,R35GM138216,"['Address', 'Algorithms', 'Artificial Intelligence', 'Biological Process', 'Biomedical Research', 'Classification', 'Clinical', 'Clinical Research', 'Computer Vision Systems', 'Computer software', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline', 'Disease', 'Forensic Medicine', 'Funding', 'Heart', 'Histology', 'Histopathology', 'Image', 'Image Analysis', 'Interobserver Variability', 'Intraobserver Variability', 'Label', 'Laboratories', 'Laboratory Research', 'Manuals', 'Methods', 'Microscopic', 'Modeling', 'Molecular Profiling', 'Multiomic Data', 'Organ', 'Outcomes Research', 'Pathology', 'Play', 'Research', 'Research Personnel', 'Slide', 'Supervision', 'System', 'Tissue Stains', 'Training', 'United States National Institutes of Health', 'Validation', 'Visualization', 'automated analysis', 'automated image analysis', 'base', 'cancer diagnosis', 'clinical decision-making', 'data fusion', 'decision research', 'deep learning', 'deep learning algorithm', 'design', 'disease diagnosis', 'experience', 'improved', 'intelligent algorithm', 'microscopic imaging', 'novel', 'online resource', 'open source', 'outcome forecast', 'pathology imaging', 'predicting response', 'prognostic', 'success', 'treatment response', 'user-friendly']",NIGMS,BRIGHAM AND WOMEN'S HOSPITAL,R35,2020,447500,-0.011015459327872837
"Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine PROJECT SUMMARY/ABSTRACT  The NIH and other agencies are funding high-throughput genomics (‘omics) experiments that deposit digital samples of data into the public domain at breakneck speeds. This high-quality data measures the ‘omics of diseases, drugs, cell lines, model organisms, etc. across the complete gamut of experimental factors and conditions. The importance of these digital samples of data is further illustrated in linked peer-reviewed publications that demonstrate its scientific value. However, meta-data for digital samples is recorded as free text without biocuration necessary for in-depth downstream scientific inquiry.  Deep learning is revolutionary machine intelligence paradigm that allows for an algorithm to program itself thereby removing the need to explicitly specify rules or logic. Whereas physicians / scientists once needed to first understand a problem to program computers to solve it, deep learning algorithms optimally tune themselves to solve problems. Given enough example data to train on, deep learning machine intelligence outperform humans on a variety of tasks. Today, deep learning is state-of-the-art performance for image classification, and, most importantly for this proposal, for natural language processing.  This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples. We will then develop and train deep learning algorithms for STARGEO digital curation based on learning the associated free text meta-data each digital sample. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.  Finally, we will demonstrate the biological utility to leverage CrADLe for digital curation with two large- scale and independent molecular datasets in: 1) The Cancer Genome Atlas (TCGA), and 2) The Accelerating Medicines Partnership-Alzheimer’s Disease (AMP-AD). We posit that CrADLe digital curation of open samples will augment these two distinct disease projects with a host big data to fuel the discovery of potential biomarker and gene targets. Therefore, successful funding and completion of this work may greatly reduce the burden of disease on patients by enhancing the efficiency and effectiveness of digital curation for biomedical big data. PROJECT NARRATIVE This proposal is about engineering Crowd Assisted Deep Learning (CrADLe) machine intelligence to rapidly scale the digital curation of public digital samples and directly translating this ‘omics data into useful biological inference. We will first use our NIH BD2K-funded Search Tag Analyze Resource for Gene Expression Omnibus (STARGEO.org) to crowd-source human annotation of open digital samples on which we will develop and train deep learning algorithms for STARGEO digital curation of free-text sample-level metadata. Given the ongoing deluge of biomedical data in the public domain, CrADLe may perhaps be the only way to scale the digital curation towards a precision medicine ideal.",Crowd-Assisted Deep Learning (CrADLe) Digital Curation to Translate Big Data into Precision Medicine,9979659,U01LM012675,"['Algorithms', 'Alzheimer&apos', 's Disease', 'Animal Model', 'Artificial Intelligence', 'Big Data', 'Big Data to Knowledge', 'Biological', 'Biological Assay', 'Categories', 'Cell Line', 'Cell model', 'Classification', 'Clinical', 'Collaborations', 'Communities', 'Controlled Vocabulary', 'Crowding', 'Data', 'Data Set', 'Defect', 'Deposition', 'Diagnosis', 'Disease', 'Disease model', 'Drug Modelings', 'E-learning', 'Effectiveness', 'Engineering', 'Funding', 'Funding Agency', 'Future', 'Gene Expression', 'Gene Targeting', 'Genomics', 'Human', 'Image', 'Intelligence', 'Label', 'Link', 'Logic', 'Machine Learning', 'Malignant Neoplasms', 'Maps', 'MeSH Thesaurus', 'Measures', 'Medicine', 'Meta-Analysis', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'National Research Council', 'Natural Language Processing', 'Ontology', 'Pathway interactions', 'Patients', 'Pattern', 'Peer Review', 'Performance', 'Pharmaceutical Preparations', 'Physicians', 'Problem Solving', 'PubMed', 'Public Domains', 'Publications', 'Resources', 'Sampling', 'Scientific Inquiry', 'Scientist', 'Source', 'Specific qualifier value', 'Speed', 'Text', 'The Cancer Genome Atlas', 'Training', 'Translating', 'United States National Institutes of Health', 'Validation', 'Work', 'base', 'big biomedical data', 'biomarker discovery', 'burden of illness', 'cell type', 'classical conditioning', 'computer program', 'crowdsourcing', 'deep learning', 'deep learning algorithm', 'digital', 'disease phenotype', 'experimental study', 'genomic data', 'human disease', 'improved', 'knockout gene', 'large scale data', 'novel therapeutics', 'open data', 'potential biomarker', 'precision medicine', 'programs', 'public repository', 'specific biomarkers']",NLM,UNIVERSITY OF CENTRAL FLORIDA,U01,2020,467177,0.0023838122583164055
"Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points Project Summary Biomedical data science data modeling is relevant to a plethora of informatics research activities, such as natural language processing, machine learning, artificial intelligence, and predictive analytics. As Electronic Health Record systems become more advanced and more mature, with the potential to incorporate a wide and diverse array of data from genomics to mobile health (mHealth) applications, the scope and nature of the biomedical data science questions researchers ask become broader. Concomitantly, the answers to their questions have the potential to impact the care of millions of patients—getting the answers right, proactively, is high stakes. However, in data modeling currently, there is no bioethics framework to guide the process of mapping key decision points and recording the rationale for choices made. Making data modeling decision points, as well as the reasoning behind them, explicit would have a twofold impact on improving biomedical data science by: 1. Enhancing transparency and reproducibility and maximizing the value of data science research and 2. Supporting the ability to assess decision points and rationales in terms of their most crucial ethical ramifications. Research in this area is particularly timely amid the interest in, and enthusiasm for, leveraging Big Data sources in the service of improving patient population health and the health of the general public. The National Institutes of Health (NIH) recently released a strategic plan for data science; there is no better time than now to create an initial bioethical framework to inform common data modeling decision points. The improvements in data quality that will derive from decision point mapping and bioethical review will enhance efforts to apply data models across a range of high-impact areas, from predictive analytics to support clinical decision-making to robust trending models in population health to better inform local, regional, and national health policies and resource allocation. To develop this initial bioethics framework, we will use well- established qualitative research methods (interviews, focus groups, and in-person deliberation) to map the decision points in biomedical data modeling research and document the rationales invoked to support those decisions (Aim 1 key informant interviews); assess those data science decision points and decision-making rationales for their bioethical ramifications (Aim 2 focus groups); and create an initial bioethics data modeling framework (Aim 3 deliberative meeting). This study would be the first to provide a bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe. This approach directly supports core scientific values of inclusivity, transparency, accountability, and reproducibility that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global. Project Narrative This study would be the first to develop an initial bioethics framework to meet a critical gap in biomedical data modeling activities, where the downstream consequences of developing data models without careful and comprehensive review of ethical issues can be severe—not least because poorly developed data models have the potential to impact adversely the health of individuals, groups, and communities. Currently, there is limited conversation around potential bioethics issues in data modeling, and as yet no implementable guidance on how biomedical data science modeling research activities should occur. The initial ethics framework developed by this study would provide a roadmap to ensure that data modeling decision points are documented and their ethical ramifications considered at the outset of model creation, thus supporting core scientific values of inclusivity, accountability, reproducibility, and transparency that, in turn, foster trust in biomedical data modeling output and potential applications, whether local, national, or global.",Creating an initial ethics framework for biomedical data modeling by mapping and exploring key decision points,10039527,R21HG011277,"['Accountability', 'Address', 'Area', 'Artificial Intelligence', 'Big Data', 'Bioethical Issues', 'Bioethics', 'Bioethics Consultants', 'Caring', 'Clinical', 'Communities', 'Data', 'Data Science', 'Data Scientist', 'Data Sources', 'Decision Making', 'Development', 'Electronic Health Record', 'Ensure', 'Ethical Issues', 'Ethical Review', 'Ethics', 'Focus Groups', 'Fostering', 'General Population', 'Health', 'Health Resources', 'Health system', 'Individual', 'Informatics', 'Interview', 'Machine Learning', 'Maps', 'Methods', 'Mobile Health Application', 'Modeling', 'National Health Policy', 'Natural Language Processing', 'Nature', 'Output', 'Patients', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Qualitative Research', 'Reproducibility', 'Research', 'Research Activity', 'Research Methodology', 'Research Personnel', 'Resource Allocation', 'Role', 'Services', 'Social Environment', 'Strategic Planning', 'Structure', 'System', 'Time', 'Trust', 'United States National Institutes of Health', 'Walking', 'base', 'biomedical data science', 'clinical decision support', 'clinical decision-making', 'data modeling', 'data quality', 'data tools', 'ethical legal social implication', 'genomic data', 'high standard', 'improved', 'individual patient', 'informant', 'interest', 'interoperability', 'meetings', 'model development', 'patient population', 'population health', 'programs', 'public trust', 'tool', 'trend', 'usability']",NHGRI,"HASTINGS CENTER, INC.",R21,2020,100000,0.02141631188238492
"Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement PROJECT SUMMARY/ABSTRACT Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing. Project Narrative Data sharing is essential to maximize the contributions of research subjects and the public’s investment in scientific research, but human subjects research also requires strong protection of the privacy and confidentiality of research subjects. This supplement will support an expert in neuroethics to undertake a rigorous ethical and regulatory analysis of data sharing policies, focusing in particular on the threats by artificial intelligence and machine learning techniques to reidentify neuroimaging datasets that have been thought to be deidentified. This research will lay the foundation for a sound data sharing policy for the OpenNeuro project and a regulatory framework to provide for the adequate protection of neuroimaging data while maximizing the benefits of data sharing",Neuroethical analysis of data sharing in the OpenNeuro project: Administrative supplement,10149058,R24MH117179,"['Address', 'Administrative Supplement', 'Archives', 'Artificial Intelligence', 'Award', 'BRAIN initiative', 'Benefits and Risks', 'Consent Forms', 'Country', 'Data', 'Data Analyses', 'Data Security', 'Data Set', 'Ensure', 'Ethics', 'Foundations', 'Funding', 'Future', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Subject Research', 'International', 'Investments', 'Laws', 'Legal', 'Light', 'Machine Learning', 'Magnetic Resonance Imaging', 'Neurosciences', 'Parents', 'Policies', 'Privacy', 'Process', 'Regulation', 'Research', 'Research Subjects', 'Risk', 'Security Measures', 'Series', 'Software Tools', 'Solid', 'Surveys', 'Techniques', 'United States', 'United States National Institutes of Health', 'data archive', 'data privacy', 'data sharing', 'design', 'human subject', 'human subject protection', 'machine learning algorithm', 'neuroethics', 'neuroimaging', 'novel', 'prevent', 'privacy protection', 'research study', 'sharing platform', 'sound', 'stem']",NIMH,STANFORD UNIVERSITY,R24,2020,126592,-0.014899216109521538
"Identifying individuals at risk of progression to active tuberculosis Project Summary Almost 2 billion people are infected with Mycobacterium tuberculosis (Mtb), the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no clinical test to distinguish those that will progress to active TB disease, from those that will not. If we are to realize the World Health Organization's (WHO) goal of a world free of TB by 2035, the massive reservoir of TB infection must be addressed with a cost-effective, ethical therapy for preventing progression, based on treating only those most likely to progress. A diagnostic test that can accurately predict the risk of progression is critical for treating these high-risk individuals and the eradication of TB. Our goal is to develop such an assay. Our central hypothesis is that five independent host immune biomarkers, combined into a single multimetric signature will predict progression from latent to active TB with at least 90% sensitivity and specificity. We will test this hypothesis and achieve our goal by implementing the following specific aims: Aim 1: Compile a comprehensive dataset of biomarkers in a prospective cohort of individuals who are at risk of progressing to active TB. Working with the Moldova Ministry of Health's National TB Program, we will enroll 3,685 close contacts of active TB cases. All participants will be followed for two years to determine who progresses to active TB. We expect to identify ≥ 140 progressors. We will assess three previously established blood-based predictors of active TB progression, and two novel assays. We will verify the performance of previously published biomarkers in this population to discriminate progressors from non-progressors and identify new candidate biomarkers using RNA-Seq of antigen stimulated PBMC and detection of Mtb-peptides by NanoDisk MS. Aim 2: Use a discovery set of samples to develop predictive models of progression to active TB. Using data from 140 progressors and 140 non-progressors from Aim 1 we will (1) Verify the performance of existing biomarkers, (2) Use a cross-validation to identify new candidate biomarkers, and (3) derive predictive models using logistic regression and machine learning methods to identify optimal biomarker signatures that best predict progression to active TB within 12 months. Aim 3: Verify the ability of the model to predict progression to active TB disease. Using the same approach as Aim 1, we will enroll a new set of 1,340 household contacts of active TB and identify at least 60 progressors and 60 matched non-progressors and verify clinically the sensitivity/specificity of our models and biosignatures (Aim 2) to predict progression to active disease. A combined host biomarker signature that can predict TB progression from a small blood volume will have significant impact on the WHO End TB Program. PROJECT NARRATIVE Almost 2 billion people are infected with Mycobacterium tuberculosis, the causative agent of tuberculosis (TB). Approximately 10% of these individuals will progress to active TB disease over their lifetimes, but there is currently no test to distinguish those that will progress from those that will not. We propose to develop a multimetric signature of host biomarkers that together will have a sensitivity and specificity of ≥ 90% for predicting progression to active TB in one year, a critical first step to developing cost-effective and ethical treatment plans in order to reach the World Health Organization goal of Ending TB by 2035.",Identifying individuals at risk of progression to active tuberculosis,9852419,R01AI137681,"['Address', 'Antigens', 'Biological Assay', 'Biological Markers', 'Blood', 'Blood Volume', 'Cells', 'Characteristics', 'Child', 'Clinical', 'Clinical Sensitivity', 'Data', 'Data Set', 'Detection', 'Diagnostic tests', 'Disease', 'Enrollment', 'Ethics', 'Event', 'Filtration', 'Flow Cytometry', 'Foundations', 'Freezing', 'Frequencies', 'Gender', 'Gene Expression', 'Genes', 'Genetic Transcription', 'Goals', 'Health', 'Household', 'Immune', 'Immune response', 'Immunologic Markers', 'Individual', 'Interferons', 'Logistic Regressions', 'Lymphocyte', 'Modeling', 'Moldova', 'Mycobacterium tuberculosis', 'Mycobacterium tuberculosis antigens', 'National Health Programs', 'Organizational Objectives', 'Outcomes Research', 'Participant', 'Patients', 'Peptide Fragments', 'Peptides', 'Performance', 'Peripheral Blood Mononuclear Cell', 'Plasma', 'Population', 'Procedures', 'Production', 'Prospective cohort', 'Proteins', 'Publications', 'Publishing', 'RNA', 'Research Personnel', 'Risk', 'Sampling', 'Sensitivity and Specificity', 'Specificity', 'T cell response', 'T-Lymphocyte', 'Testing', 'Tuberculosis', 'Validation', 'World Health Organization', 'age group', 'base', 'biobank', 'biomarker performance', 'biosignature', 'blood-based biomarker', 'candidate marker', 'classification algorithm', 'clinical Diagnosis', 'cohort', 'cost effective', 'deep neural network', 'enzyme linked immunospot assay', 'falls', 'follow-up', 'high risk', 'indexing', 'innovation', 'machine learning method', 'monocyte', 'nanodisk', 'novel', 'novel diagnostics', 'predictive modeling', 'predictive test', 'prevent', 'programs', 'progression marker', 'random forest', 'research clinical testing', 'support vector machine', 'transcriptome sequencing', 'transmission process', 'treatment planning']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R01,2020,711592,0.012964602675237098
"Integrative data science approaches for rare disease discovery in health records ABSTRACT: There are nearly 7,000 diseases that have a prevalence of only one in 2,000 individuals or less. Yet, such rare diseases are estimated to collectively affect over 300 million people worldwide, representing a significant healthcare concern. Although rare diseases have predominantly genetic origins, nearly half of them do not manifest symptoms until adulthood and frequently confound discovery and diagnosis. Even in the case of early onset disorders, the sheer number of possible diagnoses can often overwhelm clinicians. As a result, rare diseases are often diagnosed with delay, misdiagnosed or even remain undiagnosed, not only disrupting patient lives but also hindering progress on our understanding of such diseases. Data science methods that mine large-scale retrospective health record data for phenotypic information will aid in timely and accurate diagnoses of rare diseases, especially when combined with additional data types, thus, having significant real- world impact. This proposal will integrate electronic health record (EHR) data sets with publicly available vocabularies and ontologies, and genomic data for the improved identification and characterization of patients with rare diseases, using approaches from machine learning, natural language processing (NLP) and basic bioinformatics. The work has three specific aims and will be carried out in two phases. During the mentored phase, the principal investigator (PI) will develop data-driven methods to extract standardized concepts related to rare diseases from clinical notes and infer the occurrence of each disease (Aim 1). He will also develop data science approaches to compare and contrast longitudinal patterns associated with patients' journeys through the healthcare system when seeking a diagnosis for a rare disease, and aid in clinical decision-making by leveraging these patterns (Aim 2). During the independent phase (Aim 3), computational methods will be developed for the integrated modeling and analysis of genotypic (from Aim 3) and phenotypic information (from Aims 1 and 2). Cohorts to be sequenced will cover diseases for which causal genes or disease definitions are unclear (discovery), as well as those for which these are well known (validation). This work will be carried out under the mentorship of four faculty members with complementary expertise in biomedical informatics, data science, NLP, and rare disease genomics at the University of Washington, the largest medical system in the Pacific Northwest (four million EHRs), world-renowned researchers in medical genetics, and a robust data science environment. In addition, under the direction of the mentoring team, the PI will complete advanced coursework, receive training in translational bioinformatics and clinical research informatics, submit manuscripts, and seek an independent research position. This proposal will yield preliminary results for subsequent studies on data-driven phenotyping and enable the realization of the PI's career goals by providing him with the necessary training to build on his machine learning and basic bioinformatics expertise to transition into an independent investigator in biomedical data science. PROJECT NARRATIVE Rare genetic diseases are estimated to affect the lives of 25 to 30 million Americans and their families, and present a significant economic burden on the healthcare system. Currently, our knowledge of the broad spectrum of the 7,000 observed rare diseases is limited to a few well-studied ones, hindering our ability to make correct and timely diagnoses. The objective of this study is to improve the identification of patients with rare diseases in healthcare systems by developing data science approaches that automatically recognize rare disease-related patterns in patient health records and correlate them with genomic data, thus, aiding in diagnosis and discovery.",Integrative data science approaches for rare disease discovery in health records,9884791,K99LM012992,"['Adult', 'Affect', 'American', 'Award', 'Basic Science', 'Behavioral', 'Bioinformatics', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Clinical Research', 'Computing Methodologies', 'Consensus', 'Data', 'Data Science', 'Data Set', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostics Research', 'Disease', 'Economic Burden', 'Electronic Health Record', 'Environment', 'Faculty', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Goals', 'Healthcare', 'Healthcare Systems', 'Individual', 'Informatics', 'Knowledge', 'Machine Learning', 'Manuscripts', 'Markov Chains', 'Medical', 'Medical Genetics', 'Mental disorders', 'Mentors', 'Mentorship', 'Methods', 'Mining', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Pacific Northwest', 'Patient Recruitments', 'Patients', 'Pattern', 'Phase', 'Phenotype', 'Population', 'Positioning Attribute', 'Prevalence', 'Principal Investigator', 'Rare Diseases', 'Recording of previous events', 'Research', 'Research Personnel', 'Standardization', 'Symptoms', 'System', 'Testing', 'Time', 'Training', 'Universities', 'Validation', 'Vocabulary', 'Washington', 'Work', 'accurate diagnosis', 'base', 'biomedical data science', 'biomedical informatics', 'career', 'causal variant', 'clinical data warehouse', 'clinical decision-making', 'cohort', 'diagnostic accuracy', 'disease phenotype', 'early onset disorder', 'exome sequencing', 'gene discovery', 'genomic data', 'health care delivery', 'health data', 'health record', 'improved', 'member', 'multimodal data', 'novel', 'open source', 'patient health information', 'phenotypic data', 'prototype', 'psychologic', 'rare condition', 'rare genetic disorder', 'recruit', 'skills', 'software development', 'support tools', 'tool', 'trait']",NLM,UNIVERSITY OF WASHINGTON,K99,2020,92070,-0.02353007994929919
"Advanced End-to-End Relation Extraction with Deep Neural Networks ABSTRACT Relations linking various biomedical entities constitute a crucial resource that enables biomedical data science applications and knowledge discovery. Relational information spans the translational science spectrum going from biology (e.g., protein–protein interactions) to translational bioinformatics (e.g., gene–disease associations), and eventually to clinical care (e.g., drug–drug interactions). Scientists report newly discovered relations in nat- ural language through peer-reviewed literature and physicians may communicate them in clinical notes. More recently, patients are also reporting side-effects and adverse events on social media. With exponential growth in textual data, advances in biomedical natural language processing (BioNLP) methods are gaining prominence for biomedical relation extraction (BRE) from text. Most current efforts in BRE follow a pipeline approach containing named entity recognition (NER), entity normalization (EN), and relation classiﬁcation (RC) as subtasks. They typically suffer from error snowballing — errors in a component of the pipeline leading to more downstream errors — resulting in lower performance of the overall BRE system. This situation has lead to evaluation of different BRE substaks conducted in isolation. In this proposal we make a strong case for strictly end-to-end evaluations where relations are to be produced from raw text. We propose novel deep neural network architectures that model BRE in an end-to-end fashion and directly identify relations and corresponding entity spans in a single pass. We also extend our architectures to n-ary and cross-sentence settings where more than two entities may need to be linked even as the relation is expressed across multiple sentences. We also propose to create two new gold standard BRE datasets, one for drug–disease treatment relations and another ﬁrst of a kind dataset for combination drug therapies. Our main hypothesis is that our end-to-end extraction models will yield supe- rior performance when compared with traditional pipelines. We test this through (1). intrinsic evaluations based on standard performance measures with several gold standard datasets and (2). extrinsic application oriented assessments of relations extracted with use-cases in information retrieval, question answering, and knowledge base completion. All software and data developed as part of this project will be made available for public use and we hope this will foster rigorous end-to-end benchmarking of BRE systems. NARRATIVE Relations connecting biomedical entities are at the heart of biomedical research given they encapsulate mech- anisms of disease etiology, progression, and treatment. As most such relations are ﬁrst disclosed in textual narratives (scientiﬁc literature or clinical notes), methods to extract and represent them in a structured format are essential to facilitate applications such as hypotheses generation, question answering, and information retrieval. The high level objective of this project is to develop and evaluate novel end-to-end supervised machine learning methods for biomedical relation extraction using latest advances in deep neural networks.",Advanced End-to-End Relation Extraction with Deep Neural Networks,10052028,R01LM013240,"['Adverse event', 'Architecture', 'Area', 'Benchmarking', 'Bioinformatics', 'Biology', 'Biomedical Research', 'Classification', 'Clinical', 'Code', 'Collaborations', 'Combination Drug Therapy', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Set', 'Dependence', 'Disease', 'Distant', 'Drug Interactions', 'Encapsulated', 'Etiology', 'Evaluation', 'Fostering', 'Funding', 'Future', 'Generations', 'Genes', 'Gold', 'Growth', 'Hand', 'Heart', 'Information Retrieval', 'Information Sciences', 'Intramural Research', 'Joints', 'Knowledge Discovery', 'Label', 'Language', 'Lead', 'Link', 'Literature', 'Manuals', 'Maps', 'Measures', 'Methodology', 'Methods', 'Modeling', 'Molecular', 'Names', 'Natural Language Processing', 'Patients', 'Peer Review', 'Performance', 'Periodicity', 'Pharmaceutical Preparations', 'Physicians', 'Process', 'Psychological Transfer', 'Reporting', 'Research', 'Research Personnel', 'Resources', 'Review Literature', 'Scientist', 'Semantics', 'Software Tools', 'Source', 'Standardization', 'Structure', 'Supervision', 'System', 'Terminology', 'Testing', 'Text', 'Training', 'Translational Research', 'Trees', 'base', 'biomedical data science', 'clinical care', 'deep neural network', 'improved', 'insight', 'interest', 'knowledge base', 'machine learning method', 'natural language', 'neural network', 'neural network architecture', 'new therapeutic target', 'novel', 'off-label use', 'protein protein interaction', 'relating to nervous system', 'side effect', 'social media', 'supervised learning', 'syntax']",NLM,UNIVERSITY OF KENTUCKY,R01,2020,358691,-0.006369019914080364
"xARA: ARA through Explainable AI In response to the NIH FOA OTA-19009 “Biomedical Translator: Development” we propose to build an Autonomous Relay Agent (ARA) that can characterize and rate the quality of information returned from multiple multiscale heterogeneous knowledge providers (KPs). Biomedical researchers develop a trust relationship with a knowledge provider (KP) through frequent and continued use. Over time a familiarity develops that drives their understanding and insight on 1) how to structure and invoke more effective queries, 2) the quality of the results they may expect in response to different query parameters and feature values, and 3) how to assess the relevancy of a specific query’s results. Although this information retrieval paradigm has served the research community moderately well in the past it is not scalable and the number, scope and complexity of KPs is increasing at a dramatic pace (1,613 molecular biology databases reported as of Jan. 2019). Within this ever changing information landscape, a biomedical researcher now has two choices -- either continue using the few KPs they have learned to trust but remain limited in the actionable information they will receive, or invest the time and accept the risk of using a range of new information resources with little or no familiarity and thus uncertain effectiveness. If researchers are to benefit from the vast array of NIH and industry sponsored information assets now available and expanding new information retrieval and quality assessment technologies will be required. We propose to build an Explanatory Autonomous Relay Agent (xARA) that can characterize query results by rating the quality of information returned from multi-scale heterogeneous KPs. The xARA will utilize multiple information retrieval and explainable Artificial Intelligence (xAI) strategies to perform queries across multiple heterogeneous KPs and rank their results by quality and relevancy while also identifying and explaining any inconsistencies among databases for the same query response. To deliver on this promise, we will utilize case-based reasoning and language models trained with biomedical data (i.e., BioBERT and custom annotation embeddings through Reactome and UniProt) permitting a new level of query profiling and assessment. Our strategies will permit 1) information gaps to be filled by testing alternative query patterns that produce different surface syntax yet possess semantically related and actionable concepts, 2) inconsistencies to be identified for a given query feature value, and 3) the identification and elimination or merging of semantically redundant query results via similarity metrics enriched by case-based reasoning strategies employed in the explainable AI (xAI) community to identify machine learning model behavior and performance. The xARA capabilities proposed herein will be based on strategies developed in Dr. Weber’s lab for information retrieval where the desire for greater transparency when reasoning over experimental data is our primary aim. Our multi-institutional team is comprised of senior researchers and software engineers formally trained and experienced in the computer and data sciences, cheminformatics, bioinformatics, molecular biology, and biochemistry. Inherent risks in querying heterogeneous KPs include the presence of inconsistent labeling of the same biomedical concept within unique KP data structures. Manual engineering may be necessary to overcome such hurdles, but will not be a significant challenge for the initial prototype, since only two well documented KPs are being evaluated. Another noteworthy risk is that the quality of word embeddings generated from UniProt and Reactome may not be sufficient, requiring further textual analysis of biomedical text like PubMed, which is feasible within the timeframe of our project plan. n/a",xARA: ARA through Explainable AI,10057158,OT2TR003448,"['Artificial Intelligence', 'Behavior', 'Biochemistry', 'Bioinformatics', 'Communities', 'Custom', 'Data', 'Data Science', 'Databases', 'Development', 'Effectiveness', 'Engineering', 'Familiarity', 'Industry', 'Information Resources', 'Information Retrieval', 'Knowledge', 'Label', 'Language', 'Machine Learning', 'Manuals', 'Modeling', 'Molecular Biology', 'Pattern', 'Performance', 'Provider', 'PubMed', 'Reporting', 'Research', 'Research Personnel', 'Risk', 'Semantics', 'Software Engineering', 'Structure', 'Surface', 'Technology Assessment', 'Testing', 'Text', 'Time', 'Training', 'Trust', 'United States National Institutes of Health', 'base', 'case-based', 'cheminformatics', 'computer science', 'experience', 'insight', 'prototype', 'response', 'structured data', 'syntax']",NCATS,TUFTS MEDICAL CENTER,OT2,2020,795873,0.009030014415724518
"Knowledge-Based Biomedical Data Science Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. Building on decades of work in biomedical ontology development, and exploiting the architectures supporting the Semantic Web, we have demonstrated methods that allow effective querying spanning any combination of data sources in purely biological terms, without the queries having to reflect anything about the structure or distribution of information among any of the sources. These methods are also capable of representing apparently conflicting information in a logically consistent manner, and tracking the provenance of all assertions in the knowledge-base. Perhaps the most important feature of these methods is that they scale to potentially include nearly all knowledge of molecular biology.  We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data. To test this hypothesis, we propose to address the following specific aims:  1. Identify representative and significant analytical needs in knowledge-based data science, and  refine and extend our knowledge-base to address those needs in three distinct domains: clinical  pharmacology, cardiovascular disease and rare genetic disease.  2. Develop novel and implement existing symbolic, statistical, network-based, machine learning  and hybrid approaches to goal-driven inference from very large knowledge-bases. Create a goal-  directed framework for selecting and combining these inference methods to address particular  analytical problems.  3. Overcome barriers to broad external adoption of developed methods by analyzing their  computational complexity, optimizing performance of knowledge-based querying and inference,  developing simplified, biology-focused query languages, lightweight packaging of knowledge  resources and systems, and addressing issues of licensing and data redistribution. Knowledge-based biomedical data science  In the previous funding period, we designed and constructed breakthrough methods for creating a semantically coherent and logically consistent knowledge-base by automatically transforming and integrating many biomedical databases, and by directly extracting information from the literature. We now hypothesize that using these technologies we can build knowledge-bases with broad enough coverage to overcome the “brittleness” problems that stymied previous approaches to symbolic artificial intelligence, and then create novel computational methods which leverage that knowledge to provide critical new tools for the interpretation and analysis of biomedical data.",Knowledge-Based Biomedical Data Science,9955351,R01LM008111,"['Address', 'Adoption', 'Architecture', 'Area', 'Artificial Intelligence', 'Biological', 'Biology', 'Biomedical Research', 'Cardiovascular Diseases', 'Clinical Data', 'Clinical Pharmacology', 'Collaborations', 'Communities', 'Computing Methodologies', 'Conflict (Psychology)', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Databases', 'Duchenne muscular dystrophy', 'Fruit', 'Funding', 'Genomics', 'Goals', 'Heart failure', 'Hybrids', 'Information Distribution', 'Information Resources', 'Knowledge', 'Language', 'Licensing', 'Literature', 'Machine Learning', 'Methods', 'Molecular', 'Molecular Biology', 'Network-based', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Proteins', 'Proteomics', 'Publishing', 'Role', 'Semantics', 'Serum', 'Source', 'Structure', 'System', 'Techniques', 'Technology', 'Testing', 'Work', 'biomedical data science', 'biomedical ontology', 'cohort', 'computer based Semantic Analysis', 'design and construction', 'health data', 'innovation', 'knowledge base', 'large scale data', 'light weight', 'novel', 'novel diagnostics', 'novel therapeutic intervention', 'online resource', 'ontology development', 'rare genetic disorder', 'tool', 'transcriptomics']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,517554,0.01596308472639645
"Abiotic-Biotic Interfaces for Ophthalmology Symposium ABSTRACT This proposal seeks funding to support a symposium, Abiotic-Biotic Interfaces for Ophthalmology (ABI), which will bring together recognized world experts in clinical, research, vision science, engineering, industrial and pharmaceutical communities as well as junior investigators (i.e., young faculty and those in training) to discuss the current state of ABI, ranging from bioelectronic implantable and wearable devices, to nanoscale scaffolds for stem cell and gene therapies. Given the multidisciplinary nature of this field, it is essential to bring together researchers and clinicians with varying levels of expertise across many domains related to ABI to advance the progress of this novel field, identify challenges of advancement, and develop a strategic action plan to overcome these challenges. The timing to have such a symposium to further the application of implantable and/or wearable bioengineered systems in ophthalmology is now as we focus on precision and personalized medicine and leverage the revolution in deep learning artificial intelligence algorithms. Through symposium talks, sessions, and discussions we will cover the fundamentals and also identify innovative and cutting-edge strategies and methodologies to accelerate the rate of major discoveries and development of novel therapeutics. The specific aims of this symposium are: Specific Aim 1. To bring together both established and junior investigators representing a broad range of disciplines to discuss cutting edge research in this novel field, catalyze the development of cross-disciplinary and translational approaches to advance abiotic-biotic interfaces for ophthalmology, and identify gaps in knowledge and barriers to advancement. We will identify research questions and develop an agenda to guide future research that is consistent with the objectives and interests of NEI. Specific Aim 2. Develop a junior investigator program to motivate a diverse group of students and junior investigators to pursue research careers in vision science and ophthalmologic therapeutic development, who will ultimately submit grant proposals to NEI solicitations and contribute to the scientific literature. Specific Aim 3. Develop a strategic action plan to set priorities for future studies that will encourage inter-agency collaborations (e.g., NEI, NSF, DARPA, etc.). This is critical because often certain engineering tasks are best suited to be supported by NSF or DARPA whereas the biological testing of the engineered systems lends itself to funding from NEI. Hence such inter-agency or cross-agency efforts can help leverage the funding to develop sophisticated abiotic-biotic systems NARRATIVE This meeting is the first on this topic dedicated to the broad use of implantable and/or wearable bioelectronics for ophthalmological applications. It is anticipated that the strategic action plan will significantly impact the field by greatly accelerating the translation of basic science and engineering research findings to stimulate the development of novel treatments and improve clinical practice. Key topics include visual restoration, drug and gene delivery, and sensing intraocular pressure. This meeting will foster training and development of future leaders in this emerging field and promote collaboration and exchange of knowledge and ideas among junior and established investigators.",Abiotic-Biotic Interfaces for Ophthalmology Symposium,10070800,R13EY031988,"['Algorithms', 'Applications Grants', 'Artificial Intelligence', 'Basic Science', 'Biological Testing', 'Biomedical Engineering', 'Cellular Phone', 'Clinical Research', 'Collaborations', 'Communities', 'Computer software', 'Contact Lenses', 'Custom', 'Data', 'Development', 'Devices', 'Diagnosis', 'Discipline', 'Disease', 'Drug Delivery Systems', 'Electronics', 'Engineering', 'Eye', 'Faculty', 'Fostering', 'Funding', 'Future', 'Gene Delivery', 'Glass', 'Industrialization', 'Intraocular lens implant device', 'Knowledge', 'Literature', 'Medicine', 'Methodology', 'Nature', 'Neural Retina', 'Ophthalmology', 'Optics', 'Pharmacologic Substance', 'Physiologic Intraocular Pressure', 'Physiological', 'Research', 'Research Personnel', 'Route', 'Scientific Inquiry', 'Scientist', 'Senior Scientist', 'Students', 'System', 'Time', 'Training', 'Translations', 'Virtual and Augmented reality', 'Visual', 'base', 'career', 'clinical practice', 'deep learning', 'gene therapy', 'implantable device', 'improved', 'innovation', 'intelligent algorithm', 'interest', 'meetings', 'multidisciplinary', 'nanoscale', 'neural network', 'novel', 'novel therapeutics', 'personalized medicine', 'portability', 'precision medicine', 'programs', 'restoration', 'scaffold', 'stem cell therapy', 'symposium', 'therapeutic development', 'translational approach', 'vision science', 'wearable device']",NEI,UNIVERSITY OF SOUTHERN CALIFORNIA,R13,2020,42465,-0.024821224472701808
"COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data Project Summary/Abstract  The brain imaging community is greatly benefiting from extensive data sharing efforts currently underway. However, there is still a major gap in that much data is still not openly shareable, which we propose to address. In addition, current approaches to data sharing often include significant logistical hurdles both for the investigator sharing the data (e.g. often times multiple data sharing agreements and approvals are required from US and international institutions) as well as for the individual requesting the data (e.g. substantial computational re- sources and time is needed to pool data from large studies with local study data). This needs to change, so that the scientific community can create a venue where data can be collected, managed, widely shared and analyzed while also opening up access to the (many) data sets which are not currently available (see overview on this from our group7). The large amount of existing data requires an approach that can analyze data in a distributed way while (if required) leaving control of the source data with the individual investigator or the data host; this motivates a dynamic, decentralized way of approaching large scale analyses. During the previous funding period, we developed a peer-to-peer system called the Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation (COINSTAC). Our system provides an independent, open, no-strings-attached tool that performs analysis on datasets distributed across different locations. Thus, the step of actually aggregating data is avoided, while the strength of large-scale analyses can be retained. During this new phase we respond to the need for advanced algorithms such as linear mixed effects models and deep learning, by proposing to develop decentralized models for these approaches and also implement a fully scalable cloud-based framework with enhanced security features. To achieve this, in Aim 1, we will incorporate the necessary functionality to scale up analyses via the ability to work with either local or commercial private cloud environments, together with advanced visualization, quality control, and privacy and security features. This suite of new functions will open the floodgates for the use of COINSTAC by the larger neuroscience community to enable new discovery and analysis of unprecedented amounts of brain imaging data located throughout the world. We will also improve usability, training materials, engage the community in contributing to the open source code base, and ultimately facilitate the use of COINSTAC's tools for additional science and discovery in a broad range of applications. In Aim 2 we will extend the framework to handle powerful algorithms such as linear mixed effects models and deep learning, and to perform meta-learning for leveraging and updating fit models. And finally, in Aim 3, we will test this new functionality through a partnership with the worldwide ENIGMA addiction group, which is currently not able to perform advanced machine learning analyses on data that cannot be centrally located. We will evaluate the impact of 6 main classes of substances of abuse (e.g. methamphetamines, cocaine, cannabis, nicotine, opiates, alcohol and their combinations) using the new developed functionality. 3 Project Narrative  Hundreds of millions of dollars have been spent on collecting human neuroimaging data for clinical and re- search studies, many of which do not come with subject consent for sharing or contain sensitive data which are not easily shared, such as genetics. Open sharing of raw data, though desirable from the research perspective, and growing rapidly, is not a viable solution for a large number of datasets which have additional privacy risks or IRB concerns. The COINSTAC solution we propose enables us to capture this `missing data' and achieve the same performance as pooling of both open and `closed' repositories by developing privacy preserving versions of advanced and cutting edge algorithms (including linear mixed effects models and deep learning) and incorpo- rating within an easy-to-use and scalable platform which enables distributed computation. 2","COINSTAC 2.0: decentralized, scalable analysis of loosely coupled data",10058463,R01DA040487,"['Address', 'Adoption', 'Agreement', 'Alcohol or Other Drugs use', 'Alcohols', 'Algorithms', 'Atlases', 'Awareness', 'Brain', 'Brain imaging', 'Cannabis', 'Clinical Data', 'Cocaine', 'Communities', 'Consent', 'Consent Forms', 'Coupled', 'Data', 'Data Aggregation', 'Data Pooling', 'Data Set', 'Decentralization', 'Development', 'Environment', 'Family', 'Funding', 'Genetic', 'Genomics', 'Human', 'Individual', 'Informatics', 'Institution', 'Institutional Review Boards', 'International', 'Knowledge', 'Language', 'Learning', 'Legal', 'Link', 'Location', 'Logistics', 'Machine Learning', 'Measures', 'Methamphetamine', 'Modeling', 'Movement', 'Neurosciences', 'Nicotine', 'Opioid', 'Performance', 'Phase', 'Population', 'Positioning Attribute', 'Privacy', 'Privatization', 'Process', 'Public Health', 'Quality Control', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Running', 'Science', 'Security', 'Series', 'Site', 'Source', 'Source Code', 'Statistical Bias', 'Structure', 'Substance of Abuse', 'System', 'Testing', 'Time', 'Training', 'United States National Institutes of Health', 'Update', 'Visualization', 'Work', 'addiction', 'base', 'cloud based', 'computational platform', 'computerized data processing', 'computerized tools', 'data harmonization', 'data reuse', 'data sharing', 'data visualization', 'data warehouse', 'deep learning', 'distributed data', 'improved', 'large datasets', 'learning algorithm', 'life-long learning', 'negative affect', 'neuroimaging', 'novel', 'novel strategies', 'open data', 'open source', 'peer', 'privacy preservation', 'repository', 'scale up', 'structural genomics', 'success', 'supervised learning', 'tool', 'unsupervised learning', 'usability', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,627034,0.01805515187879494
"ORS-ISFR 17th Biennial Conference: Thinking big on fracture repair Abstract Bone fractures occur in more than 25 million Americans per year, and a significant percentage of those fractures fail to heal. Fractures that show delayed or failed progression to heal account for the majority of patient disability and costs. Thus, there is an urgent need to better understand the biology of bone healing, and to use that knowledge to develop new diagnostics and therapeutics. We are seeking support for the 17th Biennial conference of the International Section of Fracture Repair (ISFR). The ISFR recently became incorporated as a section within the Orthopaedic Research Society (ORS) thereby leveraging the outreach of the largest scientific organization in the world dedicated to orthopaedic research. The ISFR is dedicated to the advancement and exchange of the most current scientific ideas and research findings on fracture repair and its application to the improvement of patient care. Our mission is be the premier forum to integrate and present cutting-edge ideas related to clinical, translational, and basic science research of fracture healing. The central theme for this conference is “Thinking Big”. We will present the use of -omics approaches to solve the most significant research problems in the field and large-scale data management and advanced computational approaches to solve intractable clinical concerns. The meeting is organized around seven scientific sessions, and also includes an embedded symposium conducted with the Orthopaedic Trauma Association (OTA) focused on non-unions and a collaborative session on proximal humeral fractures conducted with the Bone and Joint Institute of the University of Western Ontario (BJI). The scientific sessions will each have a series of speakers, selected from submitted abstracts, and each will have an eminent keynote invited speaker. The sessions include: Stem Cells in Fracture; Artificial Intelligence, Machine Learning and Big data; Fracture induced pain management; Fracture repair basic research; Fracture repair clinical perspectives; Bone repair with polytrauma; and Outcomes. There will also be a short session for poster oral presentations (poster teasers), a session on how to communicate science, a practicum on data management, and an ORS Presidential Guest Speaker. Following the ISFR 17th biennial conference there will be an associated ISFR/BJI consensus workshop, which will develop a consensus white-paper on the best evidence-based treatment for proximal humeral fractures. The meeting will present the most up to date research on fracture healing basic biology, translational research and prospective clinical studies. The Program and Scientific Committees will be highly focused on fostering an inclusive environment. Surgeons, biologists, engineers, and policy makers will attend the meeting, and be drawn from academia, government, and industry. A variety of activities will be focused on career development and networking for trainees in the bone healing field. Narrative The 17th Biennial Conference of the ORS/ISFR titled, “Thinking big on fracture repair” will be held in December of 2020. The goal of the event is to foster growth and innovation in the field of fracture-induced pain management, artificial learning/machine learning/smart technology, polytrauma and stem cells, and outcomes research. Scientists, industry partners, researchers, policy makers, post-doctoral fellows and graduate students will attend the meeting to discuss the most pressing questions in the field of fracture repair, and to consider bold approaches to solving those problems.",ORS-ISFR 17th Biennial Conference: Thinking big on fracture repair,10066004,R13AR077963,"['Academia', 'American', 'Area', 'Artificial Intelligence', 'Basic Science', 'Big Data', 'Biological Process', 'Biology', 'Bone Injury', 'Bone Pain', 'Bone Regeneration', 'Canada', 'Career Mobility', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Consensus', 'Consensus Workshop', 'Development', 'Discipline', 'Engineering', 'Ensure', 'Environment', 'Event', 'Evidence based treatment', 'Exposure to', 'Fostering', 'Fracture', 'Fracture Healing', 'Germany', 'Goals', 'Government', 'Growth', 'Industry', 'Institutes', 'International', 'Japan', 'Joints', 'Knowledge', 'Laboratories', 'Logistics', 'Machine Learning', 'Mission', 'Musculoskeletal', 'Ontario', 'Oral', 'Orthopedics', 'Outcome', 'Outcome Study', 'Outcomes Research', 'Pain management', 'Paper', 'Participant', 'Patient Care', 'Patients', 'Phenotype', 'Policy Maker', 'Postdoctoral Fellow', 'Recording of previous events', 'Registries', 'Regulation', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Series', 'Shoulder Fractures', 'Societies', 'Surgeon', 'Technology', 'Thinking', 'Translational Research', 'Trauma', 'Universities', 'Work', 'base', 'bone', 'bone healing', 'career development', 'career networking', 'cost', 'data management', 'disability', 'graduate student', 'healing', 'humerus', 'industry partner', 'innovation', 'interest', 'large scale data', 'meetings', 'novel diagnostics', 'novel therapeutics', 'outreach', 'population based', 'posters', 'programs', 'prospective', 'repaired', 'scientific organization', 'stem cells', 'success', 'symposium', 'translational physician']",NIAMS,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R13,2020,5000,-0.03746436711428764
"SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community Physics-based simulations provide a powerful framework for understanding biological form and function. They harmonize heterogeneous experimental data with real-world physical constraints, helping researchers understand biological systems as they engineer novel drugs, new diagnostics, medical devices, and surgical interventions. The rise in new sensors and simulation tools is generating an increasing amount of data, but this data is often inaccessible, preventing reuse and limiting scientific progress. In 2005, we launched SimTK, a website to develop and share biosimulation tools, models, and data, to address these issues. SimTK now supports 62,000+ researchers globally and 950+ projects. Members use it to meet their grants’ data sharing responsibilities; experiment with new ways of collaborating; and build communities around their datasets and tools. However, challenges remain: many researchers still do not share their digital assets due to the time needed to prepare, document, and maintain those assets, and since SimTK hosts a growing number of diverse digital assets, the site now also faces the challenge of making these assets discoverable and reusable. Thus, we propose a plan to extend SimTK and implement new solutions to promote scientific data sharing and reuse. First, we will maintain the reliable, user-friendly foundation upon which SimTK is built, continuing to provide the excellent support our members expect and supporting the site’s existing features for sharing and building communities. Second, we will implement methods to establish a culture of model and data sharing in the biomechanics community. We will encourage researchers to adopt new habits, making sharing part of their workflow, by enabling the software and systems they use to automatically upload models and data to SimTK via an application programming interface (API) and by recruiting leading researchers in the community to serve as beta testers and role models. Third, we will create tools to easily replicate and extend biomechanics simulations. Containers and cloud computing services allow researchers to capture and share a snapshot of their computing environment, enabling unprecedented fidelity in sharing. We will integrate these technologies into SimTK and provide custom, easy-to-use interfaces to replicate and extend simulation studies. Lastly, we will develop a metadata standard for models and data for the biomechanics community, increasing reusability and discoverability of the rich set of resources shared on SimTK. We will use the new standard on SimTK and fill in the metadata fields automatically using natural language processing and machine learning, minimizing the burden and inaccuracies of manual metadata entry. We will evaluate our success in achieving these aims by tracking the number of assets shared and the frequency they are used as a springboard to new research. These changes will accelerate biomechanics research and provide new tools to increase the reusability and impact of shared resources. By lowering barriers to data sharing in the biosimulation community, SimTK will continue to serve as a model for how to create national infrastructure for scientific subdisciplines. SimTK is a vibrant hub for the development and sharing of simulation software, data, and models of biological structures and processes. SimTK-based resources are being used to design medical devices and drugs, to generate new diagnostics, to create surgical interventions, and to provide insights into biology. The proposed enhancements to SimTK will accelerate progress in the field by lowering barriers to and standardizing data and model sharing, thus 1) increasing the quantity and also, importantly, the quality of resources that researchers share and 2) enabling others to reproduce and build on the wealth of past biomechanics research studies.",SimTK: An Ecosystem for Data and Model Sharing in the Biomechanics Community,9847973,R01GM124443,"['Achievement', 'Address', 'Adopted', 'Biological', 'Biological Models', 'Biology', 'Biomechanics', 'Biophysics', 'Cloud Computing', 'Code', 'Communities', 'Computer software', 'Consumption', 'Custom', 'Data', 'Data Files', 'Data Set', 'Development', 'Documentation', 'Engineering', 'Ensure', 'Environment', 'Explosion', 'Face', 'Foundations', 'Frequencies', 'Goals', 'Grant', 'Habits', 'Infrastructure', 'Letters', 'Literature', 'Machine Learning', 'Manuals', 'Measures', 'Medical', 'Medical Device', 'Medical Device Designs', 'Metadata', 'Methods', 'Modeling', 'Natural Language Processing', 'Operative Surgical Procedures', 'Pharmaceutical Preparations', 'Physics', 'Process', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Security', 'Services', 'Site', 'Structure', 'System', 'Technology', 'Time', 'Update', 'Work', 'application programming interface', 'base', 'biological systems', 'biomechanical model', 'community building', 'complex biological systems', 'data access', 'data cleaning', 'data ecosystem', 'data reuse', 'data sharing', 'data standards', 'digital', 'experience', 'experimental study', 'insight', 'member', 'new technology', 'novel diagnostics', 'novel therapeutics', 'prevent', 'recruit', 'research study', 'response', 'role model', 'sensor', 'simulation', 'simulation software', 'software systems', 'success', 'tool', 'user-friendly', 'web site']",NIGMS,STANFORD UNIVERSITY,R01,2020,489919,0.01647639787577346
"NextGen Random Forests Project Summary/Abstract Building from the PI's current R01, we propose next generation random forests (RF) designed for unprecedented accuracy and computational scalability to meet the challenges of today's complex and big data in the health sciences. Superior accuracy is achieved using super greedy trees which circumvent limitations on local adaptivity imposed by classical tree splitting. We identify a key quantity, forest weights, and show how these can be leveraged for further improvements and generalizability. In one application, improved survival estimators are applied to worldwide esophageal cancer data to develop guidelines for clinical decision making. Richer RF inference is another issue explored. Cutting edge machine learning methods rarely consider the problem of estimating variability. For RF, bootstrapping currently exists as the only tool for reliably estimating conﬁdence intervals, but due to heavy computations is rarely applied. We introduce tools to rapidily calculate standard errors based on U-statistic theory. These will be used to increase robustness of esophageal clinical recommendations and to investigate survival temporal trends in cardiovascular disease. In another application, we make use of our new massive data scalability for discovery of tumor and immune regulators of immunotherapy in cancers. This project will set the standard for RF computational performance. Building from the core libraries of the highly accessed R-package randomForestSRC (RF-SRC), software developed under the PIs current R01, we develop open source next generation RF software, RF-SRC Everywhere, Big Data RF-SRC, and HPC RF-SRC. The software will be deployable on a number of popular machine learning workbenches, use distributed data storage technologies, and be optimized for big-p, big-n, and big-np scenarios. Project Narrative We introduce next generation random forests (RF) designed for unprecedented accuracy for complex and big data encountered in the health sciences.",NextGen Random Forests,9929599,R01GM125072,"['Atrophic', 'Benchmarking', 'Big Data', 'Biological Response Modifiers', 'Blood', 'Cancer Patient', 'Cardiovascular Diseases', 'Clinical', 'Clinical Management', 'Code', 'Combined Modality Therapy', 'Computer software', 'Confidence Intervals', 'Data', 'Data Storage and Retrieval', 'Databases', 'Development', 'Esophagus', 'Flow Cytometry', 'Guidelines', 'Health Sciences', 'Heart failure', 'Human', 'Hybrids', 'Immune', 'Immunotherapy', 'In Vitro', 'Interagency Registry for Mechanically Assisted Circulatory Support', 'Internet', 'Java', 'Laboratories', 'Language', 'Libraries', 'Machine Learning', 'Malignant Neoplasms', 'Malignant neoplasm of esophagus', 'Methodology', 'Methods', 'Modeling', 'Mus', 'Neoadjuvant Therapy', 'Operative Surgical Procedures', 'Pathologic', 'Patients', 'Performance', 'Population', 'Pump', 'Receptor Activation', 'Recommendation', 'Resistance', 'Subgroup', 'T-Lymphocyte', 'Technology', 'Therapeutic', 'Thrombosis', 'Time', 'Time trend', 'Trees', 'Weight', 'base', 'clinical decision-making', 'clinical practice', 'complex data ', 'design', 'distributed data', 'forest', 'immune checkpoint blockade', 'immunoregulation', 'improved', 'in vivo', 'lymph nodes', 'machine learning method', 'mouse model', 'next generation', 'novel', 'open source', 'outcome forecast', 'parallel processing', 'pre-clinical', 'predicting response', 'predictive modeling', 'random forest', 'receptor', 'response', 'software development', 'statistics', 'theories', 'therapeutic target', 'tool', 'tumor', 'tumor progression']",NIGMS,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,R01,2020,347834,-0.00023184537950160603
"PREMIERE: A PREdictive Model Index and Exchange REpository The confluence of new machine learning (ML) data-driven approaches; increased computational power; and access to the wealth of electronic health records (EHRs) and other emergent types of data (e.g., omics, imaging, mHealth) are accelerating the development of biomedical predictive models. Such models range from traditional statistical approaches (e.g., regression) through to more advanced deep learning techniques (e.g., convolutional neural networks, CNNs), and span different tasks (e.g., biomarker/pathway discovery, diagnostic, prognostic). Two issues have become evident: 1) as there are no comprehensive standards to support the dissemination of these models, scientific reproducibility is problematic, given challenges in interpretation and implementation; and 2) as new models are put forth, methods to assess differences in performance, as well as insights into external validity (i.e., transportability), are necessary. Tools moving beyond the sharing of data and model “executables” are needed, capturing the (meta)data necessary to fully reproduce a model and its evaluation. The objective of this R01 is the development of an informatics standard supporting the requisite information for scientific reproducibility for statistical and ML-based biomedical predictive models; from this foundation, we then develop new computational approaches to compare models' performance. We begin by extending the current Predictive Model Markup Language (PMML) standard to fully characterize biomedical datasets and harmonize variable definitions; to elucidate the algorithms involved in model creation (e.g., data preprocessing, parameter estimation); and to explain the validation methodology. Importantly, models in this PMML format will become findable, accessible, interoperable, and reusable (i.e., following FAIR principles). We then propose novel meth- ods to compare and contrast predictive models, assessing transportability across datasets. While metrics exist for comparing models (e.g., c-statistics, calibration), often the required case-level information is not available to calculate these measures. We thus introduce an approach to simulate cases based on a model's reported da- taset statistics, enabling such calculations. Different levels of transportability are then assigned to the metrics, determining the extent to which a selected model is applicable to a given population/cohort (i.e., helping answer the question, can I use this published model with my own data?). We tie these efforts together in our proposed framework, the PREdictive Model Index & Exchange REpository (PREMIERE). We will develop an online portal and repository for model sharing around PREMIERE, and our efforts will include fostering a community of users to guide its development through workshops, model-thons, and other activities. To demonstrate these efforts, we will bootstrap PREMIERE with predictive models from a targeted domain (risk assessment in imaging-based lung cancer screening). Our efforts to evaluate these developments will engage a range of stakeholders (model developers, users) to inform the completeness of our standard; and biostatisticians and clinical experts to guide assessment of model transportability. PROGRAM NARRATIVE With growing access to information contained in the electronic health record and other data sources, the appli- cation of statistical and machine learning methods are generating more biomedical predictive models. However, there are significant challenges to reproducing these models for purposes of comparison and application in new environments/populations. This project develops informatics standards to facilitate the sharing and reproducibil- ity of these models, enabling a suite of comparative methods to evaluate model transportability.",PREMIERE: A PREdictive Model Index and Exchange REpository,10016297,R01EB027650,"['Access to Information', 'Address', 'Algorithms', 'Area', 'Attention', 'Bayesian Network', 'Big Data', 'Biological Markers', 'Calibration', 'Characteristics', 'Clinical', 'Communities', 'Computational Biology', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Data Sources', 'Decision Making', 'Decision Trees', 'Dermatology', 'Development', 'Diagnosis', 'Diagnostic', 'Diagnostic Imaging', 'Ecosystem', 'Educational workshop', 'Electronic Health Record', 'Environment', 'Evaluation', 'FAIR principles', 'Fostering', 'Foundations', 'Goals', 'Human', 'Image', 'Image Analysis', 'Informatics', 'Language', 'Link', 'Literature', 'Machine Learning', 'Measures', 'Medical', 'Metadata', 'Methodology', 'Methods', 'Modeling', 'Nature', 'Ophthalmology', 'Pathway interactions', 'Performance', 'Population', 'Publications', 'Publishing', 'Radiology Specialty', 'Receiver Operating Characteristics', 'Reporting', 'Reproducibility', 'Reproduction', 'Research Personnel', 'Risk Assessment', 'Source', 'Techniques', 'Testing', 'Training', 'Validation', 'Variant', 'Work', 'base', 'bioimaging', 'biomarker discovery', 'case-based', 'cohort', 'collaborative environment', 'comparative', 'computer aided detection', 'convolutional neural network', 'data sharing', 'deep learning', 'design', 'experience', 'feature selection', 'indexing', 'innovation', 'insight', 'interest', 'interoperability', 'learning network', 'lung basal segment', 'lung cancer screening', 'mHealth', 'machine learning method', 'model development', 'novel', 'novel strategies', 'online repository', 'predictive modeling', 'prognostic', 'repository', 'software repository', 'statistical and machine learning', 'statistics', 'stem', 'tool', 'web portal']",NIBIB,UNIVERSITY OF CALIFORNIA LOS ANGELES,R01,2020,673491,0.0015158920907329836
"Scientific Questions: A New Target for Biomedical NLP Project Summary  Natural language processing (NLP) technology is now widespread (e.g. Google Translate) and has several important applications in biomedical research. We propose a new target for NLP: extraction of scientific questions stated in publications. A system that automatically captures and organizes scientific questions from across the biomedical literature could have a wide range of significant impacts, as attested to in our diverse collection of support letters from researchers, journal editors, educators and scientific foundations. Prior work focused on making binary (or probabilistic) assessments of whether a text is hedged or uncertain, with the goal of downgrading such statements in information extraction tasks—not computationally capturing what the uncertainty is about. In contrast, we propose an ambitious plan to identify, represent, integrate and reason about the content of scientific questions, and to demonstrate how this approach can be used to address two important new use cases in biomedical research: contextualizing experimental results and enhancing literature awareness. Contextualizing results is the task of linking elements of genome-scale results to open questions across all of biomedical research. Literature awareness is the ability to understand important characteristics of large, dynamic collections of research publications as a whole. We propose to produce rich computational representations of the dynamic evolution of research questions, and to prototype textual and visual interfaces to help students and researchers explore and develop a detailed understanding of key open scientific questions in any area of biomedical research. Project Narrative The scientific literature is full of statements of important unsolved questions. By using artificial intelligence systems to identify and categorize these questions, the proposed work would help other researchers discover when their findings might address an important question in another scientific area. This work would also make it easier for students, journal editors, conference organizers and others understand where science is headed by tracking the evolution of scientific questions.",Scientific Questions: A New Target for Biomedical NLP,10069773,R01LM013400,"['Address', 'Area', 'Artificial Intelligence', 'Awareness', 'Biomedical Research', 'Characteristics', 'Collection', 'Computerized Patient Records', 'Cues', 'Data', 'Elements', 'Environment', 'Evolution', 'Expert Systems', 'Foundations', 'Genes', 'Goals', 'Gold', 'Information Retrieval', 'Journals', 'Letters', 'Link', 'Literature', 'Manuals', 'Methods', 'Molecular', 'Natural Language Processing', 'Ontology', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Phenotype', 'Proteomics', 'Publications', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Role', 'Science', 'Scientist', 'Semantics', 'Services', 'Signal Transduction', 'Source', 'Students', 'System', 'Taxonomy', 'Technology', 'Text', 'Time', 'Translating', 'Uncertainty', 'Update', 'Visual', 'Work', 'design', 'dynamical evolution', 'experimental study', 'genome wide association study', 'genome-wide', 'graduate student', 'high throughput screening', 'innovation', 'journal article', 'news', 'novel', 'pharmacovigilance', 'prototype', 'symposium', 'text searching', 'tool', 'transcriptome sequencing', 'trend']",NLM,UNIVERSITY OF COLORADO DENVER,R01,2020,462393,-0.022502770704361588
"A deep learning platform to evaluate the reliability of scientific claims by citation analysis. The opioid epidemic in the United States has been traced to a 1980 letter reporting in the prestigious New England Journal of Medicine that synthetic opioids are not addictive. A belated citation analysis led the journal to append this letter with a warning this letter has been “heavily and uncritically cited” as evidence that addiction is rare with opioid therapy.” This epidemic is but one example of how unreliable and uncritically cited scientific claims can affect public health, as studies from industry report that a substantial part of biomedical reports cannot be independently verified. Yet, there is no publicly available resource or indicator to determine how reliable a scientific claim is without becoming an expert on the subject or retaining one. The total citation count, the commonly used measure, is inherently a poor proxy for research quality because confirming and refuting citations are counted as equal, while the prestige of the journal is not a guarantee that a claim published there is true. The lack of indicators for the veracity of reported claims costs the public, businesses, and governments, billions of dollars per year. We have developed a prototype that automatically classifies statements citing a scientific claim into three classes: those that provide supporting or contradicting evidence, or merely mention the claim. This unique capability enables scite users to analyze the reliability of scientific claims at an unprecedented scale and speed, helping them to make better-informed decisions. The prototype has attracted potential customers among top biotechnology and pharmaceutical companies, research institutions, academia, and academic publishers. We propose to conduct research that will refine scite into an MVP by optimizing prototype efficiency and accuracy until they reach feasible milestones, and will refine the product-market fit in our beachhead market, academic publishing, whose influence on the integrity and reliability of research is difficult to overestimate. We propose to develop a platform that can be used to evaluate the reliability of scientific claims. Our deep learning model, combined with a network of experts, automatically classifies citations as supporting, contradicting, or mentioning, allowing users to easily assess the veracity of scientific articles and consequently researchers. By introducing a system that can identify how a research article has been cited, not just how many times, we can assess research better than traditional analytical approaches, thus helping to improve public health by identifying and promoting reliable research and by increasing the return on public and private investment in research.",A deep learning platform to evaluate the reliability of scientific claims by citation analysis.,10136941,R44DA050155,"['Academia', 'Address', 'Affect', 'Architecture', 'Biotechnology', 'Businesses', 'Classification', 'Data', 'Data Set', 'Epidemic', 'Government', 'Human', 'Industry', 'Institution', 'Investments', 'Journals', 'Letters', 'Link', 'Literature', 'Machine Learning', 'Marketing', 'Measures', 'Medicine', 'Modeling', 'National Institute of Drug Abuse', 'New England', 'Performance', 'Pharmacologic Substance', 'Phase', 'Privatization', 'Program Description', 'Proxy', 'Public Health', 'Publishing', 'Readiness', 'Reporting', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'Sales', 'Small Business Innovation Research Grant', 'Speed', 'System', 'Testing', 'Text', 'Time', 'Training', 'United States', 'Vision', 'Visual system structure', 'addiction', 'commercialization', 'cost', 'dashboard', 'deep learning', 'design', 'improved', 'insight', 'interest', 'learning classifier', 'literature citation', 'opioid epidemic', 'opioid therapy', 'product development', 'programs', 'prototype', 'synthetic opioid', 'tool', 'user-friendly']",NIDA,"SCITE, INC.",R44,2020,746725,0.01013573337073919
"Image-guided Biocuration of Disease Pathways From Scientific Literature Realization of precision medicine ideas requires an unprecedented rapid pace of translation of biomedical discoveries into clinical practice. However, while many non-canonical disease pathways and uncommon drug actions, which are of vital importance for understanding individual patient-specific disease pathways, are accumulated in the literature, most are not organized in databases. Currently, such knowledge is curated manually or semi-automatically in a very limited scope. Meanwhile, the volume of biomedical information in PubMed (currently 28 million publications) keeps growing by more than a million articles per year, which demands more efficient and effective biocuration approaches.  To address this challenge, a novel biocuration method for automatic extraction of disease pathways from figures and text of biomedical articles will be developed.  Specific Aim 1: To develop focused benchmark sets of articles to assess the performance of the biocuration pipeline.  Specific Aim 2: To develop a method for extraction of components of disease pathways from articles’ figures based on deep-learning techniques.  Specific Aim 3: To develop a method for reconstruction of disease-specific pathways through enrichment and through graph neural network (GNN) approaches.  Specific Aim 4: To conduct a comprehensive evaluation of the pipeline.  The overarching goal of this project is to develop a computer-based automatic biocuration ecosystem for rapid transformation of free-text biomedical literature into a machine-processable format for medical applications.  The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. It will especially benefit cancer patients for which up-to-date knowledge of newly discovered molecular mechanisms and drug actions is critical. The overall impact of the proposed project will be to significantly improve health outcomes in individualized patient cases by efficiently bringing the latest biomedical discoveries into a precision medicine setting. In this project, a novel biocuration method for an automatic extraction of disease mechanisms from figures and text in scientific literature will be developed. These mechanisms will be stored in a database for further querying to assist in medical diagnosis and treatment.",Image-guided Biocuration of Disease Pathways From Scientific Literature,9987133,R01LM013392,"['Address', 'Architecture', 'Benchmarking', 'Biological', 'Cancer Patient', 'Communities', 'Computers', 'Databases', 'Deposition', 'Detection', 'Diagnosis', 'Dimensions', 'Disease', 'Disease Pathway', 'Ecosystem', 'Elements', 'Evaluation', 'Feedback', 'Genes', 'Goals', 'Graph', 'Health', 'Image', 'Informatics', 'Knowledge', 'Label', 'Language', 'Link', 'Literature', 'Malignant Neoplasms', 'Malignant neoplasm of lung', 'Manuals', 'Measures', 'Medical', 'Methods', 'Molecular', 'Molecular Analysis', 'Natural Language Processing pipeline', 'Ontology', 'Outcome', 'Oxidative Stress', 'Pathway interactions', 'Patients', 'Performance', 'Phenotype', 'PubMed', 'Publications', 'Regulation', 'Reporting', 'Research', 'Retrieval', 'Selection Criteria', 'Signal Pathway', 'Source', 'Structure', 'System', 'Techniques', 'Testing', 'Text', 'Training', 'Translations', 'Visual', 'Work', 'base', 'clinical practice', 'deep learning', 'design', 'detector', 'drug action', 'image guided', 'improved', 'individual patient', 'knowledge base', 'knowledge curation', 'multimodality', 'neural network', 'neural network architecture', 'novel', 'precision medicine', 'reconstruction', 'success', 'text searching', 'tool', 'usability']",NLM,UNIVERSITY OF MISSOURI-COLUMBIA,R01,2020,313495,-0.0034944140193152464
"Arkansas Bioinformatics Consortium Project Summary/Abstract The Arkansas Research Alliance proposes to hold five annual workshops on the subject of bioinformatics. The purpose is to bring six major Arkansas institutions into closer collaboration. Those institutions are: University of Arkansas-Fayetteville; Arkansas State University; University of Arkansas for Medical Sciences; University of Arkansas at Little Rock; University of Arkansas at Pine Bluff; and the National Center for Toxicological Research. The workshops will focus on capabilities at each of the six in sciences related to bioinformatics including artificial intelligence, big data, machine learning, food and agriculture, high speed computing, and visualization capabilities. As this work progresses, educational coordination and student encouragement will be important components. Principals from all six institutions are collaborating to accomplish the workshop goals. Project Narrative The FDA ability to protect the public health is directly related to its ability to access and utilize the latest scientific data. Increased proficiency in collecting, presenting, validating, understanding, and drawing quantitative inference from the massive volume of new scientific results is necessary for success in that effort. The complexity involved requires continued development of new tools available and being developed within the realm of information technology, and the workshops proposed here will address this need. Specific Aims  • Thoroughly understand the resources in Arkansas available for furthering the capabilities in  bioinformatics and its associated needs, e.g., access to high speed computing capability and use  of computational tools. • Develop a set of plans to harness and grow those capabilities, especially those that are relevant  to the needs of NCTR and FDA. • Stimulate interest and capability across Arkansas in bioinformatics to produce a larger cadre of  expertise as these plans are implemented. • Enlist NCTR’s help in directing the effort toward seeking local, national and international data  that can be more effectively analyzed to produce results needed by FDA and others, e.g.,  reviewing decades of genomic/treatment data on myeloma patients at the University of  Arkansas for Medical Sciences. • Develop ways in which the Arkansas capabilities can be combined into a coordinated, synergistic  force larger than the sum of its parts. • Encourage students and faculty in the development of new models and techniques to be used in  bioinformatics and related fields. • Improve inter-institutional communication, including developing standardized bioinformatics  curricula and more universal course acceptance.",Arkansas Bioinformatics Consortium,9961522,R13FD006690,[' '],FDA,ARKANSAS RESEARCH ALLIANCE,R13,2020,15000,-0.017377221527266545
"Development of Tools for Evaluating the National Toxicology Program's Effectiveness  NIEHS funds research grants and conducts research to evaluate agents of public health concern. NIEHS has need for research and development tools for use in its research evaluations both the Division of the National Toxicology Program (DNTP) and the Division of Extramural Research and Training (DERT). These tools will enable NTP to evaluate its effectiveness across multiple stakeholder groups to determine use and ability to affect change for public health. Additionally, NTP has interests in using natural language processing for tools that can assist with information extraction from scientific publications ultimately for use in assessing potential hazards. DERT has need for categorical evaluation of its grants portfolio by extracting information and organizing them relative to outcomes and impacts. The Department of Energy’s Oak Ridge National Laboratory (ORNL) has research experience in analysis of textual information and has developed a unique publication mining capability that enable automated evaluation of scientific publications. NIEHS wants to take advantage of these ORNL capabilities for use in its research evaluations. n/a",Development of Tools for Evaluating the National Toxicology Program's Effectiveness ,10237828,ES16002001,"['Affect', 'Area', 'Bibliometrics', 'Categories', 'Computer software', 'Department of Energy', 'Effectiveness', 'Evaluation', 'Evaluation Research', 'Extramural Activities', 'Funding', 'Grant', 'Information Retrieval', 'Internet', 'Laboratories', 'Methods', 'Mining', 'National Institute of Environmental Health Sciences', 'National Toxicology Program', 'Natural Language Processing', 'Outcome', 'Program Effectiveness', 'Public Health', 'Publications', 'Research', 'Research Project Grants', 'Research Training', 'Retrieval', 'Scientific Evaluation', 'Techniques', 'Visual', 'experience', 'hazard', 'interest', 'research and development', 'tool', 'tool development']",NIEHS,NATIONAL INSTITUTE OF ENVIRONMENTAL HEALTH SCIENCES,Y01,2020,500000,-0.01363002470758893
"Lagrangian computational modeling for biomedical data science The goal of the project is to develop a new mathematical and computational modeling framework for from biomedical data extracted from biomedical experiments such as voltages, spectra (e.g. mass, magnetic resonance, impedance, optical absorption, …), microscopy or radiology images, gene expression, and many others. Scientists who are looking to understand relationships between different molecular and cellular measurements are often faced with questions involving deciphering differences between different cell or organ measurements. Current approaches (e.g. feature engineering and classification, end-to-end neural networks) are often viewed as “black boxes,” given their lack of connection to any biological mechanistic effects. The approach we propose builds from the “ground up” an entirely new modeling framework build based on recently developed invertible transformation. As such, it allows for any machine learning model to be represented in original data space, allowing for not only increased accuracy in prediction, but also direct visualization and interpretation. Preliminary data including drug screening, modeling morphological changes in cancer, cardiac image reconstruction, modeling subcellular organization, and others are discussed. Mathematical data analysis algorithms have enabled great advances in technology for building predictive models from biological data which have been useful for learning about cells and organs, as well as for stratifying patient subgroups in different diseases, and other applications. Given their lack to fundamental biophysics properties, the modeling approaches in current existence (e.g. numerical feature engineering, artificial neural networks) have significant short-comings when applied to biological data analysis problems. The project describes a new mathematical data analysis approach, rooted on transport and related phenomena, which is aimed at greatly enhance our ability to extract meaning from diverse biomedical datasets, while augmenting the accuracy of predictions.",Lagrangian computational modeling for biomedical data science,9874005,R01GM130825,"['3-Dimensional', 'Accountability', 'Address', 'Algorithmic Analysis', 'Area', 'Biological', 'Biological Models', 'Biology', 'Biophysics', 'Brain', 'Cancer Detection', 'Cartilage', 'Cell model', 'Cells', 'Classification', 'Collaborations', 'Communication', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Reporting', 'Data Science', 'Data Scientist', 'Data Set', 'Development', 'Disease', 'Drug Screening', 'Engineering', 'Flow Cytometry', 'Fluorescence', 'Gene Expression', 'Generations', 'Goals', 'Heart', 'Image', 'Knee', 'Laboratories', 'Learning', 'Letters', 'Libraries', 'Link', 'Machine Learning', 'Magnetic Resonance', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Mass Spectrum Analysis', 'Mathematics', 'Measurement', 'Medical Imaging', 'Methodology', 'Modeling', 'Molecular', 'Morphology', 'Optics', 'Organ', 'Performance', 'Plant Roots', 'Population', 'Pythons', 'Research', 'Scientist', 'Signal Transduction', 'System', 'Techniques', 'Technology', 'Training', 'Universities', 'Virginia', 'Visualization', 'absorption', 'algorithm development', 'artificial neural network', 'base', 'biomedical data science', 'biophysical properties', 'brain morphology', 'cellular imaging', 'clinical application', 'clinical practice', 'convolutional neural network', 'cost', 'data space', 'deep learning', 'deep neural network', 'effectiveness testing', 'electric impedance', 'experimental study', 'graphical user interface', 'gray matter', 'heart imaging', 'image reconstruction', 'learning strategy', 'mathematical algorithm', 'mathematical model', 'mathematical theory', 'microscopic imaging', 'models and simulation', 'neural network', 'patient stratification', 'patient subsets', 'predictive modeling', 'radiological imaging', 'technology research and development', 'tool', 'voltage']",NIGMS,UNIVERSITY OF VIRGINIA,R01,2020,360227,-0.009718176789694789
"An Integrated Software Platform for Accelerating Image-Driven Ophthalmic Research and Driving New Insights and Endpoints to the Clinic ABSTRACT More than 20 million patients suffer from age-related macular degeneration, diabetic retinopathy, or glaucoma. These degenerative eye diseases develop over decades, and their prevalence is increasing. Retinal imaging technologies such as optical coherence tomography and adaptive optics ophthalmoscopy are essential tools in the investigation and management of eye disease. New quantitative biomarkers derived from these and other imaging modalities are critical to the clinical translation of emerging ophthalmic innovations. However, biomarker development in the era of artificial intelligence requires large volumes of annotated images and transparent, reproducible processes, which places new demands on the management of living subjects research, data sharing, and algorithm development. Unfortunately, current software platforms are not effective in integrating these data in a manner that meets specific requirements in ophthalmology, Our goal in this Direct-to-Phase II SBIR, consistent with objectives of the NIH Strategic Plan for Data Science, is to create an integrated platform (PaaS) for the collection, curation, analysis, and sharing of ocular images and data. We will extend the capabilities of systems developed by the Advanced Ocular Imaging Program (AOIP), Medical College of Wisconsin (MCW), which include: (a) LATTICE - a software solution that reduces costs, reduces errors, and improves communications in the management of living-subjects research; (b) MOSAIC - an image processing platform and algorithm library with traditional and AI-trained algorithms; and (c) The AOIP Image Bank - a Repository that houses images and data on 1578 fully-consent human research subjects. To create the integrative platform, we will address four aims: (a) Extend LATTICE to meet the workflow requirements of academic and sponsored research in local and multisite environments, including the extensible direct integration of data relevant to ocular studies; (b) Design and implement a hybrid (local + cloud) REPOSITORY architecture, data schema, knowledge ontology, and query architecture for Owners and Readers of data.; (c) Integrate and demonstrate LATTICE, REPOSITORY and MOSAIC into a continuous ocular science workflow and (d) integrate and demonstrate Lattice, Repository and Mosaic into a continuous ocular science workflow. Our Integrated Translational Imaging platform will enable ophthalmic innovators to translate sight-saving insights and interventions to the clinic faster, with less frustration, and greater confidence. Our proposal fills an important technology gap in the field of ophthalmic data science and biomarker development. While the number and type of imaging devices continues to grow, the tools to develop and deploy new biomarkers and clinical endpoints using these exquisite imaging devices has not kept pace. With this program we will enable a new generation of image-driven innovation to find its way to the clinic. Project Narrative With more than 20 million patients suffering from age-related macular degeneration, diabetic retinopathy, or glaucoma, it is crucial to develop non-invasive biomarkers as early predictors of eye disease and reliable tests of the safety and efficacy of new preventative and restorative therapies. To meet the unmet need for rapid access and analysis of ophthalmic research data for the discovery of these biomarkers, we will create an integrated platform (PaaS) for the collection, curation, sharing, and analysis of ocular images and data. If we meet our objectives, our platform will reduce the cost of clinical research and increase the speed of translating critical research insights to saving the sight of millions of patients.",An Integrated Software Platform for Accelerating Image-Driven Ophthalmic Research and Driving New Insights and Endpoints to the Clinic,9908389,R44EY031198,"['Address', 'Age related macular degeneration', 'Algorithms', 'Architecture', 'Artificial Intelligence', 'Automobile Driving', 'Biological Markers', 'Blindness', 'Clinic', 'Clinical', 'Clinical Research', 'Collection', 'Communication', 'Computer software', 'Consent', 'Data', 'Data Discovery', 'Data Science', 'Diabetic Retinopathy', 'Docking', 'Economics', 'Environment', 'Eye diseases', 'Foundations', 'Frustration', 'Funding', 'Glaucoma', 'Goals', 'Housing', 'Human Subject Research', 'Hybrids', 'Image', 'Imaging Device', 'Imaging technology', 'Influentials', 'Intervention', 'Investigation', 'Knowledge', 'Libraries', 'Mosaicism', 'Ontology', 'Ophthalmology', 'Ophthalmoscopy', 'Optical Coherence Tomography', 'Patients', 'Phase', 'Policies', 'Prevalence', 'Process', 'Reader', 'Research', 'Research Subjects', 'Savings', 'Science', 'Site', 'Small Business Innovation Research Grant', 'Speed', 'Strategic Planning', 'Structure', 'System', 'Technology', 'Translating', 'Translations', 'United States National Institutes of Health', 'Validation', 'Vision', 'Wisconsin', 'adaptive optics', 'algorithm development', 'algorithm training', 'application programming interface', 'biomarker development', 'biomarker discovery', 'clinical translation', 'cost', 'data access', 'data exchange', 'data integration', 'data sharing', 'data warehouse', 'deep learning', 'design', 'efficacy testing', 'experience', 'fighting', 'image processing', 'image reconstruction', 'imaging modality', 'imaging platform', 'imaging program', 'improved', 'innovation', 'insight', 'medical schools', 'microsystems', 'ocular imaging', 'process repeatability', 'programs', 'repository', 'retinal imaging', 'safety testing', 'software systems', 'structured data', 'tool', 'verification and validation', 'vision science']",NEI,"TRANSLATIONAL IMAGING INNOVATIONS, INC.",R44,2020,743347,0.0014933016615536995
"Neuroimaging Analysis Center (NAC) Project Summary/Abstract The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the pos- sibility for a new era in neuroimaging, disease understanding, and patient treatment. To unlock the full medical potential made possible by these new technologies, new algorithms and clinically-relevant techniques must be developed by close collaboration between computer scientists, physicians, and medical researchers. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and exten- sive collaboration. The overarching theme for this P41 renewal is the discovery and analysis of novel imaging phenotypes to characterize disease. We use the term imaging phenotypes to describe patterns or features of disease that can be detected through imaging (predominantly MRI) followed by machine learning, statistical analysis, feature detection, and correlation with other indicators of disease such as structured patient infor- mation. The three proposed Technology Research & Development (TR&D) projects address this common question us- ing a variety of complementary approaches and clinical testbeds. TR&D 1 addresses microstructure of tissue, including novel imaging methods to detect tumor microstructure. TR&D 2 investigates rich spatial patterns of disease extracted from clinical imaging with a focus on cerebrovascular and neurodegenerative conditions such as stroke. Finally, TR&D 3 proposes novel image and connectivity-based features that can be correlated with a variety of diseases, with a clinical emphasis on pediatric brain development. Technical innovation will be driven by intense collaboration between the TR&Ds and key collaborators in neurosurgery, neurology, and pe- diatrics. The TR&Ds will leverage recent important developments in the fields of image acquisition, machine learning, and data science to identify and exploit novel imaging phenotypes of disease. Building on our long history of developing clinically-relevant methods, each TR&D includes a translational and clinical validation aim to ensure our work is clinically relevant and effective at meeting the driving clinical goals. NAC's proven software engi- neering, translation, and dissemination infrastructure, along with its established network of academic, medical, and industrial partners, enhance the center's value as a national resource. Project Narrative The Neuroimaging Analysis Center is a research and technology center with the mission of advancing the role of neuroimaging in health care. The ability to access huge cohorts of patient medical records and radiology data, the emergence of ever-more detailed imaging modalities, and the availability of unprecedented computer processing power marks the possibility for a new era in neuroimaging, disease understanding, and patient treatment. We are excited to propose a national resource center with the goal of finding new ways of extracting disease characteristics from advanced imaging and computation, and to make these methods available to the larger medical community through a proven methodology of world-class research, open-source software, and extensive collaboration.",Neuroimaging Analysis Center (NAC),9997917,P41EB015902,"['Address', 'Algorithmic Analysis', 'Algorithms', 'Automobile Driving', 'Biomedical Technology', 'Biotechnology', 'Brain', 'Characteristics', 'Childhood', 'Clinical', 'Collaborations', 'Communities', 'Computational Technique', 'Computer Vision Systems', 'Computer software', 'Computers', 'Data', 'Data Science', 'Development', 'Disease', 'Educational process of instructing', 'Ensure', 'Goals', 'Healthcare', 'Image', 'Industrialization', 'Infrastructure', 'Machine Learning', 'Magnetic Resonance Imaging', 'Medical', 'Methodology', 'Methods', 'Mission', 'National Institute of Biomedical Imaging and Bioengineering', 'Nerve Degeneration', 'Neurobiology', 'Neurology', 'Patients', 'Pattern', 'Pediatrics', 'Phenotype', 'Physicians', 'Radiology Specialty', 'Recording of previous events', 'Research', 'Research Personnel', 'Resources', 'Role', 'Scientist', 'Software Engineering', 'Software Framework', 'Statistical Data Interpretation', 'Stroke', 'Structure', 'Techniques', 'Technology', 'Tissues', 'Training', 'Translations', 'Validation', 'Work', 'algorithmic methodologies', 'base', 'cerebrovascular', 'clinical application', 'clinical imaging', 'clinically relevant', 'cohort', 'disease phenotype', 'feature detection', 'imaging modality', 'innovation', 'meetings', 'neuroimaging', 'neurosurgery', 'new technology', 'novel', 'novel imaging technique', 'open source', 'patient health information', 'response', 'technology research and development', 'tumor']",NIBIB,BRIGHAM AND WOMEN'S HOSPITAL,P41,2020,1339073,0.004470181959272346
"BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy 7. Project Summary/Abstract With the wide adoption of electronic health record systems, cross-institutional genomic medicine predictive modeling is becoming increasingly important, and have the potential to enable generalizable models to accelerate research and facilitate quality improvement initiatives. For example, understanding whether a particular variable has clinical significance depends on a variety of factors, one important one being statistically significant associations between the variant and clinical phenotypes. Multivariate models that predict predisposition to disease or outcomes after receiving certain therapeutic agents can help propel genomic medicine into mainstream clinical care. However, most existing privacy-preserving machine learning methods that have been used to build predictive models given clinical data are based on centralized architecture, which presents security and robustness vulnerabilities such as single-point-of-failure. In this proposal, we will develop novel methods for decentralized privacy-preserving genomic medicine predictive modeling, which can advance comparative effectiveness research, biomedical discovery, and patient-care. Our first aim is to develop a predictive modeling framework on private Blockchain networks. This aim relies on the Blockchain technology and consensus protocols, as well as the online and batch machine learning algorithms, to provide an open-source Blockchain-based privacy-preserving predictive modeling library for further Blockchain-related studies and applications. We will characterize settings in which Blockchain technology offers advances over current technologies. The second aim is to develop a Blockchain-based privacy-preserving genomic medicine modeling architecture for real-world clinical data research networks. These aims are devoted to the mission of the National Human Genome Research Institute (NHGRI) to develop biomedical technologies with application domain of genomics and healthcare. The NIH Pathway to Independence Award provides a great opportunity for the applicant to complement his computer science background with biomedical knowledge, and specialized training in machine learning and knowledge-based systems. It will also allow him to investigate new techniques to advance genomic and healthcare privacy protection. The success of the proposed project will help his long-term career goal of obtaining a faculty position at a biomedical informatics program at a major US research university and conduct independently funded research in the field of decentralized privacy-preserving computation. 8. Project Narrative The proposed research will develop practical methods to support privacy-preserving genomic and healthcare predictive modeling, and build innovations based on Blockchain technology for secure and robust machine learning training processes. The development of such privacy technology may increase public trust in research and quality improvement. The technology we propose will also contribute to the sharing of predictive models in ways that meet the needs of genomic research and healthcare.",BECKON - Block Estimate Chain: creating Knowledge ON demand & protecting privacy,9920181,R00HG009680,"['Adoption', 'Algorithms', 'Architecture', 'Authorization documentation', 'Award', 'Biomedical Technology', 'Caring', 'Characteristics', 'Client', 'Clinical', 'Clinical Data', 'Clinical Medicine', 'Comparative Effectiveness Research', 'Complement', 'Complex', 'Consensus', 'Data', 'Data Aggregation', 'Data Collection', 'Decentralization', 'Development', 'Disease', 'Distributed Databases', 'Electronic Health Record', 'Ethics', 'Faculty', 'Failure', 'Fibrinogen', 'Funding', 'Genomic medicine', 'Genomics', 'Goals', 'Health Care Research', 'Healthcare', 'Hybrids', 'Infrastructure', 'Institution', 'Institutional Policy', 'Intuition', 'Investigation', 'Knowledge', 'Libraries', 'Machine Learning', 'Mainstreaming', 'Maintenance', 'Medicine', 'Metadata', 'Methods', 'Mission', 'Modeling', 'Monitor', 'National Human Genome Research Institute', 'Outcome', 'Pathway interactions', 'Patient Care', 'Patients', 'Population', 'Positioning Attribute', 'Predisposition', 'Privacy', 'Privatization', 'Process', 'Protocols documentation', 'Records', 'Research', 'Research Infrastructure', 'Research Personnel', 'Risk', 'Secure', 'Security', 'Site', 'Standardization', 'System', 'Techniques', 'Technology', 'Testing', 'Therapeutic Agents', 'Time', 'Training', 'Transact', 'United States National Institutes of Health', 'Universities', 'Variant', 'base', 'biomedical informatics', 'blockchain', 'career', 'clinical care', 'clinical phenotype', 'clinically significant', 'computer science', 'data sharing', 'design', 'digital', 'diverse data', 'health care delivery', 'improved', 'innovation', 'interoperability', 'knowledge base', 'machine learning algorithm', 'machine learning method', 'medical specialties', 'network architecture', 'novel', 'open source', 'peer', 'peer networks', 'point of care', 'predictive modeling', 'privacy preservation', 'privacy protection', 'programs', 'public trust', 'structural genomics', 'success', 'trend', 'web portal', 'web services']",NHGRI,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R00,2020,249000,-0.005722160581391081
"Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data This project aims to develop NeuroManager™, an innovative neuroinformatics platform for advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data. A core technology that we will develop in NeuroManager will be Image Content Analysis for Retrieval Using Semantics (ICARUS), a novel, intelligent neuroimage curation system that will enable image retrieval based on visual appearance or by semantic concept. ICARUS will use machine learning applied to content-based image retrieval - (CBIR) to build and refine models that summarize microscopic and macroscopic image appearance and automatically assign semantic concepts to neuroimages. Neuroscience research generates extensive, multifaceted data that is considerably under-utilized because access to original raw data is typically maintained by the source lab. On the other hand, there are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results. Unfortunately, none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. To solve this problem, NeuroManager will include the following distinct, significant innovations: (i) versatility for handling two-dimensional (2D) and three-dimensional neuroimaging data sets from animal models and humans; (ii) functionality to share complex datasets that extends secure, privacy-controlled paradigms from institutional, laboratory-based and even public domains; (iii) flexibility to implement NeuroManager within an institute’s IT infrastructure, or on most cloud-based virtualized environments including Azure, Google Cloud Services and Amazon Web Services; (iv) and most importantly, the ICARUS technology for CBIR in neuroimaging data sets. The benefit of NeuroManager for the neuroscience research community, pharmacological and biotechnological R&D, and society in general will be to foster collaboration between scientists and institutions, promoting innovation through combined expertise in an interdisciplinary atmosphere. This will open new horizons for better understanding the neuropathology associated with several human neuropsychiatric and neurological conditions at various levels (i.e., macroscopically, microscopically, subcellularly and functionally), ultimately leading to an improved basis for developing novel treatment and prevention strategies for complex brain diseases. In Phase I we will prove feasibility of this novel technology by developing prototype software that will perform CBIR on 2D whole slide images of coronal sections of entire mouse brains from ongoing research projects of our collaborators. Work in Phase II will focus on developing the commercial software product that will include all of the innovations mentioned above. A competing technology with comparable functionality, addressing the full breadth of needs for modern neuroscience research, is currently not available commercially or otherwise. There are many advantages in sharing complex image data in neuroscience research, including the opportunity for separate analysis of raw data by other scientists from another perspective and improved reproducibility of scientific studies and their results; however none of the neuroscience data sharing options that exist today fulfill all the needs of neuroscientists. This project commercializes an innovative software for sophisticated advanced parsing, storing, aggregating, analyzing and sharing of complex neuroscience image data, including a novel, intelligent neuroimage curation system that will enable content-based neuroscience image search powered by machine learning, thereby opening new horizons in neuroscience research collaborations. This system will allow researchers to make new discoveries based on new studies that are currently not feasible, ultimately providing the basis for developing novel treatments to prevent and fight complex brain diseases.",Neuroinformatics platform using machine learning and content-based image retrieval for neuroscience image data,9989186,R44MH118815,"['3-Dimensional', 'Address', 'Amygdaloid structure', 'Animal Model', 'Appearance', 'Archives', 'Biotechnology', 'Brain', 'Brain Diseases', 'Brain imaging', 'Chicago', 'Cloud Service', 'Collaborations', 'Communities', 'Complex', 'Computer software', 'Data', 'Data Aggregation', 'Data Files', 'Data Provenance', 'Data Set', 'Data Sources', 'Digital Imaging and Communications in Medicine', 'Dimensions', 'Fostering', 'Human', 'Image', 'Information Systems', 'Infrastructure', 'Institutes', 'Institution', 'Intelligence', 'Laboratories', 'Machine Learning', 'Manuals', 'Microscopic', 'Modeling', 'Modernization', 'Mus', 'National Institute of Mental Health', 'Neurologic', 'Neurosciences', 'Neurosciences Research', 'New York', 'Notification', 'Pharmacology', 'Phase', 'Prevention strategy', 'Privacy', 'Problem Solving', 'Production', 'Public Domains', 'Records', 'Regenerative Medicine', 'Reproducibility', 'Research Personnel', 'Research Project Grants', 'Research Subjects', 'Retrieval', 'Schools', 'Scientist', 'Secure', 'Semantics', 'Societies', 'Source', 'System', 'Technology', 'Testing', 'Universities', 'Validation', 'Visual', 'Work', 'application programming interface', 'base', 'cloud based', 'collaborative environment', 'data access', 'data format', 'data sharing', 'data warehouse', 'fighting', 'flexibility', 'hands-on learning', 'improved', 'innovation', 'interest', 'neuroimaging', 'neuroinformatics', 'neuropathology', 'neuropsychiatry', 'new technology', 'novel', 'prevent', 'prototype', 'research and development', 'stem cells', 'treatment strategy', 'two-dimensional', 'usability', 'virtual environment', 'web services', 'whole slide imaging']",NIMH,"MICROBRIGHTFIELD, LLC",R44,2020,749716,0.0036697748908127064
"Data Science Applications in Communication andSwallowing Disorders PROJECT SUMMARY/ABSTRACT The emergence of electronic medical records, large data registries and readily accessible, protected servers have resulted in an explosion of digital information with potentially high clinical impact for improving patient management and outcomes. Big data warehouses that capture standardized information within the scope of clinical practices allow trained scientists to not only engage in traditional hypothesis testing, but to also uncover new hypotheses, refine existing theories and apply new discoveries to health assessments and interventions. Despite the accessibility and potential impact of these data platforms, clinician scientists have traditionally directed experiments that incorporate relatively small sample sizes and data from individual laboratories, and have not been trained in big data analytics or in engaging appropriate team scientists who work in this space, such as computer scientists, biostatisticians and engineers. The overarching goal of this proposal is to mentor early patient oriented communication and swallowing scientists in big data analytics and to mentor and involve early data science scholars in communication and swallowing research. The PI proposes four primary mentorship and research goals in this K24 renewal proposal: 1. Train a cadre of early stage communication and swallowing scientists in data science methods, including machine learning, by an expert, interdisciplinary, collaborative data science team, 2. Engage and introduce early career data scientists from fields of biostatistics, computer science and engineering to communication and swallowing sciences, and respective data sets, toward facilitating interdisciplinary data science teams and research productivity, 3. Apply novel data science methods to identify phenotypes of swallowing impairment and severity classifications in patient groups known to be at high risk for nutritional and health complications related to dysphagia, and 4. Develop a new area of research in machine learning applications toward improving reliability of physiologic swallowing assessment. The data science theme of the career development and research plan directly align with NIDCD's Strategic Plan for Data Science which lists as its mission: Storing, managing, standardizing and publishing the vast amounts of data produced by biomedical research. NIDCD recognizes that accessible, well-organized, secure and efficiently operated data resources are critical to modern scientific inquiry…and by maximizing the value of data generated through NIH-funded efforts, the pace of biomedical discoveries and medical breakthroughs for better health outcomes can be accelerated. PROJECT NARRATIVE The emergence of electronic health records exposes clinicians to massive amounts of information about the millions of patients who suffer from communication and swallowing disorders, yet most clinical scientists do not have the training or skill to apply meaning to the data toward improving patient care. The overarching mentorship goal of this proposal is to train early, patient-oriented communication and swallowing scientists in big data analyses, including computer machine learning approaches. The research project will uncover distinct patterns and severity of swallowing impairments in large groups of patients with high risk medical diagnoses, which will have high impact on patient care planning and identification of treatments that directly target these impairments for improved outcomes.",Data Science Applications in Communication andSwallowing Disorders,9892671,K24DC012801,"['Algorithms', 'Area', 'Award', 'Barium swallow', 'Big Data', 'Big Data Methods', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Biometry', 'Chronic Obstructive Airway Disease', 'Classification', 'Clinical', 'Communication', 'Communication impairment', 'Computer Vision Systems', 'Computerized Medical Record', 'Computers', 'Data', 'Data Analyses', 'Data Science', 'Data Scientist', 'Data Set', 'Deglutition', 'Deglutition Disorders', 'Dementia', 'Development', 'Diagnosis', 'Disease', 'Economics', 'Electronic Health Record', 'Engineering', 'Explosion', 'Funding', 'Geography', 'Goals', 'Grant', 'Head and Neck Cancer', 'Health', 'Hearing', 'Impairment', 'Individual', 'Instruction', 'Intervention', 'Laboratories', 'Machine Learning', 'Medical', 'Medical Records', 'Mentored Patient-Oriented Research Career Development Award', 'Mentors', 'Mentorship', 'Methods', 'Mid-Career Clinical Scientist Award (K24)', 'Mission', 'Modernization', 'National Institute on Deafness and Other Communication Disorders', 'Nutritional', 'Outcome', 'Parkinson Disease', 'Patient Care', 'Patient Care Planning', 'Patients', 'Pattern', 'Phenotype', 'Physiological', 'Productivity', 'Publishing', 'Registries', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Sample Size', 'Science', 'Scientific Inquiry', 'Scientist', 'Secure', 'Severities', 'Speech', 'Standardization', 'Statistical Models', 'Strategic Planning', 'Stroke', 'Supervision', 'Techniques', 'Testing', 'Training', 'United States National Institutes of Health', 'Work', 'base', 'career', 'career development', 'clinical practice', 'clinically relevant', 'computer science', 'computerized', 'data registry', 'data resource', 'data warehouse', 'digital', 'experimental study', 'health assessment', 'high risk', 'human error', 'impression', 'improved', 'improved outcome', 'learning algorithm', 'novel', 'patient oriented', 'programs', 'research and development', 'skills', 'statistical learning', 'theories', 'uptake']",NIDCD,NORTHWESTERN UNIVERSITY,K24,2020,183209,-0.009741932718521229
"Nobrainer: A robust and validated neural network tool suite for imagers There is an increasing need for efficient and robust software to process, integrate, and offer insight across the diversity of population imaging efforts underway across the BRAIN Initiative and other projects. Advances in statistical learning offer a set of technologies that can address many research applications using the extensive and varied data being produced by the projects. This can transform how we analyze and integrate new data. We propose using Nobrainer, an open source Python library that leverages these new learning technologies, as a platform that greatly simplifies integrating deep learning into neuroimaging research. Using this library, we are building and distributing user-friendly and cloud enabled end-user applications for the neuroimaging community. In Aim 1, we provide neural network models. We will create robust, pre-trained neural networks for brain segmentation and time series processing using brain scans from over 65000 individuals. Once trained, these models can then be used as the basis for many other applications, especially in reducing time of processing. We will subsequently use these base networks to perform image processing, image correction, and quality control. In Aim 2, we address the ability to train on private datasets. We will use Bayesian neural network models, which support principled use of prior information. We will use these networks to help detect when the models are expected to fail on an input, and provide visualizations to better understand how the model is working. In Aim 3, we focus on the engineering needed to maintain the software infrastructure, improve efficiency, and increase the scalability of our training methods. Here, we will extend, maintain, and disseminate Nobrainer, our open source software framework, together with training materials and ready to use, cloud-friendly, applications. We will also create much faster, neural network equivalents of time consuming image processing tasks (e.g., registration, segmentation, and annotation). The Nobrainer tools developed through these aims will allow users to find and apply the most pertinent applications and developers to extend the framework to support new architectures and disseminate new models and applications. We expect these tools to be used by any neuroimaging researcher through integration with BRAIN archives and popular software packages. These tools will significantly reduce data processing and new model development time, thus allowing faster exploration of hypotheses using public data and increase reusability of data through greater trust in model outputs. 9/11/2019 ResearchPlan - Google Docs The proposal will create artificial intelligence software for scientists to analyze, integrate, and visualize data from large brain imaging projects, which inform our understanding of brain structure, function, and development. Open and reusable software helps to increase collaboration and benefits researchers, but can also be used by citizen scientists and students in high schools and colleges. An open collection of neural network tools will facilitate scientific communities and has the potential to accelerate scientific discoveries about the nervous system. https://docs.google.com/document/d/11s9ZRzy8tHs6NguzpnJdJUWRgLvdkXpEYjZchtJXSmY/edit# 2/23",Nobrainer: A robust and validated neural network tool suite for imagers,10021957,RF1MH121885,"['Address', 'Adolescent', 'Architecture', 'Archives', 'Artificial Intelligence', 'BRAIN initiative', 'Bayesian neural network', 'Brain', 'Brain imaging', 'Brain scan', 'Cognitive', 'Collaborations', 'Collection', 'Communities', 'Computer software', 'Consumption', 'Data', 'Data Set', 'Data Sources', 'Development', 'Diagnosis', 'Disease', 'Educational workshop', 'Engineering', 'Ethics', 'Evaluation', 'Failure', 'Human', 'Image', 'Individual', 'Informatics', 'Institution', 'Label', 'Learning', 'Legal', 'Libraries', 'Longevity', 'Memory', 'Mental Health', 'Methods', 'Modeling', 'Nervous system structure', 'Network-based', 'Neural Network Simulation', 'Output', 'Pattern', 'Population', 'Population Heterogeneity', 'Privacy', 'Privatization', 'Process', 'Psychological Transfer', 'Pythons', 'Quality Control', 'Research', 'Research Personnel', 'Sampling', 'Scientist', 'Series', 'Software Engineering', 'Software Framework', 'Structure', 'Students', 'Technology', 'Testing', 'Three-Dimensional Imaging', 'Time', 'Training', 'Training Support', 'Trust', 'Uncertainty', 'Visualization', 'Work', 'algorithm training', 'base', 'citizen science', 'cognitive development', 'college', 'computerized data processing', 'connectome', 'data reuse', 'deep learning', 'distributed data', 'diverse data', 'high school', 'image processing', 'imager', 'improved', 'insight', 'large datasets', 'learning strategy', 'model development', 'network models', 'neural network', 'neuroimaging', 'neurophysiology', 'open source', 'software infrastructure', 'statistical learning', 'tool', 'user-friendly']",NIMH,MASSACHUSETTS INSTITUTE OF TECHNOLOGY,RF1,2020,419908,0.002002563240357947
"A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers   Abstract Wide-spread data sharing has started to permeate the brain imaging community from funders to researchers. However, in recent years there have also been some concerns raised regarding ethical issues related to privacy and data ownership among others. In the parent award we are leveraging and extending a privacy preserving decentralized data sharing platform called COINSTAC to perform a study of gene-by-environmental effects by pooling together data from across the world, some of which is unable to be openly shared. In this supplement we will study various bioethical issues related to different data sharing strategies. This will include calculating risk scores from existing data to evaluate the effectiveness of machine learning to potentially reidentify from similar or different data types, a detailed survey of various policy makers and stakeholders including researchers, federal employees, IRB members, and more, and finally the development of a forward looking white paper addressing both privacy, policy, and regulatory aspects which attempts to frame the various aspects that arise in the contact of the spectrum of data sharing approaches including fully open, ‘trust’ based via data usage agreements, privacy preserving via tools like COINSTAC, and more. The outcomes of this supplement will provide a useful guide for the field going forward and also provide initial data necessary to develop a larger scale project on these topics going forward. Narrative The era of big data, open science, and deep learning is upon us, and data sharing can be done in various ways ranging from fully open to privacy preserving to fully closed. However bioethical issues related to risk, privacy, and data sharing strategies have not been well studied in the context of combined brain imaging, genomics, and macro-environmental data and can be especially sensitive in the context of information such as substance use. In this supplement we will quantify risk levels, survey a broad community of stakeholders, and develop recommendations for the field going forward.",A decentralized macro and micro gene-by-environment interaction analysis of substance use behavior and its brain biomarkers  ,10131528,R01DA049238,"['Address', 'Adolescence', 'Adolescent', 'Age', 'Agreement', 'Alcohol or Other Drugs use', 'Anxiety', 'Award', 'Behavior', 'Behavioral', 'Behavioral Genetics', 'Big Data', 'Bioethical Issues', 'Biological Markers', 'Brain', 'Brain imaging', 'Brain scan', 'China', 'Climate', 'Communities', 'Complex', 'Computer software', 'Consumption', 'Country', 'Data', 'Data Protection', 'Decentralization', 'Development', 'Disease', 'Dropout', 'Economic Burden', 'Employee', 'Environment', 'Environmental Risk Factor', 'Epidemiology', 'Ethical Issues', 'Ethnic group', 'Europe', 'Family', 'Genes', 'Genetic', 'Genomics', 'Genotype', 'Heritability', 'Household', 'Image', 'Income', 'India', 'Individual', 'Institutional Review Boards', 'International', 'Intervention', 'Learning', 'Legal', 'Machine Learning', 'Magnetic Resonance Imaging', 'Measures', 'Mental Depression', 'Modeling', 'Movement', 'Neurobiology', 'Neurons', 'Neurosciences', 'Outcome', 'Ownership', 'Paper', 'Parents', 'Physiological', 'Policies', 'Policy Maker', 'Population', 'Population Density', 'Privacy', 'Quality Control', 'Race', 'Recommendation', 'Regulation', 'Research Personnel', 'Risk', 'Scanning', 'Smoking', 'Source Code', 'Structure', 'Surveys', 'System', 'Time', 'Trust', 'Twin Multiple Birth', 'Update', 'Urbanization', 'Visualization', 'adolescent substance use', 'base', 'cloud based', 'cognitive development', 'cohort', 'computerized tools', 'cost', 'data access', 'data exchange', 'data sharing', 'deep learning', 'drinking', 'early life stress', 'effectiveness evaluation', 'epidemiologic data', 'gene environment interaction', 'genetic profiling', 'human subject', 'imaging genetics', 'insight', 'member', 'neuroimaging', 'open data', 'open source', 'peer', 'privacy preservation', 'psychiatric symptom', 'relating to nervous system', 'rural area', 'sex', 'sharing platform', 'statistics', 'tool', 'tool development', 'urban area', 'virtual']",NIDA,GEORGIA STATE UNIVERSITY,R01,2020,149834,0.006953947770647721
"Nathan Shock Center of Excellence in Basic Biology of Aging OVERALL PROJECT SUMMARY This application is for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington and affiliated institutions. This Center has over the past 25 years provided key resources in support of investigators who study the biology of aging. This application continues a theme that emphasizes outreach and service to the broadest community of investigators in the gerosciences. Of proximal relevance is the characterization of aging-related phenotypes of longevity and healthspan. As our Center services must be easily accessible to outside users, our Longevity and Healthspan Core (Core E) focuses on invertebrate assays, many of them novel. Two other Resources Cores focus on the high dimensional assessments that are closely related to aging phenotypes: Protein Phenotypes of Aging (Core C) and Metabolite Phenotypes of Aging (Core D). Sophisticated computational and bioinformatic tools for data analysis and optimal insight are provided by the Artificial Intelligence and Bioinformatics Core F. Each of these four Resource Cores is led by highly respected experts in that field, including Michael MacCoss and Judit Villen (Core C), Daniel Promislow (Core D), Matt Kaeberlein and Maitreya Dunham (Core E) and Su-In Lee (Core F). Each will push the envelope of appropriate technologies, developing new state-of-the art approaches for assessments that are the most applicable to gerontology and making them accessible to the national aging community. The Research Development Core (Core B) will continue to support pilot and junior faculty studies, with a firm focus on outreach of service to the national geroscience constituency. The Administrative and Program Enrichment Core (Core A) supports administrative management, an external advisory panel, courses, and data sharing and dissemination. Core A’s program of seminars and symposia will continue a focus on sponsorship and organization of national courses, meetings and pre-meetings, as well as workshops in the fields allied to our Resource Core Services. In coordination with other Nathan Shock Centers, we will support a new Geropathology Research initiative. UW NATHAN SHOCK CENTER OVERALL - PROJECT NARRATIVE We apply for renewal of the Nathan Shock Center of Excellence in the Basic Biology of Aging at the University of Washington, which has for 25 years provided key resources supporting investigators who study the biology of aging. The overarching goal of this Center is to have a positive impact on the field by accelerating research discovery and providing research support for investigators nationally and internationally, particularly junior investigators in the process of building their own research programs. We will accomplish this goal through six cores that function synergistically together: four Resource Cores with particular expertise in protein (Core C) and metabolite (Core D) phenotypes of aging, invertebrate longevity and healthspan phenotypes (Core E) and artificial intelligence and bioinformatics (Core F), along with a Research Development Core (Core B) that supports external pilot projects and junior faculty studies, and an Administrative and Program Enrichment Core (Core A) that supports administrative management, an external advisory panel, sponsorship and organization of national meetings and pre-meetings, courses, workshops and seminars, and, in coordination with other Nathan Shock Centers, a Geropathology Research initiative.",Nathan Shock Center of Excellence in Basic Biology of Aging,10042617,P30AG013280,"['Aging', 'Artificial Intelligence', 'Bioinformatics', 'Biological Assay', 'Biology of Aging', 'Collaborations', 'Communication', 'Communities', 'Consult', 'Data', 'Data Analyses', 'Development', 'Educational workshop', 'Environment', 'Experimental Designs', 'Faculty', 'Genes', 'Genetic study', 'Gerontology', 'Geroscience', 'Goals', 'Growth', 'Informatics', 'Institution', 'International', 'Invertebrates', 'Leadership', 'Longevity', 'Methodology', 'Methods', 'Microfluidics', 'Molecular Genetics', 'Office of Administrative Management', 'Pathway interactions', 'Phenotype', 'Philosophy', 'Pilot Projects', 'Post-Translational Protein Processing', 'Process', 'Proteins', 'Proteomics', 'Research', 'Research Activity', 'Research Personnel', 'Research Support', 'Resources', 'Robotics', 'Services', 'Shock', 'Statistical Data Interpretation', 'Technology', 'Transcript', 'Universities', 'Variant', 'Washington', 'bioinformatics tool', 'career development', 'cell age', 'computerized tools', 'data dissemination', 'data sharing', 'healthspan', 'high dimensionality', 'insight', 'meetings', 'metabolomics', 'multiple omics', 'novel', 'outreach', 'outreach services', 'programs', 'protein metabolite', 'research and development', 'symposium', 'tool', 'trait']",NIA,UNIVERSITY OF WASHINGTON,P30,2020,962037,0.0018700795403289935
"THE XNAT IMAGING INFORMATICS PLATFORM PROJECT SUMMARY This proposal aims to continue the development of XNAT. XNAT is an imaging informatics platform designed to facilitate common management and productivity tasks for imaging and associated data. We will develop the next generation of XNAT technology to support the ongoing evolution of imaging research. Development will focus on modernizing and expanding the current system. In Aim 1, we will implement new web application infrastructure that includes a new archive file management system, a new event bus to manage cross-service orchestration and a new Javascript library to simplify user interface development. We will also implement new core services, including a Docker Container service, a dynamic scripting engine, and a global XNAT federation. In Aim 2, we will implement two innovative new capabilities that build on the services developed in Aim 1. The XNAT Publisher framework will streamline the process of data sharing by automating the creation and curation of data releases following best practices for data publication and stewardship. The XNAT Machine Learning framework will streamline the development and use of machine learning applications by integrating XNAT with the TensorFlow machine learning environment and implementing provenance and other monitoring features to help avoid the pitfalls that often plague machine learning efforts. For both Aim 1 and 2, all capabilities will be developed and evaluated in the context of real world scientific programs that are actively using the XNAT platform. In Aim 3, we will provide extensive support to the XNAT community, including training workshops, online documentation, discussion forums, and . These activities will be targeted at both XNAT users and developers. RELEVANCE Medical imaging is one of the key methods used by biomedical researchers to study human biology in health and disease. The imaging informatics platform described in this application will enable biomedical researchers to capture, analyze, and share imaging and related data. These capabilities address key bottlenecks in the pathway to discovering cures to complex diseases such as Alzheimer's disease, cancer, and heart disease.",THE XNAT IMAGING INFORMATICS PLATFORM,10002330,R01EB009352,"['Address', 'Administrator', 'Alzheimer&apos', 's Disease', 'Architecture', 'Archives', 'Area', 'Automation', 'Biomedical Research', 'Brain', 'Cardiology', 'Categories', 'Classification', 'Communities', 'Complex', 'Data', 'Data Set', 'Databases', 'Detection', 'Development', 'Disease', 'Docking', 'Documentation', 'Educational workshop', 'Ensure', 'Event', 'Evolution', 'Goals', 'Health', 'Heart Diseases', 'Human', 'Human Biology', 'Image', 'Individual', 'Informatics', 'Infrastructure', 'Instruction', 'Internet', 'Libraries', 'Machine Learning', 'Magnetic Resonance Imaging', 'Malignant Neoplasms', 'Medical Imaging', 'Methods', 'Modality', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Newsletter', 'Optics', 'Paper', 'Pathway interactions', 'Peer Review', 'Persons', 'Plague', 'Positron-Emission Tomography', 'Principal Investigator', 'Process', 'Productivity', 'Publications', 'Publishing', 'Radiology Specialty', 'Research', 'Research Personnel', 'Security', 'Services', 'System', 'Technology', 'TensorFlow', 'Training', 'Validation', 'base', 'biomedical resource', 'computer framework', 'computing resources', 'data curation', 'data sharing', 'design', 'distributed data', 'educational atmosphere', 'hackathon', 'imaging informatics', 'imaging program', 'improved', 'informatics tool', 'innovation', 'next generation', 'online tutorial', 'open source', 'outreach program', 'pre-clinical', 'programs', 'skills', 'symposium', 'tool', 'virtual', 'web app']",NIBIB,WASHINGTON UNIVERSITY,R01,2020,653481,0.020554902630061146
"Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions PROJECT SUMMARY The research interests of my group are rooted in explorations of new and useful conceptual models to improve the control and prediction of noncovalent interactions. Our research involves the use of a variety of computational quantum chemical tools, applications of density functional theory (DFT), cheminformatics, and machine-learning methods. A premise of our research is that aromaticity may be used to modulate many types of noncovalent interactions (such as hydrogen bonding, π-stacking, anion-π interactions). The reciprocal relationship we find, between “aromaticity” in molecules and the strengths of “noncovalent interactions,” is surprising especially since they are typically considered as largely separate ideas in chemistry. The innovation of this research is that it will enable use of intuitive “back-of-the-envelope” electron-counting rules (such as the 4n+2πe Hückel rule for aromaticity) to make predictions of experimental outcomes regarding the impact of noncovalent interactions. A five-year goal is to realize the use of our conceptual models in real synthetic examples prepared by our experimental collaborators. My research vision is to bridge discoveries of innovative concepts to their practical impacts for biomedical and biomolecular research. PROJECT NARRATIVE This research proposal includes four projects that are jointly motivated by the challenge to control and predict noncovalent interactions in organic and biomolecular systems. The proposed work involves applications of a variety of computational quantum chemical tools and synergistic investigations with experimental collaborators. We seek to identify new and useful concepts to guide experimental designs of novel “non-natural” molecular systems (e.g., receptors, biosensors, and hydrogels) that have potential biomedical applications.",Computational Explorations of Unconventional Approaches to Control Noncovalent Interactions,10016376,R35GM133548,"['Anions', 'Back', 'Biosensor', 'Chemicals', 'Chemistry', 'Electrons', 'Experimental Designs', 'Goals', 'Hydrogels', 'Hydrogen Bonding', 'Intuition', 'Investigation', 'Modeling', 'Molecular', 'Outcome', 'Plant Roots', 'Research', 'Research Project Summaries', 'Research Proposals', 'System', 'Vision', 'Work', 'cheminformatics', 'density', 'improved', 'innovation', 'interest', 'machine learning method', 'novel', 'quantum computing', 'receptor', 'theories', 'tool']",NIGMS,UNIVERSITY OF HOUSTON,R35,2020,377200,-0.0141802206189185
"mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment - Phase II 1 Project Summary NIH is increasing its investment in large, multi-center brain MRI studies via projects such as the recently announced BRAIN initiative. The success of these studies depends on the quality of MRIs and the resulting image measurements, regardless of sample size. Even though quality control of MRIs and corresponding measurements could be outsourced, most neuroscience studies rely on in-house procedures that combine automatically generated scores with manually guided checks, such as visual inspection. Implementing these procedures typically requires combining several software systems. For example, the NIH NIAAA- and BD2K- funded Data Analysis Resource (DAR) of the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA) uses XNAT to consolidate the structural, diffusion, and functional MRIs acquired across five sites, and has also developed their own custom software package to comply with study requirements for a multi-tier, quality control (QC) workflow. However, these custom, one-off tools lack support for the multi-site QC workflows that will come with the unified platform that MIQA represents: a design that supports collaboration and sharing, and strong cohesion between technologies. To improve the effectiveness of QC efforts specific to multi-center neuroimaging studies, we will develop a widely accessible and broadly compatible software platform that simplifies the creation of custom QC workflows in compliance with study requirements, provides core functionality for performing QC of medical images, and automatically generates documentation compliant with the FAIR principle, i.e., making scientific results findable, accessible, interoperable, and reusable.  Specifically, our multi-site, web-based software platform for Medical Image Quality Assurance (MIQA) will enable efficient and accurate QC processing by leveraging open-source, state-of-the-art web interface technologies, such as a web-based dataset caching system and machine learning to aid in QC processes. Users will be able to configure workflows that not only reflect the specific requirements of medical imaging studies but also minimize the time spent on labor-intensive operations, such as visually reviewing scans. Issue tracking technology will enhance communication between geographically-distributed team members, as they can easily share image annotations and receive automated notifications of outstanding QC issues. The system will be easy to deploy as it will be able to interface with various imaging storage backends, such as local file systems and XNAT. While parts of this functionality have been developed elsewhere, MIQA is unique as it provides a unified, standard interface for efficient QC setup, maintenance, and review for projects analyzing multiple, independently managed data sources.  The usefulness of this unique QC system will be demonstrated on increasing the efficiency of the diverse QC team of the multi-center NCANDA study. Narrative The goal of this proposal is to develop a web-based, multi-site, open-source platform for Medical Image Quality Assurance (MIQA) to address the QC needs of geographically diverse teams using small and large medical image-based studies alike. MIQA will enable efficient and accurate QC processing by levering state-of-the-art machine learning, data management, and web interface technologies. Our effort will minimize the time spent on labor-intensive reviews and analysis operations by supporting team-oriented reviewing that is guided by highly customizable workflows seamlessly interacting with existing data management systems.",mIQa: A Highly Scalable and Customizable Platform for Medical Image Quality Assessment - Phase II,10010814,R44MH119022,"['3-Dimensional', 'Active Learning', 'Address', 'Adolescence', 'Alcohols', 'Archives', 'Area', 'BRAIN initiative', 'Big Data to Knowledge', 'Brain', 'Brain imaging', 'Classification', 'Collaborations', 'Communication', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Management Resources', 'Data Provenance', 'Data Set', 'Data Sources', 'Detection', 'Diffusion', 'Documentation', 'Effectiveness', 'Ensure', 'Evaluation', 'Evaluation Studies', 'FAIR principles', 'Four-dimensional', 'Funding', 'Generations', 'Geography', 'Goals', 'Human', 'Image', 'Intelligence', 'Internet', 'Investments', 'Iowa', 'Label', 'Learning', 'Licensing', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Manuals', 'Measurement', 'Medical Imaging', 'Modeling', 'Monitor', 'National Institute on Alcohol Abuse and Alcoholism', 'Neurosciences', 'Notification', 'Online Systems', 'Peer Review', 'Phase', 'Procedures', 'Process', 'Publications', 'Quality Control', 'Reporting', 'Research', 'Research Design', 'Research Personnel', 'Resources', 'Running', 'Sample Size', 'Scanning', 'Site', 'Structure', 'System', 'Technology', 'Time', 'United States National Institutes of Health', 'Universities', 'Update', 'Visual', 'Visualization', 'Work', 'Writing', 'annotation  system', 'base', 'cohesion', 'computing resources', 'cost', 'data management', 'deep learning', 'design', 'dexterity', 'image archival system', 'imaging study', 'improved', 'innovation', 'learning algorithm', 'learning strategy', 'member', 'nervous system disorder', 'neurodevelopment', 'neuroimaging', 'open source', 'operation', 'quality assurance', 'research study', 'software systems', 'success', 'three-dimensional visualization', 'tool', 'web interface']",NIMH,"KITWARE, INC.",R44,2020,819293,0.007826155138637924
"Precision immunoprofiling to reveal diagnostic biomarkers of latent TB infection PROJECT SUMMARY  Tuberculosis (TB) is among the leading causes of mortality worldwide with an estimated 2 billion individuals currently infected. Latent tuberculosis infection (LTBI) is the most common form of TB infection affecting 13 million Americans. While many with LTBI remain asymptomatic, an estimated 10% of immunocompetent patients with LTBI will reactivate to active TB, and will become infectious. LTBI is treatable with a prolonged antibiotic treatment; however, potential side effects motivate the development of new diagnostic approaches that can identify with high specificity patients at the highest risk of reactivation, for who therapy would be most beneficial.  The tuberculin skin test (TST) and interferon-γ release assays (IGRAs) are commonly used for TB and LTBI screening. Both tests provide good measures of TB exposure; however, neither is effective at diagnosing LTBI (positive predictive values <5%). Moreover, neither provide any prognostic stratification based upon reactivation risk. Both the TST and IGRAs probe immunological memory to TB-related antigen challenges and we hypothesize that a more nuanced and personalized approach to monitoring immune responses to both TB- specific and non-specific antigens might reveal new approaches to LTBI diagnosis and patient stratification.  Enabling a new, individualized approach to LTBI diagnostics, we propose to combine high throughput, multiplexed inflammatory biomarker detection strategies and powerful bioinformatics tools that allow for the identification of previously obscured multi-marker diagnostic signatures of LTBI status and reactivation risk. Silicon photonic microring resonators are an enabling technology for biomarker analysis due to their intrinsic scalability and multiplexing capabilities. Applied to the detection of cytokine panels, this technology supports the rapid immune profiling of individual samples under both TB-specific and non-specific antigen stimulation conditions. Machine learning algorithms will be utilized to analyze the resulting dense data streams to facilitate selection of key diagnostic signatures forming the basis for predictive model development and deployment. This powerful analytical combination is supplemented by deep expertise in clinical diagnosis and treatment of TB and LTBI, and an enabling collaboration and connection to subjects from an international location with high TB burden and exposure in a healthcare worker population subjected to regularly-scheduled and repeated LTBI screening.  The resulting diagnostic workflow and machine learning feature selection approaches will reveal multiplexed biomarker signatures that have strong positive predictive correlation with LTBI status (+ or -). This approach will also further stratify LTBI+ subjects on the basis of reactivation potential, thus providing a fundamentally new approach to identifying subjects that are most likely to benefit from therapeutic intervention. The end result of this project will be a new precision medicine-based diagnostic strategy that is vastly superior to the current state-of-the-art and offers the potential to transform current clinical practice. PROJECT NARRATIVE Tuberculosis (TB) affects an estimated one third of the world’s population and an asymptomatic latent state of tuberculosis infection (LTBI) is extremely common. Unfortunately, there are not any good clinical tests that can definitely diagnose LTBI, making it difficult to identify patients that should be treated to prevent reactivation to active TB, which is infectious. We will integrate cutting edge measurement technologies and machine learning bioinformatic approaches to identify and test multiplexed biomarker signatures that will transform clinical TB management by enabling personalized diagnosis of LTBI and the stratification of individuals with the highest potential for reactivation.",Precision immunoprofiling to reveal diagnostic biomarkers of latent TB infection,10006790,R01AI141591,"['Affect', 'Algorithms', 'American', 'Antibiotic Therapy', 'Antibiotics', 'Antigens', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Clinical', 'Clinical Treatment', 'Collaborations', 'Complex', 'Cytokine Network Pathway', 'Detection', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Eligibility Determination', 'Generations', 'Goals', 'Gold', 'Health Personnel', 'Immune', 'Immune response', 'Immunocompetent', 'Immunologic Markers', 'Immunologic Memory', 'Immunologic Monitoring', 'Individual', 'Infection', 'Inflammatory', 'Informatics', 'Interferons', 'International', 'Location', 'Machine Learning', 'Measurement', 'Measures', 'Modeling', 'Patients', 'Peripheral', 'Peripheral Blood Mononuclear Cell', 'Plasma', 'Population', 'Predictive Value', 'Prevention strategy', 'Regimen', 'Residual state', 'Risk', 'Sampling', 'Schedule', 'Silicon', 'Specificity', 'Stratification', 'Technology', 'Testing', 'Therapeutic Intervention', 'Translations', 'Tuberculin Test', 'Tuberculosis', 'Whole Blood', 'antigen challenge', 'base', 'bioinformatics tool', 'clinical Diagnosis', 'clinical practice', 'cytokine', 'data streams', 'diagnostic accuracy', 'diagnostic biomarker', 'feature selection', 'high risk', 'immune function', 'immunoregulation', 'improved', 'individual variation', 'latent infection', 'machine learning algorithm', 'model development', 'monocyte', 'mortality', 'novel diagnostics', 'novel strategies', 'patient stratification', 'personalized approach', 'personalized diagnostics', 'photonics', 'precision medicine', 'predictive marker', 'predictive modeling', 'prevent', 'prognostic', 'prospective', 'response', 'screening', 'side effect', 'targeted treatment', 'tool', 'treatment strategy', 'tuberculosis treatment']",NIAID,UNIVERSITY OF MICHIGAN AT ANN ARBOR,R01,2020,722295,0.010981072977778233
"C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative ABSTRACT The BRAIN Initiative is designed to leverage sophisticated neuromodulation, electrophysiological recording, and macroscale neuroimaging techniques in human and non-human animal models in order to develop a multilevel understanding of human brain function. However, the necessary tools for organizing, processing and analyzing neuroimaging data generated through these efforts are not widely available as coherent and easy-to- use software packages. Gaps are particularly apparent for nonhuman data (i.e., monkey, rodent), as most of the existing processing and analytic software packages are specifically designed for human imaging. Methods have been proposed for addressing the challenges inherent to the processing of nonhuman data (e.g., brain extraction, tissue segmentation, spatial normalization, brain parcellation, temporal denoising); to date, these have not been readily integrated into an easy-to-use, robust, and reproducible analysis package. Similarly, many of the sophisticated machine learning and modeling methods developed for neuroimaging analyses are inaccessible to most researchers because they have not been integrated into easy-to-use pipeline software. As a result, translational and comparative neuroimaging researchers patch together neuroinformatics pipelines that use various combinations of disparate software packages and in-house code. We propose to extend the Configurable Pipeline for the Analysis of Connectomes (C-PAC) open-source software to provide robust and reproducible pipelines for functional and structural MRI data. We will integrate the various disparate image processing and analysis methods used to handle the challenges of nonhuman imaging data, into a single, open source, configurable, easy-to-use end-to-end analysis pipeline package that is accessible locally or via the cloud. The end product will not only improve the quality, transparency and reproducibility of nonhuman translational and comparative imaging, but also enable new avenues of scientific inquiry through our inclusion of methods that are yet to be applied to nonhuman imaging data (e.g., gradient- based cortical parcellation methods, hyperalignment). Specific aims of the proposed work include to: 1) Integrate neuroimaging processing and analysis methods optimized for BRAIN Initiative data, 2) Implement strategies for carrying out comparative studies of human and non-human populations, and 3) Extend C-PAC to include cutting-edge analytical strategies for identifying mechanisms of brain function. All development will occur “in the open” using GitHub and other collaborative tools to maximally involve participation in the C-PAC project. Annual hackathons will be held to collaborate with investigators from BRAIN Initiative awards and other neuroinformatics development projects to integrate their tools with C-PAC. Hands-on training will be held to train investigators on optimal use of the newly developed tools. NARRATIVE New neuroimaging analysis software is needed to process and analyze the various human and non-human neuroimaging data collected through the BRAIN Initiative. We will address this need by extending the already mature C-PAC human brain imaging data analysis pipeline to include support for animal data, with a particular focus on providing methods for conducting comparative studies between species. The proposed work will also include a toolbox for helping to align electrophysiological data that is commonly collected in non-human studies, with the brain imaging data.","C-PAC: A configurable, compute-optimized, cloud-enabled neuroimaging analysis software for reproducible translational and comparative",9954159,R24MH114806,"['Address', 'Adoption', 'Anatomy', 'Animal Model', 'Architecture', 'Award', 'BRAIN initiative', 'Brain', 'Brain imaging', 'Capital', 'Code', 'Communities', 'Comparative Study', 'Computer software', 'Consumption', 'Data', 'Data Set', 'Development', 'Documentation', 'Electrodes', 'Electrophysiology (science)', 'Environment', 'Funding', 'High Performance Computing', 'Human', 'Image', 'Image Analysis', 'Individual', 'Learning', 'Link', 'Location', 'Machine Learning', 'Magnetic Resonance Imaging', 'Maintenance', 'Measures', 'Methods', 'Modeling', 'Modification', 'Monkeys', 'Outcome', 'Output', 'Pattern', 'Persons', 'Population', 'Process', 'Pythons', 'Readability', 'Reproducibility', 'Research Personnel', 'Rodent', 'Scientific Inquiry', 'Software Design', 'Statistical Methods', 'Structure', 'Techniques', 'Testing', 'Text', 'Time', 'Tissues', 'Training', 'United States National Institutes of Health', 'Validity of Results', 'Work', 'analysis pipeline', 'animal data', 'base', 'behavioral phenotyping', 'cloud based', 'comparative', 'computing resources', 'connectome', 'cost', 'data analysis pipeline', 'data sharing', 'data structure', 'denoising', 'design', 'flexibility', 'graphical user interface', 'hackathon', 'human imaging', 'image processing', 'improved', 'investigator training', 'learning strategy', 'multimodality', 'neuroimaging', 'neuroinformatics', 'neuroregulation', 'open source', 'software as a service', 'structured data', 'supervised learning', 'tool', 'unsupervised learning']",NIMH,"CHILD MIND INSTITUTE, INC.",R24,2020,545239,0.012287430430504091
"Statistical Methods for Ultrahigh-dimensional Biomedical Data This proposal develops novel statistics and machine learning methods for distributed analysis of big data in biomedical studies and precision medicine and for selecting a small group of molecules that are associated with biological and clinical outcomes from high-throughput data such as microarray, proteomic, and next generation sequence from biomedical research, especially for autism studies and Alzheimer’s disease research. It focuses on developing efficient distributed statistical methods for Big Data computing, storage, and communication, and for solving distributed health data collected at different locations that are hard to aggregate in meta-analysis due to privacy and ownership concerns. It develops both computationally and statistically efficient methods and valid statistical tools for exploring heterogeneity of big data in precision medicine, for studying associations of genomics and genetic information with clinical and biological outcomes, and for feature selection and model building in presence of errors-in- variables, endogeneity, and heavy-tail error distributions, and for predicting clinical outcomes and understanding molecular mechanisms. It introduces more robust and powerful statistical tests for selection of significant genes, SNPs, and proteins in presence of dependence of data, valid control of false discovery rate for dependent test statistics, and evaluation of treatment effects on a group of molecules. The strength and weakness of each proposed method will be critically analyzed via theoretical investigations and simulation studies. Related software will be developed for free dissemination. Data sets from ongoing autism research, Alzheimer’s disease, and other biomedical studies will be analyzed by using the newly developed methods and the results will be further biologically confirmed and investigated. The research findings will have strong impact on statistical analysis of high throughput big data for biomedical research and on understanding heterogeneity for precision medicine and molecular mechanisms of autism, Alzheimer’s disease, and other diseases. This proposal develops novel statistical machine learning methods and bioinformatic tools for finding genes, proteins, and SNPs that are associated with clinical outcomes and discovering heterogeneity for precision medicine. Data sets from ongoing autism research, Alzheimer’s disease and other biomedical studies will be critically analyzed using the newly developed statistical methods, and the results will be further biologically confirmed and investigated. The research findings will have strong impact on developing therapeutic targets and understanding heterogeneity for precision and molecular mechanisms of autism, Alzheimer’s diseases, and other diseases. !",Statistical Methods for Ultrahigh-dimensional Biomedical Data,9900790,R01GM072611,"['Address', 'Alzheimer&apos', 's Disease', 'Big Data', 'Big Data Methods', 'Biological', 'Biomedical Research', 'Brain', 'Classification', 'Clinical', 'Communication', 'Computer software', 'Cox Models', 'Cox Proportional Hazards Models', 'Data', 'Data Set', 'Databases', 'Dependence', 'Dimensions', 'Disease', 'Disease Progression', 'Evaluation', 'Gene Expression', 'Gene Proteins', 'Genes', 'Genomics', 'Heterogeneity', 'Internet', 'Investigation', 'Learning', 'Linear Models', 'Location', 'Meta-Analysis', 'Methods', 'Molecular', 'Outcome', 'Ownership', 'Patients', 'Polynomial Models', 'Principal Component Analysis', 'Privacy', 'Proteins', 'Proteomics', 'Research', 'Role', 'Statistical Data Interpretation', 'Statistical Methods', 'Tail', 'Techniques', 'Testing', 'Time', 'autism spectrum disorder', 'big biomedical data', 'bioinformatics tool', 'cell type', 'computing resources', 'feature selection', 'genetic information', 'health data', 'high dimensionality', 'high throughput analysis', 'improved', 'machine learning method', 'macrophage', 'model building', 'next generation', 'novel', 'precision medicine', 'predict clinical outcome', 'simulation', 'statistical and machine learning', 'statistics', 'therapeutic target', 'tool', 'transcriptome sequencing', 'treatment effect']",NIGMS,PRINCETON UNIVERSITY,R01,2020,293003,0.00018853204253641557
"Streamlining Volumetric Imaging, Analysis and Publication Using Immersive Virtual Reality Over the past 15 years, new imaging technologies and methods for high throughput imaging have revolutionized structural biology by extending the resolution and scale of collected images in 3 dimensions. The resulting image volumes are more typically hundreds of GB to even tens of TB and in some cases approach PB sizes. These file sizes pose challenges for image acquisition, image analysis, and communication of a representative set of raw data and quantification. Image acquisition runs can be lengthy and expensive, and often errors are not identified until after the completion of scanning. Large files contain many structures, and require machine learning (ML) strategies in a context that permits error correction. Scientific communication requires tools for ready access to raw data, and more efficient methods to communicate the rapidly accumulating sets of scientific information. We propose to leverage virtual reality (VR) and verbal communication within the VR environment, to streamline each of these stages of scientific work, by capitalizing on the more natural abilities for stereoscopic vision and hearing to process scenes and language. Based upon the tool base and direct volume rendering of large files that we have established in our VR software, called syGlass, we will first integrate VR into the microscope controls for tuning the microscope and then efficiently inspecting images in 3D as they are acquired (Aim 1). Next, we will introduce novel domain adaptation techniques in the ML field to scale up 3D image quantification capabilities for current acquisition sizes, by coupling them with user-optimized experiences that do not require ML expertise, and yet provide automated and accurate results (Aim 2). Finally, we will provide tools to efficiently generate narrated scientific presentations in VR for use in the lab setting, as manuscript publications, and for production of educational materials (Aim 3). In each of these activities, we will introduce paradigm shifts in the management of experiments, analysis of the resulting data, and publication of manuscripts and materials to other scientists and the general public. The goal of this project is to speed the pipeline from image acquisition to communication of analyzed data for large image files (big data). We propose to leverage virtual reality to change the way users interact with their microscope, provide new methods for more accurate quantification and make scientific data more transparent, and more accessible to specialists and the general public. These new paradigms are applicable to basic, pre-clinical and clinical research, and serve the goals of big data projects to generate more reliable and encompassing scientific conclusions.","Streamlining Volumetric Imaging, Analysis and Publication Using Immersive Virtual Reality",10011054,R44MH125238,"['3-Dimensional', '3D virtual reality', '4D Imaging', 'Address', 'Awareness', 'Basic Science', 'Big Data', 'Clinical Research', 'Collection', 'Communication', 'Communities', 'Computer software', 'Coupling', 'Data', 'Data Analyses', 'Data Collection', 'Depth Perception', 'Educational Materials', 'Foundations', 'General Population', 'Goals', 'Hearing', 'Hour', 'Image', 'Image Analysis', 'Imaging Device', 'Imaging technology', 'Information Distribution', 'Ingestion', 'Instruction', 'Investments', 'Journals', 'Language', 'Lasers', 'Lighting', 'Machine Learning', 'Manuals', 'Manuscripts', 'Marketing', 'Methods', 'Microscope', 'Microscopy', 'Modeling', 'Modernization', 'Monitor', 'Neurosciences', 'Pathway interactions', 'Positioning Attribute', 'Process', 'Production', 'Publications', 'Publishing', 'Reporting', 'Resolution', 'Resort', 'Running', 'Scanning', 'Science', 'Scientist', 'Services', 'Specialist', 'Speed', 'Structure', 'Techniques', 'Three-Dimensional Image', 'Three-Dimensional Imaging', 'Time', 'Training', 'Visual', 'Visualization', 'Work', 'adaptation algorithm', 'base', 'data dissemination', 'data exploration', 'experience', 'experimental study', 'feature detection', 'field study', 'image processing', 'imaging modality', 'improved', 'large datasets', 'learning strategy', 'machine learning algorithm', 'movie', 'novel', 'optogenetics', 'pre-clinical research', 'scale up', 'software development', 'structural biology', 'tool', 'virtual reality', 'virtual reality environment']",NIMH,ISTOVISR,R44,2020,1159612,-0.004125482890074799
"Signature of profiling and staging the progression of TB from infection to disease. Project Summary/Abstract Tuberculosis (TB) is the leading cause of infectious disease mortality worldwide. Nearly one-third of the world's population is infected with Mycobacterium tuberculosis (MTB). More than 10.4 million new cases of active TB disease develop annually, leading to 1.4 million deaths due to the disease each year. Despite widespread efforts to study of the etiology of disease, the development and global introduction of an effective treatment regimen, and sensitive diagnostics for identifying pulmonary TB disease, efforts to control this pandemic are falling short, largely due to a lack of a clear understanding of the pathogenic progression from MTB infection to active clinical disease. In addition, Existing gene expression studies have presented more than three dozen biomarkers to predict TB related outcomes such as identifying active TB disease, predicting risk of treatment failure, or predicting which patients will progress to active TB disease. These have been developed and refined using multiple technologies and using a diverse set of computational and machine learning prediction algorithms, but most are focused on two-class comparison (e.g. TB vs. LTBI). In this proposal, we propose to compile and harmonize dozens of existing RNA-sequencing datasets for TB outcomes. We will use these compiled data to develop a computational platform and interactive visualization tools for profiling TB signatures across all existing datasets. We plan to use this curated data and software platform to develop a more refined molecular map of progression from TB infection to active disease. Consistent with a recently presented models for TB disease development, we hypothesize that we will be able to identify gene expression patterns associated with stages on the TB disease spectrum, including: uninfected or eliminated infection, controlled or truly latent infection, future progressors or incipient disease, subclinical TB disease, and active clinical TB disease. We believe that existing gene expression data and signatures will allow us to identify distinct transcriptional profiles for each stage, and hence develop a multi-class machine learning approach for classifying patients into their corresponding stage. Overall, this proposal contributes to the field by compiling existing gene expression data and developing a wholistic map of TB progression from infection to active disease. In addition, we will provide a curated dataset and metadata in an accessible format for more than three dozen existing TB studies, and allow others to access and explore these data through a user-friendly profiling platform. Project Narrative We will compile existing TB gene expression data and develop a wholistic map of TB progression from infection to active disease. We will provide curated data for dozens of existing TB RNA-sequencing datasets, and allow others to access and explore these data through a user-friendly software toolkit and platform.",Signature of profiling and staging the progression of TB from infection to disease.,10048427,R21AI154387,"['Biological Markers', 'Blood specimen', 'Cessation of life', 'Clinical', 'Communicable Diseases', 'Computer software', 'Data', 'Data Set', 'Development', 'Diagnostic', 'Disease', 'Disease Progression', 'Etiology', 'Evaluation', 'Future', 'Gene Expression', 'Gene Expression Profile', 'Genes', 'Genetic Transcription', 'Immunologic Factors', 'Individual', 'Infection', 'Language', 'Learning', 'Machine Learning', 'Maps', 'Metadata', 'Methods', 'Modeling', 'Molecular', 'Molecular Profiling', 'Mycobacterium tuberculosis', 'National Institute of Allergy and Infectious Disease', 'Outcome', 'Pathogenicity', 'Pathway interactions', 'Patients', 'Performance', 'Population', 'Pulmonary Tuberculosis', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Staging', 'Technology', 'Treatment Failure', 'Treatment Protocols', 'Tuberculosis', 'Validation', 'Visualization', 'Visualization software', 'Work', 'base', 'biomarker validation', 'comorbidity', 'computational platform', 'data harmonization', 'data tools', 'demographics', 'effective therapy', 'falls', 'genetic signature', 'genomic biomarker', 'interactive tool', 'latent infection', 'molecular modeling', 'molecular pathology', 'mortality', 'multiple datasets', 'novel', 'pandemic disease', 'prediction algorithm', 'screening', 'transcriptome sequencing', 'treatment response', 'treatment risk', 'user friendly software', 'user-friendly']",NIAID,BOSTON UNIVERSITY MEDICAL CAMPUS,R21,2020,260741,-0.00554144836213008
"Boston University CCCR OVERALL ABSTRACT The Boston University CCCR will serve as a central resource for clinical research focused mostly on the most common musculoskeletal disorders, osteoarthritis and gout and will also provide research resources for investigator based research in scleroderma, spondyloarthritis, musculoskeletal pain and osteoporosis. Center grant funding has supported 30-35 papers annually in peer reviewed journals, most in the leading arthritis journals and some in leading general medical journals. This center has trained many of the leading clinical researchers in rheumatology throughout the US and internationally, and many of these former trainees have active collaborations with the center. We will include a broad research community and a core group of faculty in this CCCR. The research community's ready access to core faculty and to the sophisticated research methods and assistance they provide will enhance the clinical and translational research of the community and will increase collaborative opportunities for the core faculty and the community. The CCCR updates BU's historical focus on epidemiologic methods to include new approaches to causal inference and adds new methods in machine learning and mobile health. The Research and Evaluation Support Core Unit (RESCU) is the focal point of this CCCR. A key feature is the weekly research (RESCU meetings in which ongoing and proposed research projects are critically evaluated. This feature ensures frequent interactions between clinician researchers, epidemiologists and biostatisticians who are the core members of the CCCR. The RESCU core unit has provided critical support for other Center grants related to rheumatic and arthritic disorders at Boston University, three current R01/U01's; five current NIH K awards (one K24, 3 K23's, one K01), an R03, an NIH trial planning grant (U34), and multiple ACR RRF awards. The overall goal of this center is to carry out and disseminate high-level clinical research informed both by state of the art clinical research methods and by clinical and biological scientific discoveries. Ultimately, we aim either to prevent the diseases we are studying or to improve the lives of those living with the diseases. NARRATIVE The Boston University Core Center for Clinical Research will provide broad clinical research methods expertise to a large multidisciplinary group of investigators whose research focuses on osteoarthritis and gout with a secondary emphasis on scleroderma, spondyloarthritis, osteoporosis and musculoskeletal pain. The group, which includes persons with backgrounds in rheumatology, physical therapy, epidemiology, biostatistics and  . behavioral science, meets weekly to critically review research projects and serves a broad research community with which it actively engages. It has been successful in publishing influential papers on the diseases of focus and in training many of the clinical research faculty in the US and internationally",Boston University CCCR,10017004,P30AR072571,"['Allied Health Profession', 'Area', 'Arthritis', 'Award', 'Behavioral Sciences', 'Biological', 'Biometry', 'Boston', 'Clinical', 'Clinical Research', 'Cohort Studies', 'Collaborations', 'Communities', 'Complement', 'Computerized Medical Record', 'Consensus', 'Consultations', 'Databases', 'Degenerative polyarthritis', 'Disease', 'Ensure', 'Environment', 'Epidemiologic Methods', 'Epidemiologist', 'Epidemiology', 'Europe', 'Evaluation', 'Excision', 'Faculty', 'Funding', 'Goals', 'Gout', 'Grant', 'Health', 'Influentials', 'Infusion procedures', 'Institutes', 'Institution', 'International', 'Journals', 'K-Series Research Career Programs', 'Machine Learning', 'Medical', 'Medical Research', 'Medical center', 'Methods', 'Musculoskeletal Diseases', 'Musculoskeletal Pain', 'New England', 'Osteoporosis', 'Outcome', 'Pain', 'Paper', 'Peer Review', 'Persons', 'Physical therapy', 'Privatization', 'Productivity', 'Public Health Schools', 'Publications', 'Publishing', 'Research', 'Research Design', 'Research Methodology', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rheumatism', 'Rheumatoid Arthritis', 'Rheumatology', 'Risk Factors', 'Schools', 'Scleroderma', 'Spondylarthritis', 'Talents', 'Training', 'Translational Research', 'United States National Institutes of Health', 'Universities', 'Update', 'base', 'clinical center', 'cohort', 'design', 'epidemiology study', 'faculty community', 'faculty research', 'improved', 'innovation', 'interdisciplinary collaboration', 'mHealth', 'machine learning method', 'medical schools', 'meetings', 'member', 'multidisciplinary', 'novel', 'novel strategies', 'patient oriented', 'prevent', 'programs', 'protocol development', 'statistical service', 'success']",NIAMS,BOSTON UNIVERSITY MEDICAL CAMPUS,P30,2020,725375,-0.019109602782406185
"Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology Project Summary  Cold Spring Harbor Laboratory (CSHL) is a private, not-for-profit institution dedicated to research and education in biology, with leading research programs in genomics, neuroscience, quantitative biology, plant biology, and cancer. Many activities at CSHL depend critically on high-performance computing resources, but at present, investigators have limited access to Graphics Processing Units (GPUs) and large-memory compute nodes. This deficiency is beginning to hamper a wide variety of biomedical research activities, particularly in the key areas of genomics, neuroscience and structural biology, where such specialty hardware is becoming essential for many important computational analyses. Here, we propose to acquire four state-of-the-art GPU nodes, each equipped with eight Nvidia Tesla V100, SXM2, 32GB GPUs, two 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, and 768 GB of RAM. A second-generation Nvidia NVLink will provide for 300 GB/s inter-GPU communication. In addition, we propose to acquire one large-memory node with 3 TB of RAM and four 20-core 2.5 GHz Intel Xeon-Gold 6248 (Cascade Lake) processors, as well as a top-of-rack 10 Gb Ethernet switch to interconnect the servers with each other and with our existing computer cluster. These new resources will enable a wide variety of innovative research across fields, with direct implications for human health. In genomics, applications will include RNA-seq read mapping; alignment, base-calling, and genome assembly for long-read sequence data; clustering of single cell RNA-seq data; analysis of transposable elements; deep-learning methods for prediction of the fitness consequences of mutations; and deep-learning methods for interpreting high-throughput mutagenesis experiments. In neuroscience, they will include analysis of multi-neuron activity recordings; analysis of mouse brain images; and artificial neural network models of the human olfactory system, of audio features, and of behavior as a function of changing motivations. In structural biology, they will include image processing and 3D reconstruction from cryo-electron microscopy data. These new compute nodes will have a primary impact on the research programs of nine major users from the CSHL faculty with substantial NIH funding. They will also impact three minor users. The new GPU and large-memory nodes will be fully integrated with a soon-to-be-upgraded high-performance computer cluster and managed by the experienced Information Technology group at CSHL, with oversight from a committee of seven faculty members and two IT staff members. Altogether, these new computational resources will substantially enhance the overall computational infrastructure at CSHL. Project Narrative  Many areas of modern biomedical research depend critically on state-of-the-art computing resources. Here we propose to acquire two types of specialty computer hardware: four Graphics Processing Unit (GPU) nodes and a large-memory compute node, both of which will be fully integrated with an existing and soon-to-be-upgraded high-performance computer cluster. These resources will meet a wide variety of computing needs across research areas at Cold Spring Harbor Laboratory, particularly in the growing areas of genomics, neuroscience, and structural biology.","Graphical Processing Units and a Large-Memory Compute Node for Applications in Genomics, Neuroscience, and Structural Biology",9939826,S10OD028632,"['3-Dimensional', 'Area', 'Behavior', 'Biology', 'Biomedical Research', 'Brain imaging', 'Communication', 'Computer Analysis', 'Cryoelectron Microscopy', 'DNA Transposable Elements', 'Data', 'Data Analyses', 'Education', 'Faculty', 'Funding', 'Generations', 'Genome', 'Genomics', 'Gold', 'Health', 'High Performance Computing', 'Human', 'Information Technology', 'Institution', 'Laboratories', 'Malignant Neoplasms', 'Memory', 'Minor', 'Motivation', 'Mus', 'Mutagenesis', 'Mutation', 'Neural Network Simulation', 'Neurons', 'Neurosciences', 'Olfactory Pathways', 'Plants', 'Privatization', 'Research', 'Research Activity', 'Research Personnel', 'Resources', 'United States National Institutes of Health', 'artificial neural network', 'base', 'computer cluster', 'computer infrastructure', 'computing resources', 'deep learning', 'experience', 'experimental study', 'fitness', 'high end computer', 'image processing', 'innovation', 'learning strategy', 'medical specialties', 'member', 'programs', 'reconstruction', 'single-cell RNA sequencing', 'structural biology', 'transcriptome sequencing']",OD,COLD SPRING HARBOR LABORATORY,S10,2020,436882,-0.009879997531437083
"Developing novel technologies that ensure privacy and security in biomedical data science research Data science holds the promise of enabling new pathways to discovery and can improve the understanding, prevention and treatment of complex disorders such as cancer, diabetes, substance abuse, etc., which are significantly on the rise. The promise of data science can be fully realized only when collected data can be collaboratively shared and analyzed. However, the widespread increases in healthcare data breaches due to inappropriate access as well as the increasing number of novel privacy attacks restrict institutions from sharing data. Indeed, in some cases, the results of the analysis can themselves lead to significant privacy harm. The success of the data commons depends on ensuring the maximal access to data, subject to all of the patient privacy requirements including those mandated by legislation, and all of the constraints of the organization collecting the data itself. While there are existing solutions that can solve parts of the problem, there are significant challenges in truly incorporating these into comprehensive working solutions that are usable by the biomedical research community, and new challenges brought on by modern techniques such as deep learning. The long-term goal of this research is to develop technologies that can holistically enable data sharing while respecting privacy and security considerations and to ensure that they are implemented in existing platforms that have widespread acceptance in the research community. Towards this, the objective of this project is to develop complementary solutions for risk inference, distributed learning, and access control that can enable different modalities of data sharing. The problems studied are general in nature and will evolve depending on research successes and new impediments that arise. The proposed program of research is significant since lack of access to biomedical data can lead to fragmentation of care, resulting in higher economic and social costs, and is a significant impediment to biomedical research. The project will result in open-source, freely available software tools that will be integrated into widely used data collection, cohort identification, and distributed analytics platforms. There are several ongoing collaborations that will serve as initial pilot customers to provide use cases, identify the requirements, evaluate results, and in general validate the developed solutions. Project Narrative Statement of Relevance to Public Health Being able to ensure privacy and security while enabling data sharing and analysis is critical to pave the way forward for public health research and improve our understanding of diseases. The proposed work will address the challenges that impede the use of data across all of the different modalities of data sharing. The integration into existing platforms will ensure that the developed models, tools, and solutions directly impact the research community and improve public health interventions.",Developing novel technologies that ensure privacy and security in biomedical data science research,9851602,R35GM134927,"['Address', 'Biomedical Research', 'Collaborations', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Collection', 'Data Commons', 'Data Science', 'Diabetes Mellitus', 'Disease', 'Economics', 'Ensure', 'Goals', 'Healthcare', 'Institution', 'Lead', 'Learning', 'Malignant Neoplasms', 'Modality', 'Modeling', 'Modernization', 'Nature', 'Pathway interactions', 'Prevention', 'Privacy', 'Public Health', 'Research', 'Risk', 'Security', 'Software Tools', 'Statutes and Laws', 'Substance abuse problem', 'Techniques', 'Technology', 'Work', 'biomedical data science', 'care fragmentation', 'cohort', 'cost', 'data sharing', 'deep learning', 'improved', 'new technology', 'novel', 'open source', 'patient privacy', 'programs', 'public health intervention', 'public health research', 'social', 'success', 'tool']",NIGMS,RUTGERS THE STATE UNIV OF NJ NEWARK,R35,2020,382108,0.02365806083150353
"Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences PROJECT SUMMARY/ABSTRACT In the era of newly emerging computational tools for data science, biostatisticians need to play a fundamental role in health sciences research. There is an urgent need to encourage US Citizens and Permanent Residents to pursue graduate training in biostatistics. The design, conduct, and analysis of clinical trials and observational studies; the setting of regulatory policy; and the conception of laboratory experiments have been shaped by the fundamental contributions of biostatisticians for decades. Advances in genomics, medical imaging technologies, and computational biology; the increasing emphasis on precision and evidence-based medicine; and the widespread adoption of electronic health records; demand the skills of biostatisticians trained to collaborate effectively in a multidisciplinary environment and to develop statistical and machine learning methods to address the challenges presented by this data-rich revolutionary era of health sciences research. The proposed summer program which includes world-renowned clinical scientists and biostatisticians from two local universities, will provide an immense opportunity for student participants to learn basic yet modern statistical methods that are critical to uncovering new insights from such big and complex biomedical data and also illustrate the potential pitfalls of confounding and bias that may arise when analyzing biomedical data. A unique feature of the proposed training program is thus to expose the participants to not only basic statistical methods but also to the topics of computer science and bioinformatics which will be invaluable in creating the multidisciplinary teams required to tackle the complex research questions that often requires multipronged approaches. The proposed six-week training program will be structured around the NIH's Translation Science Spectrum and will introduce participants to opportunities in biostatistics through the lens of the science advanced by the contributions of biostatisticians. Following an initial set of weeks on basic training of biostatistical methods, the program will culminate in a data hack-a-thon style competition in which participants will employ the statistical and scientific knowledge gained during the program to produce the most innovative, statistically-sound, scientifically-relevant and effectively-communicated response to a set of research questions. The proposed research education program will enroll up to 20 such participants from across the nation and, through lectures, field trips, and opportunities to analyze data from real health sciences, inspire them to pursue graduate training. The program will draw upon considerable past collaborations and complementary resources of two local world-renowned universities to provide participants with an unparalleled view of the field, including award-winning instructors, internationally known methodological and clinical researchers, and a local area rich in opportunities to showcase careers in biostatistics. Special efforts will be made to enroll participants from underrepresented groups. Participants will be followed after completion, and the numbers attending graduate school in statistics and pursuing biostatistics careers will be documented. PROJECT NARRATIVE Biostatisticians are indispensible contributors to health sciences research. The demand for professionals with advanced training in biostatistics is high and will continue to increase, especially with the expanding challenges posed by big biomedical data. This six week summer research education program, a joint effort of North Carolina State University and Duke University, will enroll up to 20 US citizen/permanent resident participants from across the nation in the summers of 2020-2022 and expose them to the opportunities presented by careers in biostatistics and encourage them to seek graduate training in the field.",Preparing the Next Generation of Biostatisticians in the Era of Data and Translational Sciences,9888421,R25HL147228,"['Address', 'Adoption', 'Area', 'Attention', 'Award', 'Bioinformatics', 'Biomedical Research', 'Biometry', 'Biostatistical Methods', 'Clinical', 'Collaborations', 'Communities', 'Complex', 'Computational Biology', 'Conceptions', 'Data', 'Data Science', 'Development', 'Discipline', 'Electronic Health Record', 'Enrollment', 'Ensure', 'Environment', 'Evaluation', 'Evidence Based Medicine', 'Exposure to', 'Faculty', 'Future', 'Genomics', 'Goals', 'Health Sciences', 'Health system', 'Imaging technology', 'Institution', 'International', 'Joints', 'Knowledge', 'Learning', 'Medical Imaging', 'Medical center', 'Methodology', 'Methods', 'Modeling', 'Modernization', 'Names', 'National Heart, Lung, and Blood Institute', 'North Carolina', 'Observational Study', 'Participant', 'Play', 'Policies', 'Positioning Attribute', 'Principal Investigator', 'Program Effectiveness', 'Request for Applications', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'Role', 'Schools', 'Science', 'Scientist', 'Statistical Methods', 'Strategic Planning', 'Structure', 'Students', 'Talents', 'Training', 'Training Programs', 'Translational Research', 'Translations', 'Underrepresented Groups', 'United States National Institutes of Health', 'Universities', 'analytical method', 'big biomedical data', 'career', 'career development', 'clinical trial analysis', 'cohort', 'computer science', 'computerized tools', 'data resource', 'design', 'education research', 'experience', 'field trip', 'graduate student', 'health science research', 'innovation', 'insight', 'instructor', 'interest', 'investigator training', 'laboratory experiment', 'lectures', 'lens', 'machine learning method', 'multidisciplinary', 'next generation', 'programs', 'public health research', 'recruit', 'response', 'skills', 'sound', 'statistical and machine learning', 'statistics', 'summer institute', 'summer program', 'summer research', 'tool', 'undergraduate student']",NHLBI,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,249789,-0.028533088397126205
"Center for Modeling Complex Interactions Biomedical problems are innately complex, and their solutions require input from many fields. Many centers focus on a single disease or organ system. By contrast, the Center for Modeling Complex Interactions focuses on an approach that can address many biomedical problems: team-based, interdisciplinary research centered around modeling. Our goal is to support and facilitate biomedical discovery by integrating modeling into interdisciplinary research. Modeling improves research at all stages—hypothesis formulation, experimental design, analysis, and interpretation. It provides a unifying language by which exchange of ideas can highlight commonalities and uncover unforeseen connections between problems. Formalization of ideas into this unifying language also improves rigor and reproducibility. We define modeling broadly to include everything from deterministic and stochastic mathematical approaches, to physical and computational models of three- dimensional objects, to agent-based and machine learning approaches where exact solutions are not possible. We seek to support modelers by increasing their numbers, and by giving them opportunities to play on interdisciplinary teams. We seek to support empiricists by giving them access to relevant modeling expertise, and by creating a community and a culture to facilitate interdisciplinary research. In Phase I, the Center for Modeling Complex Interactions created the intellectual, cultural, and physical environment to promote team- based, interdisciplinary research. In Phase II, we will build on that foundation by maintaining a strong interdisciplinary culture to foster collaboration among people who might otherwise never connect, and by adding additional faculty to expand our modeling expertise. We have four Aims: 1) Support faculty to carry out model-based, interdisciplinary biomedical research and increase their competitiveness for external funding. Research in the Center is carried out in the context of Working Groups—zero-barrier, interdisciplinary, goal- focused teams that meet regularly to get work done. Supported research includes three Research Projects, Pilot Projects, Modeling Access Grants, and ad hoc teams. Our comprehensive plan for proposal preparation improves grantsmanship, and our staff assists with submission and grant management. 2) Increase University of Idaho’s faculty participation in biomedical research. We will add six new faculty as a commitment to this Phase II COBRE and attract broader participation from across the University. 3) Extend the reach of the Modeling Core into new areas of modeling to capitalize on emerging opportunities. The Modeling Core accelerates interdisciplinary research by placing Core Fellows at the hub of the research community. We have added new Core Initiatives in machine learning and geospatial modeling to stimulate research in these areas with high potential for future growth. 4) Establish a path to long-term sustainability under the umbrella of the Institute for Modeling Collaboration & Innovation. The major hurdle for sustainability is to maintain a robust Modeling Core. We have developed a business plan that calls for us to diversify our funding sources to include institutional, state, and private support to supplement federal grants. Human health is determined by interactions of complex biological systems at multiple scales, from the ecological to the biophysical; these are layered with spatial and temporal variation. To decipher these systems requires predictive modeling, coupled with strong empirical work, to be guided by and to feed the models. The Center for Modeling Complex Interactions generates model-based biomedical research and connects people who might otherwise never interact, which enhances the strong interdisciplinary culture of the University of Idaho.",Center for Modeling Complex Interactions,10026000,P20GM104420,"['Address', 'Area', 'Biological', 'Biomedical Research', 'Biophysics', 'Businesses', 'Centers of Research Excellence', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Models', 'Coupled', 'Data', 'Development', 'Disease', 'Ensure', 'Experimental Designs', 'Faculty', 'Feedback', 'Formulation', 'Fostering', 'Foundations', 'Funding', 'Funding Agency', 'Future', 'Generations', 'Goals', 'Grant', 'Growth', 'Health', 'Holly', 'Home environment', 'Human', 'Human Resources', 'Idaho', 'Incubators', 'Individual', 'Infrastructure', 'Institutes', 'Interdisciplinary Study', 'Language', 'Lead', 'Machine Learning', 'Modeling', 'Outcome', 'Phase', 'Physical environment', 'Pilot Projects', 'Play', 'Population', 'Postdoctoral Fellow', 'Preparation', 'Privatization', 'Progress Reports', 'Property', 'Reproducibility', 'Research', 'Research Institute', 'Research Project Grants', 'Research Support', 'Schedule', 'Scientist', 'Structure', 'Students', 'System', 'Testing', 'Time', 'Training', 'Uncertainty', 'Universities', 'Ursidae Family', 'Work', 'base', 'biological systems', 'body system', 'complex biological systems', 'experience', 'faculty support', 'improved', 'innovation', 'insight', 'interdisciplinary approach', 'mathematical methods', 'member', 'next generation', 'novel strategies', 'physical model', 'predictive modeling', 'spatial temporal variation', 'success', 'three-dimensional modeling', 'undergraduate student', 'working group']",NIGMS,UNIVERSITY OF IDAHO,P20,2020,2172986,0.008512759797386955
"Summer Institute in Neuroimaging and Data Science Project Summary/Abstract The study of the human brain with neuroimaging technologies is at the cusp of an exciting era of Big Data. Many data collection projects, such as the NIH-funded Human Connectome Project, have made large, high- quality datasets of human neuroimaging data freely available to researchers. These large data sets promise to provide important new insights about human brain structure and function, and to provide us the clues needed to address a variety of neurological and psychiatric disorders. However, neuroscience researchers still face substantial challenges in capitalizing on these data, because these Big Data require a different set of technical and theoretical tools than those that are required for analyzing traditional experimental data. These skills and ideas, collectively referred to as Data Science, include knowledge in computer science and software engineering, databases, machine learning and statistics, and data visualization.  The Summer Institute in Data Science for Neuroimaging will combine instruction by experts in data science methodology and by leading neuroimaging researchers that are applying data science to answer scientiﬁc ques- tions about the human brain. In addition to lectures on the theoretical background of data science methodology and its application to neuroimaging, the course will emphasize experiential hands-on training in problem-solving tutorials, as well as project-based learning, in which the students will create small projects based on openly available datasets. Summer Institute in Neuroimaging and Data Science: Project Narrative The Summer Institute in Neuroimaging and Data Science will provide training in modern data science tools and methods, such as programming, data management, machine learning and data visualization. Through lectures, hands-on training sessions and team projects, it will empower scientists from a variety of backgrounds in the use of these tools in research on the human brain and on neurological and psychiatric brain disorders.",Summer Institute in Neuroimaging and Data Science,9875480,R25MH112480,"['Address', 'Adopted', 'Big Data', 'Brain', 'Brain Diseases', 'Collaborations', 'Collection', 'Communities', 'Competence', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Science Core', 'Data Set', 'Databases', 'Discipline', 'Face', 'Faculty', 'Fostering', 'Funding', 'Habits', 'Home environment', 'Human', 'Image', 'Institutes', 'Institution', 'Instruction', 'Internet', 'Knowledge', 'Learning', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mental disorders', 'Methodology', 'Methods', 'Modernization', 'Neurologic', 'Neurosciences', 'Participant', 'Positioning Attribute', 'Problem Solving', 'Psychology', 'Reproducibility', 'Research', 'Research Personnel', 'Science', 'Scientist', 'Software Engineering', 'Software Tools', 'Structure', 'Students', 'Technology', 'Testing', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Universities', 'Washington', 'base', 'biomedical data science', 'career', 'classification algorithm', 'computer science', 'connectome', 'data management', 'data visualization', 'design', 'e-science', 'experimental study', 'high dimensionality', 'insight', 'instructor', 'interdisciplinary collaboration', 'knowledge base', 'large datasets', 'lectures', 'nervous system disorder', 'neurogenetics', 'neuroimaging', 'novel', 'open source', 'prediction algorithm', 'programs', 'project-based learning', 'skills', 'statistics', 'success', 'summer institute', 'theories', 'tool']",NIMH,UNIVERSITY OF WASHINGTON,R25,2020,164298,-0.019657892222836894
"N3C & All of Us Research Program Collaborative Project Project Summary/Abstract The COVID-19 pandemic presents unprecedented clinical and public health challenges. Though institutions collect large amounts of clinical data about COVID-19 cases, these datasets individually might not be diverse enough to draw population level conclusions. Also, statistical, machine learning, and causal analyses are most successful with large-scale data beyond what is available in any given organization. To tackle this problem, NCATS introduced the National COVID Cohort Collaborative (N3C), an open science, community-based initiative to share patient level data for analysis. The initiative requires participating institutions to share information about their COVID-19 patients in a standard-driven way, including demographics, vital signs, diagnoses, laboratory results, medications, and other treatments. The data from multiple institutions will be merged and consolidated, and access will be provided to investigators through a centralized analytical platform. The COVID-19 data sharing collaboration with the N3C initiative offers a mechanism to initiate collaborations with other NIH sponsored data sharing programs, such as the All of Us Research Program (AoURP). This administrative supplement will support efforts to clean and standardize data at VCU, and to transfer it to the N3C data repository. The supplement will also assist in introducing new services at the Wright Center to support our investigators to use the N3C resources. It will also enable collaboration with the AoURP by establishing a pipeline to collect and transmit consented patients' EHR data and by building on existing community outreach pathways to recruit additional participants for the AoURP. The project will be overseen by the PI/Executive Committee and supervised by the Director of Research Informatics. Procedures and services developed at our local CTSA hub will be shared and disseminated to the CTSA network. Project Narrative NIH/NCATS has been working on the National COVID Cohort Collaborative (N3C), which aims to build a centralized national data resource to be used by the research community to study the COVID-19 pandemic and identify potential treatments as the pandemic continues to evolve. The COVID-19 data sharing collaboration with the N3C initiative also offers a mechanism to initiate collaborations with the All of Us Research Program (AoURP). This administrative supplement will support the creation and management of a data extraction and transfer pipeline to the N3C and AoURP data repositories from VCU.",N3C & All of Us Research Program Collaborative Project,10217339,UL1TR002649,"['Administrative Supplement', 'All of Us Research Program', 'COVID-19', 'COVID-19 pandemic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Outreach', 'Consent', 'Data', 'Data Analyses', 'Data Set', 'Development', 'Diagnosis', 'Disease', 'Ecosystem', 'Effectiveness', 'Funding Opportunities', 'Goals', 'Health', 'Health Status', 'Individual', 'Informatics', 'Infrastructure', 'Institution', 'Laboratories', 'Outcomes Research', 'Participant', 'Pathway interactions', 'Patients', 'Pharmaceutical Preparations', 'Population', 'Positioning Attribute', 'Procedures', 'Public Health', 'Research', 'Research Personnel', 'Resource Informatics', 'Resources', 'Services', 'Supervision', 'Testing', 'Translational Research', 'United States National Institutes of Health', 'base', 'biomedical informatics', 'clinical center', 'cohort', 'coronavirus disease', 'data resource', 'data sharing', 'data standards', 'data warehouse', 'demographics', 'design', 'improved', 'informatics infrastructure', 'innovation', 'large scale data', 'multi-site trial', 'network informatics', 'open data', 'pandemic disease', 'parent grant', 'programs', 'recruit', 'response', 'statistical and machine learning', 'tool']",NCATS,VIRGINIA COMMONWEALTH UNIVERSITY,UL1,2020,346608,0.0012713116962917452
"Intelligent deployment of containerized bioinformatics workflows on the cloud PROJECT SUMMARY Cloud computing has emerged as a promising solution to address the challenges of big data. Public cloud vendors provide computing as-a-utility enabling users to pay only for the resources that are actually used. In this application, we will develop methods and tools to enable biomedical researchers to optimize the costs of cloud computing when analyzing biomedical big data. Infrastructure-as-a-Service (IaaS) cloud provides computing as a utility, on-demand, to end users, enabling cloud resources to be rapidly provisioned and scaled to meet computational and performance requirements. In addition, dynamic intelligent allocation of cloud computing resources has great potential to both improve performance and reduce hosting costs. Unfortunately, determining the most cost-effective and efficient ways to deploy modules on the cloud is non- trivial, due to a plethora of cloud vendors, each providing different types of virtual machines with different capabilities, performance trade-offs, and pricing structures. In addition, modern bioinformatics workflows consist of multiple modules, applications and libraries, each with their own set of software dependencies. Software containers package binary executables and scripts into modules with their software dependencies. With containers that compartmentalize software dependencies, modules implemented as containers can be mixed and matched to create workflows that give identical results on any platform. The high degree of reproducibility and flexibility of software containers makes them ideal instruments for disseminating complex bioinformatics workflows. Our overarching goal is to deliver the latest technological advances in containers and cloud computing to a typical biomedical researcher with limited resources who works with big data. Specifically, we will develop a user-friendly drag-and-drop interface to enable biomedical researchers to build and edit containerized workflows. Most importantly, users can choose to deploy and scale selected modules in the workflow on cloud computing platforms in a transparent, yet guided fashion, to optimize cost and performance. Our aim is to provide a federated approach that leverages resources from multiple cloud vendors. We have assembled a team of interdisciplinary scientists with expertise in bioinformatics, cloud and distributed computing, and machine learning. As part of this application, we will work closely with end users who routinely generate and analyze RNA-seq data. We will illustrate how our containerized, cloud-enabled methods and tools will benefit bioinformatics analyses. Project Narrative Cloud computing has emerged as a promising solution to address the challenge of analyzing diverse and massive data generated to advance our understanding of health and diseases. We will develop methods and tools to build and intelligently deploy modular and cloud-enabled bioinformatics workflows. These tools will allow the biomedical community to optimize the costs associated with cloud computing and to facilitate the replication of scientific results.",Intelligent deployment of containerized bioinformatics workflows on the cloud,9856493,R01GM126019,"['Address', 'Big Data', 'Bioinformatics', 'Case Study', 'Cloud Computing', 'Cloud Service', 'Communities', 'Complex', 'Computer software', 'Custom', 'Data', 'Data Analyses', 'Data Storage and Retrieval', 'Dependence', 'Development', 'Disease', 'Docking', 'Documentation', 'Drops', 'Drug toxicity', 'Educational Materials', 'Ensure', 'Feedback', 'Generations', 'Goals', 'Health', 'Hospitals', 'Image', 'Infrastructure', 'Intelligence', 'Libraries', 'Machine Learning', 'Manuals', 'Methods', 'Modeling', 'Modernization', 'Performance', 'Price', 'Privatization', 'RNA analysis', 'Reproducibility', 'Research Personnel', 'Resources', 'Schedule', 'Scientist', 'Services', 'Software Tools', 'Structure', 'Technical Expertise', 'Technology Transfer', 'Testing', 'Time', 'Vendor', 'Work', 'base', 'big biomedical data', 'biomedical scientist', 'cloud platform', 'cluster computing', 'computational platform', 'computing resources', 'cost', 'cost effective', 'data exchange', 'distributed data', 'expectation', 'flexibility', 'graphical user interface', 'improved', 'instrument', 'outreach', 'predictive modeling', 'prototype', 'tool', 'tool development', 'transcriptome sequencing', 'user-friendly', 'virtual machine', 'web site']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,339098,0.02091435468128292
"Meta-analysis in human brain mapping This is the competing renewal of R01MH074457-13, which sustains the BrainMap Project (www.brainmap.org). The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data sets, metadata, computational tools, and related resources that enable coordinate-based meta-analyses (CBMA), meta-analytic connectivity modeling (MACM), meta-data informed interpretation (“decoding”) of imaging results, and meta-analytic priors for mining (including machine learning) primary (per-subject) neuroimaging data. To date, the BrainMap Project has designed and populated two coordinate-based databases: 1) a task-activation repository (TA DB); and, 2) a voxel-based morphometry repository (VBM DB). The TA DB contains >17,200 experiments, collectively representing > 78,000 subjects and > 110 task- activation paradigms. The VBM DB contains > 3,100 experiments, collectively representing > 81,000 subjects with > 80 psychiatric, neurologic and developmental disorders with ICD-10 coding. The BrainMap Project has created, optimized and validated an integrated pipeline of multi-platform (Javascript), open-access tools to curate (Scribe), filter and retrieve (Sleuth), analyze (GingerALE), visualize (Mango) and interpret analysis output (BrainMap meta-data plugins for Mango). Several network-modeling approaches have been applied to BrainMap data -- MACM, independent components analysis (ICA), graph theory modeling (GTM), author-topic modeling (ATM), structural equation modeling (SEM), and connectivity-based parcellation (CBP) – but none are yet pipeline components. Utilization of these CBMA resources is substantial: BrainMap software, data and meta-data have been used in > 825 peer-reviewed publications. Of these, > 350 were published within the current funding period (April 2015-March 2019; brainmap.org/pubs). In this competing renewal, four tool- development aims are proposed, each of which extends this high-impact research resource. Aim 1. Database Expansion. BrainMap data repositories will be expanded. Aim 2. Meta-analytic Network Modeling. Network modeling will be added to the BrainMap pipeline. Aim 3. Large-Scale Simulations, Comparisons and Validations. Data simulations, characterizations and validations will be performed. Aim 4. Meta-data Inferential tools. Tools for mining BrainMap’s location-linked meta-data will be expanded. Data Sharing Plan. BrainMap data, meta-data, pipeline tools, and templates created by whole-database modeling (e.g., ICA and ATM network masks) are shared at BrainMap.org. Of all new data entries, more than half are contributed by BrainMap users, i.e., community data sharing via BrainMap.org. For community-coded entries, the BrainMap team provides curation and quality control. Comprehensive database images (database dumps) are available to tool developers through Collaborative Use Agreements. The overall goal of the BrainMap Project is to provide the human neuroimaging community with curated data  sets, metadata, computational tools, and related resources that enable coordinate-­based meta-­analyses  (CBMA), meta-­analytic connectivity modeling (MACM), meta-­data informed interpretation (“decoding”) of  imaging results, and meta-­analytic priors for mining (including machine learning) primary (per-­subject)  neuroimaging data.    ",Meta-analysis in human brain mapping,10056029,R56MH074457,"['Agreement', 'Area', 'Brain', 'Brain Mapping', 'Code', 'Collaborations', 'Communities', 'Computer software', 'Data', 'Data Set', 'Databases', 'Disease', 'Educational workshop', 'Equation', 'Functional disorder', 'Funding', 'Goals', 'Guidelines', 'Human', 'Image', 'Institution', 'International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10)', 'Internet', 'Java', 'Link', 'Location', 'Machine Learning', 'Mango - dietary', 'Masks', 'Mental disorders', 'Meta-Analysis', 'Metadata', 'Methods', 'Mining', 'Modeling', 'Online Systems', 'Output', 'Peer Review', 'Plug-in', 'Publications', 'Publishing', 'Quality Control', 'Research Domain Criteria', 'Resources', 'Rest', 'Site', 'Software Framework', 'Specificity', 'Structure', 'Training', 'Universities', 'Validation', 'base', 'candidate marker', 'computerized tools', 'data pipeline', 'data sharing', 'data warehouse', 'design', 'developmental disease', 'experimental study', 'graph theory', 'independent component analysis', 'interest', 'large scale simulation', 'morphometry', 'nervous system disorder', 'network architecture', 'network models', 'neuroimaging', 'neuropsychiatric disorder', 'repository', 'simulation', 'tool', 'tool development']",NIMH,UNIVERSITY OF TEXAS HLTH SCIENCE CENTER,R56,2020,543396,-0.003642847603753165
"DOCKET: accelerating knowledge extraction from biomedical data sets Component type: This Knowledge Provider project will continue and significantly extend work done by the Translator Consortium Blue Team, focusing on deriving knowledge from real-world data through complex analytic workflows, integrated to the Translator Knowledge Graph, and served via tools like Big GIM and the Translator Standard API. The problem: We aim to solve the “first mile” problem of translational research: how to integrate the multitude of dynamic small-to-large data sets that have been produced by the research and clinical communities, but that are in different locations, processed in different ways, and in a variety of formats that may not be mutually interoperable. Integrating these data sets requires significant manual work downloading, reformatting, parsing, indexing and analyzing each data set in turn. The technical and ethical challenges of accessing diverse collections of big data, efficiently selecting information relevant to different users’ interests, and extracting the underlying knowledge are problems that remain unsolved. Here, we propose to leverage lessons distilled from our previous and ongoing big data analysis projects to develop a highly automated tool for removing these bottlenecks, enabling researchers to analyze and integrate many valuable data sets with ease and efficiency, and making the data FAIR [1]. Plan: (AIM 1) We will analyze and extract knowledge from rich real-world biomedical data sets (listed in the Resources page) in the domains of wellness, cancer, and large-scale clinical records. (AIM 2) We will formalize methods from Aim 1 to develop DOCKET, a novel tool for onboarding and integrating data from multiple domains. (AIM 3) We will work with other teams to adapt DOCKET to additional knowledge domains. ■ The DOCKET tool will offer 3 modules: (1) DOCKET Overview: Analysis of, and knowledge extraction from, an individual data set. (2) DOCKET Compare: Comparing versions of the same data set to compute confidence values, and comparing different data sets to find commonalities. (3) DOCKET Integrate: Deriving knowledge through integrating different data sets. ■ Researchers will be able to parameterize these functions, resolve inconsistencies, and derive knowledge through the command line, Jupyter notebooks, or other interfaces as specified by Translator Standards. ■ The outcome will be a collection of nodes and edges, richly annotated with context, provenance and confidence levels, ready for incorporation into the Translator Knowledge Graph (TKG). ■ All analyses and derived knowledge will be stored in standardized formats, enabling querying through the Reasoner Std API and ingestion into downstream AI assisted machine learning. ■ Example questions this will allow us to address include: (Wellness) Which clinical analytes, metabolites, proteins, microbiome taxa, etc. are significantly correlated, and which changing analytes predict transition to which disease? [2,3] (Cancer) Which gene mutations in any of X pathways are associated with sensitivity or resistance to any of Y drugs, in cell lines from Z tumor types? (All data sets) Which data set entities are similar to this one? Are there significant clusters? What distinguishes between the clusters? What significant correlations of attributes can be observed? How can this set of entities be expanded by adding similar ones? How do these N versions of this data set differ, and how stable is each knowledge edge as the data set changes over time? Collaboration strengths: Our team has extensive experience with biomedical and domainagnostic data analytics, integrating multiple relevant data types: omics, clinical measurements and electronic health records (EHRs). We have participated in large collaborative consortia and have subject matter experts willing to advise on proper data interpretation. Our application synergizes with those of other Translator teams (see Letters of Collaboration). Challenges: Data can come in a bewildering diversity of formats. Our solution will be modular, will address the most common formats first, and will leverage established technologies like DataFrames and importers (like pandas.io) where possible. Mapping nodes and edge types onto standard ontologies is crucial for knowledge integration; we will collaborate with the Standards component to maximize success. n/a",DOCKET: accelerating knowledge extraction from biomedical data sets,10057127,OT2TR003443,"['Address', 'Big Data', 'Cell Line', 'Clinical', 'Collaborations', 'Collection', 'Communities', 'Complex', 'Data', 'Data Analyses', 'Data Analytics', 'Data Set', 'Disease', 'Electronic Health Record', 'Ethics', 'FAIR principles', 'Gene Mutation', 'Individual', 'Ingestion', 'Knowledge', 'Knowledge Extraction', 'Letters', 'Location', 'Machine Learning', 'Malignant Neoplasms', 'Manuals', 'Measurement', 'Methods', 'Ontology', 'Outcome', 'Pathway interactions', 'Pharmaceutical Preparations', 'Process', 'Proteins', 'Provider', 'Records', 'Research', 'Research Personnel', 'Resistance', 'Resources', 'Specific qualifier value', 'Standardization', 'Technology', 'Time', 'Translational Research', 'Work', 'experience', 'indexing', 'interest', 'interoperability', 'knowledge graph', 'knowledge integration', 'large datasets', 'microbiome', 'novel', 'success', 'tool', 'tumor']",NCATS,INSTITUTE FOR SYSTEMS BIOLOGY,OT2,2020,609068,0.003266222819648691
"Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale Project Summary This project aims to leverage the best of both computational and human expertise in neuronal reconstruction towards the goal of accelerating global neuroscience discovery from internationally-sourced imaging data. We propose to create a cloud-based unified platform for converging 3-dimensional images of neurons onto a single analysis platform to (1) train and grow a new expert community of global reconstructors to work across the data from these groups, to (2) generate a community-sourced neuronal reconstruction database of open imaging data that can be incorporated into a 3-dimensional map of neuronal interconnectivity - onto which (3) novel annotations and more complex functional and molecular data can be overlaid. Our approach will evolve with the growing needs of the neuroscience community over time. To do this, in Aim One (Neuronal Reconstruction at Scale), we will test if the newly developed crowd-sourced game-based platform Mozak can develop a collective of new human experts at scale, capable of accelerating the rate of current reconstruction by at least an order of magnitude, at the same time as increasing the robustness, quality and unbiasedness of the final reconstructions. In Aim Two (Robust Multi-Purpose Annotation), we will enhance basic neuronal reconstruction by adding specific semantic annotation— including soma volume and morphological quantification, volumetric analysis, and ongoing features (e.g. dendritic spines, axonal varicosities) requested from the neuroscience community. Experienced and high-ranking members will be given the opportunity to advance through increasingly complex neurons into full arbor brain-wide neuronal projections and multiple clustered groups of neurons in localized circuits. Finally, in Aim 3 (Creation of a Research-Adaptive Data Repository), we aim to develop a database of neuronal images reconstructed using the Mozak interface that will directly serve the general and specific needs of different research groups. Our goal is to make this database dynamically adaptive — as new research questions will invariably bring new needs for additional annotations and cross-referencing with other data modalities. This highquality unbiased processing repository will also be perfectly suited for training sets for automated algorithms, and the generation of a 3-dimensional maps such as Allen Institute for Brain Science (AIBS) common coordinate framework. We expect that the computational reconstruction methods will further improve with the new large corpus of “gold standard” reconstructions. Collectively, the completion of these three aims will create an analysis suite as well as an online community of experts capable of performing in depth analysis of large-scale datasets that will significantly accelerate neuroscience research, enhance machine learning for reconstruction analysis, and create a common platform of baseline neuronal morphology data against which aberrantly functioning neurons can be analyzed. Project Narrative  This project will create a new central nexus point for neuronal reconstruction and semantic annotation (Mozak) that can be used by all research labs via an accessible online portal. We will develop a new cadre of neuronal reconstruction experts that will— in conjunction with automated tools that are enhanced by their work — drastically increase the volume, quality and robustness of neuron reconstructions and annotations. Mozak reconstructions will be shared with existing repositories and will be continually updated and re-annotated based on emerging needs of research - ensuring perpetual relevance, and allowing us to generate a platform to establish the range of “baseline” 3-dimensional readouts of neuronal morphology against which diseased or malfunctioning neurons can be analyzed and understood. 1",Mozak: Creating an Expert Community to accelerate neuronal reconstruction at scale,10005472,R01MH116247,"['3-Dimensional', 'Adopted', 'Algorithms', 'Area', 'Axon', 'Brain', 'Characteristics', 'Classification', 'Communities', 'Complex', 'Computers', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Set', 'Data Sources', 'Databases', 'Dendritic Spines', 'Disease', 'Ensure', 'Future', 'Gap Junctions', 'Generations', 'Goals', 'Gold', 'Guidelines', 'Human', 'Image', 'Imaging technology', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Machine Learning', 'Manuals', 'Maps', 'Methods', 'Modality', 'Molecular', 'Morphology', 'Neurons', 'Neurosciences', 'Neurosciences Research', 'Outcome', 'Output', 'Process', 'Research', 'Science', 'Semantics', 'Slice', 'Source', 'Standardization', 'Structure', 'Techniques', 'Testing', 'Three-dimensional analysis', 'Time', 'Training', 'Update', 'Variant', 'Varicosity', 'Work', 'automated algorithm', 'base', 'citizen science', 'cloud based', 'crowdsourcing', 'data warehouse', 'experience', 'improved', 'large scale data', 'member', 'neuronal cell body', 'novel', 'online community', 'petabyte', 'programs', 'reconstruction', 'repository', 'tool', 'two-dimensional', 'web portal']",NIMH,UNIVERSITY OF WASHINGTON,R01,2020,628430,-0.03923074850331925
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS) Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health.",New Jersey Alliance for Clinical Translational Science: NJ ACTS,9890029,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'experience', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,4640627,-0.0008730362223495822
"BRAIN INITIATIVE RESOURCE: DEVELOPMENT OF A HUMAN NEUROELECTROMAGNETIC DATA ARCHIVE AND TOOLS RESOURCE (NEMAR) To take advantage of recent and ongoing advances in intensive and large-scale computational methods, and to preserve the scientific data created by publicly funded research projects, data archives must be created as well as standards for specifying, identifying, and annotating deposited data. The value of and interest in such archives among researchers can be greatly increased by adding to them an active computational capability and framework of analysis and search tools that support further analysis as well as larger scale meta-analysis and large scale data mining. The OpenNeuro.org archive, begun as a repository for functional magnetic resonance imaging (fMRI) data, is such an archive. We propose to build a gateway to OpenNeuro for human electrophysiology data (EEG and MEG, as well as intracranial data recorded from clinical patients to plan brain surgeries or other therapies) – herein we refer to these modalities as neuroelectromagnetic (NEM) data. The Neuroelectromagnetic Data Archive and Tools Resource (NEMAR) at the San Diego Supercomputer Center will act as a gateway to OpenNeuro for NEM data research. Such data uploaded to NEMAR at SDSC will be deposited in the OpenNeuro archive. Still- private NEM data in OpenNeuro will, on user request, be copied to the NEMAR gateway for further user processing using the XSEDE high-performance resources at SDSC in conjunction with The Neuroscience Gateway (nsgportal.org), a freely available and easy to use portal to use of high-performance computing resources for neuroscience research. Publicly available OpenNeuro NEM data will be able to be analyzed by running verified analysis applications on the OpenNeuro system. In this project we will build an application to evaluate the quality of uploaded NEM data, and another to visualize the data, for EEG and MEG at both the scalp and brain source levels, including time-domain and frequency-domain dynamics time locked to sets of experimental events learned from the BIDS- and HED-formatted data annotations. The NEMAR gateway will take a major step toward applying machine learning methods to a large store of carefully collected and stored human electrophysiologic brain data to spur new developments in basic and clinical brain research. The NEMAR gateway to the OpenNeuro.org human neuroimaging data archive will build tools to add human electrical and magnetic brain activity records to the archive, to evaluate its quality for users and visualize its features. The resulting facility will allow applications of new machine learning methods to research on human brain dynamics that can be expected to lead to breakthroughs in understanding how the human brain supports our awareness and behavior in both health and disease.",BRAIN INITIATIVE RESOURCE: DEVELOPMENT OF A HUMAN NEUROELECTROMAGNETIC DATA ARCHIVE AND TOOLS RESOURCE (NEMAR),9985198,R24MH120037,"['Archives', 'Awareness', 'BRAIN initiative', 'Base of the Brain', 'Behavior', 'Brain', 'Brain imaging', 'Cell Nucleus', 'Clinical', 'Cloud Computing', 'Communities', 'Computing Methodologies', 'Custom', 'Data', 'Data Analyses', 'Data Set', 'Data Storage and Retrieval', 'Deposition', 'Descriptor', 'Development', 'Diagnosis', 'Disease', 'Documentation', 'Educational workshop', 'Electrophysiology (science)', 'Engineering', 'Environment', 'Evaluation', 'Event', 'Frequencies', 'Functional Magnetic Resonance Imaging', 'Funding', 'Grant', 'Health', 'High Performance Computing', 'Human', 'Infrastructure', 'Internet', 'Laboratories', 'Lead', 'Libraries', 'Magnetic Resonance', 'Magnetism', 'Magnetoencephalography', 'Meta-Analysis', 'Methods', 'Mining', 'Modality', 'Neurosciences', 'Neurosciences Research', 'Patients', 'Performance', 'Privatization', 'Process', 'Quality Control', 'Records', 'Reproducibility', 'Research', 'Research Personnel', 'Research Project Grants', 'Resource Development', 'Resources', 'Running', 'Scalp structure', 'Science', 'Source', 'Specific qualifier value', 'Spottings', 'Staging', 'System', 'Testing', 'Time', 'United States National Institutes of Health', 'Validation', 'Visualization software', 'base', 'brain research', 'brain surgery', 'built environment', 'computational platform', 'computerized data processing', 'computing resources', 'cyber infrastructure', 'data analysis pipeline', 'data archive', 'data curation', 'data format', 'data mining', 'data quality', 'data structure', 'data submission', 'data tools', 'hackathon', 'interest', 'large scale data', 'machine learning method', 'neuroimaging', 'preservation', 'repository', 'response', 'sensor', 'structured data', 'supercomputer', 'support tools', 'tool', 'tool development', 'web interface', 'web services', 'web site']",NIMH,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",R24,2020,900723,-0.018185127008561487
"Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols Project Summary Biological assays are the foundation for developing chemical probes and drugs, but new Big Data approaches – which have revolutionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that scientists specify their assays through text descriptions written in scientific English, which need to be translated into standardized annotations readable by computers. This lack of standardized and machine-readable assay descriptions is a major impediment to manage, find, aggregate, compare, re-use, and learn from the ever-growing corpus of assays (e.g., >1.2 million in PubChem). Thus, there is a critical need for better annotation and curation tools for drug discovery assays. However, the process to go from a simple text protocol to highly detailed machine-readable semantic annotations is not trivial. Multiple tools and technologies are required: ontologies or the structured controlled vocabularies; templates that map specific vocabularies to properties that are to be captured; and software tools to actually apply these ontologies to a given text. Currently, each of these exists in isolation; yet, a bottleneck in any one tool or technology, or a gap between the different pieces, disrupts the overall process, resulting in poor or no annotation of the datasets. Here we propose a project to combine and integrate these three technologies (which are also the core competencies of the three groups collaborating on this proposal). We will deliver a novel, comprehensive, user-friendly data annotation and curation system that is highly interconnected, encompassing the full cycle, and real-world practice, of required tasks and decisions, by all parties within the `bioassay annotation ecosystem' (researchers performing curation, dedicated curators, IT specialists, ontology owners, and librarians/repositories). The alliance between academic and commercial collaborators, who already work together, will greatly benefit the project and minimize execution risk. Our specific aims are to: (1) Develop a bioassay-specific template editor and templates by adopting the Stanford (Center for Expanded Data Annotation and Retrieval, CEDAR) data model to the machine learning-based curation tool BioAssay Express, to exploit the broad functionality of its data structures, tools and interfaces; (2) Define and create an ontology update process and tool (`OntoloBridge') to support rapid feedback between curators/users and ontology experts and enable semi-automated incorporation of suggestions for updates to existing published ontologies; (3) Develop new tools to export annotated data into public repositories such as PubChem; and (4) Evaluate our solution across diverse audiences (pharma, academia, repositories). The system will improve bioassay curation efficiency, quality, and effectiveness, enabling scientists to generate standardized annotations for their experiments to make these data FAIR (Findable, Accessible, Interoperable, Reusable). We envision this suite of tools will encourage annotation earlier in the data lifecycle while still supporting annotation at later stages (e.g., submission to repositories or to journals). Project Narrative Biological assays are the foundation for developing drugs, but new Big Data approaches – which have revolu- tionized other areas of biomedical science – have not yet advanced this early step of biomedical research: analysis of assay data. The obstacle is that assays are written in scientific English, which need to be translated into standardized descriptions readable by computers. This lack of machine-readable annotations is a major impediment to manage, find, compare, re-use, and learn from the millions of assays. This project will develop a formal process and integrated tools to support the complete cycle of tasks and decisions required for bioassay annotation, enabling expedited (and more cost-effective) drug discovery.","Unifying Templates, Ontologies and Tools to Achieve Effective Annotation of Bioassay Protocols",9979969,U01LM012630,"['Academia', 'Address', 'Adopted', 'Adoption', 'Area', 'Big Data', 'Big Data Methods', 'Biological Assay', 'Biomedical Research', 'Chemicals', 'Communication', 'Communities', 'Competence', 'Complex', 'Computer software', 'Computers', 'Controlled Vocabulary', 'Custom', 'Data', 'Data Set', 'Data Storage and Retrieval', 'Ecosystem', 'Effectiveness', 'Elements', 'Ensure', 'Estrogen receptor positive', 'Exercise', 'FAIR principles', 'Feedback', 'Foundations', 'Hour', 'Journals', 'Learning', 'Librarians', 'Machine Learning', 'Manuals', 'Maps', 'Metadata', 'Ontology', 'Output', 'Participant', 'Pharmaceutical Preparations', 'Polishes', 'Problem Solving', 'Process', 'Property', 'Protocols documentation', 'PubChem', 'Publishing', 'Readability', 'Research', 'Research Personnel', 'Retrieval', 'Risk', 'Science', 'Scientist', 'Semantics', 'Site', 'Software Engineering', 'Software Tools', 'Specialist', 'Specific qualifier value', 'Standardization', 'Structure', 'Suggestion', 'System', 'Technology', 'Testing', 'Text', 'Time', 'Translating', 'Tweens', 'Update', 'Vocabulary', 'Work', 'base', 'cost effective', 'data modeling', 'data standards', 'design', 'drug discovery', 'drug mechanism', 'experience', 'experimental study', 'improved', 'improved functioning', 'in vivo', 'informatics training', 'novel', 'ontology development', 'open source', 'practical application', 'predictive modeling', 'public repository', 'repository', 'structured data', 'tool', 'user-friendly']",NLM,UNIVERSITY OF MIAMI SCHOOL OF MEDICINE,U01,2020,511367,0.008085287265877551
"A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive Project Summary The Brain Research through Advancing Innovative Neurotechnologies (BRAIN) Initiative promotes the development and application of technologies to describe the temporal and spatial dynamics of cell types and neural circuits in the brain. The Principal Investigator, senior personnel and staff of this project have diverse expertise required to marshall data across the BRAIN Initiative consortium, including experience in data collection from multiple institutions, large-scale quality control and analysis processing capability, familiarity with NIH policy and public archive deposition strategies. To promote smooth interactions across a large research consortium, we will develop the Neuroscience Multi-Omic Archive (NeMO Archive), a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects. We will utilize a federated model for data storage such that the physical location of data can be distributed between the NeMO local file system, public repositories, and a cloud-based storage system (e.g. Amazon S3). We will leverage this capability and distribute BRAIN Initiative data between our local filesystem and the cloud. The Nemo Archive will be a data resource consistent with the principles advanced by research community members who are launching resources in next generation NIH data ecosystem. These practices include FAIR Principles, documentation of APIs, data-indexing systems, workflow sharing, use of shareable software pipelines and storage on cloud-based systems. The information incorporating into the NeMO archive will, in part, enable understanding of 1) genomic regions associated with brain abnormalities and disease; 2) transcription factor binding sites and other regulatory elements; 3) transcription activity; 4) levels of cytosine modification; and 5) histone modification profiles and chromatin accessibility. It will enable users to answer diverse questions of relevance to brain research, such as identifying diagnostic candidates, predicting prognosis, selecting treatments, and testing hypotheses. It will also provide the basic knowledge to guide the development and execution of predictive and machine learning algorithms in the future.   Project Narrative The Neuroscience Multi-Omic Archive (NeMO Archive) is a data repository that is specifically focused on the storage and dissemination of omic data from the BRAIN Initiative and related brain research projects.",A BRAIN Initiative Resource: The Neuroscience Multi-omic Data Archive,9989180,R24MH114788,"['Archives', 'Atlases', 'BRAIN initiative', 'Binding Sites', 'Bioconductor', 'Brain', 'Brain Diseases', 'Chromatin', 'Communities', 'Computer software', 'Cytosine', 'Data', 'Data Collection', 'Data Coordinating Center', 'Data Management Resources', 'Data Storage and Retrieval', 'Databases', 'Deposition', 'Development', 'Diagnostic', 'Docking', 'Documentation', 'Elements', 'Ensure', 'Familiarity', 'Future', 'Generations', 'Genetic Transcription', 'Genomic Segment', 'Individual', 'Institution', 'Internet', 'Knowledge', 'Location', 'Metadata', 'Modeling', 'Modification', 'Multiomic Data', 'Neurosciences', 'Patients', 'Personnel Staffing', 'Phenotype', 'Policies', 'Principal Investigator', 'Procedures', 'Process', 'Quality Control', 'Regulatory Element', 'Reproducibility', 'Research', 'Research Project Grants', 'Resources', 'Running', 'Services', 'Site', 'Standardization', 'System', 'Technology', 'Testing', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analysis pipeline', 'base', 'brain abnormalities', 'brain research', 'cell type', 'cloud based', 'cloud storage', 'complex R', 'computerized data processing', 'data archive', 'data centers', 'data ecosystem', 'data ingestion', 'data integration', 'data pipeline', 'data resource', 'data standards', 'data submission portal', 'data visualization', 'data warehouse', 'database structure', 'experience', 'experimental study', 'histone modification', 'indexing', 'machine learning algorithm', 'member', 'multiple data types', 'multiple omics', 'neural circuit', 'next generation', 'online resource', 'operation', 'outcome forecast', 'programs', 'public repository', 'query tools', 'repository', 'tool', 'transcription factor', 'web site', 'working group']",NIMH,UNIVERSITY OF MARYLAND BALTIMORE,R24,2020,1263611,-0.012380558661627178
"New Jersey Alliance for Clinical Translational Science: NJ ACTS Contact PD/PI: Panettieri, Reynold Alexander Project Summary/Abstract Overview Coordinated by Rutgers Biomedical and Health Sciences (RBHS), the New Jersey Alliance for Clinical and Translational Science (NJ ACTS) comprises a consortium with Rutgers and Princeton Universities (PU), NJ Institute for Technology (NJIT), medical, nursing, dental and public health schools, hospitals, community health centers, outpatient practices, industry, policymakers and health information exchanges. All Alliance universities and affiliates have provided substantial resources and contributed to the planning, development and leadership of the consortium. With access to ~7 million people, NJ ACTS serves as a ‘natural laboratory’ for translational and clinical research. With a state population of ~9 million, New Jersey ranks 11th in the US, 1st in population density and higher than average in racial and ethnic diversity. Surprisingly, NJ has no CTSA Hub to coordinate translational and clinical research. Our CTSA Hub focuses on two overarching themes: the heterogeneity of disease pathogenesis and response to treatment, and the value of linking large clinical databases with interventional clinical investigations to identify cause-and-effect and predict therapeutic responses. NJ ACTS will provide: innovative approaches to link information from large databases and electronic health records to inform clinical trial design, execution and analysis; and novel platforms for biomarker discovery using fluorescence in situ hybridization and machine learning to identify unique neural signatures of chronic illness. NJ ACTS will access a large health system with significant member diversity; a rich legacy of community engagement and community-based research platforms; and proven approaches to enhance workforce development in clinical research. With a substantial investment in streamlining research administration and IRB practices at Rutgers and with the inception of NJ ACTS, there exists an unparalleled opportunity for logarithmic growth in clinical research in New Jersey. To build our capacity for participant and clinical interactions as a CTSA Hub, the newly established Trial Accelerator and Recruitment Office will coordinate feasibility assessment, implementation, recruitment, and evaluation of clinical studies. Additionally, our organization of five clinical research units into a cohesive network provides extraordinary expertise in strategic locations to enhance participant recruitment from diverse communities with a particular focus on: children; the elderly; those with serious mental illness or substance abuse issues; low-income individuals served by Medicaid; those with HIV/AIDS; and people of all ages who are minorities, underserved, and victims of health and environmental disparities. With a history of collaboration, partners and affiliates share unique skills, expertise, training and mentoring capabilities that will be greatly amplified within the infrastructure of a CTSA Hub. Princeton and NJIT, without medical schools or hospital affiliates, seeks collaboration with Rutgers to provide clinical research platforms; Rutgers seeks the PU and NJIT expertise in novel informatics platforms, expertise in natural language and ontology, machine learning and cognitive neurosciences. Together NJ ACTS will provide an alliance that will catalyze clinical research and training across New Jersey to improve population health and contribute to the CTSA Consortium. In this revised application, the overall themes remain unchanged but Cores leadership and direction has been markedly refined. Page 337 Project Summary/Abstract Contact PD/PI: Panettieri, Reynold Alexander New Jersey Alliance for Clinical and Translational Science (NJ ACTS)  Project Narrative The New Jersey Alliance for Clinical and Translational Science (NJ ACTS), as a member of the CTSA Consortium, unites Rutgers University, Princeton University, the New Jersey Institute of Technology, clinical, community and industry partners in a shared vision to make New Jersey a healthier state. Building on New Jersey’s already significant capabilities to promote and facilitate clinical and translational research, NJ ACTS will serve as a catalyst, inspiring new approaches to diagnose and manage disease, and fostering career development of the next generation of translational researchers, and promoting population health. Page 338 Project Narrative",New Jersey Alliance for Clinical Translational Science: NJ ACTS,10201004,UL1TR003017,"['AIDS/HIV problem', 'Address', 'Affect', 'Age', 'Asian Indian', 'Behavioral', 'Biometry', 'Child', 'Chronic Disease', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Clinical Trials Design', 'Cohort Studies', 'Collaborations', 'Communities', 'Cuban', 'Databases', 'Dental', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline of Nursing', 'Disease Management', 'Diverse Workforce', 'Elderly', 'Electronic Health Record', 'Evaluation', 'Fluorescent in Situ Hybridization', 'Fostering', 'Foundations', 'Government', 'Growth', 'Health', 'Health Insurance Portability and Accountability Act', 'Health Sciences', 'Health system', 'Healthcare', 'Hospitals', 'Image', 'Improve Access', 'Individual', 'Industry', 'Informatics', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Intervention', 'Investigation', 'Investments', 'Laboratories', 'Leadership', 'Life Style', 'Link', 'Location', 'Longevity', 'Low income', 'Machine Learning', 'Medicaid', 'Medical', 'Mentors', 'Methodology', 'Methods', 'Minority', 'Minority Groups', 'Mission', 'Muslim population group', 'Neighborhood Health Center', 'New Jersey', 'Not Hispanic or Latino', 'Ontology', 'Oral health', 'Outpatients', 'Parents', 'Participant', 'Pathogenesis', 'Patient Recruitments', 'Perception', 'Population', 'Population Density', 'Precision therapeutics', 'Prediction of Response to Therapy', 'Preventive Intervention', 'Process', 'Public Health', 'Public Health Schools', 'Recording of previous events', 'Research', 'Research Personnel', 'Research Training', 'Resources', 'School Nursing', 'Science', 'Solid', 'South Asian', 'Special Populations Research', 'Substance abuse problem', 'Technology', 'Therapeutic', 'Therapeutic Intervention', 'Training', 'Translational Research', 'Universities', 'Vision', 'Workforce Development', 'analytical tool', 'base', 'biomarker discovery', 'career development', 'catalyst', 'clinical care', 'clinical database', 'clinical investigation', 'cognitive neuroscience', 'cohesion', 'community based participatory research', 'disease heterogeneity', 'ethnic diversity', 'ethnic minority population', 'follower of religion Jewish', 'improved', 'industry partner', 'innovation', 'interdisciplinary approach', 'logarithm', 'medical schools', 'member', 'natural language', 'next generation', 'novel', 'novel strategies', 'population health', 'programs', 'racial diversity', 'racial minority', 'recruit', 'relating to nervous system', 'research clinical testing', 'response', 'severe mental illness', 'skills', 'success', 'tool', 'translational scientist', 'treatment response', 'trial design']",NCATS,RUTGERS BIOMEDICAL/HEALTH SCIENCES-RBHS,UL1,2020,1482000,-0.0008730362223495822
"Next-generation Monte Carlo eXtreme Light Transport Simulation Platform Project Summary/Abstract Abstract: The rapid evolution of the field of biophotonics has produced numerous emerging techniques for combatting diseases and addressing urgent human health challenges, offering safe, non-invasive, and portable light-based diagnostic and therapeutic methods, and attracting exponentially growing attention over the past decade. Rigorous, fast, versatile and publicly available computational tools have played pivotal roles in the success of these novel approaches, leading to breakthroughs in new instrumentation designs and extensive explorations of complex biological systems such as human brains. The Monte Carlo eXtreme (MCX, http://mcx.space) light transport simulation platform developed by our team has become one of the most widely disseminated biophotonics modeling platforms, known for its high accuracy, high speed and versatility, as attested to by its over 27,000 downloads and nearly 1,000 citations from a large (2,400+ registered users) world-wide user community. Over the past years, we have also been pushing the boundaries in cutting-edge Monte Carlo (MC) photon simulation algorithms by exploring modern GPU architectures, advanced anatomical modeling methods and systematic software optimizations. In this proposed project, we will build upon the strong momentum created in the initial funding period, and strive to further advance the state-of-the-art of GPU-accelerated MC light transport modeling with strong support from the world’s leading GPU manufacturers and experts, further expanding our platform to address a number of emerging challenges in biomedical optics applications. Specifically, we will further explore emerging GPU architecture and resources, such as ray- tracing cores, half- and mixed-precision hardware, and portable programming models, to further accelerate the MC modeling speed. We will also develop hybrid shape/mesh-based MC algorithms to dramatically advance the capability in simulating extremely complex yet realistic anatomical structures, such as porous tissues in the lung, dense vessel networks in the brain, and multi-scaled tissue domains. In parallel, we aim to make a break- through in applying deep-learning-based image denoising techniques to equivalently accelerate MC simulations by 2 to 3 orders of magnitudes, as suggested in our preliminary studies. In the continuation of this project, we strive to create a dynamic and community-engaging simulation environment by extending our software to allow users to create, share, browse, and reuse pre-configured simulations, avoiding redundant works in re-creating complex simulations and facilitating reproducible research. In addition, we will expand our well-received user training programs and widely disseminate our open-source tools via major Linux distributions and container images. At the end of this continued funding period, we will provide the community with a significantly accelerated, widely-available and well-supported biophotonics modeling platform that can handle multi-scaled tissue optical modeling ranging from microscopic to macroscopic domains. Project Narrative The Monte Carlo eXtreme (MCX) light transport modeling platform has quadrupled its user community and paper citation numbers during the initial funding period. Building upon this strong momentum, we aim to further explore computational acceleration enabled by emerging GPU architectures and resources, and spearhead novel Monte Carlo (MC) algorithms to address the emerging needs of a broad biophotonics research community. We also dedicate our efforts to the further dissemination, training and usability enhancement of our software, and provide timely support to our large (>2,400 registered users) and active (>300 mailing list subscribers) user community.",Next-generation Monte Carlo eXtreme Light Transport Simulation Platform,10052188,R01GM114365,"['Acceleration', 'Address', 'Adopted', 'Algorithms', 'Anatomic Models', 'Anatomy', 'Architecture', 'Attention', 'Benchmarking', 'Biophotonics', 'Brain', 'Communities', 'Complex', 'Computer software', 'Data', 'Development', 'Diagnostic', 'Disease', 'Documentation', 'Educational workshop', 'Environment', 'Evolution', 'Funding', 'Future Generations', 'Health', 'Human', 'Hybrids', 'Image', 'Industry', 'Letters', 'Libraries', 'Light', 'Linux', 'Lung', 'Manufacturer Name', 'Methods', 'Microscopic', 'Modality', 'Modeling', 'Modernization', 'Monte Carlo Method', 'Motivation', 'Online Systems', 'Optics', 'Output', 'Paper', 'Performance', 'Photons', 'Play', 'Readability', 'Reproducibility', 'Research', 'Resource Sharing', 'Resources', 'Role', 'Shapes', 'Speed', 'Techniques', 'Therapeutic', 'Time', 'Tissues', 'Tracer', 'Training', 'Training Programs', 'Training Support', 'United States National Institutes of Health', 'Work', 'base', 'combat', 'complex biological systems', 'computerized tools', 'cost', 'data standards', 'deep learning', 'denoising', 'design', 'flexibility', 'graphical user interface', 'improved', 'instrumentation', 'interoperability', 'next generation', 'novel', 'novel strategies', 'open data', 'open source', 'portability', 'rapid growth', 'simulation', 'simulation environment', 'software development', 'success', 'tool', 'usability']",NIGMS,NORTHEASTERN UNIVERSITY,R01,2020,347094,0.0009947329220585117
"An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions Most U.S. adults (68%) take dietary supplements (DS) and there is increasing evidence of drug-supplement interactions (DSIs); our ability to readily identify interactions between DS with prescription medications is currently very limited. To optimize the safe use of DS, there remains a critical and unmet need for informatics methods to detect DSIs. Our rationale is that an innovative informatics framework to discover potential DSIs from the large scale of biomedical literature will enable a new line of research for targeted DSI validation and will also significantly narrow the range of DSIs that must be further explored. Our long-term goal is to use informatics approaches to enhance DSI clinical research and translate its findings to clinical practice ultimately via clinical decision support systems. The objective of this application is to develop an informatics framework to enable the discovery of DSIs by creating a DS terminology and mining scientific evidence from the biomedical literature. Towards these objectives, we propose the following specific aims: (1) Compile a comprehensive DS terminology using online resources; and (2) Discover potential DSIs from the biomedical literature. The successful accomplishment of this project will deliver a novel informatics paradigm and resources for identifying most clinically significant DSI signals and their biological mechanisms. This information is critical to subsequent efforts aimed at improving patient safety and efficacy of therapeutic interventions. The results from this study are imperative in order to achieve the ultimate goal of reducing an individual’s risk of potential DSIs. PROJECT NARRATIVE This research will address a critical and unmet need to conduct large-scale clinical research in drug-supplement interactions (DSIs) and improve evidence bases for healthcare practice. Our primary overarching goal is to use informatics approaches to enhance DSI clinical research and translate our findings to clinical practice ultimately via clinical decision support. The successful accomplishment of this project will deliver a novel informatics paradigm and valuable resources for identifying novel clinically significant DSI signals and their associated scientific evidence.",An Informatics Framework for Discovery and Ascertainment of Drug-Supplement Interactions,9894759,R01AT009457,"['Address', 'Adult', 'Adverse event', 'Biological', 'Cancer Patient', 'Clinical', 'Clinical Decision Support Systems', 'Clinical Research', 'Complement', 'Data', 'Data Element', 'Databases', 'Development', 'Drug Targeting', 'Education', 'Effectiveness', 'Electronic Health Record', 'Failure', 'Food', 'Ginkgo biloba', 'Goals', 'Health', 'Healthcare', 'Herbal supplement', 'Individual', 'Informatics', 'Information Retrieval', 'Investigation', 'Knowledge', 'Label', 'Link', 'Literature', 'MEDLINE', 'Medicine', 'Methods', 'Mining', 'Minnesota', 'Natural Language Processing', 'Natural Products', 'Outcome', 'Pathway interactions', 'Patient risk', 'Pharmaceutical Preparations', 'Pharmacoepidemiology', 'Postoperative Hemorrhage', 'Probability', 'Research', 'Resources', 'Risk', 'Safety', 'Semantics', 'Signal Transduction', 'Standardization', 'Surveys', 'System', 'Targeted Research', 'Terminology', 'Therapeutic', 'Therapeutic Intervention', 'Translating', 'Treatment Efficacy', 'United States Food and Drug Administration', 'Universities', 'Validation', 'Warfarin', 'Work', 'base', 'clinical decision support', 'clinical practice', 'clinically significant', 'colon cancer patients', 'data modeling', 'design', 'dietary supplements', 'drug testing', 'evidence base', 'improved', 'individual patient', 'innovation', 'machine learning method', 'novel', 'nutrition', 'online resource', 'open source', 'patient population', 'patient safety', 'post-market', 'screening', 'structured data', 'tool', 'unstructured data']",NCCIH,UNIVERSITY OF MINNESOTA,R01,2020,269500,-0.0011648546474007262
"Laboratory of Neuro Imaging Resource (LONIR) PROJECT SUMMARY - OVERALL The LONIR is focused on developing innovative solutions for the investigation of imaging, genetics, behavioral and clinical data. The LONIR structure is designed to facilitate studies of dynamically changing anatomic frameworks, e.g., developmental, neurodegenerative, traumatic, and metastatic, by providing methods for the comprehensive understanding of the nature and extent of these processes. Specifically, TR&D1 (Data Science) focuses on methodological developments for the management and informatics of brain and related data. This project will develop and issue new methods for robust scientific data management to create an environment where scientific analyses can be reproduced and/or enhanced, data can be easily discovered and reused, and analysis results can be visualized and made publicly searchable. TR&D2 (Diffusion MRI and Connectomics) seeks to advance the study of brain connectivity using diffusion imaging and its powerful extensions. This project will go beyond traditional tensor models of diffusion for assessing tissue and fiber microstructure and connectivity, develop tract-based statistical analysis tools using Deep Learning, introduce novel adaptive connectivity mapping approaches, using L1 fusion of multiple tractography methods, and provide mechanisms to study connectivity and diffusion imaging over 10,000 subjects. (This technology and these methods will be managed and executed by the TR&D1 framework to distributed datasets totaling over 10,000 subjects). Lastly, our TR&D3 (Intrinsic Surface Mapping) develops a general framework for surface mapping in the high dimensional Laplace-Beltrami embedding space via the mathematical optimization of their Riemannian metric. Our approach here overcomes fundamental limitations in existing methods based on spherical registration by eliminating the metric distortion during the parameterization step, thus achieving much improved accuracy in mapping brain anatomy. Coupled with a mature and efficient administrative structure and comprehensive training and dissemination, this program serves a wide and important need in the scientific community. PROJECT NARRATIVE - OVERALL The comprehensive suite of technologies include algorithmic and computational methods for image management, processing, data analysis and visualization. The technologies are ideally suited to enable holistic studies of the interactions between different imaging data modalities, phenotypic population characteristics, and physiological brain connectivity.",Laboratory of Neuro Imaging Resource (LONIR),9922272,P41EB015922,"['AIDS/HIV problem', 'Address', 'Alzheimer&apos', 's Disease', 'Anatomy', 'Atlases', 'Award', 'Behavioral', 'Books', 'Brain', 'Brain Mapping', 'Brain imaging', 'Clinical Data', 'Clinical Research', 'Communities', 'Computer software', 'Computing Methodologies', 'Coupled', 'Data', 'Data Analyses', 'Data Analytics', 'Data Science', 'Data Set', 'Database Management Systems', 'Databases', 'Dementia', 'Development', 'Diffusion', 'Diffusion Magnetic Resonance Imaging', 'Disease', 'Environment', 'Equilibrium', 'Evolution', 'Fiber', 'Funding', 'Image', 'Informatics', 'Infrastructure', 'Ingestion', 'Investigation', 'Laboratory of Neuro Imaging Resource', 'Manuscripts', 'Mathematics', 'Methodology', 'Methods', 'Modality', 'Modeling', 'Nature', 'Nerve Degeneration', 'Neurobiology', 'Parkinson Disease', 'Peer Review', 'Phenotype', 'Physiological', 'Population', 'Population Characteristics', 'Process', 'Protocols documentation', 'Publications', 'Publishing', 'Reproducibility', 'Research Activity', 'Research Personnel', 'Resources', 'Schizophrenia', 'Science', 'Services', 'Software Tools', 'Specificity', 'Statistical Data Interpretation', 'Structure', 'Students', 'Surface', 'System', 'Technology', 'Tissues', 'Training', 'United States National Institutes of Health', 'Visualization software', 'algorithmic methodologies', 'analysis pipeline', 'autism spectrum disorder', 'base', 'brain shape', 'cohort', 'computer grid', 'computer infrastructure', 'computerized data processing', 'data curation', 'data management', 'data resource', 'data visualization', 'deep learning', 'design', 'high dimensionality', 'image archival system', 'imaging genetics', 'imaging modality', 'improved', 'innovation', 'morphometry', 'new technology', 'novel', 'programs', 'symposium', 'synergism', 'technology research and development', 'tool', 'tractography', 'translational study', 'web services']",NIBIB,UNIVERSITY OF SOUTHERN CALIFORNIA,P41,2020,1217435,-0.0018138236350510636
"Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software. Abstract (Proposal title: Neuroscience Gateway to Enable Dissemination of Computational and Data Processing Tools and Software.): This proposal presents a focused plan for expanding the capabilities of the Neuroscience Gateway (NSG) to meet the evolving needs of neuroscientists engaged in computationally intensive research. The NSG project began in 2012 with support from the NSF. Its initial goal was to catalyze progress in computational neuroscience by reducing technical and administrative barriers that neuroscientists faced in large scale modeling projects involving tools and software which require and run efficiently on high performance computing (HPC) resources. NSG's success is reflected in the facts that (1) its base of registered users has grown continually since it started operation in early 2013 (more than 800 at present), (2) every year the NSG team successfully acquires ever larger allocations of supercomputer time (recently more than 10,000,000 core hours/year) on academic HPC resources of the Extreme Science and Engineering Discovery (XSEDE – that coordinates NSF supercomputer centers) program by writing proposals that go through an extremely competitive peer review process, and (3) it has contributed to large number of publications and Ph.D thesis. In recent years experimentalists, cognitive neuroscientists and others have begun using NSG for brain image data processing, data analysis and machine learning. NSG now provides over 20 tools on HPC resources for modeling, simulation and data processing. While NSG is currently well used by the neuroscience community, there is increasing interest from that community in applying it to a wider range of tasks than originally conceived. For example, some are trying to use it as an environment for dissemination of lab-developed tools, even though NSG is not suitable for that use because of delays from the batch queue wait times of production HPC resources, and lack of features and resources for an interactive, graphical, and collaborative environment needed for tool development, benchmarking and testing. “Forced” use of NSG for development and dissemination makes NSG's operators a “person-in-the-middle” bottleneck in the process. Another issue is that newly developed data processing tools require high throughput computing (HTC) usage mode, as opposed to HPC, but currently NSG does not provide access to compute resources suitable for HTC. Additionally, data processing workflows require features such as the ability to transfer large size data, process shared data, and visualize output results, which are not currently available on NSG. The work we propose will enhance NSG by adding the features that it needs to be a suitable and efficient dissemination environment for lab-developed neuroscience tools to the broader neuroscience community. This will allow tool developers to disseminate their lab-developed tools on NSG taking advantage of the current functionalities that are being well served on NSG for the last six years such as a growing user base, an easy user interface, an open environment, the ability to access and run jobs on powerful compute resources, availability of free supercomputer time, a well-established training and outreach program, and a functioning user support system. All of these well-functioning features of NSG will make it an ideal environment for dissemination and use of lab-developed computational and data processing neuroscience tools. The Neuroscience Gateway (NSG) was first implemented to enable large scale computational modeling of brain cells and circuits used to study neural function in health and disease. This new project extends NSG's utility to support development, dissemination and use of new tools by the neuroscience community for analyzing enormous data sets produced by advanced experimental methods in neuroscience.",Neuroscience Gateway to Enable Dissemination of Computational And Data Processing Tools And Software.,10019388,U24EB029005,"['Behavioral', 'Benchmarking', 'Brain imaging', 'Cells', 'Cognitive', 'Communities', 'Computer Models', 'Computer software', 'Data', 'Data Analyses', 'Data Correlations', 'Data Science', 'Data Set', 'Development', 'Disease', 'Education', 'Education and Outreach', 'Educational workshop', 'Electroencephalography', 'Engineering', 'Environment', 'Foundations', 'Functional Magnetic Resonance Imaging', 'Funding', 'Future', 'Goals', 'Health', 'High Performance Computing', 'Hour', 'Human Resources', 'Image', 'Machine Learning', 'Magnetic Resonance Imaging', 'Methods', 'Modeling', 'Neurophysiology - biologic function', 'Neurosciences', 'Neurosciences Research', 'Occupations', 'Output', 'Peer Review', 'Persons', 'Process', 'Production', 'Psychologist', 'Publications', 'Reaction Time', 'Research', 'Research Personnel', 'Resources', 'Running', 'Science', 'Software Tools', 'Students', 'Support System', 'System', 'Testing', 'Time', 'Training', 'Training Programs', 'United States National Institutes of Health', 'Wait Time', 'Work', 'Workload', 'Writing', 'base', 'bioimaging', 'brain cell', 'collaborative environment', 'computational neuroscience', 'computerized data processing', 'computing resources', 'data sharing', 'image processing', 'interest', 'models and simulation', 'open data', 'operation', 'outreach program', 'programs', 'response', 'success', 'supercomputer', 'tool', 'tool development', 'trend', 'webinar']",NIBIB,"UNIVERSITY OF CALIFORNIA, SAN DIEGO",U24,2020,381282,-0.00717427821364156
"The University of Iowa Clinical and Translational Science Award ABSTRACT The Institute for Clinical and Translational Science (ICTS) at the University of Iowa (UI) was established by the Board of Regents to realize three objectives – first, to lead the development of translational science at the UI; second, to advance translational science as a distinct academic discipline; and third, to disseminate capacities in translational science across the State of Iowa. This mandate enabled us to tackle large problems affecting translational science that required institutional solutions, such as transforming regulatory processes for human subjects research, developing an informatics infrastructure for integrating electronic medical record and other health care data, establishing bi-directional relationships with community organizations, and revitalizing the pipeline of well trained clinical and translational researchers. Iowa is a rural state, which brings special health care needs and challenges. We have used these rural considerations as a catalyst for driving our approach to clinical and translational research pushing our teams to develop strategies to engage rural populations of all ages and backgrounds and to create new approaches that overcome the geographic barriers in a rural state. We are capitalizing on our established community practice networks of family physicians, clinics, school nurses and pharmacists. We utilize e Health/ e Learning platforms in novel ways and will test the efficacy of these new methods of engagement. As we move research “Beyond Our Borders,” we have created methods to capture real-time, real-life data from the home and to correlate this environmentally specific, comprehensive data to human performance. The ICTS is engaging with other CTSA hubs and national CTR systems to empirically test different approaches and to develop the evidence base of proven strategies for accelerating translation that can be more broadly disseminated. Though distance and rurality drive our approaches, the strategies that we develop are simply new and potentially better ways to generate broad representation and improved participation by patients, healthcare teams and academicians. Through our local, state and national partnerships, UI and the ICTS are poised to move clinical and translational discovery rapidly into healthcare practice in a variety of clinical settings. PROJECT NARRATIVE The mission of the University of Iowa Institute for Clinical and Translational Science (ICTS) is to accelerate translational science through programs to develop the translational workforce, promote the engagement of community members and other stakeholders, to promote research integration across the lifespan, and to catalyze innovative clinical and translational research. The ongoing development of collaborative data-based infrastructure and services is a central tenant to achieving this mission. The goals of this proposal are: for Iowa to join the N3C partnership, contributing data in an effective method that evolves and improves over time, to enable Iowa researchers to leverage the N3C data resource in their research and to leverage the N3C framework for other areas of research in the future.",The University of Iowa Clinical and Translational Science Award,10201104,UL1TR002537,"['Affect', 'Age', 'Area', 'Artificial Intelligence', 'Automobile Driving', 'Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Clinics and Hospitals', 'Communities', 'Community Practice', 'Computerized Medical Record', 'Data', 'Data Element', 'Data Set', 'Data Sources', 'Development', 'Discipline', 'Disease', 'E-learning', 'Emergency Situation', 'Family Physicians', 'Future', 'Genomics', 'Geography', 'Goals', 'Health', 'Health system', 'Healthcare', 'Home environment', 'Human', 'Human Subject Research', 'Image', 'Infrastructure', 'Institutes', 'Institutional Review Boards', 'Iowa', 'Knowledge', 'Lead', 'Life', 'Longevity', 'Machine Learning', 'Medical Care Team', 'Methods', 'Mission', 'Modeling', 'Outcome', 'Patient Participation', 'Patients', 'Performance', 'Pharmaceutical Preparations', 'Pharmacists', 'Phenotype', 'Positioning Attribute', 'Process', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Rural', 'Rural Health', 'Rural Population', 'School Nursing', 'Services', 'System', 'Testing', 'Time', 'Training', 'Transfer Agreement', 'Translational Research', 'Translations', 'Universities', 'Work', 'base', 'biomedical informatics', 'catalyst', 'clinical decision support', 'cohort', 'collaborative approach', 'community organizations', 'coronavirus disease', 'data acquisition', 'data enclave', 'data exchange', 'data resource', 'data sharing', 'data warehouse', 'efficacy testing', 'evidence base', 'health management', 'improved', 'informatics infrastructure', 'innovation', 'interest', 'member', 'novel', 'novel strategies', 'pandemic disease', 'phenotypic data', 'programs', 'rurality', 'social determinants', 'support tools', 'translational scientist']",NCATS,UNIVERSITY OF IOWA,UL1,2020,98933,-0.0026656167121733706
"Carolina Population Center PROJECT SUMMARY/ABSTRACT The Carolina Population Center requests infrastructure support that will advance population dynamics research at CPC by increasing research impact, innovation, and productivity, supporting the development of junior scientists, and reducing the administrative burden on scientists. Infrastructure support will advance science in three primary research areas: Sexuality, Reproduction, Fertility, and Families; Population, Health, and the Environment; and Inequality, Mobility, Disparities, and Well-Being. Much of the research at CPC draws on large publicly available longitudinal data sets that our faculty have designed and collected, including the National Longitudinal Study of Adolescent to Adult Health, the China Health and Nutrition Survey, newer surveys associated with the Transfer Project, and the Study of the Tsunami Aftermath and Recovery, all of which will continue to be important in work related to our primary research areas over the next five years. These projects embody several themes that have guided research at CPC since the Center's inception. These themes, which will continue to shape our work, are the importance of life course processes and longitudinal data, multi-level processes and measurement of context, interventions and natural experiments as means of learning about causal processes, and the relevance of sociodemographic variables such as age, gender, race- ethnicity, and socioeconomic status for disparities in health and well-being. By embedding these themes, our projects provide data that enable us to address barriers that otherwise impede progress in the population sciences generally, and in our primary research areas in particular. We request support for three cores which in combination will provide an institutional infrastructure that will push populations dynamics research forward by empowering CPC faculty to tackle challenging questions using state of the art measurement techniques and methods. The Administrative Core plans activities that maintain a stimulating intellectual community, streamlines administrative processes so that scientists can focus on research, coordinates activities of the Cores so that services are offered efficiently, and communicates information about research and data more broadly. The Development Core supports early stage investigators and other faculty with exciting new ideas through multiple mechanisms: workshops, access to technical expertise in measurement, and seed grants. The Research Services Core enables scientists to address complex and important population research issues by providing access to state-of-the-art research tools and professional support for programming, survey development, and analysis. NARRATIVE This project will provide infrastructure support for a cutting edge program of research on population dynamics at the Carolina Population Center. Research at the Center will analyze state-of-the art data to address fundamental questions regarding fertility, adolescent health, and links between the environment and health. Special attention will be paid to factors creating health disparities.",Carolina Population Center,10005569,P2CHD050924,"['Address', 'Adolescent', 'Adopted', 'Adult', 'Age', 'Applications Grants', 'Area', 'Attention', 'Biological Markers', 'China', 'Cognitive', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Computer Vision Systems', 'Creativeness', 'Data', 'Data Collection', 'Development', 'Diffuse', 'Educational workshop', 'Environment', 'Ethnic Origin', 'Extramural Activities', 'Faculty', 'Family', 'Fertility', 'Fostering', 'Funding', 'Gender', 'Genetic', 'Grant', 'Hand', 'Health', 'Health Surveys', 'Home environment', 'Inequality', 'Infrastructure', 'Intervention', 'Journals', 'Learning', 'Life Cycle Stages', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Mainstreaming', 'Measurement', 'Mentors', 'Methods', 'Natural experiment', 'Nutrition Surveys', 'Personal Satisfaction', 'Phase', 'Policy Making', 'Population', 'Population Dynamics', 'Population Research', 'Population Sciences', 'Postdoctoral Fellow', 'Process', 'Production', 'Productivity', 'Publishing', 'Race', 'Recovery', 'Reproduction', 'Research', 'Research Design', 'Research Infrastructure', 'Research Personnel', 'Research Project Grants', 'Resources', 'Schools', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Seeds', 'Services', 'Sexuality', 'Shapes', 'Socioeconomic Status', 'Structure', 'Students', 'Surveys', 'Talents', 'Teacher Professional Development', 'Technical Expertise', 'Techniques', 'Training Programs', 'Tsunami', 'Universities', 'Work', 'adolescent health', 'career', 'collaborative environment', 'cost', 'data access', 'design', 'empowered', 'experience', 'faculty support', 'health disparity', 'innovation', 'interdisciplinary collaboration', 'longitudinal dataset', 'novel strategies', 'population health', 'privacy protection', 'programs', 'research and development', 'response', 'sociodemographic variables', 'success', 'tool']",NICHD,UNIV OF NORTH CAROLINA CHAPEL HILL,P2C,2020,774402,9.718484520540363e-05
"Novel Statistical Inference for Biomedical Big Data Project Summary This project develops novel statistical inference procedures for biomedical big data (BBD), including data from diverse omics platforms, various medical imaging technologies and electronic health records. Statistical inference, i.e., assess- ing uncertainty, statistical signiﬁcance and conﬁdence, is a key step in computational pipelines that aim to discover new disease mechanisms and develop effective treatments using BBD. However, the development of statistical inference procedures for BBD has lagged behind technological advances. In fact, while point estimation and variable selection procedures for BBD have matured over the past two decades, existing inference procedures are either limited to simple methods for marginal inference and/or lack the ability to integrate biomedical data across multiple studies and plat- forms. This paucity is, in large part, due to the challenges of statistical inference in high-dimensional models, where the number of features is considerably larger than the number of subjects in the study. Motivated by our team's extensive and complementary expertise in analyzing multi-omics data from heterogenous studies, including the TOPMed project on which multiple team members currently collaborate, the current proposal aims to address these challenges. The ﬁrst aim of the project develops a novel inference procedure for conditional parameters in high-dimensional models based on dimension reduction, which facilitates seamless integration of external biological information, as well as biomedical data across multiple studies and platforms. To expand the application of this method to very high-dimensional models that arise in BBD applications, the second aim develops a data-adaptive screening procedure for selecting an optimal subset of relevant variables. The third aim develops a novel inference procedure for high-dimensional mixed linear models. This method expands the application domain of high-dimensional inference procedures to studies with longitu- dinal data and repeated measures, which arise commonly in biomedical applications. The fourth aim develops a novel data-driven procedure for controlling the false discovery rate (FDR), which facilitates the integration of evidence from multiple BBD sources, while minimizing the false negative rate (FNR) for optimal discovery. Upon evaluation using ex- tensive simulation experiments and application to multi-omics data from the TOPMed project, the last aim implements the proposed methods into easy-to-use open-source software tools leveraging the R programming language and the capabilities of the Galaxy workﬂow system, thus providing an expandable platform for further developments for BBD methods and tools. Public Health Relevance Biomedical big data (BBD), including large collections of omics data, medical imaging data, and electronic health records, offer unprecedented opportunities for discovering disease mechanisms and developing effective treatments. However, despite their tremendous potential, discovery using BBD has been hindered by computational challenges, including limited advances in statistical inference procedures that allow biomedical researchers to investigate uncon- founded associations among biomarkers of interest and various biological phenotypes, while integrating data from multiple BBD sources. The current proposal bridges this gap by developing novel statistical machine learning methods and easy-to-use open-source software for statistical inference in BBD, which are designed to facilitate the integration of data from multiple studies and platforms.",Novel Statistical Inference for Biomedical Big Data,9969887,R01GM133848,"['Address', 'Adoption', 'Behavioral', 'Big Data Methods', 'Biological', 'Biological Assay', 'Biological Markers', 'Code', 'Collection', 'Communities', 'Computer software', 'Data', 'Data Sources', 'Development', 'Dimensions', 'Disease', 'Electronic Health Record', 'Evaluation', 'Fostering', 'Galaxy', 'Genetic study', 'Goals', 'Heart', 'Imaging technology', 'Individual', 'Linear Models', 'Measurement', 'Measures', 'Medical Imaging', 'Methods', 'Modeling', 'Molecular', 'Multiomic Data', 'Outcome', 'Phenotype', 'Procedures', 'R programming language ', 'Research Personnel', 'Sample Size', 'Scientist', 'Screening procedure', 'Software Tools', 'Structure', 'System', 'Testing', 'Trans-Omics for Precision Medicine', 'Uncertainty', 'Work', 'base', 'big biomedical data', 'computational pipelines', 'data integration', 'design', 'diverse data', 'effective therapy', 'experimental study', 'heterogenous data', 'high dimensionality', 'interest', 'machine learning method', 'member', 'novel', 'open source', 'public health relevance', 'screening', 'simulation', 'statistical and machine learning', 'structured data', 'tool', 'treatment strategy', 'user friendly software']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,456980,0.003996279961217189
"Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences Project Abstract The engineering of ontologies that define the entities in an application area and the relationships among them has become essential for modern work in biomedicine. Ontologies help both humans and computers to manage burgeoning numbers of data. The need to annotate, retrieve, and integrate high- throughput data sets, to process natural language, and to build systems for decision support has set many communities of investigators to work building large ontologies. The Protégé system has become an indispensable open-source resource for an enormous international community of scientists—supporting the development, maintenance, and use of ontologies and electronic knowledge bases by biomedical investigators everywhere. The number of registered Protégé users has grown from 3,500 in 2002 to more than 300,000 users as of this writing. The widespread use of ontologies in biomedicine and the availability of tools, such as Protégé, have taken the biomedical field forward to a new set of challenges that current technology has not been designed to address: Biomedical ontologies have grown in size and scope, and their creation, maintenance and quality assurance have become particularly effort-intensive and error-prone. In this proposal, we will develop new methods and tools that will significantly aid biomedical researchers in easily creating and testing biomedical ontologies throughout their lifecycle. Our plan entails four specific aims. First, we will develop methods and tools to allow biomedical scientist to easily create ontologies directly from their source documents, such as spreadsheets, tab indented hierarchies, and document outlines. Second, we will provide the methods and tools to allow biomedical scientist to identify potential “hot spots” in their ontologies that might affect their quality. Third, we will implement a comprehensive, automated testing framework for ontologies that will assist biomedical researchers in performing ontology and data quality assurance throughout the development cycle. Fourth, we will continue to expand and support the thriving Protégé user community, as it grows to include new clinicians and biomedical scientists as they build the ontologies needed to support clinical care, data-driven research, and the elucidation of new discoveries. Project Narrative Protégé is a software system that helps a burgeoning user community to develop ontologies that enhance biomedical research and improve patient care. Protégé supports scientists, clinician researchers, and workers in informatics in data annotation, data integration, information retrieval, natural-language processing, electronic patient record systems, and decision-support systems. The Protégé resource provides critical semantic-technology infrastructure and expertise for biomedical research and the development of advanced clinical information systems.",Protege: A Knowledge-Engineering Environment for Advancing Biomedical Sciences,9848600,R01GM121724,"['Address', 'Adopted', 'Advanced Development', 'Affect', 'Applications Grants', 'Area', 'Biomedical Research', 'Clinical', 'Communities', 'Computer software', 'Computerized Patient Records', 'Computers', 'Data', 'Data Set', 'Data Sources', 'Decision Support Systems', 'Development', 'Education and Outreach', 'Engineering', 'Ensure', 'Environment', 'Foundations', 'Goals', 'Head', 'Hot Spot', 'Human', 'Informatics', 'Information Retrieval', 'Information Systems', 'Infrastructure', 'International', 'Investigation', 'Knowledge', 'Knowledge Discovery', 'Letters', 'Maintenance', 'Manuals', 'Methods', 'Modernization', 'Natural Language Processing', 'Ontology', 'Patient Care', 'Process', 'Publications', 'Research', 'Research Personnel', 'Resources', 'Science', 'Scientist', 'Semantics', 'Source', 'System', 'Technology', 'Terminology', 'Testing', 'Time', 'Update', 'Work', 'Writing', 'biomedical ontology', 'biomedical scientist', 'clinical care', 'data integration', 'data quality', 'data sharing', 'design', 'document outlines', 'improved', 'innovation', 'interoperability', 'knowledge base', 'large datasets', 'natural language', 'next generation', 'ontology development', 'open source', 'quality assurance', 'software systems', 'tool']",NIGMS,STANFORD UNIVERSITY,R01,2020,559088,0.02752511428024238
"CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA) ABSTRACT - OVERALL Total joint arthroplasty (TJA) is the most common and fastest growing surgery in the nation. There are currently more than 7 million Americans living with artificial joints. Despite the high surgery volume, the evidence base for TJA procedures, technologies and associated interventions are limited. Many surgical approaches and implant technologies in TJA are adopted based on theoretical grounds with limited clinical evidence. The wider TJA research community needs access to large, high quality and rich data sources and state-of-the-art clinical research standards and information technologies to overcome methodological and practical challenges in studies of surgical and nonsurgical interventions in TJA. The overarching goal of Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) is to facilitate innovative, methodologically rigorous and interdisciplinary clinical research that will directly improve TJA care and the outcomes. The CORE-TJA will serve as a disease (TJA) and theme-focused Center providing shared methodological expertise, education and data resources. The CORE-TJA will leverage big data resources for TJA research, provide customized methodology resources in epidemiology, biostatistics, health services research and medical informatics, and establish synergistic interactions around an integrated Core (American Joint Replacement Registry – AJRR). The Specific Aims of CORE-TJA are: (1) To provide administrative and scientific oversight of CORE-TJA activities (Administrative Core), (2) To provide integrated services, access to large databases and novel analytical methods for clinical research in TJA (Methodology Core); and (3) To meet the unique data needs of the TJA research community and to strengthen the national capacity for large-scale observational and interventional studies in TJA using national registry data (Resource Core). The CORE-TJA will be integrated within the long-standing and highly centralized clinical research environment of the Mayo Clinic, thereby leveraging existing expertise and infrastructure resources, including the Center for Clinical and Translational Science. All CORE-TJA activities will be evaluated using robust metrics to ensure continuous evaluation, flexibility and improvement in response to the most pressing needs of the TJA research community. NARRATIVE The Mayo Core Center for Clinical Research in Total Joint Arthroplasty (CORE-TJA) will provide methodological expertise and access to nationwide data resources to facilitate innovative, methodologically rigorous and interdisciplinary clinical research in TJA. The clinical research needs of the TJA research community that will be addressed by the CORE-TJA include training of the next generation of TJA researchers, customized consultations, facilitated access to high quality, rich data sources and national TJA registry data as well as informatics and methodology support.",CORE CENTER FOR CLINICAL RESEACH IN TOTAL JOINT ARTHROPLASTY (CORE-TJA),10019333,P30AR076312,"['Address', 'Adopted', 'Adoption', 'Advisory Committees', 'American', 'Area', 'Berry', 'Big Data', 'Biometry', 'Characteristics', 'Clinic', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communication', 'Communities', 'Consultations', 'Custom', 'Data', 'Data Analyses', 'Data Collection', 'Data Element', 'Data Sources', 'Databases', 'Development', 'Disease', 'Documentation', 'Education', 'Electronic Health Record', 'Ensure', 'Environment', 'Epidemiology', 'Evaluation', 'Future', 'Goals', 'Health Services Research', 'Hip Prosthesis', 'Implant', 'Informatics', 'Information Technology', 'Infrastructure', 'Intervention', 'Intervention Studies', 'Joint Prosthesis', 'Knee Prosthesis', 'Leadership', 'Link', 'Medical Informatics', 'Methodology', 'Modeling', 'Musculoskeletal', 'Natural Language Processing', 'Observational Study', 'Operative Surgical Procedures', 'Outcome', 'Patient Care', 'Patients', 'Policies', 'Positioning Attribute', 'Procedures', 'Productivity', 'Registries', 'Replacement Arthroplasty', 'Research', 'Research Methodology', 'Research Personnel', 'Resources', 'Secure', 'Services', 'Surgeon', 'Technology', 'Time', 'Training', 'Translating', 'Translational Research', 'Translations', 'United States', 'Vision', 'analytical method', 'base', 'care outcomes', 'clinical center', 'cost', 'data registry', 'data resource', 'education resources', 'evidence base', 'experience', 'flexibility', 'improved', 'improved outcome', 'innovation', 'next generation', 'novel', 'outreach', 'programs', 'response', 'skills', 'tool', 'willingness']",NIAMS,MAYO CLINIC ROCHESTER,P30,2020,629384,0.013901090309570674
"Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning Project Summary/Abstract Engaging adolescents' interest in pursuing careers in health science and the health professions offers significant promise for building our nation's healthcare and health research capacity. The goal of this project is to create Health Quest, an intelligent game-based learning environment that increases adolescents' knowledge of, interest in, and self-efficacy to pursue health science careers. Three specific aims will be accomplished by the project: 1. Design and develop Health Quest to engage adolescents' interest in the health sciences utilizing  personalized learning technologies that integrate the following components: (a) the Health Quest Career  Adventure Game, an intelligent game-based learning environment that leverages AI technologies to  create personalized health career adventures; (b) the Health Quest Student Discovery website, which  will feature interactive video interviews with health professionals about their biomedical, behavioral, and  clinical research careers; and (c) the Health Quest Teacher Resource Center website, which will provide  online professional development materials and in-class support for teachers' classroom implementation of  Health Quest. 2. Investigate the impact of Health Quest on adolescents' (1) knowledge of biomedical, behavioral, and  clinical research careers; (2) interest in biomedical, behavioral, and clinical research careers; and (3) self-  efficacy for pursuing biomedical, behavioral, and clinical research careers by conducting a matched  comparison study in middle school classes. ! 3. Examine the effect of Health Quest on diverse adolescents by gender and racial/ethnicity. Working closely  with underrepresented minorities throughout all design and development phases of the project, the project  team will specifically design Health Quest to develop girls' and members of underrepresented groups'  knowledge of, interest in, and self-efficacy to pursue health science careers. Project Narrative    The goal of this project is to create Health Quest, an immersive career adventure game that deeply engages  adolescents’ interest in health science careers. Health Quest will leverage significant advances in personalized  learning technologies to create online interactions that enable adolescents to virtually explore health research  careers in action. The project will investigate the impact of Health Quest on adolescents’ knowledge of, interest  in,  and  self-­efficacy  to  pursue  health  science  careers/research  and  examine  the  effect  of  Health  Quest  on  diverse adolescents by gender and racial/ethnicity. ",Health Quest: Engaging Adolescents in Health Careers with Technology-Rich Personalized Learning,9984446,R25GM129215,"['Address', 'Adolescence', 'Adolescent', 'Adolescent Medicine', 'Artificial Intelligence', 'Behavioral Research', 'Biomedical Research', 'California', 'Clinical Research', 'Collaborations', 'Dentistry', 'Development', 'Dietetics', 'Ethnic Origin', 'Game Based Learning', 'Gender', 'Goals', 'Health', 'Health Occupations', 'Health Professional', 'Health Sciences', 'Healthcare', 'Immersion', 'Informatics', 'Intelligence', 'Interdisciplinary Study', 'Intervention', 'Interview', 'Knowledge', 'Medicine', 'Mental Health', 'Middle School Student', 'North Carolina', 'Patients', 'Pediatrics', 'Phase', 'Play', 'Preventive', 'Primary Health Care', 'Public Health', 'Race', 'Recording of previous events', 'Research', 'Research Support', 'Resources', 'Role', 'San Francisco', 'Science, Technology, Engineering and Mathematics Education', 'Self Efficacy', 'Students', 'Technology', 'Testing', 'Underrepresented Groups', 'Underrepresented Minority', 'Universities', 'Woman', 'adolescent health', 'behavior change', 'career', 'career awareness', 'computer science', 'design', 'distinguished professor', 'educational atmosphere', 'ethnic minority population', 'experience', 'game development', 'girls', 'health science research', 'interest', 'junior high school', 'member', 'nutrition', 'outreach', 'personalized learning', 'professor', 'racial minority', 'teacher', 'virtual', 'web site']",NIGMS,NORTH CAROLINA STATE UNIVERSITY RALEIGH,R25,2020,253654,0.0020426651608675193
"A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks New advances in biomedical research have made it possible to collect multiple data “views” — for example, genetic, metabolomic, and clinical data — for a single patient. Such multi-view data promises to offer deeper insights into a patient's health and disease than would be possible if just one data view were available. However, in order to achieve this promise, new statistical methods are needed.  This proposal involves developing statistical methods for the analysis of multi-view data. These methods can be used to answer the following fundamental question: do the data views contain redundant information about the observations, or does each data view contain a different set of information? The answer to this question will provide insight into the data views, as well as insight into the observations. If two data views contain redundant information about the observations, then those two data views are related to each other. Furthermore, if each data view tells the same “story” about the observations, then we can be quite conﬁdent that the story is true.  The investigators will develop a uniﬁed framework for modeling multi-view data, which will then be applied in a number of settings. In Aim 1, this framework will be applied to multi-view multivariate data (e.g. a single set of patients, with both clinical and genetic measurements), in order to determine whether a single clustering can adequately describe the patients across all data views, or whether the patients cluster separately in each data view. In Aim 2, the framework will be applied to multi-view network data (e.g. a single set of proteins, with both binary and co-complex interactions measured), in order to determine whether the nodes belong to a single set of communities across the data views, or a separate set of communities in each data view. In Aim 3, the framework will be applied to multi-view multivariate data in order to determine whether the observations can be embedded in a single latent space across all data views, or whether they belong to a separate latent space in each data view. In Aims 1–3, the methods developed will be applied to the Pioneer 100 study, and to the protein interactome. In Aim 4(a), the availability of multiple data views will be used in order to develop a method for tuning parameter selection in unsupervised learning. In Aim 4(b), protein communities that were identiﬁed in Aim 2 will be validated experimentally. High-quality open source software will be developed in Aim 5.  The methods developed in this proposal will be used to determine whether the ﬁndings from multiple data views are the same or different. The application of these methods to multi-view data sets, including the Pioneer 100 study and the protein interactome, will improve our understanding of human health and disease, as well as fundamental biology. Biomedical researchers often collect multiple “types” of data (e.g. clinical data and genetic data) for a single patient, in order to get a fuller picture of that patient's health or disease status than would be possible using any single data type. This proposal involves developing new statistical methods that can be used in order to analyze data sets that consist of multiple data types. Applying these methods will lead to new insights and better understanding of human health and disease.","A Modeling Framework for Multi-View Data, with Applications to the Pioneer 100 Study and Protein Interaction Networks",9962426,R01GM123993,"['Address', 'Adoption', 'Agreement', 'Algorithms', 'Biology', 'Biomedical Research', 'Clinical Data', 'Communities', 'Complex', 'Computer software', 'Conflict (Psychology)', 'Data', 'Data Pooling', 'Data Set', 'Detection', 'Development', 'Dimensions', 'Disease', 'Foundations', 'Future', 'Gene Expression', 'Genetic', 'Genomics', 'Goals', 'Health', 'Human', 'Individual', 'Measurement', 'Measures', 'Medical Genetics', 'Methodology', 'Methods', 'Modeling', 'Participant', 'Patients', 'Principal Component Analysis', 'Proteins', 'Proteomics', 'Records', 'Research Personnel', 'Resources', 'Set protein', 'Statistical Data Interpretation', 'Statistical Methods', 'Technology', 'Testing', 'Time', 'Trust', 'Validation', 'Variant', 'genomic data', 'improved', 'insight', 'metabolomics', 'multiple data types', 'novel strategies', 'open source', 'unsupervised learning']",NIGMS,UNIVERSITY OF WASHINGTON,R01,2020,323659,-0.029070203374494993
"Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes PROJECT SUMMARY Overview: We will extend and develop implementations of foundational methods for analyzing populations of attributed connectomes. Our toolbox will enable brain scientists to (1) infer latent structure from individual connectomes, (2) identify meaningful clusters among populations of connectomes, and (3) detect relationships between connectomes and multivariate phenotypes. The methods we develop and extend will naturally overcome the challenges inherent in connectomics: high-dimensional non-Euclidean data with multi-level nonlinear interactions. Our implementations will comply with the highest open-source standards by: providing extensive online documentation and extended tutorials, hosting workshops to demonstrate our tools on an annual basis, and merging our implementations into commonly used packages such as scikit-learn [1], scipy [2], and networkx [3]. All of the code we develop is open source. We strive to ensure that our code is shared in accordance with the strictest guiding principles. We chose to implement these algorithms in Python due to its wide adoption in the neuroscience and data science fields. In particular, many other neuroscience tools applicable to connectomics, including NetworkX DiPy, mindboggle, nilearn, and nipy, are also implemented in Python. This will enable researchers to chain our analysis tools onto pre-existing pipelines for data preprocessing and visualization. Nonetheless, we feel that sharing our code in our own public repositories is insufficient for global reach. We have also begun reaching out to developers of the leading data science packages in python, including scipy, sklearn, networkx, scikit-image, and DiPy. For each of those packages, we have informal approval to begin integrating algorithms that we have developed. Those packages are collectively used by >220,000 other packages, so merging our algorithms into those packages will significantly extend our global reach. All researchers investigating connectomics, including all the authors of the 24,000 papers that mention the word “connectome”, will be able to apply state-of-the-art statistical theory and methods to their data. Currently, we have about 150 open source software projects on our NeuroData GitHub organization. Collectively, these projects get about 2,000 downloads and >11,000 views per month. As we incorporate additional functionality as described in this proposal, we expect far more researchers across disciplines and sectors will utilize our software. 20 ​ ​​ ​ ​​ Project Narrative Connectomes are an increasingly important modality for characterizing the structure of the brain, to complement behavior, genetics, and physiology. We and others have developed foundational statistical theory and methods over the last decade for the analysis of networks, networks with edge, vertex, and other attributes, and populations thereof, with preliminary implementations of those tools that we leverage in our laboratory for various application papers. In this project, we will extend our package, called graspy, to be of professional quality, implementing key functionality to include (1) estimating latent structure from attributed connectomes, (2) identifying meaningful clusters among populations of connectomes, and (3) detecting relationships between connectomes and multivariate phenotypes, such as behavior, genetics, and physiology. 18",Graspy: A python package for rigorous statistical analysis of populations of attributed connectomes,10012519,RF1MH123233,"['Adoption', 'Algorithms', 'Behavioral Genetics', 'Brain', 'Code', 'Coin', 'Complement', 'Complex', 'Computer software', 'Data', 'Data Science', 'Data Set', 'Development', 'Discipline', 'Documentation', 'Educational workshop', 'Ensure', 'Foundations', 'Funding', 'Genes', 'Human', 'Image', 'Individual', 'Journals', 'Laboratories', 'Learning', 'Link', 'Machine Learning', 'Methodology', 'Methods', 'Modality', 'Modernization', 'Motivation', 'Neurosciences', 'Paper', 'Pathway Analysis', 'Phenotype', 'Physiology', 'Population', 'Population Analysis', 'Population Study', 'Property', 'PubMed', 'Publishing', 'Pythons', 'Research Personnel', 'Scientist', 'Statistical Data Interpretation', 'Statistical Methods', 'Statistical Study', 'Structure', 'Telecommunications', 'Testing', 'Visualization', 'Work', 'brain research', 'connectome', 'data pipeline', 'design', 'high dimensionality', 'high standard', 'open source', 'public repository', 'software development', 'theories', 'tool', 'user-friendly']",NIMH,JOHNS HOPKINS UNIVERSITY,RF1,2020,1246005,0.0016382963895978553
"CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience The field of network neuroscience has developed powerful analysis tools for studying brain networks and holds promise for deepening our understanding of the role played by brain networks in health, disease, development, and cognition. Despite widespread interest, barriers exist that prevent these tools from having broader impact. These include (1) unstandardized practices for sharing and documenting software, (2) long delays from when a method is first introduced to when it becomes publicly available, and (3) gaps in theoretic knowledge and understanding leading to incorrect, delays due to mistakes, and errors in reported results. These barriers ultimately slow the rate of neuroscientific discovery and stall progress in applied domains. To overcome these challenges, we will use open science methods and cloud-computing, to increase the availability of network neuroscience tools. We will use the platform ""brainlife.io"" for sharing these tools, which will be packaged into self-contained, standardized, reproducible Apps, shared with and modified by a community of users, and integrated into existing brainlife.io analysis pipelines. Apps will also be accompanied by links to primary sources, in-depth tutorials, and documentation, and worked-through examples, highlighting their correct usage and offering solutions for mitigating possible pitfalls. In standardizing and packaging network neuroscience tools as Apps, this proposed research will engage a new generation of neuroscientists, providing them powerful new and leading to new discoveries. Second, the proposed research will contribute growing suite of modeling analysis that can be modified to suit specialized purposes. Finally, the Brainlife.io platform will serve as part of the infrastructure supporting neuroscience research. Altogether, these advances will lead to new opportunities in network neuroscience research and further stimulate its growth while increasing synergies with other domains in neuroscience. Structural and functional networks support cognitive processes. Miswiring networks lead to maladaptive behavior and neuropsychicatric disorders. Network neuroscience is a young field that provides a quantitative framework for modeling brain networks. This project will make network neuroscientific tools available to new users via open science and cloud-computing. New applications of these tools this will lead deeper insight into the role of networks in health as well as in clinical disorders.",CRCNS: US-France Data Sharing Proposal: Lowering the barrier of entry to network neuroscience,10019389,R01EB029272,"['Address', 'Aging', 'Behavior', 'Biophysics', 'Brain', 'Clinical', 'Cloud Computing', 'Cognition', 'Communities', 'Complex Analysis', 'Computer software', 'Data', 'Data Set', 'Development', 'Disease', 'Documentation', 'Ecosystem', 'Education', 'Elements', 'France', 'Funding', 'Generations', 'Graph', 'Growth', 'Health', 'Infrastructure', 'Instruction', 'Knowledge', 'Language', 'Lead', 'Libraries', 'Link', 'Literature', 'Mathematics', 'Methods', 'Modeling', 'Neurosciences', 'Neurosciences Research', 'Pathway Analysis', 'Play', 'Process', 'Reporting', 'Reproducibility', 'Research', 'Research Personnel', 'Resources', 'Role', 'Running', 'Science', 'Sociology', 'Source', 'Standardization', 'Structure', 'Study models', 'System', 'Techniques', 'Time', 'Training', 'Work', 'analysis pipeline', 'brain computer interface', 'cloud based', 'cognitive process', 'cyber infrastructure', 'data sharing', 'experience', 'innovation', 'insight', 'interest', 'machine learning method', 'network architecture', 'network models', 'neuroimaging', 'open data', 'prevent', 'relating to nervous system', 'statistics', 'support network', 'synergism', 'tool']",NIBIB,INDIANA UNIVERSITY BLOOMINGTON,R01,2020,218594,-0.0005907115390546822
"University of Buffalo Clinical and Translational Science Institute The Buffalo Translational Consortium (BTC), which includes the University at Buffalo (UB) health sciences schools, the major healthcare institutions in our region, four key research institutes and five influential community partners, have embarked on a comprehensive strategic plan to build a strong foundation for clinical and translational research in response to our community needs. Buffalo is the second most populous city in New York State and has a rich cultural history. The proportion of underrepresented minorities in Buffalo in 2018 (50%) parallels that projected for the US in 2050, making Buffalo a microcosm of what the US will look like in 30 years. A similar proportion of our population experiences health disparities. The vision for our CTSA hub is to perform innovative research across the translational spectrum to improve the health of our community and the nation. We will develop, test and share novel approaches to engage difficult-to-engage populations and reduce health disparities in our community, which represents a “population of the future”. Guided by our vision, the CTSA has catalyzed a transformation of our environment since our CTSA was first funded in August 2015 with remarkable growth in clinical and translational research. Further, in just the past year, the UB medical school has moved into a spectacular new building and our clinical partner, Kaleida Health, the largest healthcare system in the region, opened the new Oishei Children’s Hospital, both on the Buffalo Niagara Medical Campus and connected to the Clinical and Translational Research Center devoted entirely to clinical and translational research that opened in 2012. This rapid and continuing trajectory of growth in healthcare and research in the region has resulted in a new 21st century Academic Health Center with healthcare, medical education and clinical and translational research on one campus in the heart of Buffalo, creating a foundation to enhance the impact of our CTSA even further. While launching our CTSA, we have prioritized participation in the national consortium through hosting and testing Innovation Labs as a team science tool, working with multiple hubs on initiatives to solve translational research barriers and sharing tools that we have developed with the CTSA consortium, including novel health informatics tools. Our CTSA has five ambitious but achievable aims, including: 1) Accelerate innovative translational research with teams that engage communities, regional stakeholders and the national consortium; 2) Train an excellent, diverse workforce to advance translation of discoveries; 3) Enhance inclusion of special populations across the lifespan and difficult-to-engage populations; 4) Streamline clinical research processes focusing on quality and efficiency with emphasis on multisite studies; 5) Develop, test and share biomedical informatics tools to integrate data from multiple sources to speed translation. Guided by our vision to perform research to improve the health of our community and the nation, we will continue our momentum to expand translational research, train our diverse workforce, streamline processes, engage our community, and actively contribute to the national consortium. The University at Buffalo Clinical and Translational Science Institute (CTSI) is the coordinating center of the Buffalo Translational Consortium, which includes the region's premier research, educational and clinical institutions with influential community partners. The vision of the CTSI is to perform innovative clinical and translational research to reduce health disparities and improve the health of our community and the nation. We engage our community as research partners to create a shared environment to bring discoveries in the laboratory, clinic and community to benefit individual and public health.",University of Buffalo Clinical and Translational Science Institute,10053435,UL1TR001412,"['Achievement', 'Address', 'Adopted', 'African American', 'Buffaloes', 'Center for Translational Science Activities', 'Cities', 'Clinic', 'Clinical', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials', 'Collaborations', 'Communities', 'Community Health', 'County', 'Coupled', 'Cultural Backgrounds', 'Data', 'Diverse Workforce', 'Ensure', 'Environment', 'Fostering', 'Foundations', 'Funding', 'Future', 'Goals', 'Growth', 'Health', 'Health Care Research', 'Health Personnel', 'Health Professional', 'Health Sciences', 'Healthcare', 'Healthcare Systems', 'Heart', 'Image', 'Imaging technology', 'Individual', 'Influentials', 'Informatics', 'Institutes', 'Institution', 'Knowledge', 'Laboratories', 'Learning', 'Life Expectancy', 'Longevity', 'Medical', 'Medical Education', 'Medical center', 'Methods', 'Natural Language Processing', 'New York', 'Outcomes Research', 'Participant', 'Pediatric Hospitals', 'Phenotype', 'Population', 'Poverty', 'Process', 'Program Development', 'Prospective Studies', 'Public Health', 'Public Health Informatics', 'Recording of previous events', 'Recruitment Activity', 'Refugees', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Schools', 'Science', 'Sensitivity and Specificity', 'Site', 'Speed', 'Strategic Planning', 'System', 'Testing', 'Training', 'Translational Research', 'Translations', 'Underrepresented Minority', 'Universities', 'Vision', 'Work', 'Workforce Development', 'base', 'biomedical informatics', 'clinical center', 'clinical data warehouse', 'community partnership', 'data sharing', 'education research', 'experience', 'health care disparity', 'health disparity', 'imaging genetics', 'improved', 'informatics tool', 'innovation', 'interoperability', 'medical schools', 'multidisciplinary', 'multiple data sources', 'named group', 'novel', 'novel strategies', 'recruit', 'response', 'sharing platform', 'skills', 'social health determinants', 'structured data', 'tool', 'translational impact', 'translational pipeline', 'translational scientist', 'unstructured data']",NCATS,STATE UNIVERSITY OF NEW YORK AT BUFFALO,UL1,2020,4118079,0.004943483246164309
"Center for Clinical and Translational Science The COVID-19 global emergency raises many difficult patient care and healthcare management questions. Which drugs are the most viable candidates for a given patient? How can we efficiently and effectively assemble the right cohort for a trial? What social determinants impact course and outcome? How can we rapidly deploy clinical decision support tools when new knowledge is available every day? The N3C is a partnership across the Centers for Translational Science Award (CTSA) hubs, several HHS agencies, distributed clinical data networks (PCORnet, OHDSI, ACT/i2b2, TriNetX), and other partner organizations. The N3C aims to improve the efficiency and accessibility of analyses with COVID-19 clinical data, expand our ability to analyze and understand COVID, and demonstrate a novel approach for collaborative pandemic data sharing. Under this proposal we will contribute electronic health record data on patients afflicted with COVID-19 and appropriate control patients. We will also participate in three workstreams: (a) Phenotype and Data Acquisition (brining our extensive experience with development of patient registries and data repositories), (b) Data Ingestion and Harmonization (contributing our experience with harmonizing and terminologies for UAB, Columbia University, the NIH’s Biomedical Translational Research Information System (BTRIS) and the Unified Medical Language System), and (c) Collaborative Analytics (with experience in developing collaborative platforms for team-based translational science and analytics for precision medicine). This project seeks to contribute data and expertise to the National COVID Cohort Collaborative (N3C), a national database of clinical data from the health records of patients with COVID-19. Data will be drawn from the UAB Hospital data repository, which includes records on over 500 patients with documented COVID-19. Project team members will contribute to N3C Workstreams, including Phenotype and Data Acquisition, Data Curation, and Collaborative Analytics.",Center for Clinical and Translational Science,10169828,UL1TR003096,"['Award', 'COVID-19', 'Center for Translational Science Activities', 'Clinical Data', 'Clinical Sciences', 'Data', 'Development', 'Electronic Health Record', 'Emergency Situation', 'Hospitals', 'Information Systems', 'Knowledge', 'Machine Learning', 'Outcome', 'Patient Care', 'Patients', 'Pharmaceutical Preparations', 'Process', 'Records', 'Science', 'Terminology', 'Translational Research', 'Unified Medical Language System', 'United States National Institutes of Health', 'Universities', 'analytical tool', 'base', 'clinical center', 'clinical database', 'clinical decision support', 'cohort', 'collaborative approach', 'collaboratory', 'coronavirus disease', 'data acquisition', 'data curation', 'data enclave', 'data harmonization', 'data ingestion', 'data sharing', 'data warehouse', 'experience', 'health data', 'health management', 'improved', 'member', 'novel strategies', 'pandemic disease', 'patient health information', 'patient registry', 'phenotypic data', 'precision medicine', 'repository', 'social determinants', 'support tools']",NCATS,UNIVERSITY OF ALABAMA AT BIRMINGHAM,UL1,2020,148500,-0.011450183824622331
"Development of a novel method for cryopreservation of Drosophila melanogaster PROJECT SUMMARY This proposal seeks to develop a resource for the preservation of the fruit fly, Drosophila melanogaster. This insect is a foundational model organism for biological research. Over a century of work, an enormous number of fly strains harboring different mutant alleles or transgenic constructs have been generated. However, one limitation of working with flies is that there is as yet no practical method for cryopreservation of Drosophila strains. Conventional methods of vitrifying Drosophila were developed in the early 1990s and were never widely adopted due to the difficulty in performing the protocols. This is a problem from a practical perspective since all these strains need to be individually maintained in continuous culture at substantial cost and labor, and also from a scientific perspective, since in the process of continuous culture mutations can accumulate and contamination can occur, degrading the value of these resources for future experiments. A novel approach for cryopreservation of Drosophila is proposed for this R24 resource center. Isolated embryonic nuclei, rather than intact embryos, will be cryopreserved and then nuclear transplantation via microinjection will be used to create clones derived from the cryopreserved nuclei. This approach avoids the issues associated with the impermeability of embryonic membranes that have prevented the use of conventional cryopreservation approaches that have been used with other organisms. Embryonic nuclei will be cryopreserved using a naturally inspired approach. Diverse biological systems (plants, insects, etc.) survive dehydration, drought, freezing temperatures and other stresses through the use of osmolytes. On an applied level, the proposed investigation has the potential to transform preservation of Drosophila lines by 1) preserving subcellular components (specifically nuclei) as opposed to embryos; and 2) automating much of the workflow. In the long- term, the goal of this resource center is to develop a robust and scalable protocol for cryopreservation of Drosophila, thus reducing the cost and improving the quality of long-term strain maintenance. PROJECT NARRATIVE The fruit fly, Drosophila melanogaster, is a very important model organism for biomedical research. The goal of this resource center is to develop effective methods of preserving fruit flies in order to lower the costs and improve the quality of stock maintenance. The approach leverages recent scientific advances to develop a new, highly automated approach for preserving fruit flies.",Development of a novel method for cryopreservation of Drosophila melanogaster,9935719,R24OD028444,"['Adopted', 'Algorithms', 'Alleles', 'Animal Model', 'Asses', 'Automation', 'Biological', 'Biomedical Research', 'Cell Nucleus', 'Cells', 'Cellular biology', 'Communities', 'Cryopreservation', 'Dehydration', 'Development', 'Developmental Biology', 'Drosophila genus', 'Drosophila melanogaster', 'Droughts', 'Embryo', 'Engineering', 'Evolution', 'Formulation', 'Foundations', 'Freezing', 'Future', 'Genetic', 'Genome', 'Genotype', 'Goals', 'Image', 'Individual', 'Insecta', 'Investigation', 'Machine Learning', 'Maintenance', 'Mechanics', 'Membrane', 'Methods', 'Microinjections', 'Molecular Biology', 'Monoclonal Antibody R24', 'Mutation', 'Neurosciences', 'Nuclear', 'Organism', 'Plants', 'Process', 'Protocols documentation', 'Raman Spectrum Analysis', 'Recovery', 'Resources', 'Robotics', 'Scientific Advances and Accomplishments', 'Spectrum Analysis', 'Stress', 'System', 'Techniques', 'Temperature', 'Testing', 'Transgenic Organisms', 'Work', 'biological research', 'biological systems', 'cold temperature', 'cost', 'epigenome', 'experimental study', 'fly', 'genetic technology', 'high throughput screening', 'improved', 'individual response', 'mutant', 'novel', 'novel strategies', 'nuclear transfer', 'preservation', 'prevent', 'tool']",OD,UNIVERSITY OF MINNESOTA,R24,2020,599090,0.003853138651352388
"South Carolina Clinical & Translational Research Institute (SCTR) PROJECT SUMMARY – SCTR INSTITUTE Parent Award UL1-TR001450 Since 2009, the South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina (SC) by creating a Learning Health System that supports high- quality clinical and translational research (CTR) and fosters collaboration and innovation. Headquartered at the Medical University of South Carolina (MUSC), SCTR has engaged stakeholders and created statewide partnerships to improve care and address social determinants of health across SC. However, greater than 75% of SC is rural, and all 46 counties contain areas designated as medically underserved, so health disparities remain an issue. Over the next five years, SCTR will strengthen its outreach to these medically underserved areas through collaboration with the Clemson University Health Extension Program and the MUSC Telehealth Center of Excellence. With a focus on implementation and dissemination as well as discovery, we will develop and demonstrate innovative technologies and outreach to improve the health of our stakeholders. We will build on prior successes and introduce innovative approaches to expand CTR across SC through the following aims: Aim 1. Extend and enhance high-quality, innovative, flexible curricula and training experiences for all levels of the CTR workforce, with particular emphasis on enhancing workforce heterogeneity and team science. Aim 2. Engage a diverse group of stakeholders as active partners in CTR to address health care priorities while enhancing the scientific knowledge base about collaboration and engagement. Aim 3. Promote greater inclusion across the full translational spectrum of research by engaging investigators from many disciplines and patient populations from diverse demographic backgrounds and geographic areas. Aim 4. Develop, demonstrate and disseminate innovative methods and processes to address barriers and accelerate the translation of research discoveries to improvements in human health that can be generalized to a variety of practice settings. Aim 5. Enhance the conduct of translational research through the development of secure and innovative informatics and digital health solutions, tools and methodologies that affect every aspect of CTR. SCTR’s vision is to be a major force in facilitating the translation of innovative science into practice to address the health priorities of the citizens of SC and beyond. To achieve this vision, SCTR’s mission is to catalyze the development of methods and technologies that lead to more efficient translation of biomedical discoveries into interventions that improve individual and public health. SCTR will serve as the statewide academic home for CTR, one that is well-integrated with SC’s healthcare systems and provides essential support for innovative, efficient, multidisciplinary research and research training. We will work within SCTR, with our partners across SC and with the CTSA Consortium to realize this vision. PROJECT NARRATIVE The South Carolina Clinical and Translational Research Institute (SCTR) has transformed the research environment across South Carolina by creating a Learning Health System characterized by strong training and infrastructure resources that stimulate collaboration and innovation as a means to accelerate the translation of biomedical research discoveries into human health improvements. A major focus of this application is to strengthen SCTR’s statewide collaborations through innovative partnerships and initiatives with an emphasis on rural and medically underserved communities where significant health disparities exist. We will develop, demonstrate and disseminate innovative ways to train a diverse workforce and address barriers to translational research, and we will continue to collaborate across the CTSA Consortium to maximize impact and improve the health of the nation.",South Carolina Clinical & Translational Research Institute (SCTR),10241088,UL1TR001450,"['2019-nCoV', 'Address', 'Administrative Supplement', 'Affect', 'Area', 'Award', 'Biomedical Research', 'COVID-19', 'COVID-19 pandemic', 'Caring', 'Clinical', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Clinical Trials Network', 'Clinical and Translational Science Awards', 'Clinical effectiveness', 'Collaborations', 'Communities', 'County', 'Databases', 'Development', 'Discipline', 'Diverse Workforce', 'Educational Curriculum', 'Electronic Health Record', 'Emergency Situation', 'Environment', 'Fast Healthcare Interoperability Resources', 'Fostering', 'Geographic Locations', 'Health', 'Health Priorities', 'Health Sciences', 'Health system', 'Healthcare', 'Healthcare Systems', 'Heterogeneity', 'Home environment', 'Human', 'Individual', 'Informatics', 'Information Retrieval', 'Interdisciplinary Study', 'Intervention', 'Lead', 'Learning', 'Link', 'Medical', 'Medically Underserved Area', 'Methodology', 'Methods', 'Mission', 'Modeling', 'Natural Language Processing', 'Outcome', 'Parents', 'Patients', 'Phenotype', 'Population', 'Population Heterogeneity', 'Process', 'Public Health', 'Research', 'Research Institute', 'Research Personnel', 'Research Training', 'Resources', 'Rural', 'Science', 'Secure', 'South Carolina', 'Support System', 'Technology', 'Terminology', 'Text', 'Training', 'Training and Infrastructure', 'Translating', 'Translational Research', 'Translations', 'Universities', 'Vision', 'Work', 'base', 'biomedical informatics', 'cohort', 'coronavirus disease', 'cost effective', 'data access', 'data enclave', 'data interoperability', 'data modeling', 'data sharing', 'digital', 'expectation', 'experience', 'flexibility', 'health disparity', 'improved', 'innovation', 'innovative technologies', 'interest', 'knowledge base', 'medically underserved', 'member', 'method development', 'outreach', 'parent grant', 'patient population', 'practice setting', 'programs', 'response', 'rural underserved', 'social health determinants', 'success', 'synergism', 'telehealth', 'tool', 'translational pipeline', 'web services']",NCATS,MEDICAL UNIVERSITY OF SOUTH CAROLINA,UL1,2020,100000,0.0036815936184711514
"Tufts Clinical and Translational Science Institute (N3C Supplement) PROJECT SUMMARY Tufts Clinical and Translational Science Institute (Tufts CTSI) is based on the conviction that authentic involvement of the entire spectrum of clinical and translational research (CTR) is critical to fulfilling the promise of biomedical science for meeting the public's needs. This includes not only from translation from bench to bedside (T1 translation), but also, crucially for having health impact, translation into effective clinical practice (T2), care delivery and public health (T3), and health policy (T4). Advances on all of these fronts is increasingly dependent on making effective use of scientific data from multiple domains. The COVID-19 global emergency presents both an immediate challenge and an opportunity to progress on important data sharing aims emphasized by NIH. In response, NCATS and the Centers for Translational Science Award (CTSA) hubs, several HHS agencies, and other partnering organizations have committed to developing a next-generation repository for clinical data related to COVID-19, the National COVID Cohort Collaborative (N3C), as a means of accelerating global research into the disease and aiding the development of diagnostics, therapeutics, and effective vaccines. The N3C initiative's goal of improving the efficiency and accessibility of analyses with clinical data is consistent with the primary informatics objectives of Tufts CTSI, which am to reduce barriers to the integration of healthcare and research by providing innovative systems, data repositories, and analytical tools, and by enabling greater exchange and collaboration through interoperability, standardization, and resource sharing. In- line with shared objectives, in this supplement we seek to contribute to the N3C initiative as a data provider and thought partner through the following specific aims: (1) continue to play an important role providing tools and resources for N3C's analytics platform; and (2) ensure Tufts CTSI's Informatics Program has sufficient staff and technical resources to continue to provide COVID-specific patient data from our hub to the N3C repository. PROJECT NARRATIVE An integrated, continuously updated data repository and a platform of putting powerful analytics capabilities at the disposal of the scientific community can generate insights into COVID-19 and accelerate the development of effective treatments and vaccines to counter the disease. In this project, Tufts Clinical and Translational Science Institute plans to contribute to this goal by providing carefully structured clinical data and innovative informatics tools to the National COVID Cohort Collaborative (N3C), an initiative demonstrating a novel approach for collaborative pandemic data sharing.",Tufts Clinical and Translational Science Institute (N3C Supplement),10172199,UL1TR002544,"['Award', 'COVID-19', 'Caring', 'Center for Translational Science Activities', 'Clinical Data', 'Clinical Research', 'Clinical Sciences', 'Collaborations', 'Communities', 'Data', 'Development', 'Diagnostic', 'Disease', 'Emergency Situation', 'Ensure', 'Environment', 'Funding', 'Goals', 'Health', 'Health Care Research', 'Health Policy', 'Informatics', 'Institutes', 'Knowledge', 'Machine Learning', 'Mission', 'Outcome', 'Patients', 'Pharmaceutical Preparations', 'Play', 'Provider', 'Public Health', 'Research', 'Research Personnel', 'Resource Sharing', 'Resources', 'Risk', 'Role', 'Science', 'Secure', 'Speed', 'Standardization', 'Structure', 'System', 'Time', 'Translational Research', 'Translations', 'Treatment Efficacy', 'United States National Institutes of Health', 'Update', 'Vaccines', 'analytical tool', 'base', 'bench to bedside', 'care delivery', 'clinical data warehouse', 'clinical decision support', 'clinical practice', 'cohort', 'collaborative approach', 'convict', 'coronavirus disease', 'data integration', 'data sharing', 'data warehouse', 'effective therapy', 'health management', 'improved', 'informatics tool', 'innovation', 'insight', 'interoperability', 'meetings', 'next generation', 'novel strategies', 'pandemic disease', 'programs', 'repository', 'response', 'social determinants', 'support tools', 'tool']",NCATS,TUFTS UNIVERSITY BOSTON,UL1,2020,100000,0.014538828621257969
"AUGS/DUKE UrogynCREST program PROJECT SUMMARY Health Services Research (HSR) and predictive analytics are rapidly growing fields and will have enormous implications for women’s health research in pelvic floor disorders (PFDs). The AUGS/DUKE Urogynecology Clinical Research Educational Scientist Training (UrogynCREST) program will prepare participants to recognize the critical role that data play in delivering high quality health care. It brings together expertise in health service and women’s health research, medical informatics and prediction modeling. This program will target Urogynecology Faculty at the Assistant Professor level who seek successful careers in health services research (HSR) and analytics. Participants will obtain skills through a combination of didactic and interactive coursework; hands-on manipulation of data through extraction, cleaning, and analysis; and project-based one on one mentoring. The UrogynCREST program will be an interactive, hands-on educational program with centralized activities organized and delivered by distance through a popular on-line learning platform called Sakai, with educational software designed to support teaching, research and collaboration. A diverse faculty with expertise in data sciences teaches courses and the advanced methodology required to perform HSR. Yearly in-person meetings at the annual American Urogynecologic Society meeting enhance networking and the development of partnerships between participants from various institutions, as well as, interactions with the mentors and other HSR in the field. The program’s strategy allows national leaders with particular skills in the field to provide their knowledge to the participants and help mentor them through development of a relevant research question and identification of an appropriate and existing database(s) to address the question. With the guidance of a dedicated statistician and analyst programmer, participants will learn and perform the necessary computer programming needed to extract, clean and analyze these data. Participants whose projects involve the development of prediction models in the form of scores, nomograms or other tools will learn how to build and validate such tools in the existing project. Each participant’s project will culminate in the completion of a submitted manuscript to a peer- reviewed journal or study proposal and publicly available tools when relevant. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and resources for invigorating data discovery and tools for investigations in HSR specifically addressing (PFDs). PROJECT NARRATIVE: The AUGS/DUKE UrogynCREST program will prepare participants to recognize the critical role that data play in delivering high quality health care for pelvic floor disorders. It will add structure to the health data science education for Assistant Professor Level Faculty in Urogynecology by bringing together expertise in health service and women’s health research, medical informatics, and prediction modeling. Overall, the program will shape future scientific leaders in Urogynecology by encouraging the development of clinical-scientists and provide the skills and mentorship for invigorating data discovery and tools for investigations in health service research specifically addressing pelvic floor disorders.",AUGS/DUKE UrogynCREST program,9918946,R25HD094667,"['Address', 'Age', 'American', 'Area', 'Caring', 'Clinical Research', 'Collaborations', 'Communities', 'Connective Tissue', 'Data', 'Data Discovery', 'Data Science', 'Databases', 'Development', 'E-learning', 'Educational process of instructing', 'Faculty', 'Fecal Incontinence', 'Fostering', 'Future', 'Goals', 'Health Services', 'Health Services Research', 'Healthcare', 'Infrastructure', 'Institution', 'Instruction', 'Investigation', 'Journals', 'Knowledge', 'Lead', 'Learning', 'Manuscripts', 'Medical Informatics', 'Mentors', 'Mentorship', 'Methodology', 'Methods', 'Modernization', 'Muscle', 'Nomograms', 'Participant', 'Peer Review', 'Pelvic Floor Disorders', 'Pelvis', 'Persons', 'Play', 'Predictive Analytics', 'Process', 'Public Health', 'Research', 'Resources', 'Role', 'Science', 'Scientific Advances and Accomplishments', 'Scientist', 'Shapes', 'Societies', 'Software Design', 'Structure', 'Techniques', 'Testing', 'Training Programs', 'Urinary Incontinence', 'Woman', 'Women&apos', 's Health', 'base', 'career', 'clinical decision-making', 'clinical development', 'computer program', 'computer science', 'data tools', 'design', 'health care quality', 'health data', 'improved', 'injured', 'innovation', 'meetings', 'pelvic organ prolapse', 'predictive modeling', 'professor', 'programs', 'recruit', 'science education', 'skills', 'social', 'statistical and machine learning', 'tool']",NICHD,DUKE UNIVERSITY,R25,2020,153800,-0.021053079230440214
"Omics for TB:  Response to Infection and Treatment Abstract – Overview With about 10 million new cases of active disease and 1.8 million deaths annually, TB is a global health emergency. A distinguishing feature of TB disease is its biological heterogeneity, which manifests at the clinical level chiefly in 2 forms: disease progression and treatment response. The premise of this Program is that the heterogeneous outcomes of TB infection and treatment are determined by the interplay of competing regulatory networks between the pathogen and the host. Our primary goal is to apply systems biology approaches to elucidate the biological control underlying the variability of disease outcome and response to treatment. Our first specific aim is to define novel host regulators of TB disease progression in vivo, and the innate and adaptive networks they control. We will also seek to define novel Mtb regulators of TB treatment response, and the Mtb regulatory networks that they control. This work will allow us to produce and validate host and Mtb models of TB disease progression and treatment response. Altogether, this program addresses key unanswered questions that stymie efforts to combat the TB pandemic. Our team has perfected the required platforms and scientific approaches to execute this ambitious research plan in a timely and cost- effective manner. All the participating investigators have strong records of interacting productively, and of disseminating their data and reagents to the scientific community. Project Narrative - Omics for TB: Response to Infection and Treatment Mycobacterium tuberculosis causes ~10 million new cases of active disease and 1.8 million deaths each year, and our tools to combat tuberculosis (TB) disease are universally outdated and overmatched. This project combines separate advances in systems biology and network modeling to produce experimentally grounded and verifiable systems-level models of the host and MTB regulatory networks that affect disease progression and response to treatment.",Omics for TB:  Response to Infection and Treatment,9878754,U19AI135976,"['Address', 'Affect', 'Bacteria', 'Biological', 'Cessation of life', 'Clinical', 'Collection', 'Communicable Diseases', 'Communities', 'Complex', 'Data', 'Data Set', 'Disease', 'Disease Outcome', 'Disease Progression', 'Eicosanoids', 'Elements', 'Genetic Transcription', 'Goals', 'Human', 'Immunologic Receptors', 'Infection', 'Inflammatory Response', 'Infrastructure', 'Machine Learning', 'Mass Spectrum Analysis', 'Methodology', 'Modeling', 'Molecular Profiling', 'Mouse Strains', 'Multiplexed Ion Beam Imaging', 'Mus', 'Mycobacterium tuberculosis', 'Network-based', 'Outcome', 'Pharmaceutical Preparations', 'Phenotype', 'Predisposition', 'Proteomics', 'Reagent', 'Receptor Activation', 'Records', 'Regulator Genes', 'Research', 'Research Personnel', 'Resources', 'Sampling', 'Symptoms', 'System', 'Systems Analysis', 'Systems Biology', 'Technology', 'Time', 'Treatment outcome', 'Tuberculosis', 'Vaccines', 'Work', 'base', 'biological heterogeneity', 'chemotherapy', 'combat', 'cost effective', 'data tools', 'design', 'drug-sensitive', 'global health emergency', 'high risk', 'in vivo', 'member', 'metabolomics', 'mouse model', 'network models', 'novel', 'pandemic disease', 'pathogen', 'predictive signature', 'programs', 'protein protein interaction', 'response', 'tool', 'transcriptomics', 'treatment response', 'tuberculosis treatment']",NIAID,SEATTLE CHILDREN'S HOSPITAL,U19,2020,3351478,0.009195080837275781
"The Center for Innovation in Intensive Longitudinal Studies (CIILS) PROJECT SUMMARY Significance. The Intensive Longitudinal Behavior Network (ILHBN) provides an unprecedented opportunity to advance and shape the future landscape of health behavior science and related intervention practice. The proposed Research Coordinating Center, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University (Penn State), will bring together an interdisciplinary team to synergistically support and coordinate research activities across a diverse portfolio of anticipated U01 projects to accomplish the Network’s larger goal of sustained innovation in the use of intensive longitudinal data (ILD) and associated methods in the study of health behavior change, and in informing prevention and intervention designs. Innovation. The proposed organizational structure of the ILHBN as a small-world network is motivated by our team’s collective decades of experience with multidisciplinary and multi-site collaborations, and is designed to facilitate information flow, collective decision making, and coordination of goals and effort within the ILHBN. Approach. CIILS consists of five Cores with expertise in management of multi-site projects and coordinating centers (Administrative Core); development of novel methods for analysis of ILD (Methods Core); ILD collection, harmonization, sharing, security, as well as collection of digital footprints (Data Core); ILD design, harmonization and instrumentation support (Design Core); and integration of health behavior theories, translation, and implementation of within-person health preventions/interventions (Theory Core). Key personnel with rich and complementary expertise are supported by a roster of advisory Co-Is at Penn State and distributed consultants who are leaders and innovators in their respective fields. Institutional support and contributed staff time by Penn State provide robust infrastructure, expertise, and “boots on the ground” to support the operation and coordination activities of ILHBN; and a wealth of additional resources to elevate and broaden the collective impacts of the Network. PROJECT NARRATIVE This project proposes an RCC, the Center for Innovation in Intensive Longitudinal Studies (CIILS), housed at the Pennsylvania State University, to provide a repertoire of expertise and resources to support the Intensive Longitudinal Health Behavior Network (ILHBN). Our interdisciplinary team – consisting of social scientists with expertise in design and management of intensive longitudinal studies; methodological experts who are leading figures in developing novel within-person analytic techniques; health theorists and prevention/intervention experts well-versed in the translation of health theories into within-person health intervention; cyberscience experts with expertise in collection of digital footprints, data security and data sharing issues; and administrative personnel with expertise in management and coordination of network activities – is uniquely poised to advance the collective innovations of the ILHBN by synergistically supporting and coordinating research activities across a diverse portfolio of anticipated U01 projects.",The Center for Innovation in Intensive Longitudinal Studies (CIILS),10007746,U24AA027684,"['Administrative Personnel', 'Algorithms', 'Behavior', 'Big Data', 'Collaborations', 'Collection', 'Communication', 'Communities', 'Consultations', 'Data', 'Data Analyses', 'Data Analytics', 'Data Collection', 'Data Security', 'Databases', 'Decision Making', 'Development', 'Devices', 'Future', 'General Population', 'Goals', 'Health', 'Health Sciences', 'Health behavior', 'Health behavior change', 'Healthcare', 'Human Resources', 'Individual', 'Infrastructure', 'Intervention', 'Lead', 'Link', 'Longitudinal Studies', 'Machine Learning', 'Manuals', 'Measures', 'Methodology', 'Methods', 'Neurobiology', 'Pennsylvania', 'Persons', 'Positioning Attribute', 'Preventive Intervention', 'Process', 'Production', 'Progress Reports', 'Protocols documentation', 'Publications', 'Records', 'Regulation', 'Reporting', 'Research', 'Research Activity', 'Research Design', 'Resources', 'Science', 'Scientist', 'Security', 'Shapes', 'Site', 'Social Work', 'Source', 'Structure', 'Technical Expertise', 'Techniques', 'Testing', 'Time', 'Training', 'Translations', 'United States National Institutes of Health', 'Universities', 'Update', 'Visualization software', 'Workplace', 'control theory', 'data de-identification', 'data harmonization', 'data management', 'data portal', 'data privacy', 'data sharing', 'data tools', 'data visualization', 'data warehouse', 'design', 'digital', 'dynamic system', 'experience', 'human subject', 'innovation', 'instrumentation', 'member', 'multidisciplinary', 'novel', 'operation', 'organizational structure', 'preservation', 'social', 'success', 'theories', 'therapy design']",NIAAA,PENNSYLVANIA STATE UNIVERSITY-UNIV PARK,U24,2020,103799,-0.007240429844719558
"WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",WASHINGTON UNIVERSITY SCHOOL OF MEDICINE UNDIAGNOSED DISEASES NETWORK CLINICAL SITE,10124937,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,150000,-0.006690567338544198
"Washington University School of Medicine Undiagnosed Diseases Network Clinical Site 1.0 PROJECT SUMMARY The scientific premise of this application is that the individualized translational research process of the Undiagnosed Diseases Network (UDN) developed during Phase I is scalable and that its impact on patients, families, and disease discovery can be advanced and sustained by addition of a Clinical Site at Washington University School of Medicine (WUSM). WUSM represents a large academic medical center that is fully integrated with world-renowned basic science capabilities and demonstrated expertise in a gene first approach for patients with undiagnosed diseases. The highly collaborative clinical and biomedical research culture at WUSM promotes interactions within and across Departments, with institutional genomic, clinical, computational, and model system experts, and with colleagues regionally, nationally, and internationally. These interactions support recruitment, selection, evaluation, diagnosis discovery, and follow up of pediatric and adult patients with undiagnosed diseases through both established networks and individual referrals. Building on this infrastructure, WUSM faculty and staff will advance the success of the UDN in diagnosing and managing disease in undiagnosed patients by, first, using, refining, and improving protocols designed during Phase I of the UDN for comprehensive, timely clinical evaluations of 30 undiagnosed patients annually. Secondly, we will collect, securely store, and share standardized, high-quality clinical and laboratory data including genotyping, phenotyping, and documentation of environmental exposures and promote an integrated and collaborative community across the UDN and among laboratory and clinical investigators focused on defining the pathophysiology, cell biologic, and molecular mechanisms that cause these difficult to diagnose diseases. Thirdly, the WUSM UDN Clinical Site will propose a bioinformatics plan for leveraging institutional infrastructure and expertise to develop innovative strategies to improve discovery of pathogenic variants. Fourthly, the assessment, dissemination, outreach, and training plan will accelerate assessment and dissemination of data, protocols, consent materials, and methods, availability of educational and outreach materials for participants, clinicians, and other researchers, engagement of underrepresented minorities, and training for students, fellows, staff, and faculty in collaboration with WUSM’s Clinical and Translational Science Award infrastructure. Finally, WUSM will make a clear institutional commitment to maintain its Clinical Site, to adapt UDN Phase I practices for sustainability, to contribute to formation of a sustainable national UDN resource, and to adapt to unique needs and unexpected circumstances that may arise once Common Fund support ends in fiscal year 2022. 2.0 PROJECT NARRATIVE Undiagnosed diseases in children and adults represent frustrating and costly challenges for patients, families, physicians, and society. Building on established institutional infrastructure similar to the Undiagnosed Diseases Network (UDN), Washington University School of Medicine (WUSM) will establish a UDN Clinical Site to improve the level of diagnosis and care for patients with undiagnosed diseases, facilitate research into the etiology of undiagnosed diseases, and promote an integrated and collaborative community across multiple UDN Clinical Sites, Sequencing Cores, Model Organisms Screening Centers, and among laboratory and clinical investigators. Specifically, the WUSM UDN Clinical Site will annually recruit, select, evaluate, and follow 30 participants with disorders in any clinical specialty, adult and pediatric, provide comprehensive clinical evaluations that require <5 days and follow up, and participate in all UDN protocols, data management and sharing, and sustainability planning.",Washington University School of Medicine Undiagnosed Diseases Network Clinical Site,9977220,U01HG010215,"['Academic Medical Centers', 'Accreditation', 'Adult', 'Animal Model', 'Basic Science', 'Bioinformatics', 'Biological', 'Biomedical Research', 'Businesses', 'Cells', 'Child', 'Childhood', 'Classification', 'Clinical', 'Clinical Investigator', 'Clinical Research', 'Clinical Sciences', 'Clinical and Translational Science Awards', 'Collaborations', 'Communities', 'Computer Models', 'Consent', 'DNA Sequencing Facility', 'Data', 'Development', 'Diagnosis', 'Diagnostic', 'Disease', 'Disease Management', 'Documentation', 'Environmental Exposure', 'Etiology', 'Evaluation', 'Expert Systems', 'Faculty', 'Family', 'Family Physicians', 'Functional disorder', 'Funding', 'Genes', 'Genomics', 'Genotype', 'Geographic Locations', 'Individual', 'Infrastructure', 'Institutes', 'International', 'Laboratories', 'Methods', 'Molecular', 'Monitor', 'Network-based', 'Participant', 'Pathogenicity', 'Patient Care', 'Patients', 'Phase', 'Phase I Clinical Trials', 'Phenotype', 'Process', 'Protocols documentation', 'Research', 'Research Personnel', 'Resources', 'Review Committee', 'Secure', 'Societies', 'Standardization', 'Time', 'Training', 'Translational Research', 'Underrepresented Minority', 'Universities', 'Variant', 'Washington', 'clinical practice', 'clinical research site', 'cost', 'data dissemination', 'data management', 'data sharing', 'deep learning', 'design', 'disease diagnosis', 'exome sequencing', 'experience', 'follow-up', 'genetic variant', 'genome sequencing', 'improved', 'innovation', 'medical schools', 'medical specialties', 'network models', 'operation', 'outreach', 'recruit', 'research clinical testing', 'screening', 'sequencing platform', 'student training', 'success', 'transcriptome sequencing']",NHGRI,WASHINGTON UNIVERSITY,U01,2020,550000,-0.006690567338544198
"Big Flow Cytometry Data: Data Standards, Integration and Analysis PROJECT SUMMARY Flow cytometry is a single-cell measurement technology that is data-rich and plays a critical role in basic research and clinical diagnostics. The volume and dimensionality of data sets currently produced with modern instrumentation is orders of magnitude greater than in the past. Automated analysis methods in the field have made great progress in the past five years. The tools are available to perform automated cell population identification, but the infrastructure, methods and data standards do not yet exist to integrate and compare non-standardized big flow cytometry data sets available in public repositories. This proposal will develop the data standards, software infrastructure and computational methods to enable researchers to leverage the large amount of public cytometry data in order to integrate, re-analyze, and draw novel biological insights from these data sets. The impact of this project will be to provide researchers with tools that can be used to bridge the gap between inference from isolated single experiments or studies, to insights drawn from large data sets from cross-study analysis and multi-center trials. PROJECT NARRATIVE The aims of this project are to develop standards, software and methods for integrating and analyzing big and diverse flow cytometry data sets. The project will enable users of cytometry to directly compare diverse and non-standardized cytometry data to each other and make biological inferences about them. The domain of application spans all disease areas where cytometry is utilized.","Big Flow Cytometry Data: Data Standards, Integration and Analysis",9969443,R01GM118417,"['Address', 'Adoption', 'Advisory Committees', 'Archives', 'Area', 'Basic Science', 'Bioconductor', 'Biological', 'Biological Assay', 'Cells', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computing Methodologies', 'Cytometry', 'Data', 'Data Analyses', 'Data Analytics', 'Data Files', 'Data Set', 'Development', 'Dimensions', 'Disease', 'Environment', 'Flow Cytometry', 'Foundations', 'Genes', 'Goals', 'Heterogeneity', 'Immune System Diseases', 'Immunologic Monitoring', 'Industry', 'Informatics', 'Infrastructure', 'International', 'Knock-out', 'Knowledge', 'Manuals', 'Measurable', 'Measurement', 'Measures', 'Meta-Analysis', 'Metadata', 'Methods', 'Modernization', 'Mouse Strains', 'Multicenter Trials', 'Mus', 'Output', 'Phenotype', 'Play', 'Population', 'Procedures', 'Protocols documentation', 'Reagent', 'Research', 'Research Personnel', 'Retrieval', 'Role', 'Societies', 'Software Tools', 'Standardization', 'Technology', 'Testing', 'Validation', 'Work', 'automated analysis', 'base', 'bioinformatics tool', 'body system', 'cancer diagnosis', 'clinical diagnostics', 'community based evaluation', 'computerized tools', 'data exchange', 'data integration', 'data standards', 'data submission', 'data warehouse', 'experimental study', 'human disease', 'insight', 'instrument', 'instrumentation', 'large datasets', 'mammalian genome', 'multidimensional data', 'novel', 'operation', 'phenotypic data', 'public repository', 'repository', 'research and development', 'software development', 'software infrastructure', 'statistics', 'supervised learning', 'tool', 'vaccine development']",NIGMS,FRED HUTCHINSON CANCER RESEARCH CENTER,R01,2020,158388,-0.005553148915553494
"Computational Methods for Enhancing Privacy in Biomedical Data Sharing Project Summary Data sharing is essential to modern biomedical data science. Access to a large amount of genomic and clinical data can help us better understand human genetics and its impact on health and disease. However, the sensitive nature of biomedical information presents a key bottleneck in data sharing and collection efforts, limiting the utility of these data for science. The goal of this project is to leverage cutting-edge advances in cryptography and information theory to develop innovative computational frameworks for privacy-preserving sharing and analysis of biomedical data. We will draw upon our recent success in developing secure pipelines for collaborative biomedical analyses to address the imminent need to share sensitive data securely and at scale.  Practical adoption of existing privacy-preserving techniques in biomedicine has thus far been largely limited due to two major pitfalls, which this project overcomes with novel technical advances. First, emerging cryptographic data sharing frameworks, which promise to enable collaborative analysis pipelines that securely combine data across multiple institutions with theoretical privacy guarantees, are too costly to support complex and large-scale computations required in biomedical analyses. In this project, we will build upon recent advances in cryptography (e.g., secure distributed computation, pseudorandom correlation, zero-knowledge proofs) to significantly enhance the scalability and security of cryptographic biomedical data sharing pipelines. Second, existing approaches that locally transform data to protect sensitive information before sharing (e.g. de-identification techniques) either offer insufficient levels of protection or require excessive perturbation in order to ensure privacy. We will draw upon recent tools from information theory to develop effective local privacy protection methods that achieve superior utility-privacy tradeoffs on a range of biomedical data including genomes, transcriptomes, and medical images by directly exploiting the latent correlation structure of the data.  To promote the use of our privacy techniques, we will create production-grade software of our tools and publicly release them. We will also actively participate in international standard-setting organizations in genomics, e.g. GA4GH and ICDA, to incorporate our insights into community guidelines for biomedical privacy. Successful completion of these aims will result in computational methods and software tools that open the door to secure sharing and analysis of massive sets of sensitive genomic and clinical data. Our long-term goal is to broadly enable data sharing and collaboration efforts in biomedicine, thus empowering researchers to better understand the molecular basis of human health and to drive translation of new biological insights to the clinic. Project Narrative Rapidly-growing volume of biomedical datasets around the world promises to enable unprecedented insights into human health and disease. However, increasing concerns for individual privacy severely limited the extent of data sharing in the field. This project draws upon cutting-edge tools from cryptography and information theory to develop effective privacy- preserving methods for collecting, sharing, and analyzing sensitive biomedical data to empower advances in genomics and medicine.",Computational Methods for Enhancing Privacy in Biomedical Data Sharing,10017554,DP5OD029574,"['Address', 'Adoption', 'Biological', 'Biology', 'Biomedical Research', 'Brain', 'Clinic', 'Clinical Data', 'Code', 'Collaborations', 'Communication', 'Communities', 'Complex', 'Complex Analysis', 'Computational Biology', 'Computer software', 'Computing Methodologies', 'Data', 'Data Analyses', 'Data Collection', 'Data Science', 'Data Set', 'Disease', 'Electronic Health Record', 'Engineering', 'Enhancement Technology', 'Ensure', 'Foundations', 'Genome', 'Genomic medicine', 'Genomics', 'Goals', 'Guidelines', 'Health', 'Human', 'Human Genetics', 'Image', 'Individual', 'Information Theory', 'Institutes', 'Institution', 'Interdisciplinary Study', 'International', 'Knowledge', 'Letters', 'Machine Learning', 'Magnetic Resonance Imaging', 'Mainstreaming', 'Mathematics', 'Medical Genetics', 'Medical Imaging', 'Mentorship', 'Methods', 'Modernization', 'Molecular', 'Nature', 'Pattern', 'Pharmacology', 'Policies', 'Polynomial Models', 'Preservation Technique', 'Privacy', 'Privatization', 'Production', 'Research', 'Research Personnel', 'Resources', 'Risk', 'Science', 'Secure', 'Security', 'Software Tools', 'Structure', 'Techniques', 'Technology', 'Translations', 'Vision', 'Work', 'analysis pipeline', 'base', 'biomedical data science', 'computer framework', 'computing resources', 'cost', 'cryptography', 'data sharing', 'design', 'empowered', 'experience', 'experimental study', 'genetic analysis', 'genome wide association study', 'genomic data', 'infancy', 'innovation', 'insight', 'novel', 'privacy preservation', 'privacy protection', 'software development', 'statistics', 'structured data', 'success', 'task analysis', 'tool', 'transcriptome', 'transcriptomics', 'web server']",OD,"BROAD INSTITUTE, INC.",DP5,2020,392799,0.028986758800873147
"Predicting tuberculosis outcomes using genotypic and biomarker signatures PROJECT SUMMARY/ABSTRACT Tuberculosis (TB) is caused by an infectious pathogen, Mycobacterium tuberculosis (M.tb) in susceptible individuals, but we cannot yet classify or predict outcomes in those prone to pulmonary TB disease versus those prone to resistance. In part, this reflects knowledge gaps regarding genotypes that may increase susceptibility, and in validated disease correlates (e.g. serum of lung protein biomarkers) measured individually, or combined signatures. We address these knowledge gaps by using Diversity Outbred (DO) mice, a population with abundant genetic diversity and heterozygosity, like the human population. Also, like humans, a low dose M.tb infection of DO mice produces a spectrum of outcomes, from highly susceptible to highly resistant, and many intermediate outcomes. In this proposal, we use the DO population to: 1) Identify and test the capacity of genotypic (alleles and statistically significant loci) to predict outcomes such as diagnostic category (class); and 2) To identify and test lung and serum biomarker (protein) and granuloma signatures to determine diagnostic category (class); and 3) To identify and test serum biomarker (protein) signatures that can forecast disease onset, within a 3-week window before illness manifests clinically. The best performing signatures will be tested using samples from humans. Collectively, results from these studies will generate new translatable knowledge regarding correlates of pulmonary TB (useful for diagnostics), and genotypic and serum protein signatures (useful for prognostics). PROJECT NARRATIVE Mycobacterium tuberculosis (M.tb) causes tuberculosis (TB) in millions of susceptible humans each year. It is well known that humans respond variably to M.tb infection, yet we are unable to predict outcomes with accuracy. Here, we use the Diversity Outbred (DO) mouse population to identify and test genotypic, serum, and lung biomarker signatures to accurately predict outcomes. Findings are also validated in samples from humans.",Predicting tuberculosis outcomes using genotypic and biomarker signatures,9849329,R01HL145411,"['AIDS/HIV problem', 'Address', 'Adult', 'Aerosols', 'Alleles', 'Animal Model', 'Bacillus', 'Biological Markers', 'Blood', 'Categories', 'Classification', 'Clinical', 'Consensus', 'Data', 'Databases', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Disease Outcome', 'Dose', 'Genetic Variation', 'Genotype', 'Granuloma', 'Harvest', 'Heterozygote', 'Human', 'Image', 'Image Analysis', 'Inbred Strain', 'Individual', 'Infection', 'Intervention', 'Knowledge', 'Lung', 'Malaria', 'Malignant Neoplasms', 'Measures', 'Minority', 'Modeling', 'Morbidity - disease rate', 'Mus', 'Mycobacterium tuberculosis', 'Necrosis', 'Onset of illness', 'Outcome', 'Patient Care', 'Patient-Focused Outcomes', 'Patients', 'Population', 'Predisposition', 'Process', 'Production', 'Proteins', 'Pulmonary Tuberculosis', 'Quantitative Trait Loci', 'Resistance', 'Sampling', 'Serum', 'Serum Proteins', 'Structure', 'Testing', 'Time', 'Training', 'Tuberculosis', 'Vehicle crash', 'base', 'human pathogen', 'improved', 'individual patient', 'learning algorithm', 'model development', 'novel diagnostics', 'novel marker', 'outcome forecast', 'outcome prediction', 'pathogen', 'predictive marker', 'predictive modeling', 'prognostic', 'protein biomarkers', 'public health intervention', 'response', 'supervised learning', 'survival outcome', 'tool', 'transmission process', 'tuberculosis diagnostics']",NHLBI,TUFTS UNIVERSITY BOSTON,R01,2020,655091,-0.029030742258978694
"Research Resource for Complex Physiologic Signals PhysioNet, established in 1999 as the NIH-sponsored Research Resource for Complex Physiologic Signals, has attained a preeminent status among biomedical data and software resources. Its data archive was the first, and remains the world's largest, most comprehensive and widely used repository of time-varying physiologic signals. Its software collection supports exploration and quantitative analyses of its own and other databases by providing a wide range of well-documented, rigorously tested open-source programs that can be run on any platform. PhysioNet's team of researchers drive the creation and enrichment of: i) Data collections that provide comprehensive, multifaceted views of pathophysiology over long time intervals, such as the MIMIC (Medical Information Mart for Intensive Care) Databases of critical care patients; ii) Analytic methods for quantification of information encoded in physiologic signals relevant to risk stratification and health status assessment; iii) User interfaces, reference materials and services that add value and improve access to the resource’s data and software; and iv) unique annual Challenges focusing on high priority clinical problems, such as early prediction of sepsis, detection and quantification of sleep apnea syndromes from a single lead electrocardiogram (ECG), false alarm detection in the intensive care unit (ICU), continuous fetal ECG monitoring, and paroxysmal atrial fibrillation detection and prediction. PhysioNet is a proven enabler and accelerator of innovative research by investigators with a diverse range of interests, working on projects made possible by data that are otherwise inaccessible. The creation and development of PhysioNet were recognized with the 2016 highest honor of the Association for the Advancement of Medical Instrumentation (AAMI). PhysioNet's world-wide, growing community of researchers, clinicians, educators, trainees, and medical instrument and software developers retrieve about 380 GB of data per day and publish a yearly average of nearly 300 new scholarly articles. Over the next five years we aim to: 1) Enhance PhysioNet’s impact with new data and technology; 2) Develop new methods to quantify dynamical information in physiologic signals relevant for health status assessment, and for acute and chronic risk stratification, and 3) Harness the research community through our international Challenges that address key clinical problems and a new data annotation initiative. PhysioNet, the Research Resource for Complex Physiological Signals, maintains the world's largest, most comprehensive and most widely used repository of physiological data and data analysis software, making them freely available to the research community. PhysioNet is a proven enabler and accelerator of innovative biomedical research through its unique role in providing data and other resources that otherwise would be inaccessible.",Research Resource for Complex Physiologic Signals,10050843,R01EB030362,"['Acute', 'Address', 'Adult', 'Area', 'Arrhythmia', 'Atrial Fibrillation', 'Biological Markers', 'Biomedical Research', 'Cardiovascular system', 'Chronic', 'Clinical', 'Clinical Data', 'Collection', 'Communities', 'Complex', 'Computer software', 'Computerized Medical Record', 'Coupling', 'Critical Care', 'Data', 'Data Analyses', 'Data Collection', 'Data Set', 'Databases', 'Detection', 'Development', 'Doctor of Philosophy', 'Documentation', 'Educational Background', 'Electrocardiogram', 'Entropy', 'Functional disorder', 'Funding', 'Future', 'Goals', 'Growth', 'Health Status', 'Heart failure', 'Image', 'Improve Access', 'Intensive Care', 'Intensive Care Units', 'International', 'Label', 'Lead', 'Legal patent', 'Life', 'Link', 'Machine Learning', 'Measures', 'Medical', 'Methods', 'Monitor', 'Neonatal', 'Operative Surgical Procedures', 'Outcome', 'Pathologic', 'Patient Care', 'Physiological', 'Publishing', 'Research', 'Research Personnel', 'Resolution', 'Resources', 'Risk stratification', 'Role', 'Running', 'Sepsis', 'Services', 'Signal Transduction', 'Sleep Apnea Syndromes', 'Source Code', 'Stroke', 'Students', 'System', 'Techniques', 'Technology', 'Testing', 'Time', 'Time Series Analysis', 'United States National Institutes of Health', 'Visualization', 'Visualization software', 'Work', 'analytical method', 'base', 'clinical care', 'cloud based', 'data archive', 'data exploration', 'data resource', 'fetal', 'graphical user interface', 'high school', 'innovation', 'instrument', 'instrumentation', 'interest', 'open source', 'opioid use', 'programs', 'repository', 'response', 'time interval', 'tool']",NIBIB,BETH ISRAEL DEACONESS MEDICAL CENTER,R01,2020,759918,-0.004162764454502622
"Childhood ‘Omics’ and Mycobacterium tuberculosis-derived BiOsignatures (COMBO) for TB diagnosis in high HIV prevalence settings PROJECT SUMMARY There is an urgent need to develop non-sputum biomarker-based triage and diagnostic tests for childhood tuberculosis (TB). This is particularly important for young children with HIV infection, who have high TB-related mortality but often cannot produce sputum and have lower sputum bacillary burden. Biomarker discovery for childhood TB requires ultra-sensitive platforms to measure low-abundant Mycobacterium tuberculosis (Mtb) proteins in clinical samples and greater investigation of host proteins, post-translational modifications of proteins, and metabolites that are more likely than upstream RNA expression to reflect the host-pathogen interactions that lead to TB disease. The overall objective of the proposed project is to identify Mtb- and/or host-derived biosignatures in children that can achieve World Health Organization (WHO) target product profile (TPP) accuracy thresholds for a non-sputum biomarker-based triage or diagnostic test for childhood TB. We hypothesize that biosignatures that combine Mtb proteins and host biomarkers with evidence of functional relevance to TB pathogenesis or immunity will have the best diagnostic performance. To assess this hypothesis, we will conduct biomarker discovery and initial clinical validation studies using samples from three well- characterized pediatric TB cohorts in Uganda, The Gambia and South Africa. In Aim 1, we will use an ultra- sensitive electrochemoluminescence (ECL)-based immunoassay to assess the presence of Mtb proteins ESAT- 6, CFP-10, MPT64, MPT32, and Ag85B in a discovery set of banked blood and urine samples from 100 children under 5 years old with confirmed TB and 200 with unlikely TB per NIH consensus definitions (50% HIV prevalence in both groups). In Aim 2, we will use the same discovery set to perform targeted and untargeted mass spectrometry with functional assessment through pathway analysis, in vitro models and in vivo mouse models to identify host proteins, post-translational modifications and metabolites that distinguish children with confirmed versus unlikely TB. In Aim 3, we will use the candidate Mtb and host biomarkers identified in Aims 1 and 2 to derive biosignatures with up to 10 analytes consisting of Mtb proteins only, host biomarkers only, and both Mtb- and host-derived biomarkers. Biosignatures that meet WHO TPP criteria in the discovery set will 1) be evaluated in an independent test set of banked samples from 300 children under 5 years old (100 with confirmed TB, 200 with unlikely TB; 50% HIV prevalence in each group) to verify diagnostic accuracy and establish cut- offs and 2) be evaluated in a prospective cohort of 350 children under 5 years old using the pre-select cut-offs and both microbiological and clinical reference standards. Completion of these aims will result in identification of promising biosignatures that can be further validated in large-scale field studies and translated into point-of-care triage and/or diagnostic tests for childhood TB. PROJECT NARRATIVE The inability to quickly and accurately diagnose tuberculosis (TB) in children and initiate treatment is a major contributor to the high mortality of childhood TB worldwide. This study will identify novel Mtb- and host-derived biomarkers, and determine the best biomarker panels (i.e., biosignatures) that can meet the World Health Organization-recommended diagnostic accuracy thresholds for a TB triage or diagnostic test. If successful, the results will the first step needed to make progress toward addressing the critical unmet need for non-sputum biomarker-based tests for childhood TB.",Childhood ‘Omics’ and Mycobacterium tuberculosis-derived BiOsignatures (COMBO) for TB diagnosis in high HIV prevalence settings,9986338,R01AI152161,"['5 year old', 'Address', 'Africa South of the Sahara', 'Biological Assay', 'Biological Markers', 'Blood', 'Blood Banks', 'Cause of Death', 'Child', 'Childhood', 'Clinical', 'Communicable Diseases', 'Consensus', 'Data', 'Detection', 'Diagnosis', 'Diagnostic', 'Diagnostic tests', 'Disease', 'Enrollment', 'Evaluation', 'Failure', 'Gambia', 'Gene Expression', 'Gene Targeting', 'Genetic Transcription', 'HIV', 'HIV Infections', 'HIV/TB', 'Human', 'Immunity', 'Immunoassay', 'In Vitro', 'Investigation', 'Knock-out', 'Lead', 'Machine Learning', 'Mass Spectrum Analysis', 'Measurement', 'Measures', 'Methods', 'Microbiology', 'Modeling', 'Mus', 'Mycobacterium tuberculosis', 'Pathogenesis', 'Pathway Analysis', 'Pathway interactions', 'Performance', 'Plasma', 'Post-Translational Protein Processing', 'Prevalence', 'Probability', 'Prospective cohort', 'Proteins', 'ROC Curve', 'Reference Standards', 'Resolution', 'Role', 'Sampling', 'South Africa', 'Specificity', 'Sputum', 'Symptoms', 'Testing', 'Translating', 'Translations', 'Triage', 'Tryptophan', 'Tuberculosis', 'Uganda', 'United States National Institutes of Health', 'Urine', 'Validation', 'World Health Organization', 'accurate diagnosis', 'base', 'bioinformatics pipeline', 'biomarker discovery', 'biomarker panel', 'biosignature', 'clinical research site', 'clinically relevant', 'co-infection', 'cohort', 'cost', 'diagnostic accuracy', 'field study', 'in vitro Model', 'in vivo', 'in vivo Model', 'ion mobility', 'macrophage', 'mortality', 'mouse model', 'multiple omics', 'next generation', 'novel', 'pathogen', 'point of care', 'point-of-care diagnostics', 'prospective', 'protein metabolite', 'success', 'targeted biomarker', 'transcriptomics', 'tuberculosis diagnostics', 'validation studies']",NIAID,"UNIVERSITY OF CALIFORNIA, SAN FRANCISCO",R01,2020,1503586,-0.018677564039610583
"Mentoring in Immunometabolic Dysregulation in TB and TB/HIV Tuberculosis (TB) is the leading cause of death among people living with HIV (PLWH) worldwide. Despite recent scientific advances, significant gaps remain in our understanding of the immune mechanisms responsible for control and eradication of Mycobacterium tuberculosis (Mtb) infection. PLWH with latent TB infection (LTBI) have a ~10% annual risk of progressing to TB disease, however currently available tests for LTBI diagnosis have reduced sensitivity in this population and are not able to predict which latently infected individuals are at highest risk for developing TB for targeted preventive therapy. Emerging data from clinically relevant animal models suggest that LTBI and active TB represent a spectrum of immune responses and host pathology, with increasing metabolic changes and immune dysregulation during the transition to TB disease. We have identified unique serum metabolite and microRNA (miRNA) profiles that are able to discriminate between patients with TB and those with non-TB lung disease. However, these novel TB signatures have not been assessed prospectively to identify PLWH and HIV-negative persons with LTBI who are at increased risk for TB progression. In order to address this significant knowledge gap, in Aim 1 of the current research program, trainees will leverage the Indian and South African RePORT longitudinal biorepositories of household contacts of TB index cases to test the hypothesis that TB is a chronic inflammatory disease associated with profound changes in immune regulation and metabolism prior to the onset of clinical signs and symptoms. Another major barrier to global TB eradication efforts is the lengthy and complicated current anti-tubercular regimen, which is associated with medical nonadherence and the emergence of drug resistance. Recently, attention has focused on host-directed adjunctive therapies aimed at optimizing immune responses to the pathogen and improving lung damage. Lipid-laden macrophages (foam cells) are central to maintaining chronic TB infection by providing a favorable niche in which antimicrobial functions are down-regulated, and by inducing caseation and tissue damage. Recent work has shown that foam-cell-rich and necrotic areas of TB granulomas are particularly enriched in triglycerides. Mtb infection is associated with dysregulation of two cellular pathways involved in triglyceride homeostasis: a pro-lipogenic pathway involving protein kinase B and mTOR complex 1 (Akt/mTORC1), and an anti- lipogenic pathway involving AMP-activated protein kinase and the sirtuins (AMPK/SIRT). In Aim 2, trainees will use longitudinal clinical samples from RePORT study participants and experimental infections ex vivo to characterize: (i) the relationship between activation of these pathways and control of clinical Mtb infection, and the effect of anti-lipogenic treatments on antimycobacterial functions of human macrophages infected ex vivo. The research aims will be integrated with a mentoring strategy for mentees that fosters development of high impact patient-oriented research with a pathway to independence. Tuberculosis (TB) remains among the most deadly infections worldwide, especially among people living with HIV. Current available tests are not able to accurately detect persons at the highest risk of developing TB, and curing the disease requires at least 6 months of therapy because the TB bacteria can avoid being killed by the immune system and currently available drugs. In the current proposal, physician scientists will receive training in a variety of complementary disciplines, and use several cutting-edge experimental and modeling techniques to analyze samples from patients with TB and TB/HIV, with the ultimate goal of identifying new biomarkers that can predict TB disease and host-directed therapies that can shorten TB treatment.",Mentoring in Immunometabolic Dysregulation in TB and TB/HIV,9975724,K24AI143447,"['5&apos', '-AMP-activated protein kinase', 'Address', 'Animal Model', 'Antibiotics', 'Antimycobacterial Agents', 'Archives', 'Area', 'Attention', 'Automobile Driving', 'Bacteria', 'Bioinformatics', 'Biological Assay', 'Biological Markers', 'Cause of Death', 'Chronic', 'Clinical', 'Clinical Data', 'Complex', 'Development', 'Diagnosis', 'Diagnostic', 'Discipline', 'Disease', 'Drug resistance', 'Environment', 'Experimental Models', 'FRAP1 gene', 'Foam Cells', 'Fostering', 'Goals', 'HIV', 'HIV Seronegativity', 'HIV Seropositivity', 'HIV/TB', 'Homeostasis', 'Household', 'Human', 'Immune', 'Immune response', 'Immune system', 'Immunocompetent', 'Individual', 'Infection', 'International', 'Knowledge', 'Lesion', 'Lipid-Laden Macrophage', 'Lipids', 'Lung diseases', 'Machine Learning', 'Medical', 'Mentors', 'Metabolic', 'Metabolism', 'MicroRNAs', 'Mycobacterium tuberculosis', 'Necrosis', 'Outcome', 'Participant', 'Pathology', 'Pathway interactions', 'Patients', 'Persons', 'Pharmaceutical Preparations', 'Physicians', 'Population', 'Prevention', 'Preventive therapy', 'Proto-Oncogene Proteins c-akt', 'Pulmonary Tuberculosis', 'Regimen', 'Reporting', 'Research', 'Resources', 'Risk', 'Sampling', 'Scientific Advances and Accomplishments', 'Scientist', 'Serum', 'Signs and Symptoms', 'Sirtuins', 'South African', 'Specimen', 'Systems Biology', 'Techniques', 'Testing', 'Tissues', 'Training', 'Triglycerides', 'Tuberculosis', 'Validation', 'Whole Blood', 'Work', 'World Health Organization', 'antimicrobial', 'biobank', 'biosignature', 'career', 'chronic inflammatory disease', 'clinically relevant', 'cohort', 'cytokine', 'high risk', 'immunoregulation', 'improved', 'indexing', 'lifetime risk', 'lung injury', 'macrophage', 'monocyte', 'mycobacterial', 'novel', 'pathogen', 'patient oriented research', 'peripheral blood', 'prevent', 'programs', 'prospective', 'tool', 'transcriptome', 'transcriptome sequencing', 'tuberculosis granuloma', 'tuberculosis treatment']",NIAID,JOHNS HOPKINS UNIVERSITY,K24,2020,191195,-0.025378156051191557
"Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt Founded in 1965 as one of the original Intellectual and Developmental Disorders Research Centers (IDDRC), the Vanderbilt Kennedy Center (VKC) IDDRC serves as the central nexus across Vanderbilt for interdisciplinary research, communication, and training in intellectual and developmental disabilities (IDD). The VKC IDDRC serves as a trans-institutional institute that brings together over 200 faculty from 38 departments in 10 schools at Vanderbilt. The VKC’s mission to facilitate discoveries that inform best practices to improve the lives of people with IDD and their families. This mission is met by leveraging our outstanding institutional resources and support, partnering with disability communities, and capitalizing on synergistic interactions across the VKC’s federally-designated centers: the VKC IDDRC, a University Center of Excellence in Developmental Disabilities and a Leadership Education in Neurodevelopmental Disabilities program. The IDDRC as the centerpiece of the VKC is the foundational organizing structure that creates a “Center culture” wherein research and discovery permeates the VKC’s broader training and service activities, thus enhancing the translational research goals of the IDDRC. Demonstrable IDDRC success includes 976 investigator- authored publications and robust NIH funding to Vanderbilt to support IDD-related research ($52.6M in FY20). Harnessing and leveraging this trans-institutional strength to focus on unique challenges in IDD, the overarching goal of the next phase of the IDDRC is to develop precision care for IDD by providing infrastructure and scientific leadership to enable rapid translation of basic discoveries into high- impact IDD interventions and treatments. Three global Aims guide the IDDRC’s work. Aim 1 provides core services to enable and disseminate impactful research on individualizing treatments based upon the causes, mechanisms, and contributing co-morbid sequelae of IDD; Aim 2 focuses on incorporating innovative methods and approaches to enhance multidisciplinary IDD research; and Aim 3 proposes to conduct a signature research project to improve the precision use of antipsychotic medication in people with autism. Across these Aims and five Cores supported by the IDDRC (Administrative, Clinical Translational, Translational Neuroscience, Behavioral Phenotyping, and Data Sciences), three themes permeate our work: (1) recruitment of highly-skilled researchers not currently conducting IDD research (non-traditional researchers); (2) inclusion of IDD participants into research studies that currently do not include IDD (non-traditional subjects); and (3) incorporation of novel scientific approaches and methods (non-traditional approaches). Our IDDRC is ideally posed to enable rapid discovery of precision care approaches by supporting 50 investigators leading 70 research projects (15 from NICHD) and, as highlighted by the Signature Research Project, to promote and implement generative, novel, and impactful research directions, thus meeting the NICHD’s vision of applying newly evolved technologies and approaches to rapidly accelerate the prevention and/or amelioration of IDDs. PUBLIC HEALTH RELEVANCE: As a group, intellectual and developmental disabilities, including Down syndrome and autism spectrum disorder, have dramatic effects on affected people’s and their caregiver’s lives. Unfortunately, there remains a lack of understanding about what causes these disabilities and, critically, how to treat them with targeted therapies. The Vanderbilt Kennedy Center’s Intellectual and Developmental Disabilities Research Center serves as the hub for Vanderbilt’s research efforts focusing on improving the lives of people with intellectual and developmental disabilities by understanding the causes of these disorders and developing and testing therapies tailored to each individual’s precise needs.",Overall: Eunice Kennedy Shriver Intellectual and Developmental Disabilities Research Center at Vanderbilt,10085550,P50HD103537,"['Academic Medical Centers', 'Affect', 'Antipsychotic Agents', 'Basic Science', 'Behavioral', 'Biomedical Research', 'Caregivers', 'Caring', 'Clinical', 'Clinical Research', 'Clinical Trials', 'Communication', 'Communities', 'Computerized Medical Record', 'Data', 'Data Science', 'Development', 'Developmental Disabilities', 'Diagnosis', 'Disease', 'Disease model', 'Down Syndrome', 'Education', 'Evaluation', 'Faculty', 'Family', 'Foundations', 'Funding', 'Future', 'Gap Junctions', 'Genotype', 'Goals', 'Image', 'Individual', 'Infrastructure', 'Institutes', 'Intellectual and Developmental Disabilities Research Centers', 'Intellectual functioning disability', 'Interdisciplinary Study', 'Intervention', 'Leadership', 'Longevity', 'Machine Learning', 'Medical Records', 'Methods', 'Mission', 'Modeling', 'National Institute of Child Health and Human Development', 'Neurodevelopmental Disability', 'Obesity', 'Outcome', 'Participant', 'Pharmaceutical Preparations', 'Pharmacogenetics', 'Phase', 'Pilot Projects', 'Policy Research', 'Prevention', 'Problem behavior', 'Publications', 'Research', 'Research Personnel', 'Research Project Grants', 'Resources', 'Risk', 'Sampling', 'Schools', 'Series', 'Services', 'Structure', 'Techniques', 'Technology', 'Testing', 'Training', 'Translational Research', 'Translations', 'United States National Institutes of Health', 'Universities', 'Vision', 'Weight Gain', 'Work', 'autism spectrum disorder', 'base', 'behavioral phenotyping', 'clinical translation', 'comorbidity', 'cost effective', 'developmental disease', 'disability', 'drug-induced weight gain', 'experience', 'image processing', 'implementation science', 'improved', 'individualized medicine', 'innovation', 'large datasets', 'lectures', 'meetings', 'multidisciplinary', 'novel', 'personalized approach', 'personalized care', 'personalized medicine', 'population based', 'pragmatic trial', 'predictive modeling', 'programs', 'public health relevance', 'recruit', 'research study', 'success', 'targeted treatment', 'translational neuroscience', 'trial comparing']",NICHD,VANDERBILT UNIVERSITY MEDICAL CENTER,P50,2020,1387605,-0.007772339974677761

text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"The goal of the proposed research is the analysis of biological sequence
 data to address the molecular mechanisms of evolution and the origin(s)
 of all viruses and related genetic elements. Phylogenetic trees will
 provide a framework for the mapping of cell and tissue tropism,
 pathogenicity and virulence, modes of transmission and geographical
 distributions, and many other higher order characteristics of viruses.
 The specific aims of proposed analytical studies are: i) determining
 functionally equivalent networks and frequency of exchange among and
 between retroid elements, and their potential cellular homologues,
 including new studies on 300 retroviral env proteins; 2) inferring
 functionally important regions of all proteins of paramyxo-, rhabdo- and
 filoviruses, (with privileged access to new Ebola sequences), and Borna
 Disease virus, (including potential BDV sequences from schizophrenic
 patients); and 3) the analysis of the dUTPase gene, as a model system,
 to address issues relevant to the structure, function and evolution of
 duplicated sequences, and potential horizontal transfer among and between
 host and viral genomes. The specific aims of the technical studies are:
 i) evaluation of stochastic production model approaches for generation
 of multiple alignments, detection of recombination, and calculation of
 evolutionary distances; and 2) development and testing of new and
 existing methods for historical reconstruction of functionally equivalent
 networks.
 
 RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,
 animal and plant viral diseases world wide. The heterogeneous nature of
 RNA populations makes it difficult to develop effective, anti-viral
 agents. The sequence database is now large enough to conduct comparative
 studies on natural variants versus chemotherapeutically induced mutants
 for several retroviral proteins. This model study will provide new
 information on the nature of selected mutations which will be useful in
 future anti-viral drug development.
 
 Computational analysis of primary sequence data is an area of intense
 interest in biology, mathematics, statistics and systems science. In the
 last few years new approaches to problem solving and classification, such
 as machine learning, neural networks, genetic algorithms, and stochastic
 production models or, ""intelligent systems"" as they are referred to
 collectively, have become available. Unfortunately most biologists are
 unaware of these developments. Application of these methods to real data
 remains unexplored. The proposed studies will go a long way in rectifying
 this gap in technological utilization. These studies will continue to
 define important evolutionary' relationships and events, provide
 biologically informative sequence relationships for bench-marking new
 software, and contribute new information relevant to the structure and
 function of viral proteins suggesting new directions in laboratory
 experimentation. Strategies and techniques developed for the analysis of
 highly divergent genomes can also be applied to the study of the wealth
 of sequence information generated under the auspices of the Human Genome
 Project.
 DNA directed DNA polymerase; DNA directed RNA polymerase; DNA topoisomerases; Paramyxovirus; RNA directed DNA polymerase; RNA virus; Rhabdoviridae; bacterial proteins; biochemical evolution; computer assisted sequence analysis; genetic recombination; integrase; method development; nuclease; protein sequence; ribonuclease III; virus DNA; virus RNA; virus protein COMPUTER-BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION","The goal of the proposed research is the analysis of biological sequence
 data to address the molecular mechanisms of evolution and the origin(s)
 of all viruses and related genetic elements. Phylogenetic trees will
 provide a framework for the mapping of cell and tissue tropism,
 pathogenicity and virulence, modes of transmission and geographical
 distributions, and many other higher order characteristics of viruses.
 The specific aims of proposed analytical studies are: i) determining
 functionally equivalent networks and frequency of exchange among and
 between retroid elements, and their potential cellular homologues,
 including new studies on 300 retroviral env proteins; 2) inferring
 functionally important regions of all proteins of paramyxo-, rhabdo- and
 filoviruses, (with privileged access to new Ebola sequences), and Borna
 Disease virus, (including potential BDV sequences from schizophrenic
 patients); and 3) the analysis of the dUTPase gene, as a model system,
 to address issues relevant to the structure, function and evolution of
 duplicated sequences, and potential horizontal transfer among and between
 host and viral genomes. The specific aims of the technical studies are:
 i) evaluation of stochastic production model approaches for generation
 of multiple alignments, detection of recombination, and calculation of
 evolutionary distances; and 2) development and testing of new and
 existing methods for historical reconstruction of functionally equivalent
 networks.
 
 RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,
 animal and plant viral diseases world wide. The heterogeneous nature of
 RNA populations makes it difficult to develop effective, anti-viral
 agents. The sequence database is now large enough to conduct comparative
 studies on natural variants versus chemotherapeutically induced mutants
 for several retroviral proteins. This model study will provide new
 information on the nature of selected mutations which will be useful in
 future anti-viral drug development.
 
 Computational analysis of primary sequence data is an area of intense
 interest in biology, mathematics, statistics and systems science. In the
 last few years new approaches to problem solving and classification, such
 as machine learning, neural networks, genetic algorithms, and stochastic
 production models or, ""intelligent systems"" as they are referred to
 collectively, have become available. Unfortunately most biologists are
 unaware of these developments. Application of these methods to real data
 remains unexplored. The proposed studies will go a long way in rectifying
 this gap in technological utilization. These studies will continue to
 define important evolutionary' relationships and events, provide
 biologically informative sequence relationships for bench-marking new
 software, and contribute new information relevant to the structure and
 function of viral proteins suggesting new directions in laboratory
 experimentation. Strategies and techniques developed for the analysis of
 highly divergent genomes can also be applied to the study of the wealth
 of sequence information generated under the auspices of the Human Genome
 Project.
",2057512,K04AI001277,['K04AI001277'],AI,https://reporter.nih.gov/project-details/2057512,K04,1995,63885,0.7272202778919414
"DESCRIPTION: Over  the past decade  a variety of  alternative computer          
based  modeling  techniques  have  been   introduced  which  show               
promise  for   the  construction of clinical decision aids. These               
techniques include statistical  regression  approaches such as                  
generalized additive  modeling, classification  tree induction such as          
ID3 or CART, and multi-layer neural  networks. Logistic  regression             
models (LR) are currently central to most probabilistic predictive              
clinical  decision aids and are fundamental to comparative analyses of          
medical  care based  risk adjusted events. These newer techniques               
have been applied on  a larger scale in the last few years. They                
appear to have unique advantages in  selected circumstances. The                
successful use  of these methods, however, depends  on understanding            
their accuracy, performance, and model transportability.                        
                                                                                
A  formal assessment of  these new techniques with four specific  aims          
is  proposed:  (1) to assess  and compare the  performance of                   
different  models to  determine  the  factors which affect                      
performance; (2)  to  develop automated  computer  based procedures             
for exploratory model  development for each method;  (3)  to develop            
hybrid models incorporating the strengths of each of the existing               
techniques, and  (4) to  determine the situations  that restrict  the           
transportability of these models.                                               
                                                                                
These specific aims will be  achieved in a three stage project. In              
the first  stage four approaches will be pursued:  (1) the                      
mathematical properties of the  different  computational  algorithms            
for the  modeling  techniques will be studied;  (2) automated                   
modeling procedures will be developed and utilized; (3)  the  factors           
that  affect  performance for  each  modeling technique  will  be               
explored and(4) new hybrid techniques will be developed and assessed.           
In the  second stage the methods  developed in the first stage will             
be used to create  and  test  models  that predict  cardiovascular              
events  on  data from  15,000  patients in  a prospective clinical              
trial. In the third stage the factors that  affect  the                         
generalizability and transportability of models to  new datasets  will          
be explored  by repeated sampling and  model construction  on                   
different  subsets of the cardiovascular database  including                    
separating the database into  subsets from each of ten different                
hospitals.                                                                      
                                                                                
This   work  will  broaden  the  understanding of  these  important             
modeling  techniques  and their  potential contributions for                    
clinical decision making,  health policy research,  and medical                 
informatics.   New modeling  techniques  might be developed which               
incorporate elements from different techniques.                                 
 artificial intelligence; cardiovascular disorder epidemiology; cardiovascular function; computer assisted medical decision making; computer simulation; disease /disorder proneness /risk; health care facility information system; human data; mathematical model; model design /development NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS","DESCRIPTION: Over  the past decade  a variety of  alternative computer          
based  modeling  techniques  have  been   introduced  which  show               
promise  for   the  construction of clinical decision aids. These               
techniques include statistical  regression  approaches such as                  
generalized additive  modeling, classification  tree induction such as          
ID3 or CART, and multi-layer neural  networks. Logistic  regression             
models (LR) are currently central to most probabilistic predictive              
clinical  decision aids and are fundamental to comparative analyses of          
medical  care based  risk adjusted events. These newer techniques               
have been applied on  a larger scale in the last few years. They                
appear to have unique advantages in  selected circumstances. The                
successful use  of these methods, however, depends  on understanding            
their accuracy, performance, and model transportability.                        
                                                                                
A  formal assessment of  these new techniques with four specific  aims          
is  proposed:  (1) to assess  and compare the  performance of                   
different  models to  determine  the  factors which affect                      
performance; (2)  to  develop automated  computer  based procedures             
for exploratory model  development for each method;  (3)  to develop            
hybrid models incorporating the strengths of each of the existing               
techniques, and  (4) to  determine the situations  that restrict  the           
transportability of these models.                                               
                                                                                
These specific aims will be  achieved in a three stage project. In              
the first  stage four approaches will be pursued:  (1) the                      
mathematical properties of the  different  computational  algorithms            
for the  modeling  techniques will be studied;  (2) automated                   
modeling procedures will be developed and utilized; (3)  the  factors           
that  affect  performance for  each  modeling technique  will  be               
explored and(4) new hybrid techniques will be developed and assessed.           
In the  second stage the methods  developed in the first stage will             
be used to create  and  test  models  that predict  cardiovascular              
events  on  data from  15,000  patients in  a prospective clinical              
trial. In the third stage the factors that  affect  the                         
generalizability and transportability of models to  new datasets  will          
be explored  by repeated sampling and  model construction  on                   
different  subsets of the cardiovascular database  including                    
separating the database into  subsets from each of ten different                
hospitals.                                                                      
                                                                                
This   work  will  broaden  the  understanding of  these  important             
modeling  techniques  and their  potential contributions for                    
clinical decision making,  health policy research,  and medical                 
informatics.   New modeling  techniques  might be developed which               
incorporate elements from different techniques.                                 
",2032352,R01LM005607,['R01LM005607'],LM,https://reporter.nih.gov/project-details/2032352,R01,1997,278143,0.7272202778919414
"The overall goal of this project is to develop a comprehensive computer         
model of neural coding of auditory space in the auditory thalamocortical        
system.  A self-organizing neural network model of auditory cortical maps       
will be studied.  The output of the network will be that of a simulated         
planar cortex that will afford direct comparisons of simulated virtual          
space receptive fields (VSRF) with those of actual VSRFs of direction-          
sensitive neurons in primary auditory cortex (AI).  These computational         
simulations will greatly benefit from the ability to synthesize virtual         
auditory space from measured Head-Related Transfer Functions (HRTFs). The       
same stimuli used for microelectrode recordings in auditory cortex of cat       
will be used in the development and stimulation of the neural-network           
models.  These network models assume that primary auditory cortex is            
subject to experience-dependent changes.  Currently available                   
computational models of neural signal processing in the auditory periphery      
and brain stem will be used to provide a neural representation of binaural      
stimuli to the self-organizing thalamocortical model.  The simulation of        
self-organizing processes operating on input data with intrinsic structure      
leads to the emergence of topographical maps.  These maps afford the            
opportunity to examine overlays of functional organization.  Currently,         
the ability to corroborate the emergence of a spatial auditory map with a       
detailed map of an auditory cortical field is quite limited, but the            
simulated development of maps will demonstrate how global topographic           
order can emerge, in principle, from local cooperative and competitive          
interactions within the cortical field.  It is anticipated that these           
simulation studies will help guide neurophysiology research with regard to      
deciphering the neural code of auditory space.  Specifically, the               
simulations may suggest where to probe the cortex with microelectrodes and      
with what types of stimulation.  Interactions between the tonotopic             
frequency organization and orthogonal iso-frequency organization will be        
investigated computationally.  This computational modeling work may             
provide a better understanding of the representation of complex sounds in       
general at higher levels in the auditory system.  Given the nature of the       
model to re-organize, effects of cochlear lesions can be studied and thus       
aid in the study of sensorineural hearing impairment.                           
 artificial intelligence; auditory pathways; cats; computational neuroscience; computer simulation; computer system design /evaluation; model design /development; neuroanatomy; sensorineural hearing loss; thalamocortical tract COMPUTATIONAL MODEL OF SELF ORGANIZING AUDITORY MAPS","The overall goal of this project is to develop a comprehensive computer         
model of neural coding of auditory space in the auditory thalamocortical        
system.  A self-organizing neural network model of auditory cortical maps       
will be studied.  The output of the network will be that of a simulated         
planar cortex that will afford direct comparisons of simulated virtual          
space receptive fields (VSRF) with those of actual VSRFs of direction-          
sensitive neurons in primary auditory cortex (AI).  These computational         
simulations will greatly benefit from the ability to synthesize virtual         
auditory space from measured Head-Related Transfer Functions (HRTFs). The       
same stimuli used for microelectrode recordings in auditory cortex of cat       
will be used in the development and stimulation of the neural-network           
models.  These network models assume that primary auditory cortex is            
subject to experience-dependent changes.  Currently available                   
computational models of neural signal processing in the auditory periphery      
and brain stem will be used to provide a neural representation of binaural      
stimuli to the self-organizing thalamocortical model.  The simulation of        
self-organizing processes operating on input data with intrinsic structure      
leads to the emergence of topographical maps.  These maps afford the            
opportunity to examine overlays of functional organization.  Currently,         
the ability to corroborate the emergence of a spatial auditory map with a       
detailed map of an auditory cortical field is quite limited, but the            
simulated development of maps will demonstrate how global topographic           
order can emerge, in principle, from local cooperative and competitive          
interactions within the cortical field.  It is anticipated that these           
simulation studies will help guide neurophysiology research with regard to      
deciphering the neural code of auditory space.  Specifically, the               
simulations may suggest where to probe the cortex with microelectrodes and      
with what types of stimulation.  Interactions between the tonotopic             
frequency organization and orthogonal iso-frequency organization will be        
investigated computationally.  This computational modeling work may             
provide a better understanding of the representation of complex sounds in       
general at higher levels in the auditory system.  Given the nature of the       
model to re-organize, effects of cochlear lesions can be studied and thus       
aid in the study of sensorineural hearing impairment.                           
",2414667,R03DC002804,['R03DC002804'],DC,https://reporter.nih.gov/project-details/2414667,R03,1997,33508,0.7272202778919414
"DESCRIPTION (Taken from application abstract):  This proposed study will        
replicate and extend methodology used in earlier studies and will use           
extensive clinical data repositories, informatics tools, and expert             
practitioners for perinatal medical knowledge building.                         
                                                                                
Clinical Data Repository:  Duke University's Medical Center (DUMC) TMR (The     
Medical Record) data repository will be used for this study, and contains       
45,922 electronic medical records for both low and high-risk pregnant women     
(and their infants) who have received prenatal care at DUMC, and its            
affiliated regional clinics, between 1/1/86 and 12/3l/95.  Each patient's       
electronic data is used for clinical patient care and contains a potential      
4000 variables per record.  This volume of data requires new approaches for     
data analysis and medical decision support, since human information             
processing limitations become quickly overloaded by both an individual          
patient s data and the aggregate information collected for the perinatal        
patient population.                                                             
                                                                                
lnformatics Tools:  Informatics techniques for knowledge acquisition and        
data mining will use machine learning programs, statistical analysis, and       
domain expert input to articulate relationships between the data and            
perinatal patent outcomes.  The goal is to provide decision support for         
perinatal care providers to accurately identify patients at risk and assist     
them with modifiable preterm birth ask factors.  An expert system will use      
data-generated and verified knowledge bases to test its predictive validity     
when new patient cases are induced to the expert system.  Earlier studies       
found 53-90% predictive accuracies for an expert system prototype, as           
compared to 17-38% accuracies, reported in the literature, using current        
manual techniques.  Mapping the expert system's knowledge base terms to         
medical library resources will be explored for additional decision support.     
                                                                                
Expert Practitioner:  The perinatal expert panel will consist of the            
Principal Investigator, a Board Certified OB-Gyn Physician, and a certified     
Perinatal RN.  Each of the panel members has more than 20 years of perinatal    
experience.  Participating informatics experts are known, both nationally       
and internationally for their expertise in the field of Medical Informatics.    
 artificial intelligence; computer system design /evaluation; human data; information systems; prenatal care INFORMATICS TOOLS & MEDICAL PERINATAL KNOWLEDGE BUILDING","DESCRIPTION (Taken from application abstract):  This proposed study will        
replicate and extend methodology used in earlier studies and will use           
extensive clinical data repositories, informatics tools, and expert             
practitioners for perinatal medical knowledge building.                         
                                                                                
Clinical Data Repository:  Duke University's Medical Center (DUMC) TMR (The     
Medical Record) data repository will be used for this study, and contains       
45,922 electronic medical records for both low and high-risk pregnant women     
(and their infants) who have received prenatal care at DUMC, and its            
affiliated regional clinics, between 1/1/86 and 12/3l/95.  Each patient's       
electronic data is used for clinical patient care and contains a potential      
4000 variables per record.  This volume of data requires new approaches for     
data analysis and medical decision support, since human information             
processing limitations become quickly overloaded by both an individual          
patient s data and the aggregate information collected for the perinatal        
patient population.                                                             
                                                                                
lnformatics Tools:  Informatics techniques for knowledge acquisition and        
data mining will use machine learning programs, statistical analysis, and       
domain expert input to articulate relationships between the data and            
perinatal patent outcomes.  The goal is to provide decision support for         
perinatal care providers to accurately identify patients at risk and assist     
them with modifiable preterm birth ask factors.  An expert system will use      
data-generated and verified knowledge bases to test its predictive validity     
when new patient cases are induced to the expert system.  Earlier studies       
found 53-90% predictive accuracies for an expert system prototype, as           
compared to 17-38% accuracies, reported in the literature, using current        
manual techniques.  Mapping the expert system's knowledge base terms to         
medical library resources will be explored for additional decision support.     
                                                                                
Expert Practitioner:  The perinatal expert panel will consist of the            
Principal Investigator, a Board Certified OB-Gyn Physician, and a certified     
Perinatal RN.  Each of the panel members has more than 20 years of perinatal    
experience.  Participating informatics experts are known, both nationally       
and internationally for their expertise in the field of Medical Informatics.    
",2032587,R01LM006488,['R01LM006488'],LM,https://reporter.nih.gov/project-details/2032587,R01,1997,279649,0.4576444938534036
"Two-dimensional polyacrylamide gel electrophoresis (2DGE) can detect            
thousands of polypeptides, separating them by apparent molecular weight         
and isoelectric point.  It provides a more realistic and global review          
of cellular genetic expression than any other technique.  Computer 2DGE         
analysis software for a variety of platforms has been developed to deal         
with what can easily be an unwieldy data management problem.  However,          
these programs require significant user input for gel matching and do           
not adequately correct for routine gel-to-gel distortions.  The best            
approach for gel matching and spot detection is pixel matching from one         
gel to another.  This procedure is computationally demanding and is             
currently limited to high-end workstations costing 80,000 dollars and           
above.  This Phase I effort proposes to utilize a pixel matching                
algorithm and integrate an enhanced image processor that will fully             
automate spot detection for a PC-based system.  This system will perform        
at the level of the expensive workstations but cost 40-50 percent less.         
Phase I goals include hardware integration of a new image processor PC          
card into a Pentium class computer, software development for pixel              
matching of reference to study gel and preliminary testing of the               
software to meet expected benchmark processing speeds for 2D plasma             
protein maps.  Full automation and increased performance in a PC-based          
product will facilitate the use of 2DGE in the clinical setting and in          
proteome research where the focus is on understanding the order,                
regulation, and coordination of the human genome.                               
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The proposed project will result in a PC-based system capable of fully          
automated spot detection for 2D gel analysis.  The algorithm used will          
correct for gel-to-gel variations without significant user input.  This         
affordable system will speed up 2D gel analysis and will allow for wider        
clinical use of this valuable analytical technique.                             
 artificial intelligence; biomedical automation; charge coupled device camera; computer data analysis; computer program /software; computer system design /evaluation; computer system hardware; gel electrophoresis; image processing; isoelectric point; molecular weight AUTOMATED PC-BASED SPOT DETECTION FOR 2D GEL ANALYSIS","Two-dimensional polyacrylamide gel electrophoresis (2DGE) can detect            
thousands of polypeptides, separating them by apparent molecular weight         
and isoelectric point.  It provides a more realistic and global review          
of cellular genetic expression than any other technique.  Computer 2DGE         
analysis software for a variety of platforms has been developed to deal         
with what can easily be an unwieldy data management problem.  However,          
these programs require significant user input for gel matching and do           
not adequately correct for routine gel-to-gel distortions.  The best            
approach for gel matching and spot detection is pixel matching from one         
gel to another.  This procedure is computationally demanding and is             
currently limited to high-end workstations costing 80,000 dollars and           
above.  This Phase I effort proposes to utilize a pixel matching                
algorithm and integrate an enhanced image processor that will fully             
automate spot detection for a PC-based system.  This system will perform        
at the level of the expensive workstations but cost 40-50 percent less.         
Phase I goals include hardware integration of a new image processor PC          
card into a Pentium class computer, software development for pixel              
matching of reference to study gel and preliminary testing of the               
software to meet expected benchmark processing speeds for 2D plasma             
protein maps.  Full automation and increased performance in a PC-based          
product will facilitate the use of 2DGE in the clinical setting and in          
proteome research where the focus is on understanding the order,                
regulation, and coordination of the human genome.                               
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The proposed project will result in a PC-based system capable of fully          
automated spot detection for 2D gel analysis.  The algorithm used will          
correct for gel-to-gel variations without significant user input.  This         
affordable system will speed up 2D gel analysis and will allow for wider        
clinical use of this valuable analytical technique.                             
",2536786,R43HG001749,['R43HG001749'],HG,https://reporter.nih.gov/project-details/2536786,R43,1998,95053,0.7272202778919414
"This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
 artificial intelligence; behavioral /social science research tag; biomechanics; computed axial tomography; computer program /software; computer simulation; electrical measurement; human subject; magnetic resonance imaging; psychoacoustics; respiratory airflow measurement; respiratory imaging /visualization; speech; speech synthesizers; vocal cords; vocalization; voice SIMULATION OF VOICE QUALITIES IN SPEECH","This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
",2733685,R01DC002532,['R01DC002532'],DC,https://reporter.nih.gov/project-details/2733685,R01,1998,229724,0.4576444938534036

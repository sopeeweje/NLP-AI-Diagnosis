text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
"Knowledge management (KM) may be defined as ""the ability to organize and
structure knowledge, access it, navigate through it, and adapt it as
needed"".  This research seeks to develop and refine a ""building block""
approach for creation of KM applications to facilitate medical education
and decision support, and to evaluate its effectiveness.  Potential uses
include query, browsing, testing, stimulation, didactic instruction,
problem solving, and personal file management.  Knowledge is considered to
be available in multiple forms, both non-adaptive and adaptive.  A
challenge is to assemble units of knowledge of the forms pertinent to a
particular domain, organize them to support user needs, provide consistent
access to them, and facilitate linkage among them.  This proposal seeks to
build on and extend previous work on architectural issues for KM in two
principal ways:

(1) To refine and further develop software architectures and methodologies.
This research will explore the role of a kernel set of functions to
facilitate organization and combination of disparate components in building
complex KM applications.  If properly designed, such a kernel can provide a
consistent set of services for all applications.  A prototype kernel
architecture for KM applications will be designed and tested.  Research
issues include methods for decomposition of applications to separate their
storage, processing, and presentation components; data base requirements
for indexing and composing complex structures from disparate, disjoint data
elements; and methods to support multi-user cooperative development.
Important goals will be to facilitate functional integration of
independently developed units, extensibility, and portability and/or
interoperability.

(2) To validate and evaluate the architectures and methodologies.  This
will be done principally via on-going participation in collaborative KM
applications, where the kernel architecture developed is applied and
experience with its use gained, in order to identify problems, delineate
other needed capabilities, and prioritize future developments.  In
addition, aid will be provided for evaluation activities of the
collaborative projects themselves, via development of tools for collecting,
summarizing, and analyzing usage data.

In conjunction with the above activities, a number of key non-technical
issues will be explored, which bear upon the practical impact of the
technical developments.  These include adoption of guidelines for knowledge
module definition and interchange; understanding of user information needs
and information seeking behavior; and requirements for supporting,
maintaining, updating, and disseminating knowledge bases.  The approach to
these tasks will be to collect and synthesize experiences, review and
analyze those of others, and participate in task forces and workshops aimed
at elucidating these issues.
 artificial intelligence; computer assisted medical decision making; computer program /software; computer system design /evaluation; indexing; information display; information dissemination; information retrieval; information system analysis; information systems; medical education KNOWLEDGE MANAGEMENT","Knowledge management (KM) may be defined as ""the ability to organize and
structure knowledge, access it, navigate through it, and adapt it as
needed"".  This research seeks to develop and refine a ""building block""
approach for creation of KM applications to facilitate medical education
and decision support, and to evaluate its effectiveness.  Potential uses
include query, browsing, testing, stimulation, didactic instruction,
problem solving, and personal file management.  Knowledge is considered to
be available in multiple forms, both non-adaptive and adaptive.  A
challenge is to assemble units of knowledge of the forms pertinent to a
particular domain, organize them to support user needs, provide consistent
access to them, and facilitate linkage among them.  This proposal seeks to
build on and extend previous work on architectural issues for KM in two
principal ways:

(1) To refine and further develop software architectures and methodologies.
This research will explore the role of a kernel set of functions to
facilitate organization and combination of disparate components in building
complex KM applications.  If properly designed, such a kernel can provide a
consistent set of services for all applications.  A prototype kernel
architecture for KM applications will be designed and tested.  Research
issues include methods for decomposition of applications to separate their
storage, processing, and presentation components; data base requirements
for indexing and composing complex structures from disparate, disjoint data
elements; and methods to support multi-user cooperative development.
Important goals will be to facilitate functional integration of
independently developed units, extensibility, and portability and/or
interoperability.

(2) To validate and evaluate the architectures and methodologies.  This
will be done principally via on-going participation in collaborative KM
applications, where the kernel architecture developed is applied and
experience with its use gained, in order to identify problems, delineate
other needed capabilities, and prioritize future developments.  In
addition, aid will be provided for evaluation activities of the
collaborative projects themselves, via development of tools for collecting,
summarizing, and analyzing usage data.

In conjunction with the above activities, a number of key non-technical
issues will be explored, which bear upon the practical impact of the
technical developments.  These include adoption of guidelines for knowledge
module definition and interchange; understanding of user information needs
and information seeking behavior; and requirements for supporting,
maintaining, updating, and disseminating knowledge bases.  The approach to
these tasks will be to collect and synthesize experiences, review and
analyze those of others, and participate in task forces and workshops aimed
at elucidating these issues.
",2237628,R01LM004572,['R01LM004572'],LM,https://reporter.nih.gov/project-details/2237628,R01,1994,404825,1.0
"Despite the capability to collect sophisticated multiparameter listmode         
data, both rectilinear or bit-map cell sorting boundaries still are             
usually chosen manually and in a rather arbitrary fashion. Usually the          
experimenter performs visual clustering prior to drawing boundaries which       
have no statistical prediction of successful classification.                    
Visualization of complex multiparameter data is also difficult. One way         
to deal with the visualization problem is to view the first three               
principal components of the data and use this information to estimate the       
number and approximate centroids for ""guided"" cluster analysis. Cluster         
membership probabilities will then be used to make sort decisions.              
                                                                                
Another way to deal with the problem of placing sort boundaries on the          
basis of arbitrary ""visual classifications"" is to apply statistical             
methods of classifying cells, e.g. discriminant analysis with Bayes             
decision boundaries. Discriminant functions will be calculated and Bayes        
decision boundaries will be used to sort cells on the basis of                  
discriminant function scores which will be calculated in real-time by           
hardware and/or software lookup tables. A cost of misclassification will        
also be included in the cell sorting decision.                                  
                                                                                
For all classifier systems developed, classifier performance will be            
measured through ROC (""receiver operating characteristics"") analyses of         
true-positives and false-positives. To accomplish this we will use a            
well-defined system of data and model cell systems whereby all                  
classifiers can be checked for correctness against ""tagged"" parameters.         
All sorted model cells can be unequivocally identified by PCR (polymerase       
chain reaction) or by FISH (fluorescence in-situ hybridization).                
                                                                                
While the main focus of the proposal is to develop real-time cell               
classifiers useful for cell sorting, many of the techniques can also be         
used by other researchers for off-line analysis of conventional listmode        
flow cytometry data.  Hence many of these techniques should prove               
important to other researchers even if they are unable to perform the           
sophisticated cell sorting described in this proposal.                          
                                                                                
To demonstrate the importance of these new techniques to many problems          
in biology and medicine we will attempt to apply these new techniques to        
several important applications including: (1) high-resolution sorting of        
single fetal cells from human maternal blood for prenatal diagnosis; (2)        
molecular characterizations of oncogene, tumor suppresser, metastatic,          
and multi-drug resistance genes in rare human metastatic breast cancer          
cells isolated from peripheral blood and bone marrow by high-speed              
enrichment or high-resolution cell sorting; and (3) bone marrow purging         
of metastatic cells to allow for autologous transplantations in breast          
cancer patients undergoing high-dose chemotherapy.                              
 artificial intelligence; bone marrow purging; breast neoplasms; cell sorting; classification; computer system design /evaluation; confocal scanning microscopy; female; human subject; in situ hybridization; metastasis; multidrug resistance; neoplasm /cancer genetics; oncogenes; polymerase chain reaction; pregnancy circulation; statistics /biometry; tumor suppressor genes; women's health CLASSIFIERS FOR HIGH RESOLUTION CELL SORTING","Despite the capability to collect sophisticated multiparameter listmode         
data, both rectilinear or bit-map cell sorting boundaries still are             
usually chosen manually and in a rather arbitrary fashion. Usually the          
experimenter performs visual clustering prior to drawing boundaries which       
have no statistical prediction of successful classification.                    
Visualization of complex multiparameter data is also difficult. One way         
to deal with the visualization problem is to view the first three               
principal components of the data and use this information to estimate the       
number and approximate centroids for ""guided"" cluster analysis. Cluster         
membership probabilities will then be used to make sort decisions.              
                                                                                
Another way to deal with the problem of placing sort boundaries on the          
basis of arbitrary ""visual classifications"" is to apply statistical             
methods of classifying cells, e.g. discriminant analysis with Bayes             
decision boundaries. Discriminant functions will be calculated and Bayes        
decision boundaries will be used to sort cells on the basis of                  
discriminant function scores which will be calculated in real-time by           
hardware and/or software lookup tables. A cost of misclassification will        
also be included in the cell sorting decision.                                  
                                                                                
For all classifier systems developed, classifier performance will be            
measured through ROC (""receiver operating characteristics"") analyses of         
true-positives and false-positives. To accomplish this we will use a            
well-defined system of data and model cell systems whereby all                  
classifiers can be checked for correctness against ""tagged"" parameters.         
All sorted model cells can be unequivocally identified by PCR (polymerase       
chain reaction) or by FISH (fluorescence in-situ hybridization).                
                                                                                
While the main focus of the proposal is to develop real-time cell               
classifiers useful for cell sorting, many of the techniques can also be         
used by other researchers for off-line analysis of conventional listmode        
flow cytometry data.  Hence many of these techniques should prove               
important to other researchers even if they are unable to perform the           
sophisticated cell sorting described in this proposal.                          
                                                                                
To demonstrate the importance of these new techniques to many problems          
in biology and medicine we will attempt to apply these new techniques to        
several important applications including: (1) high-resolution sorting of        
single fetal cells from human maternal blood for prenatal diagnosis; (2)        
molecular characterizations of oncogene, tumor suppresser, metastatic,          
and multi-drug resistance genes in rare human metastatic breast cancer          
cells isolated from peripheral blood and bone marrow by high-speed              
enrichment or high-resolution cell sorting; and (3) bone marrow purging         
of metastatic cells to allow for autologous transplantations in breast          
cancer patients undergoing high-dose chemotherapy.                              
",2331967,R01GM038645,['R01GM038645'],GM,https://reporter.nih.gov/project-details/2331967,R01,1997,231327,1.0
"Delays in language acquisition are the most prevalent developmental problem     
in preschool children.  The acquisition of verbs poses a special challenge      
for children with language problems, perhaps because verbs are richly           
endowed with syntactic and semantic information and play a central role in      
the development of grammatical competency.  We plan to develop and publish      
a microcomputer-based system designed to facilitate the development of a        
rich verb lexicon in children at risk for chronic language disorders.  The      
curriculum will be based on linguistic research indicating a central role       
for verbs in language acquisition.  The system will feature animation,          
digital speech, interface options for special needs, and an intelligent         
computer-aided training (JCAT) system that uses artificial intelligence to      
generate individualized intervention strategies.  The system will include       
modules designed to increase the size and diversity of the verb lexicon,        
increase semantic and syntactic accuracy, and promote the proper use of         
inflected verb forms.  Our Phase II objectives are to (a) develop               
additional program modules for children at various stages of language           
development, (b) field test, improve, and validate the effectiveness of         
program components, and (c) integrate all components with the ICAT system.      
We envision the system being useful in clinical, school, and preschool          
settings, as well as in the home.                                               
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Speech-Language Pathologists, teachers        
and parents recognize athe importance of early intervention when a child's      
language status is impaired, and such intervention is mandated by Federal       
law.  Our proposal is to produce a clinically effective language                
intervention system that will be a commercial success because it provides       
a cost effective means to supplement the efforts of professional service        
providers, and can fulfill an urgent and largely unmet need for intensive,      
individualized early language intervention.                                     
 behavioral /social science research tag; child (0-11); computer assisted instruction; computer program /software; education evaluation /planning; human subject; language development; language disorders SOFTWARE FOR VERB BASED EARLY LANGUAGE INTERVENTION","Delays in language acquisition are the most prevalent developmental problem     
in preschool children.  The acquisition of verbs poses a special challenge      
for children with language problems, perhaps because verbs are richly           
endowed with syntactic and semantic information and play a central role in      
the development of grammatical competency.  We plan to develop and publish      
a microcomputer-based system designed to facilitate the development of a        
rich verb lexicon in children at risk for chronic language disorders.  The      
curriculum will be based on linguistic research indicating a central role       
for verbs in language acquisition.  The system will feature animation,          
digital speech, interface options for special needs, and an intelligent         
computer-aided training (JCAT) system that uses artificial intelligence to      
generate individualized intervention strategies.  The system will include       
modules designed to increase the size and diversity of the verb lexicon,        
increase semantic and syntactic accuracy, and promote the proper use of         
inflected verb forms.  Our Phase II objectives are to (a) develop               
additional program modules for children at various stages of language           
development, (b) field test, improve, and validate the effectiveness of         
program components, and (c) integrate all components with the ICAT system.      
We envision the system being useful in clinical, school, and preschool          
settings, as well as in the home.                                               
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Speech-Language Pathologists, teachers        
and parents recognize athe importance of early intervention when a child's      
language status is impaired, and such intervention is mandated by Federal       
law.  Our proposal is to produce a clinically effective language                
intervention system that will be a commercial success because it provides       
a cost effective means to supplement the efforts of professional service        
providers, and can fulfill an urgent and largely unmet need for intensive,      
individualized early language intervention.                                     
",2403648,R44HD035255,['R44HD035255'],HD,https://reporter.nih.gov/project-details/2403648,R44,1997,324379,1.0
"The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: 1) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
1) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotheraputically induced mutants          
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary relationships and events, provide                 
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
 DNA replication; Mononegavirales; RNA biosynthesis; biochemical evolution; computer assisted sequence analysis; computer program /software; nucleic acid sequence; virus genetics; virus protein COMPUTER BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION","The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: 1) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
1) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotheraputically induced mutants          
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary relationships and events, provide                 
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
",2667706,R01AI028309,['R01AI028309'],AI,https://reporter.nih.gov/project-details/2667706,R01,1998,231222,1.0

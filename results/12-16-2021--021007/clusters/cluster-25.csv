text,title,id,project_number,terms,administration,organization,mechanism,year,funding,score
 abstracting; artificial intelligence; information retrieval; information systems; literature survey; online computer ONLINE REFERENCE WORKS IN MEDICINE,,2320340,01LM083531,['N01LM083531'],LM,https://reporter.nih.gov/project-details/2320340,N01,1988,67038,-0.05135937078388693
"The purpose of this contract is to develop methods to analyze and
represent information in biomedical texts.  This will require
sophisticated natural language processing capabilities, involving
lexical, syntactic, semantic and pragmatic analysis of these texts.
The Natural Language Systems group of the Lister Hill Center for
Biomedical Communications is pursuing this work as an intramural
research project and seeks to collaborate with outside
organizations presently conducting closely related research.

The overall objective of this contract is to establish methods for
testing the hypothesis that access to a bibliographic database,
such as the National Library of Medicine's MEDLINE database,
can be improved by automated analysis of the free test in the
system.  The project work will involve modification and extension
of aspects of a natural language parser.

The MEDLINE database has citation records for several million
articles in biomedicine, representing several thousand journals.
Each citation record includes the title, an author prepared
abstract when available, author and journal names, and a set of
Medical Subject Headings under which the article has been
indexed by expert indexers.  The free text in the system is found
in the title and abstract fields of the citation records.  Titles are
normally complex noun phrases, while abstracts are composed of
well-formed, although highly specialized, English sentences.  The
purpose of the work under this contract is to develop methods for
parsing this text.  The parsing procedure should result in a set of
well-specified logical forms, representing the meaning of the
phrases and sentences in the citation record.
 abstracting; information retrieval; information systems; language; literature survey; semantics AUTOMATED ANALYSIS OF BIOMEDICAL TEXT","The purpose of this contract is to develop methods to analyze and
represent information in biomedical texts.  This will require
sophisticated natural language processing capabilities, involving
lexical, syntactic, semantic and pragmatic analysis of these texts.
The Natural Language Systems group of the Lister Hill Center for
Biomedical Communications is pursuing this work as an intramural
research project and seeks to collaborate with outside
organizations presently conducting closely related research.

The overall objective of this contract is to establish methods for
testing the hypothesis that access to a bibliographic database,
such as the National Library of Medicine's MEDLINE database,
can be improved by automated analysis of the free test in the
system.  The project work will involve modification and extension
of aspects of a natural language parser.

The MEDLINE database has citation records for several million
articles in biomedicine, representing several thousand journals.
Each citation record includes the title, an author prepared
abstract when available, author and journal names, and a set of
Medical Subject Headings under which the article has been
indexed by expert indexers.  The free text in the system is found
in the title and abstract fields of the citation records.  Titles are
normally complex noun phrases, while abstracts are composed of
well-formed, although highly specialized, English sentences.  The
purpose of the work under this contract is to develop methods for
parsing this text.  The parsing procedure should result in a set of
well-specified logical forms, representing the meaning of the
phrases and sentences in the citation record.
",2320317,01LM083521,['N01LM083521'],LM,https://reporter.nih.gov/project-details/2320317,N01,1988,68141,-0.002890067195314146
 abstracting; artificial intelligence; information retrieval; information systems; literature survey; online computer ONLINE REFERENCE WORKS (ORW) IN MEDICINE,,2320342,01LM083531,['N01LM083531'],LM,https://reporter.nih.gov/project-details/2320342,N01,1989,76114,-0.012564877027417955
"The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
 abstracting; artificial intelligence; evaluation /testing; information retrieval; information system analysis; information systems; language; medicine; vocabulary development for information system UNIFIED MEDICAL LANGUAGE SYSTEM R&D SUPPORT","The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
",2319079,01LM013538,['N01LM013538'],LM,https://reporter.nih.gov/project-details/2319079,N01,1991,109966,-0.05354004040095797
  EXPERT SYSTEM FOR MANAGING ANTICOAGULANT THERAPY,,2224025,R43HL047977,['R43HL047977'],HL,https://reporter.nih.gov/project-details/2224025,R43,1992,50000,-0.029667342213398094
"Present anesthesia machines have many displays and alarms but in fact do
little to support the anesthesiologist's decision making in a crisis.  The
objective of this proposal is to develop an anesthesia workstation which
includes an expert alarm system and central display in an effort to
significantly reduce the anesthesiologists reaction time to of the
anesthesiologist critical events.  This system would detect critical events
very quickly and provide a clearly defined cause for the problem.

Phase I studies demonstrated in animals that an expert alarm system can
detect 94% of the anesthesia machine failures and 89.4% of the breathing
circuit failures.  Phase II studies are planned to complete the development
of the alarm system, focusing effort on training the alarms algorithm and
developing simple disposable sensors.  The alarm system will use neural
network type artificial intelligence to identify failures in the patient
breathing circuit and in the anesthesia machine.  Input to the system will
come from a small CO2/flow/pressure transducer array.  Phase II includes
the animal and clinical testing required to obtain FDA marketing approval.
 anesthesia; apnea; artificial intelligence; clinical biomedical equipment; computer system design /evaluation; diagnosis design /evaluation; dogs; heart arrest; human subject; patient monitoring device DEVELOPMENT OF AN ANESTHESIA WORKSTATION","Present anesthesia machines have many displays and alarms but in fact do
little to support the anesthesiologist's decision making in a crisis.  The
objective of this proposal is to develop an anesthesia workstation which
includes an expert alarm system and central display in an effort to
significantly reduce the anesthesiologists reaction time to of the
anesthesiologist critical events.  This system would detect critical events
very quickly and provide a clearly defined cause for the problem.

Phase I studies demonstrated in animals that an expert alarm system can
detect 94% of the anesthesia machine failures and 89.4% of the breathing
circuit failures.  Phase II studies are planned to complete the development
of the alarm system, focusing effort on training the alarms algorithm and
developing simple disposable sensors.  The alarm system will use neural
network type artificial intelligence to identify failures in the patient
breathing circuit and in the anesthesia machine.  Input to the system will
come from a small CO2/flow/pressure transducer array.  Phase II includes
the animal and clinical testing required to obtain FDA marketing approval.
",2180905,R44GM041558,['R44GM041558'],GM,https://reporter.nih.gov/project-details/2180905,R44,1992,212244,-0.1200055002332332
  MULTIMEDIA EXPERT SYSTEM MODEL FOR SENIOR HOUSING,,2051963,R43AG010739,['R43AG010739'],AG,https://reporter.nih.gov/project-details/2051963,R43,1992,49985,-0.036394980283091
  AN EXPERT SYSTEM ADVISOR FOR DRUG DETOXIFICATION,,2118305,R44DA005889,['R44DA005889'],DA,https://reporter.nih.gov/project-details/2118305,R44,1992,159030,-0.19656155294646946
"The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
 abstracting; artificial intelligence; evaluation /testing; information retrieval; information system analysis; information systems; language; medicine; vocabulary development for information system UNIFIED MEDICAL LANGUAGE SYSTEM R&D SUPPORT","The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
",2319082,01LM013538,['N01LM013538'],LM,https://reporter.nih.gov/project-details/2319082,N01,1993,70406,-0.05527907954130868
  GUIDING COGNITIVE DEVELOPMENT--AN EXPERT SYSTEM APPROACH,,2249310,R43MH049943,['R43MH049943'],MH,https://reporter.nih.gov/project-details/2249310,R43,1993,50000,-0.021241869843107043
  PROGRAM PLANNING AND EVALUATION EXPERT SYSTEM,,2095601,R44CA053977,['R44CA053977'],CA,https://reporter.nih.gov/project-details/2095601,R44,1993,296387,-0.04386970145405733
"The proposed research has the broad long-term objective of providing useful     
computer-based consultative assistance to clinicians faced with complex         
choices among diagnostic and therapeutic options. This advice will be           
provided by an intelligent decision system, a computer program employing        
artificial intelligence techniques to automate the generation of decision       
models and analysis of those models to produce a recommendation. The            
advantages of incorporating decision analytic principles are explicit           
consideration of uncertainty and patients' preferences, and an axiomatic        
approach which links the decision elements to a recommendation. The system      
will permit users to control the elements of a decision model and will thus     
constitute a decision analysis workbench rather than a traditional expert       
system.                                                                         
                                                                                
The project will focus on the evaluation of pulmonary infiltrates in            
patients with the Acquired Immunodeficiency Syndrome (AIDS) or suspected        
AIDS. This is an important and increasingly common problem which involves       
high stakes for individual patients and a bewildering array of diagnostic       
and therapeutic options with complex trade-offs for clinicians. In this         
medical area, knowledge of disease prognoses and efficacy of therapy is         
rapidly accumulating. Thus, diagnostic and therapeutic strategies must          
continuously evolve in response to new data.                                    
                                                                                
The system is to be implemented in the Common LISP programming language on      
an Intel 80386-based microcomputer. The system will employ separate             
knowledge bases for decision analytic knowledge and medical domain              
knowledge. The modularity inherent in this organization will facilitate         
expansion and refinement of the knowledge base in response to new research      
findings and the availability of new techniques. The system will use a          
frame-based representation of diseases, diagnostic tests and treatments.        
These frames and their relations will determine the alternatives and            
outcomes modeled by the system. A network representation of probabilistic       
dependencies will ensure that the consistent updating of probabilities is       
performed in each decision tree context. Generation of a decision model         
will be guided by context-dependent rules which will determine at any given     
point in the tree which events to consider and how deeply to expand the         
decision model. The system will also contain facilities to tailor               
preference functions and probabilities to individual patients. Abstracted       
cases from the medical records of patients seen in our institution who have     
pulmonary infiltrates and AIDS or suspected AIDS will be used for system        
evaluation.                                                                     
 AIDS; Pneumocystis pneumonia; artificial intelligence; computer assisted medical decision making; computer program /software; human immunodeficiency virus 1; medical complication INTELLIGENT DECISION SYSTEM FOR LUNG DISEASE IN AIDS","The proposed research has the broad long-term objective of providing useful     
computer-based consultative assistance to clinicians faced with complex         
choices among diagnostic and therapeutic options. This advice will be           
provided by an intelligent decision system, a computer program employing        
artificial intelligence techniques to automate the generation of decision       
models and analysis of those models to produce a recommendation. The            
advantages of incorporating decision analytic principles are explicit           
consideration of uncertainty and patients' preferences, and an axiomatic        
approach which links the decision elements to a recommendation. The system      
will permit users to control the elements of a decision model and will thus     
constitute a decision analysis workbench rather than a traditional expert       
system.                                                                         
                                                                                
The project will focus on the evaluation of pulmonary infiltrates in            
patients with the Acquired Immunodeficiency Syndrome (AIDS) or suspected        
AIDS. This is an important and increasingly common problem which involves       
high stakes for individual patients and a bewildering array of diagnostic       
and therapeutic options with complex trade-offs for clinicians. In this         
medical area, knowledge of disease prognoses and efficacy of therapy is         
rapidly accumulating. Thus, diagnostic and therapeutic strategies must          
continuously evolve in response to new data.                                    
                                                                                
The system is to be implemented in the Common LISP programming language on      
an Intel 80386-based microcomputer. The system will employ separate             
knowledge bases for decision analytic knowledge and medical domain              
knowledge. The modularity inherent in this organization will facilitate         
expansion and refinement of the knowledge base in response to new research      
findings and the availability of new techniques. The system will use a          
frame-based representation of diseases, diagnostic tests and treatments.        
These frames and their relations will determine the alternatives and            
outcomes modeled by the system. A network representation of probabilistic       
dependencies will ensure that the consistent updating of probabilities is       
performed in each decision tree context. Generation of a decision model         
will be guided by context-dependent rules which will determine at any given     
point in the tree which events to consider and how deeply to expand the         
decision model. The system will also contain facilities to tailor               
preference functions and probabilities to individual patients. Abstracted       
cases from the medical records of patients seen in our institution who have     
pulmonary infiltrates and AIDS or suspected AIDS will be used for system        
evaluation.                                                                     
",2237728,R29LM005266,['R29LM005266'],LM,https://reporter.nih.gov/project-details/2237728,R29,1994,114010,-0.0305898437699804
  PROTEIN NMR SPECTRA USING ARTIFICIAL INTELLIGENCE,,2237590,F37LM000036,['F37LM000036'],LM,https://reporter.nih.gov/project-details/2237590,F37,1994,25608,-0.01976090018682204
"Hyperthermia has been shown to be efficacious in the treatment
of many tumors despite the limited capabilities of the available
heating technologies, which have been constrained by the short
history of modern clinical hyperthermia.  One developing, non-
invasive heating technology with the potential to tailor power
deposition patterns to individual tumors and to allow multipoint
temperature control is scanned, focussed ultrasound.  We have
developed such a system and propose (1) to perform phase 1
studies involving human tumors at a variety of locations to
determine which tumors and sites can be effectively treated by
this modality, (2) to develop a computerized treatment planning
system to allow the ultrasound scanning pattern to be
individualized and optimized for each patient, (3) to extend our
current, single point feedback control system to a multipoint
controller to give improved control over tumor temperature
distributions, and (4) to perform animal experiments to obtain
important knowledge concerning the interactions between
ultrasound and normal and tumor tissues.  The clinical studies will
investigate the temperature distributions in different tumors and
locations as a function of our system parameters (transducer
choice, feedback algorithm, scanning speed and pattern), and the
limitations of scanned focussed ultrasound imposed by pain and
other normal tissue toxicity, abdominal gas, tissue
inhomogeneities and interfaces, and the available utltrasonic
treatment window.  The treatment planning approach will use
serial CT scans of the patient, pretreatment measurements of the
tumor blood perfusion values and ultrasonic absorption
coefficients, and computerized treatment simulations.  The
multipoint feedback control system will utilize the know locations
of the temperature sensors to vary power as a function of
position.  The animal studies will investigate the possible
utilization of nonlinear effects (including a determination of
potential cavitation problems), the in vivo absorption coefficients
in different tissues, the distortion of the high power ultrasound
beam while passing through multiple tissues, and the power
deposition at soft tissuebone and-gas interfaces.  These studies
will give a scientific basis for the improved design and application
of scanned focussed ultrasound.
 abdomen neoplasm; artificial intelligence; clinical biomedical equipment; clinical trials; combination cancer therapy; computed axial tomography; computer assisted diagnosis; computer program /software; computer system design /evaluation; cytotoxicity; dogs; human subject; human therapy evaluation; image processing; neoplasm /cancer thermotherapy; ultrasonography; ultrasound biological effect; ultrasound therapy ULTRASOUND HYPERTHERMIA SYSTEM FOR CANCER THERAPY","Hyperthermia has been shown to be efficacious in the treatment
of many tumors despite the limited capabilities of the available
heating technologies, which have been constrained by the short
history of modern clinical hyperthermia.  One developing, non-
invasive heating technology with the potential to tailor power
deposition patterns to individual tumors and to allow multipoint
temperature control is scanned, focussed ultrasound.  We have
developed such a system and propose (1) to perform phase 1
studies involving human tumors at a variety of locations to
determine which tumors and sites can be effectively treated by
this modality, (2) to develop a computerized treatment planning
system to allow the ultrasound scanning pattern to be
individualized and optimized for each patient, (3) to extend our
current, single point feedback control system to a multipoint
controller to give improved control over tumor temperature
distributions, and (4) to perform animal experiments to obtain
important knowledge concerning the interactions between
ultrasound and normal and tumor tissues.  The clinical studies will
investigate the temperature distributions in different tumors and
locations as a function of our system parameters (transducer
choice, feedback algorithm, scanning speed and pattern), and the
limitations of scanned focussed ultrasound imposed by pain and
other normal tissue toxicity, abdominal gas, tissue
inhomogeneities and interfaces, and the available utltrasonic
treatment window.  The treatment planning approach will use
serial CT scans of the patient, pretreatment measurements of the
tumor blood perfusion values and ultrasonic absorption
coefficients, and computerized treatment simulations.  The
multipoint feedback control system will utilize the know locations
of the temperature sensors to vary power as a function of
position.  The animal studies will investigate the possible
utilization of nonlinear effects (including a determination of
potential cavitation problems), the in vivo absorption coefficients
in different tissues, the distortion of the high power ultrasound
beam while passing through multiple tissues, and the power
deposition at soft tissuebone and-gas interfaces.  These studies
will give a scientific basis for the improved design and application
of scanned focussed ultrasound.
",2088613,R01CA033922,['R01CA033922'],CA,https://reporter.nih.gov/project-details/2088613,R01,1994,308081,-0.025009605770888066
"The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
 abstracting; artificial intelligence; evaluation /testing; information retrieval; information system analysis; information systems; language; medicine; vocabulary development for information system UNIFIED MEDICAL LANGUAGE SYSTEM RESEARCH & DEVELOPMENT","The purpose of this contract is to provide research and development support
for the Unified Medical Language System.  Specifically, the contractor will
support (1) the definition of the functional components and algorithms
needed: to relate the user's terms to information in the UMLS Knowledge
Sources; to interact with the user to clarify the information needed; and
to select and access the information sources relevant to the user's
inquiry, and (2) the evaluation of the utility of the UMLS Knowledge
Sources and proposed functional components in a variety of environments.
",2319084,01LM013538,['N01LM013538'],LM,https://reporter.nih.gov/project-details/2319084,N01,1994,68457,-0.2679461933733747
"The first project objective is directed to developing a comprehensive           
computer interactive neuropsychologic test battery that is sensitive for        
detecting subtle disturbances of cerebral integrity.  This task will be         
accomplished accessing a panel of neuropsychologists, an expert systems         
specialist and a software architect.  Next, standardization will be             
conducted on 500 normal adolescents and adults.  This sample will be            
stratified by age and gender and demographically representative of the          
population.  Concurrent to these activities, an expert system will be           
created.  This latter task will enable dissemination of the final               
protocol to the widest possible user audience.  Finally, the interactive        
test battery will be used to ascertain the prevalence, types and severity       
of neuropsychologic deficit in samples of adolescent and adult drug             
abusers.                                                                        
                                                                                
The ultimate goal of this research program is to devise a standardized,         
quantitative and comprehensive method for characterizing the cognitive          
and psychomotor capacities of drug abusers.  Computer interactive               
testing, because of its precision in measurement, will enable the               
detection of subtle or occult impairment residual to chronic drug abuse.        
For this reason, the battery is to be created specifically for this             
population.  The information obtained from such an assessment has               
potentially important ramifications for determining the timing of               
treatment, type of treatment and post-treatment vocational rehabilitation       
of drug abusers.                                                                
 adolescence (12-20); adult human (21+); artificial intelligence; attention; auditory threshold; cognition; cognition disorders; computer assisted diagnosis; computer human interaction; computer program /software; computer system design /evaluation; dementia; diagnosis quality /standard; drug abuse; human subject; memory disorders; neural information processing; neuropsychological tests; neuropsychology; psychometrics; psychomotor disorders; psychomotor function; sensory discrimination; stroke; visual threshold NEUROPSYCHOLOGIC ASSESSMENT--CHRONIC DRUG ABUSE EFFECTS","The first project objective is directed to developing a comprehensive           
computer interactive neuropsychologic test battery that is sensitive for        
detecting subtle disturbances of cerebral integrity.  This task will be         
accomplished accessing a panel of neuropsychologists, an expert systems         
specialist and a software architect.  Next, standardization will be             
conducted on 500 normal adolescents and adults.  This sample will be            
stratified by age and gender and demographically representative of the          
population.  Concurrent to these activities, an expert system will be           
created.  This latter task will enable dissemination of the final               
protocol to the widest possible user audience.  Finally, the interactive        
test battery will be used to ascertain the prevalence, types and severity       
of neuropsychologic deficit in samples of adolescent and adult drug             
abusers.                                                                        
                                                                                
The ultimate goal of this research program is to devise a standardized,         
quantitative and comprehensive method for characterizing the cognitive          
and psychomotor capacities of drug abusers.  Computer interactive               
testing, because of its precision in measurement, will enable the               
detection of subtle or occult impairment residual to chronic drug abuse.        
For this reason, the battery is to be created specifically for this             
population.  The information obtained from such an assessment has               
potentially important ramifications for determining the timing of               
treatment, type of treatment and post-treatment vocational rehabilitation       
of drug abusers.                                                                
",2119052,R01DA006752,['R01DA006752'],DA,https://reporter.nih.gov/project-details/2119052,R01,1994,518746,-0.0680023514699271
  EXPERT SYSTEM FOR AUTOMATED SLEEP STAGE SCORING,,2272263,R43NS033437,['R43NS033437'],NS,https://reporter.nih.gov/project-details/2272263,R43,1995,98020,-0.07768352122422495
"This proposal aims to develop an improved understanding of the mechanisms
 involved in functional MRI of the brain and to optimize imaging and data
 analysis strategies for the detection of neuronal activity.  Functional MRI
 relies on the ability to detect the changes in NMR signal that are produced
 in discrete regions of cortex in response to specific activating stimuli,
 and are believed to reflect changes in local blood flow, volume and
 oxygenation.  Functional MRI promises to be a major addition to the methods
 available for studying brain activation.  Despite the widespread claims for
 the power and successes of the method, there remain several unanswered
 questions regarding its optimal mode of use, the tissue and technical
 factors that are important in determining the signal changes detected, and
 the significance and interpretation of these signal changes.  The research
 proposed would systematically address such issues.  The underlying
 mechanism may include both susceptibility contrast effects, based on the
 BOLD effect, as well as wash-in effects, and these will be separately
 quantified.  The factors that affect each mechanism will be separately
 identified and measured.  For the BOLD effect, extensive computer modeling
 and measurements in phantoms and animals brains will be used to establish
 the relative sensitivity to vascular structures of different sizes,
 spacings and orientations, as well as other tissue properties such as the
 rate of water diffusion.  The separate sensitivities to s-called static
 field effects (T2*),  diffusive losses and other mechanisms will also be
 established.  The performance of different pulse sequences will be compared
 to devise optimal methods of scanning and detection at 1.5T.  Echo planar
 imaging, conventional gradient echo and fast spin echo imaging as well as
 more novel schemes will be compared in phantoms, animal brains and examples
 of human activation.  Human and animal activations will be produced in vivo
 using  visual and motor stimuli as well as by alteration of global blood
 flow by acetazolamide and hypercarbia.  A critical feature of current
 paradigms for detecting activation is the method of data analysis, which is
 interrelated with the nature of the task and imaging method used.  We will
 compare different methods of analyzing functional data sets, including
 statistical parameter mapping, time-correlation analyses, and principal
 component analysis.  The sensitivity of each to motion and other artifacts
 will be established by in in vivo comparisons and by computer simulations.
 From these studies, we anticipate being able to improve strategies for the
 use and interpretation of functional MRI in human studies of function and
 cognition.
 acetazolamide; biophysics; blood flow measurement; blood vessels; blood volume; brain electrical activity; capillary; computer data analysis; computer simulation; human subject; hypercapnia; laboratory rat; magnetic resonance imaging; method development; motor neurons; nuclear magnetic resonance spectroscopy; phantom model; respiratory oxygenation; statistics /biometry; visual stimulus; water flow BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI","This proposal aims to develop an improved understanding of the mechanisms
 involved in functional MRI of the brain and to optimize imaging and data
 analysis strategies for the detection of neuronal activity.  Functional MRI
 relies on the ability to detect the changes in NMR signal that are produced
 in discrete regions of cortex in response to specific activating stimuli,
 and are believed to reflect changes in local blood flow, volume and
 oxygenation.  Functional MRI promises to be a major addition to the methods
 available for studying brain activation.  Despite the widespread claims for
 the power and successes of the method, there remain several unanswered
 questions regarding its optimal mode of use, the tissue and technical
 factors that are important in determining the signal changes detected, and
 the significance and interpretation of these signal changes.  The research
 proposed would systematically address such issues.  The underlying
 mechanism may include both susceptibility contrast effects, based on the
 BOLD effect, as well as wash-in effects, and these will be separately
 quantified.  The factors that affect each mechanism will be separately
 identified and measured.  For the BOLD effect, extensive computer modeling
 and measurements in phantoms and animals brains will be used to establish
 the relative sensitivity to vascular structures of different sizes,
 spacings and orientations, as well as other tissue properties such as the
 rate of water diffusion.  The separate sensitivities to s-called static
 field effects (T2*),  diffusive losses and other mechanisms will also be
 established.  The performance of different pulse sequences will be compared
 to devise optimal methods of scanning and detection at 1.5T.  Echo planar
 imaging, conventional gradient echo and fast spin echo imaging as well as
 more novel schemes will be compared in phantoms, animal brains and examples
 of human activation.  Human and animal activations will be produced in vivo
 using  visual and motor stimuli as well as by alteration of global blood
 flow by acetazolamide and hypercarbia.  A critical feature of current
 paradigms for detecting activation is the method of data analysis, which is
 interrelated with the nature of the task and imaging method used.  We will
 compare different methods of analyzing functional data sets, including
 statistical parameter mapping, time-correlation analyses, and principal
 component analysis.  The sensitivity of each to motion and other artifacts
 will be established by in in vivo comparisons and by computer simulations.
 From these studies, we anticipate being able to improve strategies for the
 use and interpretation of functional MRI in human studies of function and
 cognition.
",2272080,R01NS033332,['R01NS033332'],NS,https://reporter.nih.gov/project-details/2272080,R01,1995,331160,-0.12198828140100831
"The fundamental issue underlying this proposal is the mechanism
 responsible for associative learning in mammals.  Two of the most
 successful model systems today involve adaptions of the vestibulo-ocular
 reflex (VOR) and classical conditioning of the nictitating membrane
 response (NMR).  This proposal seeks to significantly extend our
 understanding of associative learning mechanisms by an in-depth
 computational and neurophysiological analysis of VOR and its adaptation
 in the cat combined with an effort to bridge the conceptual gap between
 this model system and NMR conditioning.  Three key questions will be
 explored:
 
 1.  Can a computational model of neural function constrained by known
 physiology, account for the currently described behavior as well as serve
 as a heuristic tool in directing further experimentation? This defines
 a bottom-up approach which has not been utilized on this scale previously
 in analyzing a specific response system.  Our prior modeling work in this
 project.  These predictions include identification of: a) likely
 anatomical site for learning to take place and b) a physiological
 substrate for the ""teacher"" signal in a supervised learning algorithm
 used to represent reflex plasticity.
 
 2.  What is the precise nature of the changes which take place at the
 single neuron level which ar responsible for the development of motor
 learning?  A two-prong approach to help answer this critical questions
 is proposed.  First, an in-depth characterization of the changes in the
 spatial characteristic of neurons crucial to expression of the VOR will
 be conducted. Second, u sing stimulation and lesion techniques the nature
 of the training signal responsible for guiding plasticity in the reflex
 will be explore.
 
 3.  Can mechanisms accounting for both VOR adaption and classical
 conditioning of the NMR be unified under a single learning scheme?
 Different ways in which to map the VOR adaption paradigm to a classical
 conditioning paradigm will be explored.  A clear demonstration of VOR
 adaptation development under conditions defined by classical conditioning
 would lead the way to unifying these two significant bodies of
 investigation into associative learning mechanisms.
 
 The benefits of a clear understanding of how the brain express
 associative learning are enormous form a societal viewpoint; from better
 recovery of function in stroke patients, to improvements in the
 behavioral capabilities of the mentally retarded to, potentially, an
 improvement in the ability of our society, as a whole, to increase its
 intellectual capacities.
 association learning; brain electrical activity; cats; computational neuroscience; electrophysiology; electrostimulus; eye movements; model design /development; neural plasticity; nictitating membrane; vestibuloocular reflex VOR LEARNING ALGORITHMS AND NEURONAL PLASTICITY","The fundamental issue underlying this proposal is the mechanism
 responsible for associative learning in mammals.  Two of the most
 successful model systems today involve adaptions of the vestibulo-ocular
 reflex (VOR) and classical conditioning of the nictitating membrane
 response (NMR).  This proposal seeks to significantly extend our
 understanding of associative learning mechanisms by an in-depth
 computational and neurophysiological analysis of VOR and its adaptation
 in the cat combined with an effort to bridge the conceptual gap between
 this model system and NMR conditioning.  Three key questions will be
 explored:
 
 1.  Can a computational model of neural function constrained by known
 physiology, account for the currently described behavior as well as serve
 as a heuristic tool in directing further experimentation? This defines
 a bottom-up approach which has not been utilized on this scale previously
 in analyzing a specific response system.  Our prior modeling work in this
 project.  These predictions include identification of: a) likely
 anatomical site for learning to take place and b) a physiological
 substrate for the ""teacher"" signal in a supervised learning algorithm
 used to represent reflex plasticity.
 
 2.  What is the precise nature of the changes which take place at the
 single neuron level which ar responsible for the development of motor
 learning?  A two-prong approach to help answer this critical questions
 is proposed.  First, an in-depth characterization of the changes in the
 spatial characteristic of neurons crucial to expression of the VOR will
 be conducted. Second, u sing stimulation and lesion techniques the nature
 of the training signal responsible for guiding plasticity in the reflex
 will be explore.
 
 3.  Can mechanisms accounting for both VOR adaption and classical
 conditioning of the NMR be unified under a single learning scheme?
 Different ways in which to map the VOR adaption paradigm to a classical
 conditioning paradigm will be explored.  A clear demonstration of VOR
 adaptation development under conditions defined by classical conditioning
 would lead the way to unifying these two significant bodies of
 investigation into associative learning mechanisms.
 
 The benefits of a clear understanding of how the brain express
 associative learning are enormous form a societal viewpoint; from better
 recovery of function in stroke patients, to improvements in the
 behavioral capabilities of the mentally retarded to, potentially, an
 improvement in the ability of our society, as a whole, to increase its
 intellectual capacities.
",2269743,R29NS031805,['R29NS031805'],NS,https://reporter.nih.gov/project-details/2269743,R29,1995,93701,-0.04752029029124983
  AUTOMATED DATABASE EXPERT SYSTEM FOR ACUTE STROKE,,2266667,R29NS027924,['R29NS027924'],NS,https://reporter.nih.gov/project-details/2266667,R29,1995,87848,-0.20983318566325157
"In the STARE (STructured Analysis of the REtina) project, we are
 developing a computerized image-interpreting system with hierarchical
 inferencing to measure, compare, and diagnose images of the ocular
 fundus. The system will have sufficient depth of imaging tools to be a
 resource for researchers or clinicians.
 
 The STARE system is designed to find objects of interest (normal
 anatomical structures and lesions) in digitized ocular fundus images and
 to use these objects to diagnose an image, to detect changes in
 sequential images, and to make clinically useful measurements that are
 currently tedious or costly. To accomplish these difficult goals, we
 must segment and identify the objects of interest. Identified objects
 can be used to compare images, and the objects can be assembled to
 describe the image. Image interpretation incorporating expert systems
 and neural networks can provide the structure for cross-sectional
 epidemiological studies.
 
 There was no established paradigm to follow to construct a system for
 image interpretation. We designed the overall structure of the process
 and determined how each task was to be accomplished. We have broken the
 project into steps, each of which has been accomplished. We are able to
 find objects of importance and correctly identify and localize them on
 a fundus coordinate system that we designed. We have created and tested
 a neural network and an expert system (INTELLEYE) to handle the
 interpretation of an image and its contents.
 
 We will now improve the accuracy of each step, increase the number of
 lesions we can identify, and integrate the image analysis steps with the
 expert system to allow smooth progression from image to diagnosis and
 change detection. We will validate the usefulness and accuracy of the
 system by comparing its diagnosis and image comparison to trained
 readers of ophthalmic images.
 
 The goal of this project is a system with multiple imaging tools and
 inferencing ability that can be adapted to a variety of imaging tasks.
 The outcome will be an image-interpreting system for use in clinical and
 research settings that will build annotated image databases, screen
 images of the ocular fundus for health care systems, furnish decision
 support for primary care providers, and extend the capability and
 productivity of the ophthalmologist.
 artificial intelligence; computer system design /evaluation; diagnosis design /evaluation; digital imaging; eye fundus photography; human subject; image processing STRUCTURED ANALYSIS OF THE RETINA","In the STARE (STructured Analysis of the REtina) project, we are
 developing a computerized image-interpreting system with hierarchical
 inferencing to measure, compare, and diagnose images of the ocular
 fundus. The system will have sufficient depth of imaging tools to be a
 resource for researchers or clinicians.
 
 The STARE system is designed to find objects of interest (normal
 anatomical structures and lesions) in digitized ocular fundus images and
 to use these objects to diagnose an image, to detect changes in
 sequential images, and to make clinically useful measurements that are
 currently tedious or costly. To accomplish these difficult goals, we
 must segment and identify the objects of interest. Identified objects
 can be used to compare images, and the objects can be assembled to
 describe the image. Image interpretation incorporating expert systems
 and neural networks can provide the structure for cross-sectional
 epidemiological studies.
 
 There was no established paradigm to follow to construct a system for
 image interpretation. We designed the overall structure of the process
 and determined how each task was to be accomplished. We have broken the
 project into steps, each of which has been accomplished. We are able to
 find objects of importance and correctly identify and localize them on
 a fundus coordinate system that we designed. We have created and tested
 a neural network and an expert system (INTELLEYE) to handle the
 interpretation of an image and its contents.
 
 We will now improve the accuracy of each step, increase the number of
 lesions we can identify, and integrate the image analysis steps with the
 expert system to allow smooth progression from image to diagnosis and
 change detection. We will validate the usefulness and accuracy of the
 system by comparing its diagnosis and image comparison to trained
 readers of ophthalmic images.
 
 The goal of this project is a system with multiple imaging tools and
 inferencing ability that can be adapted to a variety of imaging tasks.
 The outcome will be an image-interpreting system for use in clinical and
 research settings that will build annotated image databases, screen
 images of the ocular fundus for health care systems, furnish decision
 support for primary care providers, and extend the capability and
 productivity of the ophthalmologist.
",2238143,R01LM005759,['R01LM005759'],LM,https://reporter.nih.gov/project-details/2238143,R01,1995,274013,-0.07716516601123014
"Efforts to apply computer methods to assess and improve the quality of
 care in the hospital have been stymied by limited access to clinical
 data.  Free-text data have detailed clinical descriptions of patients
 that would be useful in computer altering systems and computer reminder
 systems.  However, free-text data cannot be interpreted by most clinical
 computer systems.  In this proposal, we describe research specifically
 aimed at making free-text data accessible to computer-based applications
 for assessing and improving the quality of care.  In particular the
 research plan focuses on the development of technologies that would allow
 free-text data to be used in clinical alert systems for critical test
 results; in reminder systems to encourage adherence to practice
 guidelines; and in data collection systems for severity of illness models
 applied in the assessment of risk adjusted outcomes.  The approach
 described in the research plan emphasizes the development of statistical
 and probabilistic methods for interpretation of data derived from medical
 language processing systems.  We will test the methods developed for
 language processing and interpretation developed under this proposal in
 3 area: 1) the identification of concepts related to severity from the
 MedisGroups and the Computerized Severity Index models of patient
 severity of illness; 2) the identification of chest x ray reports and
 mammography reports with potentially malignant findings that require
 radiological follow-up; 3) and the automatic assessment of
 appropriateness of coronary artery bypass grafting (CABG) surgery from
 free-text descriptions of patients based on the application of a clinical
 practice guideline for CABG surgery.
 abstracting; artificial intelligence; automated medical record system; computer assisted diagnosis; computer assisted medical decision making; computer human interaction; coronary bypass; health care model; health care quality; human data; information retrieval; mammography; method development; statistics /biometry; thoracic radiography; vocabulary development for information system COMPUTER INTERPRETATION OF FREE-TEXT DATA","Efforts to apply computer methods to assess and improve the quality of
 care in the hospital have been stymied by limited access to clinical
 data.  Free-text data have detailed clinical descriptions of patients
 that would be useful in computer altering systems and computer reminder
 systems.  However, free-text data cannot be interpreted by most clinical
 computer systems.  In this proposal, we describe research specifically
 aimed at making free-text data accessible to computer-based applications
 for assessing and improving the quality of care.  In particular the
 research plan focuses on the development of technologies that would allow
 free-text data to be used in clinical alert systems for critical test
 results; in reminder systems to encourage adherence to practice
 guidelines; and in data collection systems for severity of illness models
 applied in the assessment of risk adjusted outcomes.  The approach
 described in the research plan emphasizes the development of statistical
 and probabilistic methods for interpretation of data derived from medical
 language processing systems.  We will test the methods developed for
 language processing and interpretation developed under this proposal in
 3 area: 1) the identification of concepts related to severity from the
 MedisGroups and the Computerized Severity Index models of patient
 severity of illness; 2) the identification of chest x ray reports and
 mammography reports with potentially malignant findings that require
 radiological follow-up; 3) and the automatic assessment of
 appropriateness of coronary artery bypass grafting (CABG) surgery from
 free-text descriptions of patients based on the application of a clinical
 practice guideline for CABG surgery.
",2237957,R29LM005626,['R29LM005626'],LM,https://reporter.nih.gov/project-details/2237957,R29,1995,113251,-0.19601508726274847
"The proposed research seeks to understand the process by which an older
 adult who is experiencing an episode of illness decides whether to seek
 the care of a physician. Two complementary methodological approaches are
 taken.  (l) Detailed illness episode data from a seven wave longitudinal
 panel study of 1009 Medicare recipients enrolled in a Health Maintenance
 Organization (HMO) will be used to develop a series of event history
 models of physician contact.  The effects of various explanatory variables
 (sociodemographic, prior health history) and time.varying covariates
 (other illness response strategies) on the risk of seeking medical care
 will be estimated in proportional hazards models that control for specific
 illness types and respondent categories. (2) A sample of 150 ethnographic
 informants will be selected from the original group of respondents to
 represent theoretically relevant categories of the population (e.g., men,
 women, married, not married, U.S. born, immigrants, health-status).
 Detailed ethnographic data on illnesses experienced in later life, their
 causes and symptoms, and the range of available and appropriate treatment
 alternatives for each illness will be collected systematically using free
 lists, card sorts, paired comparisons, and sentence frames.  These data
 will be analyzed using techniques such as consensus analysis,
 multidimensional scaling and hierarchical clustering. Expressed rules for
 deciding among treatment alternatives, particularly whether to contact a
 physician, will be explored using ethnographic decision tree modeling
 techniques. Both approaches should yield information useful to planners
 and evaluators of health education programs aimed at increasing the match
 between need for services as defined by the medical profession, and actual
 use of medical services by this population.
 Medicare /Medicaid; age difference; alternative medicine; decision making; gender difference; health behavior; health care model; health care service utilization; human data; human old age (65+); human subject; human very old age (85+); interview; longitudinal human study; managed care; mathematical model; population survey; racial /ethnic difference; religion; self care; socioeconomics DECISIONS ABOUT HMO SERVICE USE FOR LATE LIFE ILLNESS","The proposed research seeks to understand the process by which an older
 adult who is experiencing an episode of illness decides whether to seek
 the care of a physician. Two complementary methodological approaches are
 taken.  (l) Detailed illness episode data from a seven wave longitudinal
 panel study of 1009 Medicare recipients enrolled in a Health Maintenance
 Organization (HMO) will be used to develop a series of event history
 models of physician contact.  The effects of various explanatory variables
 (sociodemographic, prior health history) and time.varying covariates
 (other illness response strategies) on the risk of seeking medical care
 will be estimated in proportional hazards models that control for specific
 illness types and respondent categories. (2) A sample of 150 ethnographic
 informants will be selected from the original group of respondents to
 represent theoretically relevant categories of the population (e.g., men,
 women, married, not married, U.S. born, immigrants, health-status).
 Detailed ethnographic data on illnesses experienced in later life, their
 causes and symptoms, and the range of available and appropriate treatment
 alternatives for each illness will be collected systematically using free
 lists, card sorts, paired comparisons, and sentence frames.  These data
 will be analyzed using techniques such as consensus analysis,
 multidimensional scaling and hierarchical clustering. Expressed rules for
 deciding among treatment alternatives, particularly whether to contact a
 physician, will be explored using ethnographic decision tree modeling
 techniques. Both approaches should yield information useful to planners
 and evaluators of health education programs aimed at increasing the match
 between need for services as defined by the medical profession, and actual
 use of medical services by this population.
",2052121,R29AG010887,['R29AG010887'],AG,https://reporter.nih.gov/project-details/2052121,R29,1995,94704,-0.1721673878287746
"The overall goal of this continuing project is to develop efficient
 algorithms which will permit the construction of realistic and
 comprehensive mathematical models that relate normal and pathological
 renal function to the underlying membrane transport and flow processes in
 the renal tubules and their associated vasculature. The primary thrust of
 our research during the next period will be:
 
 1. To use our present inner medullary models to develop fairly detailed
 architectural models of the inner stripe of the outer medulla and then
 integrate these new models with our present central core and vasa recta n-
 nephron inner medullary models.
 
 2. To include additional solutes and osmolytes and investigate the role of
 osmolyte production in the inner medulla.
 
 3. To incorporate a more detailed representation of transmural movement of
 water and solutes that takes into account both cellular and paracellular
 pathways and also the exchange of electrolytes and water between red blood
 cells and plasma in the vasa recta.
 
 4. Adapt current algorithms for serial computers, - if necessary, develop
 new ones based on our split system solvers - to fully exploit the parallel
 and vector processing capabilities of supercomputers. This is necessary to
 handle the size and complexity of our current and future n-nephron models.
 
 5. If necessary - for parallel and/or vector algorithms - (a) improve
 stability and accuracy of numerical methods, (b) develop hierarchal
 solution strategies, and (c) incorporate continuation and smoothing
 methods.
 
 6. To make the non-linear Schur Complement type methods developed by us
 readily available to other biomedical modelers, for use on minicomputers
 and/or workstations. These include: (a) reduced models of the whole kidney
 and medulla, (b) models of epithelia and isolated perfused tubules, (c)
 tubuloglomerular feedback response models, and (d) neural network models.
 artificial intelligence; computer simulation; kidney circulation; mathematical model; model design /development; parallel processing; renal medulla; renal tubular transport; renal tubule; supercomputer NUMERICAL SOLUTION OF RENAL TRANSPORT EQUATIONS","The overall goal of this continuing project is to develop efficient
 algorithms which will permit the construction of realistic and
 comprehensive mathematical models that relate normal and pathological
 renal function to the underlying membrane transport and flow processes in
 the renal tubules and their associated vasculature. The primary thrust of
 our research during the next period will be:
 
 1. To use our present inner medullary models to develop fairly detailed
 architectural models of the inner stripe of the outer medulla and then
 integrate these new models with our present central core and vasa recta n-
 nephron inner medullary models.
 
 2. To include additional solutes and osmolytes and investigate the role of
 osmolyte production in the inner medulla.
 
 3. To incorporate a more detailed representation of transmural movement of
 water and solutes that takes into account both cellular and paracellular
 pathways and also the exchange of electrolytes and water between red blood
 cells and plasma in the vasa recta.
 
 4. Adapt current algorithms for serial computers, - if necessary, develop
 new ones based on our split system solvers - to fully exploit the parallel
 and vector processing capabilities of supercomputers. This is necessary to
 handle the size and complexity of our current and future n-nephron models.
 
 5. If necessary - for parallel and/or vector algorithms - (a) improve
 stability and accuracy of numerical methods, (b) develop hierarchal
 solution strategies, and (c) incorporate continuation and smoothing
 methods.
 
 6. To make the non-linear Schur Complement type methods developed by us
 readily available to other biomedical modelers, for use on minicomputers
 and/or workstations. These include: (a) reduced models of the whole kidney
 and medulla, (b) models of epithelia and isolated perfused tubules, (c)
 tubuloglomerular feedback response models, and (d) neural network models.
",2137100,R01DK017593,['R01DK017593'],DK,https://reporter.nih.gov/project-details/2137100,R01,1995,102598,-0.025697222342378888
"DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed
 to  develop adaptive imaging algorithms and instrumentation to compensate
 for  tissue-induced ultrasonic image degradation.  Theoretical and
 simulation  studies are proposed to optimize the accuracy, stability, and
 speed of  adaptive algorithms and to explore the impact of transducer
 design on adaptive  imaging.  In addition, the applicants proposed to
 collect high-quality tissue  echo data and through-transmission data to
 investigate the nature of tissue- induced image degradation.  The
 adaptive imaging techniques would be implemented in real-time on an
 advanced engineering prototype scanner.  Synthetic receive aperture (SRA)
 techniques, combined with adaptive imaging,  would be used to address
 1000 and 2000 element two dimensional (2-D) arrays  and to form very high
 resolution images.  Specialized analog multiplexers and  other hardware
 would be constructed for this system.  Clinical trials would  evaluate
 the performance of the adaptive/SRA system in imaging breast lesions  and
 breast microcalcifications, and in renal and adrenal gland imaging
 studies.  The applicants hypothesized that the proposed techniques and
 system  would markedly improve ultrasonic image quality in a wide variety
 of clinical  applications.
 adrenal disorder; artificial intelligence; calcification; computer simulation; computer system design /evaluation; endocrine disorder diagnosis; human subject; image processing; kidney disorder diagnosis; mammary disorder; nephrolithiasis; reproductive system disorder diagnosis; ultrasonography ADAPTIVE ULTRASONIC IMAGING","DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed
 to  develop adaptive imaging algorithms and instrumentation to compensate
 for  tissue-induced ultrasonic image degradation.  Theoretical and
 simulation  studies are proposed to optimize the accuracy, stability, and
 speed of  adaptive algorithms and to explore the impact of transducer
 design on adaptive  imaging.  In addition, the applicants proposed to
 collect high-quality tissue  echo data and through-transmission data to
 investigate the nature of tissue- induced image degradation.  The
 adaptive imaging techniques would be implemented in real-time on an
 advanced engineering prototype scanner.  Synthetic receive aperture (SRA)
 techniques, combined with adaptive imaging,  would be used to address
 1000 and 2000 element two dimensional (2-D) arrays  and to form very high
 resolution images.  Specialized analog multiplexers and  other hardware
 would be constructed for this system.  Clinical trials would  evaluate
 the performance of the adaptive/SRA system in imaging breast lesions  and
 breast microcalcifications, and in renal and adrenal gland imaging
 studies.  The applicants hypothesized that the proposed techniques and
 system  would markedly improve ultrasonic image quality in a wide variety
 of clinical  applications.
",2091175,R01CA043334,['R01CA043334'],CA,https://reporter.nih.gov/project-details/2091175,R01,1995,206746,-0.022860743805596162
"The goal of the proposed research is to develop computer-aided diagnostic
 (CAD) schemes for detection of lung nodules, interstitial infiltrates, and
 pneumothoraces in digital chest images. We plan to develop advanced
 computerized schemes and software for improvements in sensitivity,
 specificity and efficiency in order to implement and evaluate such schemes
 in a controlled clinical environment. We believe that these computer-aided
 diagnostic schemes, which provide the radiologist with the location and/or
 quantitative measures of highly suspected lesions, have the potential to
 improve diagnostic accuracy in the detection of cancer by reducing human
 errors associated with radiologic diagnoses.
 
 Specifically, we plan to (l) develop an improved scheme for automated
 detection of lung nodules by (a) combinations of linear and nonlinear
 morphological filtering techniques based on a difference image method for
 enhancement and suppression of lung nodules, (b) reduction of false
 positive detections by detailed analysis of image features by chest
 radiologists and also use of artificial neural networks, (c) analysis of
 posterior ribs for reduction of false positives, (d) application of
 wavelet transform for increasing the sensitivity, and (e) observer
 performance studies for optimal use of CAD methods; (2) develop an
 improved scheme for automated lung texture analysis by (a) devising an
 automated technique for sampling numerous regions of interest (ROIs) in
 the lung fields, (b) investigation of new texture measures based on
 analysis of the shape and anisotropic properties of the power spectrum of
 lung textures, and (c) application of artificial neural networks for
 detection and classification of interstitial infiltrates; (3) develop an
 automated scheme for detection of pneumothorax by (a) application of the
 Hough transform in conjunction with an edge enhancement technique for
 detection of subtle curved lines, and (b) ROC analysis of radiologists'
 performances for evaluation of the usefulness of the CAD scheme; and (4)
 implement and evaluate the CAD schemes in a high-resolution. high-speed
 image processing system by (a) development of a prototype intelligent
 workstation with efficient algorithms and efficient man-machine
 interfaces, and (b) carrying out pilot studies on clinical evaluation of
 our chest CAD schemes in comparison with conventional readings in terms of
 the three types of abnormalities related to lung nodules, interstitial
 infiltrates, and pneumothoraces.
 artificial intelligence; computer assisted diagnosis; computer human interaction; computer system design /evaluation; digital imaging; disease /disorder classification; human data; image processing; lung neoplasms; neoplasm /cancer radiodiagnosis; pneumothorax disorder; thoracic radiography COMPUTER AIDED DIAGNOSIS IN CHEST RADIOGRAPHY","The goal of the proposed research is to develop computer-aided diagnostic
 (CAD) schemes for detection of lung nodules, interstitial infiltrates, and
 pneumothoraces in digital chest images. We plan to develop advanced
 computerized schemes and software for improvements in sensitivity,
 specificity and efficiency in order to implement and evaluate such schemes
 in a controlled clinical environment. We believe that these computer-aided
 diagnostic schemes, which provide the radiologist with the location and/or
 quantitative measures of highly suspected lesions, have the potential to
 improve diagnostic accuracy in the detection of cancer by reducing human
 errors associated with radiologic diagnoses.
 
 Specifically, we plan to (l) develop an improved scheme for automated
 detection of lung nodules by (a) combinations of linear and nonlinear
 morphological filtering techniques based on a difference image method for
 enhancement and suppression of lung nodules, (b) reduction of false
 positive detections by detailed analysis of image features by chest
 radiologists and also use of artificial neural networks, (c) analysis of
 posterior ribs for reduction of false positives, (d) application of
 wavelet transform for increasing the sensitivity, and (e) observer
 performance studies for optimal use of CAD methods; (2) develop an
 improved scheme for automated lung texture analysis by (a) devising an
 automated technique for sampling numerous regions of interest (ROIs) in
 the lung fields, (b) investigation of new texture measures based on
 analysis of the shape and anisotropic properties of the power spectrum of
 lung textures, and (c) application of artificial neural networks for
 detection and classification of interstitial infiltrates; (3) develop an
 automated scheme for detection of pneumothorax by (a) application of the
 Hough transform in conjunction with an edge enhancement technique for
 detection of subtle curved lines, and (b) ROC analysis of radiologists'
 performances for evaluation of the usefulness of the CAD scheme; and (4)
 implement and evaluate the CAD schemes in a high-resolution. high-speed
 image processing system by (a) development of a prototype intelligent
 workstation with efficient algorithms and efficient man-machine
 interfaces, and (b) carrying out pilot studies on clinical evaluation of
 our chest CAD schemes in comparison with conventional readings in terms of
 the three types of abnormalities related to lung nodules, interstitial
 infiltrates, and pneumothoraces.
",2104011,R01CA062625,['R01CA062625'],CA,https://reporter.nih.gov/project-details/2104011,R01,1995,336226,-0.1553676206435571
"MVBS Inc. proposes to continue development of Hera, TM a microcomputer-         
based expert system that takes a preventive medicine history from a lay         
user, the makes recommendations to the user about various preventive            
health activities.  These recommendations, tailored to the user's               
history, are based on authoritative practice guidelines stored in Hera's        
knowledge base.  The user is strongly urged to discuss the                      
recommendations with a physician or other trained health care provider          
before acting on them.                                                          
                                                                                
Hera's knowledge base is also capable of storing multimedia prevention-         
related information which the user can explore using hypertext links.           
This comprehensive functionality of the Hera system compels us to label         
it nothing less than a 'prevention workstation.'                                
                                                                                
Hera will be sold to consumers for use on home microcomputers.                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Hera is intended to be a product for          
the mainstream commercial software market.  Several derivative products         
are possible, too.                                                              
 artificial intelligence; case history; computer assisted patient care; computer human interaction; computer system design /evaluation; disease /disorder prevention /control; human subject; information systems HERA-AN EXPERT SYSTEM FOR PREVENTIVE CARE","MVBS Inc. proposes to continue development of Hera, TM a microcomputer-         
based expert system that takes a preventive medicine history from a lay         
user, the makes recommendations to the user about various preventive            
health activities.  These recommendations, tailored to the user's               
history, are based on authoritative practice guidelines stored in Hera's        
knowledge base.  The user is strongly urged to discuss the                      
recommendations with a physician or other trained health care provider          
before acting on them.                                                          
                                                                                
Hera's knowledge base is also capable of storing multimedia prevention-         
related information which the user can explore using hypertext links.           
This comprehensive functionality of the Hera system compels us to label         
it nothing less than a 'prevention workstation.'                                
                                                                                
Hera will be sold to consumers for use on home microcomputers.                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Hera is intended to be a product for          
the mainstream commercial software market.  Several derivative products         
are possible, too.                                                              
",2111504,R43CA067740,['R43CA067740'],CA,https://reporter.nih.gov/project-details/2111504,R43,1995,92967,-0.03924565157177024
"     We propose to develop a machine-vision system for the diagnostic
 interpretation of histopathologic sections and cytologic preparations. in
 continuation of an ongoing research project grant.  The system will have
 ""image understanding capability."" that is, it will follow in its reasoning
 a model of the histology of a given application.  The interpretive
 expert-system module will integrate concepts from human diagnostic
 knowledge with machine-computable histometric features.  Systems of this
 kind can provide objective. quantitative, consistent evaluation of lesions,
 yielding more reliable interpretation and diagnostic, as well as
 prognostic, assessment.  Using a combination of large data bases of
 digitized imagery for given diagnostic situations. and relational data
 bases (including patient history. treatment, and outcome), it is hoped that
 the objective assessment eventually will be related reliably to truth in
 diagnosis.
      The requirements for representative sampling are in the multimegapixel
 range, and require fully automatic scene segmentation and histometric
 feature extraction.  This difficult problem has been largely resolved
 through ongoing support, employing an AI-based, adaptive segmentation
 approach.
      The major challenges for the proposed research are: 1) to gain an
 understanding of the necessary and sufficient human diagnostic clues and
 corresponding histometric analogs, that is, to determine the library of
 transforms to be used by the interpretive expert-system module; 2) to
 develop learning capability of the system for conceptual data; and 3) to
 gain an understanding of the functional requirements. dependence structure,
 and decision control capabilities of such a system.
 artificial intelligence; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; diagnosis quality /standard; histopathology; information system analysis; prognosis KNOWLEDGE BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY","     We propose to develop a machine-vision system for the diagnostic
 interpretation of histopathologic sections and cytologic preparations. in
 continuation of an ongoing research project grant.  The system will have
 ""image understanding capability."" that is, it will follow in its reasoning
 a model of the histology of a given application.  The interpretive
 expert-system module will integrate concepts from human diagnostic
 knowledge with machine-computable histometric features.  Systems of this
 kind can provide objective. quantitative, consistent evaluation of lesions,
 yielding more reliable interpretation and diagnostic, as well as
 prognostic, assessment.  Using a combination of large data bases of
 digitized imagery for given diagnostic situations. and relational data
 bases (including patient history. treatment, and outcome), it is hoped that
 the objective assessment eventually will be related reliably to truth in
 diagnosis.
      The requirements for representative sampling are in the multimegapixel
 range, and require fully automatic scene segmentation and histometric
 feature extraction.  This difficult problem has been largely resolved
 through ongoing support, employing an AI-based, adaptive segmentation
 approach.
      The major challenges for the proposed research are: 1) to gain an
 understanding of the necessary and sufficient human diagnostic clues and
 corresponding histometric analogs, that is, to determine the library of
 transforms to be used by the interpretive expert-system module; 2) to
 develop learning capability of the system for conceptual data; and 3) to
 gain an understanding of the functional requirements. dependence structure,
 and decision control capabilities of such a system.
",2095545,R35CA053877,['R35CA053877'],CA,https://reporter.nih.gov/project-details/2095545,R35,1995,480297,-0.029855055518916304
"Our goal in this continuing Program Project is still to facilitate the
 transfer of basic science information from the laboratory to help deal with
 clinical problems in breast cancer, but our focus broadens from primarily
 prognostic and mechanistic considerations in two directions.  First, we
 will study the molecular correlates of the developmental stages in early
 breast cancer, to provide not only greater understanding of how the full
 malignant phenotype is acquired, but also greater clinical ability to
 distinguish and thus to treat those benign or early malignant lesions
 destined to cause serious trouble.  Second, at the other end of the
 clinical course, we will be dissecting the components of growth factor
 pathways as potential targets of new treatment strategies and testing such
 strategies directly in preclinical model systems.
 
 The projects will study:  1) the problem of how best to integrate both new
 and existing prognostic factors into a coherent assessment of risk for
 individual patients, comparing expanded forms of the traditional Cox
 multivariate analysis with newer recursive partitioning and artificial
 intelligence (neural network) approaches; 2) the occurrence of mutations
 and variants in the functional domains of the steroid receptor genes, which
 could result in failures to respond to hormone manipulation even though
 receptor is present, or in failure to detect receptor, or even in an
 oncogene-like receptor variant which would stimulate the tumor cell even in
 the absence of hormone; 3) potential molecular markers -- activated
 oncogenes, proliferation markers, antigens associated with metastatic
 behavior, etc.--in well defined early and later lesions at several stages,
 in order to see which markers might identify high risk lesions and to learn
 more about the evolutionary pathway (or pathways) by which breast cancer
 progresses; 4) antigens associated with proliferation as new prognostic
  as new prognostic
 markers; 5) TGFalpha as an autocrine mediator of estrogen action, as a
 factor in the development of hyperplastic lesions in transgenic mice
 expressing a TGFalpha gene (and/or an EGF-R gene) in the mammary glands,
 and as a target for inhibiting malignant progression; 6) the insulin-like
 growth factor (IGF) system, especially the receptors, IGFRI and IGFRII and
 the IGF binding proteins, as targets for blocking breast cancer growth and
 tumorigenesis.  These projects will all be supported by a tumor/data
 network core function, flow cytometry, factor assay, and tissue culture
 core laboratories, and an administrative core.
 
 Many of the factors and markers studied in these interactive projects will
 have important clinical significance.  Indeed, our major effort will be to
 help oncologists integrate and appropriately utilize basic science
 information in the 1990's in diagnostic and treatment strategies for the
 breast cancer patient.
 breast neoplasms; female; growth factor; hormone related neoplasm /cancer; molecular oncology MEDICAL ONCOLOGY PROGRAM PROJECT--THERAPEUTIC RESEARCH","Our goal in this continuing Program Project is still to facilitate the
 transfer of basic science information from the laboratory to help deal with
 clinical problems in breast cancer, but our focus broadens from primarily
 prognostic and mechanistic considerations in two directions.  First, we
 will study the molecular correlates of the developmental stages in early
 breast cancer, to provide not only greater understanding of how the full
 malignant phenotype is acquired, but also greater clinical ability to
 distinguish and thus to treat those benign or early malignant lesions
 destined to cause serious trouble.  Second, at the other end of the
 clinical course, we will be dissecting the components of growth factor
 pathways as potential targets of new treatment strategies and testing such
 strategies directly in preclinical model systems.
 
 The projects will study:  1) the problem of how best to integrate both new
 and existing prognostic factors into a coherent assessment of risk for
 individual patients, comparing expanded forms of the traditional Cox
 multivariate analysis with newer recursive partitioning and artificial
 intelligence (neural network) approaches; 2) the occurrence of mutations
 and variants in the functional domains of the steroid receptor genes, which
 could result in failures to respond to hormone manipulation even though
 receptor is present, or in failure to detect receptor, or even in an
 oncogene-like receptor variant which would stimulate the tumor cell even in
 the absence of hormone; 3) potential molecular markers -- activated
 oncogenes, proliferation markers, antigens associated with metastatic
 behavior, etc.--in well defined early and later lesions at several stages,
 in order to see which markers might identify high risk lesions and to learn
 more about the evolutionary pathway (or pathways) by which breast cancer
 progresses; 4) antigens associated with proliferation as new prognostic
  as new prognostic
 markers; 5) TGFalpha as an autocrine mediator of estrogen action, as a
 factor in the development of hyperplastic lesions in transgenic mice
 expressing a TGFalpha gene (and/or an EGF-R gene) in the mammary glands,
 and as a target for inhibiting malignant progression; 6) the insulin-like
 growth factor (IGF) system, especially the receptors, IGFRI and IGFRII and
 the IGF binding proteins, as targets for blocking breast cancer growth and
 tumorigenesis.  These projects will all be supported by a tumor/data
 network core function, flow cytometry, factor assay, and tissue culture
 core laboratories, and an administrative core.
 
 Many of the factors and markers studied in these interactive projects will
 have important clinical significance.  Indeed, our major effort will be to
 help oncologists integrate and appropriately utilize basic science
 information in the 1990's in diagnostic and treatment strategies for the
 breast cancer patient.
",2088029,P01CA030195,['P01CA030195'],CA,https://reporter.nih.gov/project-details/2088029,P01,1995,1748122,-0.037999647845973795
"This research program will develop accurate theoretical methods for
 analyzing secondary structural equilibria in superhelical DNA molecules
 of kilobase length and specified sequence, in which all transitions
 compete to which the sequence is susceptible.  These include B-Z
 transitions, cruciform extrusions, B-H transitions, and strand
 separation.  Methods also will be developed for handling local sequence
 effects, known to occur in practice, that complicate the energetics of
 transitions and the calculation of equilibria.  Examples include chemical
 adducts, abasic sites or other disruptions of base pairing, and imperfect
 susceptible sequences such as imprecise inverted repeat symmetry or
 purine-pyrimidine alternation.  Methods based on Monte Carlo techniques
 will be developed for the analysis of superhelical secondary structural
 transitions at high temperatures or in extremely long DNA sequences
 (approximately 105 base pairs).  Monte Carlo methods also will be
 developed to analyze the interplay between transitions and bending
 deformations in superhelical  DNA molecules.  Transition state theories
 of the kinetics of superhelical transconformation reactions will be
 developed and tested against available data.  Collaborations with several
 experimental groups will illuminate roles that superhelical DNA
 conformational transitions play in normal and pathological processes.
 These include projects examining:  1) the role of superhelical strand
 separation in the initiation of replication; 2) mechanisms by which
 superhelicity enhances DNA sensitivity to single strand breakage by x-
 rays, and; 3) superhelical cruciform formation at orthopoxviral telomere
 sequences and its role in replication.  The analytic techniques developed
 in this research will be used to deduce from experimental data the values
 of important energetic and conformational parameters governing
 superhelical transitions.  The effects of sequence modifications and
 imperfections on the energetics of superhelical transitions will be found
 in several specific cases.  These will include determining the influence
 of violations of perfect inverted repeat symmetry on cruciform extrusion,
 the effects of base methylation on strand separation, and the energetics
 of strand separation in molecules containing abasic sites or chemical
 adducts.  Transition and destabilization profiles will be calculated for
 a variety of DNAs to determine how local susceptibilities to specific
 transitions correlate with regulatory regions, mutational hotspots,
 chromosomal breakpoints and other sites of biological activity.
 DNA; DNA damage; DNA methylation; DNA replication; DNA replication origin; analytical method; artificial intelligence; chemical kinetics; chemical structure function; computer assisted sequence analysis; computer data analysis; computer simulation; conformation; mathematical model; method development; model design /development; molecular size; nucleic acid sequence; nucleic acid structure; radiation genetics; structural biology; telomere; thermodynamics; virus genetics THEORETICAL ANALYSIS OF DNA SUPERHELICAL EQUILIBRIA","This research program will develop accurate theoretical methods for
 analyzing secondary structural equilibria in superhelical DNA molecules
 of kilobase length and specified sequence, in which all transitions
 compete to which the sequence is susceptible.  These include B-Z
 transitions, cruciform extrusions, B-H transitions, and strand
 separation.  Methods also will be developed for handling local sequence
 effects, known to occur in practice, that complicate the energetics of
 transitions and the calculation of equilibria.  Examples include chemical
 adducts, abasic sites or other disruptions of base pairing, and imperfect
 susceptible sequences such as imprecise inverted repeat symmetry or
 purine-pyrimidine alternation.  Methods based on Monte Carlo techniques
 will be developed for the analysis of superhelical secondary structural
 transitions at high temperatures or in extremely long DNA sequences
 (approximately 105 base pairs).  Monte Carlo methods also will be
 developed to analyze the interplay between transitions and bending
 deformations in superhelical  DNA molecules.  Transition state theories
 of the kinetics of superhelical transconformation reactions will be
 developed and tested against available data.  Collaborations with several
 experimental groups will illuminate roles that superhelical DNA
 conformational transitions play in normal and pathological processes.
 These include projects examining:  1) the role of superhelical strand
 separation in the initiation of replication; 2) mechanisms by which
 superhelicity enhances DNA sensitivity to single strand breakage by x-
 rays, and; 3) superhelical cruciform formation at orthopoxviral telomere
 sequences and its role in replication.  The analytic techniques developed
 in this research will be used to deduce from experimental data the values
 of important energetic and conformational parameters governing
 superhelical transitions.  The effects of sequence modifications and
 imperfections on the energetics of superhelical transitions will be found
 in several specific cases.  These will include determining the influence
 of violations of perfect inverted repeat symmetry on cruciform extrusion,
 the effects of base methylation on strand separation, and the energetics
 of strand separation in molecules containing abasic sites or chemical
 adducts.  Transition and destabilization profiles will be calculated for
 a variety of DNAs to determine how local susceptibilities to specific
 transitions correlate with regulatory regions, mutational hotspots,
 chromosomal breakpoints and other sites of biological activity.
",2184483,R01GM047012,['R01GM047012'],GM,https://reporter.nih.gov/project-details/2184483,R01,1995,113947,-0.07931367097925773
"Structural domains in proteins are regions that are thought to fold
 autonomously.  Previous work by the principal investigator has linked
 domain structure with physical compactness.  Large compact units correspond
 well to domains while small compact units seem to be folding intermediates
 or possible folding nucleation sites.  Small compact units may also serve
 as useful entities in protein design.
 
 This research will study both experimental and theoretical aspects of the
 compact domain problem.  Experimentally, a series of peptides corresponding
 to compact and noncompact structures will be obtained using solid phase
 synthesis or chemical cleavage of the native protein.  The peptides'
 structure will be analyzed using circular dichroism and high resolution
 two-dimensional NMR.  If the structure of the compact peptides closely
 corresponds to that observed for the peptide within the native protein,
 while the noncompact peptides have little or no structure, then the compact
 unit theory will be proven.
 
 Theoretical work will focus on discontinuous units containing two different
 polypeptide chains.  Recent advances in computer technology and a new
 approach to performing calculations should make it possible to analyze for
 discontinuous compact domains.  The examination of discontinuous protein
 domains should provide new insights into protein structure.
 
 Once a good theoretical model for discontinuous compact domains is
 formulated, it will be experimentally tested using a similar methodology to
 that used for continuous domains.
 artificial intelligence; bacterial proteins; chemical cleavage; chemical stability; circular dichroism; computer program /software; computer simulation; conformation; mathematical model; molecular site; nonwater solvent; nuclear magnetic resonance spectroscopy; nuclease; peptide chemical synthesis; peptide structure; protein folding; protein sequence; protein structure; solutions; synthetic peptide COMPACT DOMAINS IN PROTEINS","Structural domains in proteins are regions that are thought to fold
 autonomously.  Previous work by the principal investigator has linked
 domain structure with physical compactness.  Large compact units correspond
 well to domains while small compact units seem to be folding intermediates
 or possible folding nucleation sites.  Small compact units may also serve
 as useful entities in protein design.
 
 This research will study both experimental and theoretical aspects of the
 compact domain problem.  Experimentally, a series of peptides corresponding
 to compact and noncompact structures will be obtained using solid phase
 synthesis or chemical cleavage of the native protein.  The peptides'
 structure will be analyzed using circular dichroism and high resolution
 two-dimensional NMR.  If the structure of the compact peptides closely
 corresponds to that observed for the peptide within the native protein,
 while the noncompact peptides have little or no structure, then the compact
 unit theory will be proven.
 
 Theoretical work will focus on discontinuous units containing two different
 polypeptide chains.  Recent advances in computer technology and a new
 approach to performing calculations should make it possible to analyze for
 discontinuous compact domains.  The examination of discontinuous protein
 domains should provide new insights into protein structure.
 
 Once a good theoretical model for discontinuous compact domains is
 formulated, it will be experimentally tested using a similar methodology to
 that used for continuous domains.
",2184157,R29GM046664,['R29GM046664'],GM,https://reporter.nih.gov/project-details/2184157,R29,1995,94536,-0.06632263102076681
"We propose to work in the development and application of mathematical,
 statistical, and computational methods for the analysis of nucleic acid
 and amino acid sequence data. The long range goals can be placed into
 three categories. (1) Computational analysis is essential to our
 approaches to sequence data. Algorithms are being developed for shotgun
 sequence assembly, to search for tandem repeats of length up to 32
 basepairs, to find the consensus local alignment of an unknown region
 common to an unknown subset of sequences, to study the
 thermodynamic/statistical behavior of experiments that repeatedly select
 and amplify DNA molecules, and to weight multiple and suboptimal sequence
 alignment paths. (2) Physical mapping of DNA is important in genome
 analysis. Studies include the PEP procedure to amplify single chromosomes,
 PCR is a branching process including both amplification errors and
 efficiency less than 1, the mathematical analysis of physical mapping
 using end characterized clones, and classification of multiple solutions
 of the double digest problem. (3) As sequence data increase, estimating
 statistical significance becomes more central. We will develop methods for
 estimating statistical significance of scores of tandem repeats, Poisson
 distributional results for sequence alignment in certain cases where the
 Chen-Stein method fails, the statistical distribution of correctly
 inferred sequence in shotgun sequencing projects as a function of depth
 and accuracy, and the growth of minimum free energy of secondary
 structures of a random RNA.
 RNA; RNA splicing; artificial intelligence; computer assisted sequence analysis; computer program /software; computer simulation; computer system design /evaluation; gene mutation; genetic mapping; genetic models; mathematical model; model design /development; molecular genetics; nucleic acid sequence; polymerase chain reaction; protein sequence; ribosomal RNA; statistics /biometry PATTERN RECOGNITION FOR ANALYSIS OF MOLECULAR SEQUENCES","We propose to work in the development and application of mathematical,
 statistical, and computational methods for the analysis of nucleic acid
 and amino acid sequence data. The long range goals can be placed into
 three categories. (1) Computational analysis is essential to our
 approaches to sequence data. Algorithms are being developed for shotgun
 sequence assembly, to search for tandem repeats of length up to 32
 basepairs, to find the consensus local alignment of an unknown region
 common to an unknown subset of sequences, to study the
 thermodynamic/statistical behavior of experiments that repeatedly select
 and amplify DNA molecules, and to weight multiple and suboptimal sequence
 alignment paths. (2) Physical mapping of DNA is important in genome
 analysis. Studies include the PEP procedure to amplify single chromosomes,
 PCR is a branching process including both amplification errors and
 efficiency less than 1, the mathematical analysis of physical mapping
 using end characterized clones, and classification of multiple solutions
 of the double digest problem. (3) As sequence data increase, estimating
 statistical significance becomes more central. We will develop methods for
 estimating statistical significance of scores of tandem repeats, Poisson
 distributional results for sequence alignment in certain cases where the
 Chen-Stein method fails, the statistical distribution of correctly
 inferred sequence in shotgun sequencing projects as a function of depth
 and accuracy, and the growth of minimum free energy of secondary
 structures of a random RNA.
",2178223,R01GM036230,['R01GM036230'],GM,https://reporter.nih.gov/project-details/2178223,R01,1995,394394,-0.006200551819820932
"The long term goal of this project is to improve computer based simulation
 methods that can be applied to study the structures, kinetics and
 thermodynamics of nucleic acids and their interaction with ligands.  In the
 next period of grant support, we anticipate making major improvements in
 the energy function used to describe nucleic acids, and in the methodology
 to determine conformational free energies, free energies for mutations, and
 absolute free energies of association for nucleic acids and nucleic
 acid-ligand complexes using molecular dynamics/free energy approaches.  We
 plan to use available X-ray and NMR data to evaluate our models.
 Applications to a number of the most interesting physical chemical
 phenomena in DNA and RNA structure and thermodynamics will be carried out
 including the Z phobicity of AT base pairs, the thermodynamics of RNA
 loops, the sequence selectivity and binding of DNA minor groove binding
 ligands, and the neighbor exclusion rule for ligand binding to DNA.
 
 Recent advances are allowing a most exciting synergy between computer based
 theoretical methods to study intermolecular inter-actions and experiments.
 These computer simulations are yielding more accurate insights than ever
 before and are often correctly predicting the results of subsequent
 experiments. The work proposed here has such a synergistic relationship to
 experiments and is aimed at a continued improvement of the predictive power
 of these computer based theoretical methods. The long term objective of our
 studies is to make the computer based approaches truly reliable and
 predictive and to use them in anti-cancer and anti-AIDS drug design.
 DNA; artificial intelligence; biophysics; chemical association; chemical models; chemical stability; computer simulation; conformation; drug interactions; intermolecular interaction; ionic bond; mathematical model; molecular dynamics; netropsin; nucleic acid denaturation; nucleic acid sequence; nucleic acid structure; solutions; thermodynamics THEORETICAL STUDIES OF DRUG-NUCLEIC ACID INTERACTIONS","The long term goal of this project is to improve computer based simulation
 methods that can be applied to study the structures, kinetics and
 thermodynamics of nucleic acids and their interaction with ligands.  In the
 next period of grant support, we anticipate making major improvements in
 the energy function used to describe nucleic acids, and in the methodology
 to determine conformational free energies, free energies for mutations, and
 absolute free energies of association for nucleic acids and nucleic
 acid-ligand complexes using molecular dynamics/free energy approaches.  We
 plan to use available X-ray and NMR data to evaluate our models.
 Applications to a number of the most interesting physical chemical
 phenomena in DNA and RNA structure and thermodynamics will be carried out
 including the Z phobicity of AT base pairs, the thermodynamics of RNA
 loops, the sequence selectivity and binding of DNA minor groove binding
 ligands, and the neighbor exclusion rule for ligand binding to DNA.
 
 Recent advances are allowing a most exciting synergy between computer based
 theoretical methods to study intermolecular inter-actions and experiments.
 These computer simulations are yielding more accurate insights than ever
 before and are often correctly predicting the results of subsequent
 experiments. The work proposed here has such a synergistic relationship to
 experiments and is aimed at a continued improvement of the predictive power
 of these computer based theoretical methods. The long term objective of our
 studies is to make the computer based approaches truly reliable and
 predictive and to use them in anti-cancer and anti-AIDS drug design.
",2087391,R01CA025644,['R01CA025644'],CA,https://reporter.nih.gov/project-details/2087391,R01,1995,144861,-0.1105568232339216
"The goal of this project is to develop rehabilitation guidelines for
 restoration of ambulation in patients following a stroke. Specifically
 the aims are to identify patterns of gait deviations in both the sound
 and paretic sides among patients with hemiplegia, form classifications
 of motor control strategies used during walking and relate these findings
 to the patient's potential for recovery of ambulation and response to
 intensive rehabilitation. Three gait training strategies will be
 compared: i) supported treadmill gait training, 2) intensified-use of the
 paretic leg and 3) functional independence and endurance training using
 motor learning principles. Variables that predict improvement in walking
 ability after rehabilitation will be identified. Three clinical tests
 (Functional Independence Measure, Fugl-meyer and Upright Motor Control)
 will be used to identify predictor-criteria. Testing will be done within
 one week of admission to rehabilitation and at the 6 month-post-stroke
 anniversary. Gait analysis will be performed to define outcome measures
 (level and community velocities) and associated gait impairment variables
 (muscle and motion patterns). The comprehensive baseline gait evaluation
 will be conducted within the first week the patient is able to walk six
 meters with assistance. Follow-up testing will be conducted at discharge
 from inpatient rehabilitation and at 6 months and i year post-stroke.
 Function of lower gluteus maximus, gluteus medius, long head of biceps
 femoris, semimembranosus, rectus femoris, adductor longus, vastus
 intermedius, soleus, anterior tibialis and peroneus brevis muscles will
 be recorded with dynamic EMG using intramuscular fine wire electrodes.
 Motion of the trunk, pelvis, hip, thigh, knee and ankle will be recorded
 for both the sound and paretic limbs with the Vicon Motion Analysis
 system. Stride characteristics and foot-floor contact patterns will be
 recorded with Stride Analyzer footswitch system. Subjects will ambulate
 on level surfaces and over a curb both in bare feet and in shoes with
 their customary orthoses. Patterned muscle strength will be documented
 with the Upright Motor Control test and selective strength of both lower
 extremities will be recorded with the LIDO isokinetic dynamometer (knee
 and ankle) and a strain gauge tensiometer (hip). Walking endurance will
 be measured in a fifteen minute test on an outdoor track. Clinical
 factors and gait impairment variables that best predict level and
 community walking velocities will be identified. Patterns of gait errors
 and substitutions will be formulated. Patients will be classified by
 their gait motor control strategy with a data analysis/expert system.
 Effectiveness of the three treatment programs will be discerned by
 comparing functional outcome measures (gait velocities) and gait
 impairment variables (EMG and motion). Patients in the three treatment
 groups will be subdivided based on initial motor control strategy to
 determine if initial severity affects the response to the rehabilitation
 strategies.
 electromyography; gait; hemiplegia; human subject; human therapy evaluation; longitudinal human study; muscle function; muscle strength; rehabilitation; stroke RECOVERY AND REHABILITATION OF GAIT IN STROKE PATIENTS","The goal of this project is to develop rehabilitation guidelines for
 restoration of ambulation in patients following a stroke. Specifically
 the aims are to identify patterns of gait deviations in both the sound
 and paretic sides among patients with hemiplegia, form classifications
 of motor control strategies used during walking and relate these findings
 to the patient's potential for recovery of ambulation and response to
 intensive rehabilitation. Three gait training strategies will be
 compared: i) supported treadmill gait training, 2) intensified-use of the
 paretic leg and 3) functional independence and endurance training using
 motor learning principles. Variables that predict improvement in walking
 ability after rehabilitation will be identified. Three clinical tests
 (Functional Independence Measure, Fugl-meyer and Upright Motor Control)
 will be used to identify predictor-criteria. Testing will be done within
 one week of admission to rehabilitation and at the 6 month-post-stroke
 anniversary. Gait analysis will be performed to define outcome measures
 (level and community velocities) and associated gait impairment variables
 (muscle and motion patterns). The comprehensive baseline gait evaluation
 will be conducted within the first week the patient is able to walk six
 meters with assistance. Follow-up testing will be conducted at discharge
 from inpatient rehabilitation and at 6 months and i year post-stroke.
 Function of lower gluteus maximus, gluteus medius, long head of biceps
 femoris, semimembranosus, rectus femoris, adductor longus, vastus
 intermedius, soleus, anterior tibialis and peroneus brevis muscles will
 be recorded with dynamic EMG using intramuscular fine wire electrodes.
 Motion of the trunk, pelvis, hip, thigh, knee and ankle will be recorded
 for both the sound and paretic limbs with the Vicon Motion Analysis
 system. Stride characteristics and foot-floor contact patterns will be
 recorded with Stride Analyzer footswitch system. Subjects will ambulate
 on level surfaces and over a curb both in bare feet and in shoes with
 their customary orthoses. Patterned muscle strength will be documented
 with the Upright Motor Control test and selective strength of both lower
 extremities will be recorded with the LIDO isokinetic dynamometer (knee
 and ankle) and a strain gauge tensiometer (hip). Walking endurance will
 be measured in a fifteen minute test on an outdoor track. Clinical
 factors and gait impairment variables that best predict level and
 community walking velocities will be identified. Patterns of gait errors
 and substitutions will be formulated. Patients will be classified by
 their gait motor control strategy with a data analysis/expert system.
 Effectiveness of the three treatment programs will be discerned by
 comparing functional outcome measures (gait velocities) and gait
 impairment variables (EMG and motion). Patients in the three treatment
 groups will be subdivided based on initial motor control strategy to
 determine if initial severity affects the response to the rehabilitation
 strategies.
",2204774,R01HD031931,['R01HD031931'],HD,https://reporter.nih.gov/project-details/2204774,R01,1995,190437,-0.051227764049953925
"This proposal is directed toward improving tomographic imaging in
 diagnostic radiology and nuclear medicine. It is predicated on the claim
 that significant advances will be achieved in the fidelity of the images
 that are reconstructed from the raw detector measurements of the
 tomographic scanner by changing the basic elements (called ""basis
 functions"") with which the image is built in the computer. The
 conventional basic elements for computerized tomographic imaging are the
 voxel basis functions, and the sinusoidal basis functions of Fourier
 analysis. Two classes of promising new basis functions have been
 developed: functions that are localized in space (as are the voxel basis
 functions), and functions that are not localized (similar in many respects
 to sinusoids). The new classes of basis functions are well-suited to
 constructing faithful digital image representations of the biological
 structures that have influenced the raw tomographic scanner data. The new
 localized basis functions have a number of very desirable properties not
 shared by voxels: they are rotationally symmetric, their Fourier
 transforms are effectively localized, and they have continuous derivatives
 of any desired order.  The new non-localized basis functions are designed
 to perform a spatially-variant filtering operation that is required by a
 non-iterative method of 3D image reconstruction developed by the Principal
 Investigator.
 
 The specific aims are to develop mathematical theory, efficient computer
 algorithms, application-specific implementations and evaluation criteria
 for (1) methods of iterative reconstruction from projections, (2) methods
 of estimating the fundamental limits on the performance of the
 reconstruction process, and (3) methods of non-iterative 3D reconstruction
 from projections. For specified imaging tasks, the level of statistical
 significance will be found for rejection of the null hypothesis that two
 methods perform a task equally well, in favor of the alternative
 hypothesis that one method performs the task better.
 
 The basis functions of the image representation are the essential core of
 all methods for computerized image reconstruction, irrespective of the
 medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of
 new computer algorithms and their associated image representations will
 enable the full potential of scanners for functional imaging in emission
 tomography (PET and SPECT) to be realized by extracting as much
 information as possible from fully-3D low-statistics projection data.
 artificial intelligence; computer simulation; computer system design /evaluation; digital imaging; model design /development; phantom model; positron emission tomography DIGITAL IMAGE REPRESENTATIONS FOR TOMOGRAPHIC RADIOLOGY","This proposal is directed toward improving tomographic imaging in
 diagnostic radiology and nuclear medicine. It is predicated on the claim
 that significant advances will be achieved in the fidelity of the images
 that are reconstructed from the raw detector measurements of the
 tomographic scanner by changing the basic elements (called ""basis
 functions"") with which the image is built in the computer. The
 conventional basic elements for computerized tomographic imaging are the
 voxel basis functions, and the sinusoidal basis functions of Fourier
 analysis. Two classes of promising new basis functions have been
 developed: functions that are localized in space (as are the voxel basis
 functions), and functions that are not localized (similar in many respects
 to sinusoids). The new classes of basis functions are well-suited to
 constructing faithful digital image representations of the biological
 structures that have influenced the raw tomographic scanner data. The new
 localized basis functions have a number of very desirable properties not
 shared by voxels: they are rotationally symmetric, their Fourier
 transforms are effectively localized, and they have continuous derivatives
 of any desired order.  The new non-localized basis functions are designed
 to perform a spatially-variant filtering operation that is required by a
 non-iterative method of 3D image reconstruction developed by the Principal
 Investigator.
 
 The specific aims are to develop mathematical theory, efficient computer
 algorithms, application-specific implementations and evaluation criteria
 for (1) methods of iterative reconstruction from projections, (2) methods
 of estimating the fundamental limits on the performance of the
 reconstruction process, and (3) methods of non-iterative 3D reconstruction
 from projections. For specified imaging tasks, the level of statistical
 significance will be found for rejection of the null hypothesis that two
 methods perform a task equally well, in favor of the alternative
 hypothesis that one method performs the task better.
 
 The basis functions of the image representation are the essential core of
 all methods for computerized image reconstruction, irrespective of the
 medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of
 new computer algorithms and their associated image representations will
 enable the full potential of scanners for functional imaging in emission
 tomography (PET and SPECT) to be realized by extracting as much
 information as possible from fully-3D low-statistics projection data.
",2095856,R01CA054356,['R01CA054356'],CA,https://reporter.nih.gov/project-details/2095856,R01,1995,221032,-0.04355516736949109
"The research plan outlined in this proposal is part of a long-range study
 to understand the role of the cerebellum in the control and regulation of
 posture and movement. The specific aims of this proposal focus on the
 organization and representation of sensory information in spinal
 projection pathways and in the cerebellum.
 
 The dorsal spinocerebellar tract (DSCT) transmits to the cerebellum
 information derived from sensory receptors in muscle, joints and skin of
 the hind limbs. The content of the signals transmitted and their
 utilization by the cerebellum are not known, yet these are central to an
 understanding of cerebellar function as it relates to spinal motor
 functions.
 
 The proposed research will investigate neuronal population behavior in the
 DSCT and cerebellum by recording from large numbers of randomly sampled
 neurons, and by using statistical methods based on principal component
 analysis and correlation analysis. The main issues to be addressed are the
 nature of the information encoded and the way in which it is encoded.
 These issues will be examined in the context of a new working hypothesis
 about the functional role of the spinocerebellar system: It is proposed
 that the information transmitted by spinocerebellar pathways provides a
 sensory representation of limb stiffness which may be used to regulate
 stiffness as a background for posture and movement. Experiments will test
 specific issues related to this hypothesis. The research is expected to
 provide new evidence about the role of the nervous system in motor
 control.
 
 Motor control disorders constitute a major health problem with
 considerable economic consequences. Basic research, such as that proposed
 here, is needed to understand better the role played by neuronal
 structures like the spinal cord and the cerebellum in the normal control
 of posture and movement.
 brain electrical activity; brain mapping; cats; central neural pathway /tract; cerebellum; dorsal column; electrophysiology; electrostimulus; limb movement; neural information processing; neuromuscular function; neurophysiology; sensorimotor system; sensory receptors; sensory signal detection; single cell analysis; spinal cord; spinal cord mapping NEUROPHYSIOLOGICAL STUDY OF DORSAL SPINOCEREBELLAR TRACT","The research plan outlined in this proposal is part of a long-range study
 to understand the role of the cerebellum in the control and regulation of
 posture and movement. The specific aims of this proposal focus on the
 organization and representation of sensory information in spinal
 projection pathways and in the cerebellum.
 
 The dorsal spinocerebellar tract (DSCT) transmits to the cerebellum
 information derived from sensory receptors in muscle, joints and skin of
 the hind limbs. The content of the signals transmitted and their
 utilization by the cerebellum are not known, yet these are central to an
 understanding of cerebellar function as it relates to spinal motor
 functions.
 
 The proposed research will investigate neuronal population behavior in the
 DSCT and cerebellum by recording from large numbers of randomly sampled
 neurons, and by using statistical methods based on principal component
 analysis and correlation analysis. The main issues to be addressed are the
 nature of the information encoded and the way in which it is encoded.
 These issues will be examined in the context of a new working hypothesis
 about the functional role of the spinocerebellar system: It is proposed
 that the information transmitted by spinocerebellar pathways provides a
 sensory representation of limb stiffness which may be used to regulate
 stiffness as a background for posture and movement. Experiments will test
 specific issues related to this hypothesis. The research is expected to
 provide new evidence about the role of the nervous system in motor
 control.
 
 Motor control disorders constitute a major health problem with
 considerable economic consequences. Basic research, such as that proposed
 here, is needed to understand better the role played by neuronal
 structures like the spinal cord and the cerebellum in the normal control
 of posture and movement.
",2264079,R01NS021143,['R01NS021143'],NS,https://reporter.nih.gov/project-details/2264079,R01,1995,161960,-0.12998730458869884
"This application seeks funding for Phase II of a three-phase program of
 research to identify, classify, and test nursing sensitive patient
 outcomes and their indicators for use in standardized language
 development, practice, research, and education.  The purposes of the
 research are to:  1) identify, label, validate, and classify nursing
 sensitive patient outcomes and indicators, 2) evaluate the validity and
 usefulness of the classification in clinical field testing, and 3) define
 and test measurement procedures for the outcomes and indicators.  The
 classification is expected to contain patient outcomes, indicators, and
 measurement activities at three to four levels of abstraction and to
 identify those patient outcomes most influenced by nursing.  The research
 uses both inductive and deductive approaches.  An inductive approach will
 be used to extract outcomes, indicators, and measures from current
 nursing literature and instruments.  A combined inductive and deductive
 approach will be used to label outcomes, specify indicators for the
 outcomes, and group the outcomes in broad categories based on the Medical
 Outcomes framework and categories identified by nurses.  Delphi
 techniques and surveys of random samples of masters prepared nurses will
 be used to validate the outcomes and indicators prior to field testing
 the outcomes and indicators in four sites, a tertiary care hospital, a
 community hospital, a nursing home, and a community agency.  Hierarchical
 clustering techniques and nonmetric scaling analysis will be used to
 develop the classification of nursing sensitive patient outcomes and a
 survey of nurse experts will be used for initial validation of the
 classification.
 data collection; method development; nursing care evaluation; nursing research; prognosis CLASSIFICATION OF NURSING-SENSITIVE PATIENT OUTCOMES","This application seeks funding for Phase II of a three-phase program of
 research to identify, classify, and test nursing sensitive patient
 outcomes and their indicators for use in standardized language
 development, practice, research, and education.  The purposes of the
 research are to:  1) identify, label, validate, and classify nursing
 sensitive patient outcomes and indicators, 2) evaluate the validity and
 usefulness of the classification in clinical field testing, and 3) define
 and test measurement procedures for the outcomes and indicators.  The
 classification is expected to contain patient outcomes, indicators, and
 measurement activities at three to four levels of abstraction and to
 identify those patient outcomes most influenced by nursing.  The research
 uses both inductive and deductive approaches.  An inductive approach will
 be used to extract outcomes, indicators, and measures from current
 nursing literature and instruments.  A combined inductive and deductive
 approach will be used to label outcomes, specify indicators for the
 outcomes, and group the outcomes in broad categories based on the Medical
 Outcomes framework and categories identified by nurses.  Delphi
 techniques and surveys of random samples of masters prepared nurses will
 be used to validate the outcomes and indicators prior to field testing
 the outcomes and indicators in four sites, a tertiary care hospital, a
 community hospital, a nursing home, and a community agency.  Hierarchical
 clustering techniques and nonmetric scaling analysis will be used to
 develop the classification of nursing sensitive patient outcomes and a
 survey of nurse experts will be used for initial validation of the
 classification.
",2257355,R01NR003437,['R01NR003437'],NR,https://reporter.nih.gov/project-details/2257355,R01,1995,290972,-0.0912069863224728
"The electronic medical record (EMR) holds great allure to both the
 medical informatics and health services research communities. In this
 project, we propose to enhance the capability of electronic medical
 record (EMR) systems by creating and evaluating tools to extract
 clinical vocabularies as well as patient data from narrative text
 reports. We will apply advanced natural language processing tools from
 the CLARIT system to both of the above problems. We contend that fast
 and robust automated text processing methods are the only way that the
 problems of vocabulary construction and narrative text extraction can
 be solved.
 
 We will address the clinical vocabulary problem by utilizing the
 thesaurus extraction techniques already present in the CLARIT system.
 Using several gigabytes of narrative text, including discharge
 summaries, progress notes, radiology reports, and other clinical text,
 we plan to:
 l. Identify empirically the terminology used in medicine.
 2. Compare the coverage of that terminology in several existing large
 medical vocabularies: UMLS, SNOMED, and the Medical Entities
 Dictionary.
 3. Discern the semantic characteristics of that terminology to allow
 other structured vocabularies a richer substrate of terms as well as
 providing us the opportunity to implement a clinical vocabulary schema
 based on the methods of the MedSORT-II Project.
 4. Evaluate how well our tools assist the vocabulary building efforts
 of ourselves and others.
 
 The narrative extraction problem will be approached differently than
 in the past, building on the efforts of previous investigators who
 have tackled this problem before but changing the perspective by
 focusing on the development of tools specific to researchers and
 others with a need to extract data from narrative text. This approach
 will be applied in two domains:
 l.Consortium-based research in the use of esophogastroduodenoscopy
 (EGD).
 2.Practice guidelines implementation in blood product transfusion.
 abstracting; automated medical record system; blood transfusion; cooperative study; endoscopy; vocabulary development for information system VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR","The electronic medical record (EMR) holds great allure to both the
 medical informatics and health services research communities. In this
 project, we propose to enhance the capability of electronic medical
 record (EMR) systems by creating and evaluating tools to extract
 clinical vocabularies as well as patient data from narrative text
 reports. We will apply advanced natural language processing tools from
 the CLARIT system to both of the above problems. We contend that fast
 and robust automated text processing methods are the only way that the
 problems of vocabulary construction and narrative text extraction can
 be solved.
 
 We will address the clinical vocabulary problem by utilizing the
 thesaurus extraction techniques already present in the CLARIT system.
 Using several gigabytes of narrative text, including discharge
 summaries, progress notes, radiology reports, and other clinical text,
 we plan to:
 l. Identify empirically the terminology used in medicine.
 2. Compare the coverage of that terminology in several existing large
 medical vocabularies: UMLS, SNOMED, and the Medical Entities
 Dictionary.
 3. Discern the semantic characteristics of that terminology to allow
 other structured vocabularies a richer substrate of terms as well as
 providing us the opportunity to implement a clinical vocabulary schema
 based on the methods of the MedSORT-II Project.
 4. Evaluate how well our tools assist the vocabulary building efforts
 of ourselves and others.
 
 The narrative extraction problem will be approached differently than
 in the past, building on the efforts of previous investigators who
 have tackled this problem before but changing the perspective by
 focusing on the development of tools specific to researchers and
 others with a need to extract data from narrative text. This approach
 will be applied in two domains:
 l.Consortium-based research in the use of esophogastroduodenoscopy
 (EGD).
 2.Practice guidelines implementation in blood product transfusion.
",2238284,U01LM005879,['U01LM005879'],LM,https://reporter.nih.gov/project-details/2238284,U01,1995,375570,-0.06634009470812134
"This renewal application for Project Years 4-6 proposes continuation of
 the development of new computer algorithms for reasoning about time-
 varying data from underlying causal models. The signals consist of two or
 more channels of events, each event having several properties including
 time of occurrence and event shape class. The models can be represented as
 graphs in which the nodes are digital components, some of which give rise
 to observable events. The arcs are physical connections associated with
 time delays. The application domain is the electrocardiogram, in which the
 P waves and QRS complexes represent two event channels, and optional
 intracardiac recordings contribute additional channels. The P waves and
 QRS complexes are the results of the all-or-nothing depolarization of the
 atria and ventricles, respectively. These cardiac structures are connected
 anatomically and functionally by a series of other structures, each
 associated with a characteristic conduction time.
 
 This application builds on the results of Project Years 1-2, in which an
 analytic approach based on a variation of the hypothesize-and-test
 paradigm was used with a hierarchy of models to track the events in the
 cardiac rhythm on an event-by-event basis. The output of the system is one
 or more complete causal explanations of the observed signal, expressed in
 the standard clinical ladder diagram format and consisting of an
 instantiated model and event-by-event annotation of causality based on the
 model. When a signal admits of more than one explanatory model, each is
 developed and displayed separately. The initial hypothesis, that a cardiac
 arrhythmia monitor constructed using knowledge-based programming methods
 can perform substantially better than current clinical arrhythmia
 monitors, has largely been confirmed.
 
 The hypothesis to be tested in this proposal is that the programming
 techniques being used not only can perform better qualitatively than do
 current clinical arrhythmia monitors, but that they can do so at a level
 of performance that is likely to become suitable for the clinical use.
 This hypothesis will be tested in a trial comparing the program with
 physicians at several levels of experience.
 
 This project is attractive for two reasons. First, it offers new
 knowledge-based algorithms for model-based reasoning about time-varying
 signals. This is an unsolved problem in the expert systems field. Second,
 current cardiac arrhythmia monitors do not perform nearly as well as do
 expert nurses, technicians, and physicians. The proposed project will
 contribute to the development of an improved generation of arrhythmia
 interpretation systems that should result in improved care of patients
 with disorders of the heart rhythm, particularly in medically underserved
 settings.
 arrhythmia; artificial intelligence; computer assisted diagnosis; computer assisted patient care; computer network; computer program /software; diagnosis design /evaluation; diagnosis quality /standard; electrocardiographic monitor; human subject KNOWLEDGE-BASED INTERPRETATION OF CARDIAC ARRHYTHMIAS","This renewal application for Project Years 4-6 proposes continuation of
 the development of new computer algorithms for reasoning about time-
 varying data from underlying causal models. The signals consist of two or
 more channels of events, each event having several properties including
 time of occurrence and event shape class. The models can be represented as
 graphs in which the nodes are digital components, some of which give rise
 to observable events. The arcs are physical connections associated with
 time delays. The application domain is the electrocardiogram, in which the
 P waves and QRS complexes represent two event channels, and optional
 intracardiac recordings contribute additional channels. The P waves and
 QRS complexes are the results of the all-or-nothing depolarization of the
 atria and ventricles, respectively. These cardiac structures are connected
 anatomically and functionally by a series of other structures, each
 associated with a characteristic conduction time.
 
 This application builds on the results of Project Years 1-2, in which an
 analytic approach based on a variation of the hypothesize-and-test
 paradigm was used with a hierarchy of models to track the events in the
 cardiac rhythm on an event-by-event basis. The output of the system is one
 or more complete causal explanations of the observed signal, expressed in
 the standard clinical ladder diagram format and consisting of an
 instantiated model and event-by-event annotation of causality based on the
 model. When a signal admits of more than one explanatory model, each is
 developed and displayed separately. The initial hypothesis, that a cardiac
 arrhythmia monitor constructed using knowledge-based programming methods
 can perform substantially better than current clinical arrhythmia
 monitors, has largely been confirmed.
 
 The hypothesis to be tested in this proposal is that the programming
 techniques being used not only can perform better qualitatively than do
 current clinical arrhythmia monitors, but that they can do so at a level
 of performance that is likely to become suitable for the clinical use.
 This hypothesis will be tested in a trial comparing the program with
 physicians at several levels of experience.
 
 This project is attractive for two reasons. First, it offers new
 knowledge-based algorithms for model-based reasoning about time-varying
 signals. This is an unsolved problem in the expert systems field. Second,
 current cardiac arrhythmia monitors do not perform nearly as well as do
 expert nurses, technicians, and physicians. The proposed project will
 contribute to the development of an improved generation of arrhythmia
 interpretation systems that should result in improved care of patients
 with disorders of the heart rhythm, particularly in medically underserved
 settings.
",2237886,R01LM005530,['R01LM005530'],LM,https://reporter.nih.gov/project-details/2237886,R01,1995,166321,-0.18192386257050658
"The long-term objective is to develop computer technology needed to
 accomplish the objectives of the Human Genome Project and to apply the
 technology to the analysis and management of sequencing data.  Currently,
 a database search for sequence similarities represents the most direct
 computational approach to the analysis of genomic information.  However,
 the search is becoming ever more forbidding due to the accelerating
 growth of sequencing data.  The goal of the proposed research is to
 further develop and enhance a software tool for speedy classification of
 unknown sequences, and make it available to the genome community.  The
 research will build upon a pilot system designed and developed by the
 principal investigator that has shown great promise.  The specific aims
 are (1) to enhance the tool for speedy identification of PIR
 superfamilies and ProSite patterns, (2) to develop a pilot DNA/RNA
 classification system, (3) to distribute the tool, and (4) to aid PIR
 protein database and RDP ribosomal RNA database organization.  In
 contrast to other search methods whose search time grows linearly with
 the number of entries in the database, the time of the proposed tool
 grows with the number of families, which is likely to remain low.  The
 tool would automate family assignment which is especially important for
 managing the influx of new data in a timely manner.
 
 The proposed research applies neural network technology to solving the
 database search/organization problem.  The major design principles
 involve an encoding schema to extract sequence information and a modular
 architecture to scale up backpropagation networks.  The encoding
 algorithm is a hashing function similar to the k-tuple method.  A pilot
 system has been implemented on a Cray supercomputer to classify electron
 transfer proteins and enzymes.  The system achieves about 90% accuracy
 and 50 times speed of other search methods.  The speed may be 1000 times
 faster than others in a decade if the database continues to grow at the
 current rate.  In the proposed research, the sensitivity of the tool
 would be improved and a full-scale system would be developed.  The
 automated software tool would be portable at the source code, user
 interface, and hardware levels.  The system would be updated in
 accordance with database releases, and distributed to the research
 community via anonymous ftp.  The tool would be used to classify PIR
 sequences according to superfamilies and to classify ribosomal RNA
 sequences according to phylogenetic relations.
 artificial intelligence; computer assisted sequence analysis; computer program /software; computer system design /evaluation; electron transport; genome; information systems; nucleic acid sequence; protein sequence CLASSIFICATION NEURAL NETWORKS FOR GENOME RESEARCH","The long-term objective is to develop computer technology needed to
 accomplish the objectives of the Human Genome Project and to apply the
 technology to the analysis and management of sequencing data.  Currently,
 a database search for sequence similarities represents the most direct
 computational approach to the analysis of genomic information.  However,
 the search is becoming ever more forbidding due to the accelerating
 growth of sequencing data.  The goal of the proposed research is to
 further develop and enhance a software tool for speedy classification of
 unknown sequences, and make it available to the genome community.  The
 research will build upon a pilot system designed and developed by the
 principal investigator that has shown great promise.  The specific aims
 are (1) to enhance the tool for speedy identification of PIR
 superfamilies and ProSite patterns, (2) to develop a pilot DNA/RNA
 classification system, (3) to distribute the tool, and (4) to aid PIR
 protein database and RDP ribosomal RNA database organization.  In
 contrast to other search methods whose search time grows linearly with
 the number of entries in the database, the time of the proposed tool
 grows with the number of families, which is likely to remain low.  The
 tool would automate family assignment which is especially important for
 managing the influx of new data in a timely manner.
 
 The proposed research applies neural network technology to solving the
 database search/organization problem.  The major design principles
 involve an encoding schema to extract sequence information and a modular
 architecture to scale up backpropagation networks.  The encoding
 algorithm is a hashing function similar to the k-tuple method.  A pilot
 system has been implemented on a Cray supercomputer to classify electron
 transfer proteins and enzymes.  The system achieves about 90% accuracy
 and 50 times speed of other search methods.  The speed may be 1000 times
 faster than others in a decade if the database continues to grow at the
 current rate.  In the proposed research, the sensitivity of the tool
 would be improved and a full-scale system would be developed.  The
 automated software tool would be portable at the source code, user
 interface, and hardware levels.  The system would be updated in
 accordance with database releases, and distributed to the research
 community via anonymous ftp.  The tool would be used to classify PIR
 sequences according to superfamilies and to classify ribosomal RNA
 sequences according to phylogenetic relations.
",2237878,R29LM005524,['R29LM005524'],LM,https://reporter.nih.gov/project-details/2237878,R29,1995,91266,-0.09420353544267186
"We propose creating a shared computing resource to support research
 activities in biomedical informatics at the Stanford University School of
 Medicine.  The School has established the Center for Advanced Medical
 Informatics at Stanford (CAMIS), which unites academic, administrative,
 and research computing.  The requested funds will support an active core
 research program and the infrastructure and day-to-day computing needs of
 a large number of researchers, each of whom has external support from NIH
 or other agencies through grants or contracts.  No single research group
 could support the complex set of infrastructural and system capabilities
 needed to conduct advanced medical informatics research in the 1990's.
 Our approach to designing, managing, and building the CAMIS resource will
 be guided by a number of overall goals: (1) to foster scientific
 communication, collaboration, and sharing within a distributed research
 environment; (2) to provide a superb setting for exploring biomedical
 informatics topics; (3) to pursue a strong basic replications systems; (4)
 to help define new ways to disseminate computing and
 information-management technologies into real-world settings; and (5) to
 create a state-of-the-art computing and communication environment for our
 research work.
 
 The specific aims for this undertaking fall under four categories that
 define the functions of the CAMIS resources:
 
 Core Research and Development:  We are proposing core research projects in
 several areas: (1) Clinical Trials Workstation (CTW) Project-generalize
 and abstract system-design concepts and implementation solutions from our
 ONCOCIN and T-HELPER projects so that they can be applied to facilitate
 the most rapid development of integrated clinical-trials workstations in
 any domain of clinical-trials management; (2) Understanding the Code for
 Biological Molecules--development and generalize improved tools for
 discovering and for understanding the structure relationships so these can
 be used to understand the functions of genes identified in genome mapping
 investigation; (3) Advanced Computing Systems and Environments--build upon
 the extensive distributed-computing environment already in place and
 improve, extend, and adapt new computing tools as necessary to support the
 CAMIS community.  Specific systems research areas include gesture-based
 portable computing information resource navigation and retrieval tools,
 email management tools, high-speed networks, large-volume file storage,
 and new workstations technologics.
 
 Collaborative Research:  A large community of researchers will be
 supported by the CAMIS resource, doing work in areas such as clinical
 trial protocol management, methods for uncertain reasoning,
 bioinformatics, health care outcomes and economics, basic artificial
 intelligence research, human-computer interfaces, multimedia authoring and
 delivery tools for  medical education programs, and distributed library
 resources. We will accordingly encourage collaborations through improved
 mechanisms for inter- and intra-group communications.
 
 Service and Resource Operations: CAMIS will provide effective and widely
 accessible communication and computing facilities, consulting services,
 and managerial support and will serve in an advisory capacity to Medical
 School administration.
 
 Training, Education, and Dissemination: CAMIS will emphasize a strong user
 orientation in its facilities management, will continue to run two formal
 graduate degree programs (one in medical informatics and one in health
 services research), and will actively disseminate research results through
 publications, software exports, video presentations, visiting scholar
 programs, and an annual CAMIS Symposium.
 biomedical facility; computer center; information systems CENTER FOR ADVANCED MEDICAL INFORMATICS","We propose creating a shared computing resource to support research
 activities in biomedical informatics at the Stanford University School of
 Medicine.  The School has established the Center for Advanced Medical
 Informatics at Stanford (CAMIS), which unites academic, administrative,
 and research computing.  The requested funds will support an active core
 research program and the infrastructure and day-to-day computing needs of
 a large number of researchers, each of whom has external support from NIH
 or other agencies through grants or contracts.  No single research group
 could support the complex set of infrastructural and system capabilities
 needed to conduct advanced medical informatics research in the 1990's.
 Our approach to designing, managing, and building the CAMIS resource will
 be guided by a number of overall goals: (1) to foster scientific
 communication, collaboration, and sharing within a distributed research
 environment; (2) to provide a superb setting for exploring biomedical
 informatics topics; (3) to pursue a strong basic replications systems; (4)
 to help define new ways to disseminate computing and
 information-management technologies into real-world settings; and (5) to
 create a state-of-the-art computing and communication environment for our
 research work.
 
 The specific aims for this undertaking fall under four categories that
 define the functions of the CAMIS resources:
 
 Core Research and Development:  We are proposing core research projects in
 several areas: (1) Clinical Trials Workstation (CTW) Project-generalize
 and abstract system-design concepts and implementation solutions from our
 ONCOCIN and T-HELPER projects so that they can be applied to facilitate
 the most rapid development of integrated clinical-trials workstations in
 any domain of clinical-trials management; (2) Understanding the Code for
 Biological Molecules--development and generalize improved tools for
 discovering and for understanding the structure relationships so these can
 be used to understand the functions of genes identified in genome mapping
 investigation; (3) Advanced Computing Systems and Environments--build upon
 the extensive distributed-computing environment already in place and
 improve, extend, and adapt new computing tools as necessary to support the
 CAMIS community.  Specific systems research areas include gesture-based
 portable computing information resource navigation and retrieval tools,
 email management tools, high-speed networks, large-volume file storage,
 and new workstations technologics.
 
 Collaborative Research:  A large community of researchers will be
 supported by the CAMIS resource, doing work in areas such as clinical
 trial protocol management, methods for uncertain reasoning,
 bioinformatics, health care outcomes and economics, basic artificial
 intelligence research, human-computer interfaces, multimedia authoring and
 delivery tools for  medical education programs, and distributed library
 resources. We will accordingly encourage collaborations through improved
 mechanisms for inter- and intra-group communications.
 
 Service and Resource Operations: CAMIS will provide effective and widely
 accessible communication and computing facilities, consulting services,
 and managerial support and will serve in an advisory capacity to Medical
 School administration.
 
 Training, Education, and Dissemination: CAMIS will emphasize a strong user
 orientation in its facilities management, will continue to run two formal
 graduate degree programs (one in medical informatics and one in health
 services research), and will actively disseminate research results through
 publications, software exports, video presentations, visiting scholar
 programs, and an annual CAMIS Symposium.
",2237752,P41LM005305,['P41LM005305'],LM,https://reporter.nih.gov/project-details/2237752,P41,1995,1335028,-0.01711705758203758
"The goal of this project is to refine and evaluate techniques that
 automatically construct, from clinical databases, Bayesian belief networks
 that can be used as diagnostic and prognostic aids.  The amount of
 clinical information stored in databases has increased markedly in the
 last two decades, and it seems likely that this trend will continue.
 Belief networks are able to represent the probabilistic dependencies among
 clinical variables in a relatively general manner.  Researchers have
 developed algorithms for performing probabilistic inference using belief
 networks, and they have applied these algorithms to perform medical
 diagnosis and prognosis.  Although advances have been made in developing
 the theory and application of belief networks, the manual construction of
 these networks often remains a difficult, time-consuming task.  The
 automated generation of belief networks from high-quality databases may
 facilitate significantly the construction of diagnostic and prognostic
 systems, which can serve as clinical decision aids, after their accuracy
 and usefulness are validated.
 
 The long-range goal of this research is to advance our understanding and
 development of probabilistic systems that can serve as useful diagnostic
 and prognostic tools for physicians.  Such systems can serve as one method
 for disseminating the clinical knowledge captured in high-quality
 databases, such as those developed from PORT studies.  Within this
 context, the specific aims of the current, proposed research project are
 to:
 
 * refine and extend current methods for automatically constructing belief
 networks from large databases;
 
 * test the diagnostic and prognostic accuracy of systems that are based on
 belief networks constructed automatically from high quality databases,
 compared to several standard statistical techniques;
 
 * test whether a combination of automated and expert-based methods for
 constructing belief networks will yield diagnostic and prognostic systems
 that are more accurate than systems that are based on belief networks that
 are constructed automatically.
 
 These three aims will be pursued using large, high-quality clinical-
 research databases at the University of Pittsburgh that contain
 information on patients with syncope and patients in a PORT study with
 community-acquired-pneumonia.
 artificial intelligence; computer assisted diagnosis; computer assisted medical decision making; diagnosis quality /standard; human data; information systems; pneumonia; prognosis; statistics /biometry; syncope STRUCTURING MEDICAL KNOWLEDGE--PROBABILISTIC INFERENCE","The goal of this project is to refine and evaluate techniques that
 automatically construct, from clinical databases, Bayesian belief networks
 that can be used as diagnostic and prognostic aids.  The amount of
 clinical information stored in databases has increased markedly in the
 last two decades, and it seems likely that this trend will continue.
 Belief networks are able to represent the probabilistic dependencies among
 clinical variables in a relatively general manner.  Researchers have
 developed algorithms for performing probabilistic inference using belief
 networks, and they have applied these algorithms to perform medical
 diagnosis and prognosis.  Although advances have been made in developing
 the theory and application of belief networks, the manual construction of
 these networks often remains a difficult, time-consuming task.  The
 automated generation of belief networks from high-quality databases may
 facilitate significantly the construction of diagnostic and prognostic
 systems, which can serve as clinical decision aids, after their accuracy
 and usefulness are validated.
 
 The long-range goal of this research is to advance our understanding and
 development of probabilistic systems that can serve as useful diagnostic
 and prognostic tools for physicians.  Such systems can serve as one method
 for disseminating the clinical knowledge captured in high-quality
 databases, such as those developed from PORT studies.  Within this
 context, the specific aims of the current, proposed research project are
 to:
 
 * refine and extend current methods for automatically constructing belief
 networks from large databases;
 
 * test the diagnostic and prognostic accuracy of systems that are based on
 belief networks constructed automatically from high quality databases,
 compared to several standard statistical techniques;
 
 * test whether a combination of automated and expert-based methods for
 constructing belief networks will yield diagnostic and prognostic systems
 that are more accurate than systems that are based on belief networks that
 are constructed automatically.
 
 These three aims will be pursued using large, high-quality clinical-
 research databases at the University of Pittsburgh that contain
 information on patients with syncope and patients in a PORT study with
 community-acquired-pneumonia.
",2237740,R29LM005291,['R29LM005291'],LM,https://reporter.nih.gov/project-details/2237740,R29,1995,95370,-0.09163583318306943
"Heart disease is a vital health care problem, affecting millions of
 Americans each year. Clinical decisions regarding the diagnosis of heart
 disease rely on determining the amount of diseased myocardium,with
 myocardial perfusion imaging representing the most widespread clinical
 procedure for assessing myocardial infarction and/or ischemia. However,
 interpreting this image information, and integrating it with other
 clinical data, remains a difficult and ill-defined problem. With this in
 mind, this competing renewal application proposes to continue a research
 program with the overall objective of developing a clinically useful,
 computer-based methodology to aid in the diagnosis of heart disease.  The
 methodology consists of a novel framework combining well established
 mathematical methods, visualization techniques, and artificial
 intelligence approaches for representing medical knowledge and integrating
 visual, numeric, textual, and temporal information. The principal
 hypothesis underlying the research is that medical decision-making tasks
 involving multidimensional information can be facilitated through the
 integration of both basic and applied concepts of medical informatics.
 
 During this next funding period, we propose to continue focussing our
 efforts on the development, implementation, and validation of this
 methodology through these specific aims: (1) automatic determination of
 the orientation of the left ventricular myocardium; (2) extension and
 enhancement of a knowledge base for interpreting myocardial perfusion
 imagery and other relevant information; (3) prediction of resting
 perfusion from resting thickening distributions through connectionist
 methods; (4) integration of connectionist and symbolic methods; (5)
 implementation and automation of the methodology into a fully integrated
 system; and (6) clinical testing and evaluation of this system. The
 extensive technical progress thus far achieved in these aims during the
 initial funding period is strong evidence of the merits of this highly
 interdisciplinary and interinstitutional research program.
 artificial intelligence; cardiovascular imaging /visualization; computer assisted diagnosis; computer assisted medical decision making; diagnosis design /evaluation; heart disorder diagnosis; human data; information system analysis; mathematical model; myocardium; perfusion KNOWLEDGE-BASED SYSTEM FOR CARDIAC IMAGE INTERPRETATION","Heart disease is a vital health care problem, affecting millions of
 Americans each year. Clinical decisions regarding the diagnosis of heart
 disease rely on determining the amount of diseased myocardium,with
 myocardial perfusion imaging representing the most widespread clinical
 procedure for assessing myocardial infarction and/or ischemia. However,
 interpreting this image information, and integrating it with other
 clinical data, remains a difficult and ill-defined problem. With this in
 mind, this competing renewal application proposes to continue a research
 program with the overall objective of developing a clinically useful,
 computer-based methodology to aid in the diagnosis of heart disease.  The
 methodology consists of a novel framework combining well established
 mathematical methods, visualization techniques, and artificial
 intelligence approaches for representing medical knowledge and integrating
 visual, numeric, textual, and temporal information. The principal
 hypothesis underlying the research is that medical decision-making tasks
 involving multidimensional information can be facilitated through the
 integration of both basic and applied concepts of medical informatics.
 
 During this next funding period, we propose to continue focussing our
 efforts on the development, implementation, and validation of this
 methodology through these specific aims: (1) automatic determination of
 the orientation of the left ventricular myocardium; (2) extension and
 enhancement of a knowledge base for interpreting myocardial perfusion
 imagery and other relevant information; (3) prediction of resting
 perfusion from resting thickening distributions through connectionist
 methods; (4) integration of connectionist and symbolic methods; (5)
 implementation and automation of the methodology into a fully integrated
 system; and (6) clinical testing and evaluation of this system. The
 extensive technical progress thus far achieved in these aims during the
 initial funding period is strong evidence of the merits of this highly
 interdisciplinary and interinstitutional research program.
",2237643,R01LM004692,['R01LM004692'],LM,https://reporter.nih.gov/project-details/2237643,R01,1995,227530,-0.04443540130095903
"The fundamental objective of our proposed research project is to approach       
biomedical image interpretation from a very new perspective:  that of           
knowledge-intensive experimental design of the segmentation process             
itself.  We use methods of artificial intelligence, specifically of             
knowledge representation, diagnostic decision-making, planning and              
learning, to carry out our objectives.                                          
                                                                                
Our central hypothesis is simple:  to make significant progress in              
automating image recognition and measurement tasks we need to treat             
recognition problems at the level of experimental design, so the best           
solutions to various types of imaging problems can be derived by a              
process of explicit specification, testing, and evaluation of different         
segmentation strategies.  We have already built a preliminary prototype         
of the proposed system, and have tested it on brain lesion recognition          
problems from multimodality magnetic resonance imaging (MRI).  We are now       
proposing to test both the methodological and practical assumptions             
underlying the system.  We will concentrate on automatic segmentation and       
interpretation techniques for individual and serial MRI examinations,           
which will be applied to automatically quantitate CNS changes in patients       
with tumors, AIDS-related lesions, MS lesions, and other conditions.            
                                                                                
The significance of this research for MR image interpretation lies in its       
ability to provide both the clinical researcher and the laboratory              
investigator the tools needed to carry out their work more efficiently          
and effectively.  In the clinical case we are focusing on the assessment        
of volume changes in AIDS-related and other lesions to quantitate their         
response to treatment, and in an industrial laboratory application the          
quantitation of lesion volumes is also critical in assessing the                
effectiveness of drugs undergoing testing.  In both cases there is a            
clear potential contribution to biomedical knowledge and future health          
care.                                                                           
 artificial intelligence; bioimaging /biomedical imaging; biomedical automation; computer graphics /printing; computer human interaction; computer system design /evaluation; human data; image processing; lymphoma; magnetic resonance imaging; meningioma; neoplasm /cancer diagnosis; phantom model; progressive multifocal leukoencephalopathy RUTGERS KNOWLEDGE-BASED BIOMEDICAL IMAGING PROJECT","The fundamental objective of our proposed research project is to approach       
biomedical image interpretation from a very new perspective:  that of           
knowledge-intensive experimental design of the segmentation process             
itself.  We use methods of artificial intelligence, specifically of             
knowledge representation, diagnostic decision-making, planning and              
learning, to carry out our objectives.                                          
                                                                                
Our central hypothesis is simple:  to make significant progress in              
automating image recognition and measurement tasks we need to treat             
recognition problems at the level of experimental design, so the best           
solutions to various types of imaging problems can be derived by a              
process of explicit specification, testing, and evaluation of different         
segmentation strategies.  We have already built a preliminary prototype         
of the proposed system, and have tested it on brain lesion recognition          
problems from multimodality magnetic resonance imaging (MRI).  We are now       
proposing to test both the methodological and practical assumptions             
underlying the system.  We will concentrate on automatic segmentation and       
interpretation techniques for individual and serial MRI examinations,           
which will be applied to automatically quantitate CNS changes in patients       
with tumors, AIDS-related lesions, MS lesions, and other conditions.            
                                                                                
The significance of this research for MR image interpretation lies in its       
ability to provide both the clinical researcher and the laboratory              
investigator the tools needed to carry out their work more efficiently          
and effectively.  In the clinical case we are focusing on the assessment        
of volume changes in AIDS-related and other lesions to quantitate their         
response to treatment, and in an industrial laboratory application the          
quantitation of lesion volumes is also critical in assessing the                
effectiveness of drugs undergoing testing.  In both cases there is a            
clear potential contribution to biomedical knowledge and future health          
care.                                                                           
",2283216,R01RR006235,['R01RR006235'],RR,https://reporter.nih.gov/project-details/2283216,R01,1995,429076,-0.04277414671863614
"1. Bayesian Learning in ME and HME Architectures - This Specific Aim will
 be to develop and apply Markov Chain Monte Carlo methodology to two
 specific types of neural networks: Mixtures-of-Experts (ME) and
 Hierarchical Mixtures-of-Experts (HME) Architectures. Recently, Peng,
 Jacobs and Tanner (1994) developed a Bayesian learning scheme for such
 architectures. The overall goal will be to further study and extend this
 methodology. The first specific subaim is to extend and investigate the ME
 and HME architectures in the binary response case. The second subaim will
 be to extend this ME and HME methodology to handle censored response data.
 The third subaim will be to compare this ME and HME methodology with
 competing nonparametric methods such as CART, MARS, generalized additive
 models (GAM's), and projection pursuit regression (PPR). The fourth
 specific subaim will be to develop methods to prune the ME and HME
 architectures.
 
 2. Strategies for Computing the Marginal Likelihood - While algorithms
 such as data augmentation, the Gibbs sampler, the Metropolis algorithm and
 the Metropolis-Hastings algorithm have facilitated computations regarding
 estimation and prediction, the problem of computing the marginal
 likelihood remains an open problem. Such Markov Chain Monte Carlo
 algorithms yield a sample from the posterior- the marginal likelihood (or
 marginal density of the data) is obtained by integrating the likelihood
 function with respect to the prior density. The first subaim will be to
 develop and assess a new approach to this problem. The second subaim will
 be to validate this methodology using real data sets. The third subaim
 will be to compare this approach to those developed by other researchers.
 
 3. Development of Software to Accompany Methodology - This Specific Aim is
 to develop transportable, documented, efficient code to. support the
 methodology developed under this grant. In particular, software will be
 written to apply the mixtures-of-experts and hierarchical mixtures-of-
 experts architectures to regression problems, binary response problems,
 categorical response problems and censored regression problems. Code will
 also be written to evaluate the marginal density of the sample data, thus
 allowing the comparison of competing models.
 artificial intelligence; computer system design /evaluation; human data; mathematical model; model design /development; statistics /biometry NONPARAMETRIC ANALYSIS OF CENSORED DATA","1. Bayesian Learning in ME and HME Architectures - This Specific Aim will
 be to develop and apply Markov Chain Monte Carlo methodology to two
 specific types of neural networks: Mixtures-of-Experts (ME) and
 Hierarchical Mixtures-of-Experts (HME) Architectures. Recently, Peng,
 Jacobs and Tanner (1994) developed a Bayesian learning scheme for such
 architectures. The overall goal will be to further study and extend this
 methodology. The first specific subaim is to extend and investigate the ME
 and HME architectures in the binary response case. The second subaim will
 be to extend this ME and HME methodology to handle censored response data.
 The third subaim will be to compare this ME and HME methodology with
 competing nonparametric methods such as CART, MARS, generalized additive
 models (GAM's), and projection pursuit regression (PPR). The fourth
 specific subaim will be to develop methods to prune the ME and HME
 architectures.
 
 2. Strategies for Computing the Marginal Likelihood - While algorithms
 such as data augmentation, the Gibbs sampler, the Metropolis algorithm and
 the Metropolis-Hastings algorithm have facilitated computations regarding
 estimation and prediction, the problem of computing the marginal
 likelihood remains an open problem. Such Markov Chain Monte Carlo
 algorithms yield a sample from the posterior- the marginal likelihood (or
 marginal density of the data) is obtained by integrating the likelihood
 function with respect to the prior density. The first subaim will be to
 develop and assess a new approach to this problem. The second subaim will
 be to validate this methodology using real data sets. The third subaim
 will be to compare this approach to those developed by other researchers.
 
 3. Development of Software to Accompany Methodology - This Specific Aim is
 to develop transportable, documented, efficient code to. support the
 methodology developed under this grant. In particular, software will be
 written to apply the mixtures-of-experts and hierarchical mixtures-of-
 experts architectures to regression problems, binary response problems,
 categorical response problems and censored regression problems. Code will
 also be written to evaluate the marginal density of the sample data, thus
 allowing the comparison of competing models.
",2088966,R01CA035464,['R01CA035464'],CA,https://reporter.nih.gov/project-details/2088966,R01,1995,92224,-0.05054806496811393
"This is an application for an NCHGR/SERCA grant (K01) to support
 interdisciplinary research in genomic science.  The immediate objective of
 this research project is to develop a fully automated/robust algorithm for
 computer assembly of high density cosmid contigs to serve as templates for
 large-scale sequence-based mapping.  This includes integrating the cosmid
 contigs with chromosome-wide YAC based maps and incorporating diverse
 sources of data (e.g., DNA sequence from cosmid ends, STSs and other
 markers, or results of specific gap-filling efforts) to resolve
 ambiguities and verify self-consistency.  Initial efforts will be focused
 on the high resolution mapping and sampled sequencing of human chromosome
 11.  The mapping strategy employs high density cosmid contig assembly over
 individual 200 kb to 1 Mb regions of the chromosome coupled with DNA
 sequencing of the cosmid ends.  The relative order and spacing of the
 sequence fragments is determined from the template contig resulting in a
 physical map of 1 to 5 kb resolution which contains up to 40% of the
 entire sequence at one-pass accuracy.  A simple restriction-site based
 approach to cosmid fingerprinting will be effective, provided that the
 contig-building algorithm correlates fragment data from multiple
 restriction digests and includes labeled vector/insert end fragments to
 determine the orientation of individual cosmids, i.e., the orientation of
 the genomic insert relative to the cloning vector.
 
 The candidate is a physicist with nearly a ten year record of achievement
 and publication in the field of plasma fusion energy--theory/computation.
 The research advisor, Prof. Glen Evans, is the Principal Investigator of
 the Genome Science and Technology Center (GESTEC) at the University of
 Texas Southwestern Medical Center at Dallas, one of the top academic
 medical centers in the nation and the performance site of this grant.
 Prof. Evans is an accomplished biologist with an established record of
 coordinating multi-disciplinary research to develop informatics and
 automation for the Human Genome Project (HGP).  The candidate's skills
 also compliment those of Prof. Harold Garner, physicist and co-Principal
 Investigator of GESTEC.  The candidate will receive explicit training
 through an intensive 18 months program that includes formal course-work in
 the Division of Cell and Molecular Biology (at UT Southwestern), and
 laboratory rotations in the mapping, sequencing, and informatics units of
 GESTEC.
 DNA; artificial chromosomes; artificial intelligence; chemical fingerprinting; chromosomes; computer assisted sequence analysis; computer program /software; genetic mapping; human genetic material tag; molecular cloning; nucleic acid sequence; plasmids; restriction fragment length polymorphism ADVANCED ALGORITHMS AND TOOLS FOR CONTIG BUILDING","This is an application for an NCHGR/SERCA grant (K01) to support
 interdisciplinary research in genomic science.  The immediate objective of
 this research project is to develop a fully automated/robust algorithm for
 computer assembly of high density cosmid contigs to serve as templates for
 large-scale sequence-based mapping.  This includes integrating the cosmid
 contigs with chromosome-wide YAC based maps and incorporating diverse
 sources of data (e.g., DNA sequence from cosmid ends, STSs and other
 markers, or results of specific gap-filling efforts) to resolve
 ambiguities and verify self-consistency.  Initial efforts will be focused
 on the high resolution mapping and sampled sequencing of human chromosome
 11.  The mapping strategy employs high density cosmid contig assembly over
 individual 200 kb to 1 Mb regions of the chromosome coupled with DNA
 sequencing of the cosmid ends.  The relative order and spacing of the
 sequence fragments is determined from the template contig resulting in a
 physical map of 1 to 5 kb resolution which contains up to 40% of the
 entire sequence at one-pass accuracy.  A simple restriction-site based
 approach to cosmid fingerprinting will be effective, provided that the
 contig-building algorithm correlates fragment data from multiple
 restriction digests and includes labeled vector/insert end fragments to
 determine the orientation of individual cosmids, i.e., the orientation of
 the genomic insert relative to the cloning vector.
 
 The candidate is a physicist with nearly a ten year record of achievement
 and publication in the field of plasma fusion energy--theory/computation.
 The research advisor, Prof. Glen Evans, is the Principal Investigator of
 the Genome Science and Technology Center (GESTEC) at the University of
 Texas Southwestern Medical Center at Dallas, one of the top academic
 medical centers in the nation and the performance site of this grant.
 Prof. Evans is an accomplished biologist with an established record of
 coordinating multi-disciplinary research to develop informatics and
 automation for the Human Genome Project (HGP).  The candidate's skills
 also compliment those of Prof. Harold Garner, physicist and co-Principal
 Investigator of GESTEC.  The candidate will receive explicit training
 through an intensive 18 months program that includes formal course-work in
 the Division of Cell and Molecular Biology (at UT Southwestern), and
 laboratory rotations in the mapping, sequencing, and informatics units of
 GESTEC.
",2208336,K01HG000018,['K01HG000018'],HG,https://reporter.nih.gov/project-details/2208336,K01,1995,81100,-0.13213551341279017
"This proposal seeks support for our studies on the effect of spinal cord
 injury on male reproduction.  Although anejaculation is a relatively
 uncommon occurrence in the general population, over 12,000 new cases are
 reported annually.  The major cause of ejaculatory dysfunction is spinal
 cord injury, usually occurring as a result of a motor vehicle accident.
 Infertility in spinal cord injured men is reported to approach 95%.  This
 impaired fertility is thought to be due to anejaculation secondary to
 neuromuscular dysfunction, obstruction of the genital passages secondary
 to infections and/or impaired spermatogenesis.  Electroejaculation, a
 procedure for inducing the seminal fluid emission by electrically
 stimulating specific nerves of the male reproductive tract with rectal
 probe electrostimulation, has been used to obtain sperm from these
 patients.  Nevertheless, poor sperm quality has been a consistent finding
 and pregnancy rates remain low.  Although adequate sperm densities
 usually can be obtained, the low sperm motility appears to be a limiting
 factor.  The information on variables that correlate with semen quality
 and ultimate pregnancy success or failure is far from complete.  We have
 proposed to develop a standardized protocol to be used in a large multi--
 institutional study of electroejaculation (EEJ) of SCI men to correlate
 patient evaluation, treatment and data collection.  We will evaluate the
 epididymal secretion of specific proteins, as well as the hormonal and
 spermatogenic characteristics of a large population of spinal cord
 injured men.  Changes in genital tract protein secretion after neurologic
 injury will provide the potential to develop a useful prognostic assay
 for damage to the proximal ductal system.  Most significantly we have
 proposed to devise a new approach for the multivariate analysis of the
 data using an artificial neural network Traditional statistical analysis
 of fertility has proven very unsatisfactory, with life table analysis
 commonly employed as a means of approximating the evaluation of fertility
 potential.  By ""learning"" and ""generalizing"", the neural network model
 provides an ideal method of addressing such a complex issue.  Upon
 completion of these aims we will have performed the largest study to date
 on the effect of SCI on male reproductive function and will have used
 this information to develop a powerful new system for the diagnosis of
 fertility potential in this unique patient population.
 SDS polyacrylamide gel electrophoresis; artificial intelligence; biopsy; computational neuroscience; data collection; electronic stimulator; electrostimulus; epididymis; fertility; genital secretion; gonadotropin releasing factor; hormone regulation /control mechanism; human subject; male; male reproductive system disorder; neuromuscular disorder; neurophysiology; prognosis; semen; sperm capacitation; sperm motility; spermatogenesis; spinal cord injury; testis CORRELATES OF FERTILITY IN SPINAL CORD-INJURED MEN","This proposal seeks support for our studies on the effect of spinal cord
 injury on male reproduction.  Although anejaculation is a relatively
 uncommon occurrence in the general population, over 12,000 new cases are
 reported annually.  The major cause of ejaculatory dysfunction is spinal
 cord injury, usually occurring as a result of a motor vehicle accident.
 Infertility in spinal cord injured men is reported to approach 95%.  This
 impaired fertility is thought to be due to anejaculation secondary to
 neuromuscular dysfunction, obstruction of the genital passages secondary
 to infections and/or impaired spermatogenesis.  Electroejaculation, a
 procedure for inducing the seminal fluid emission by electrically
 stimulating specific nerves of the male reproductive tract with rectal
 probe electrostimulation, has been used to obtain sperm from these
 patients.  Nevertheless, poor sperm quality has been a consistent finding
 and pregnancy rates remain low.  Although adequate sperm densities
 usually can be obtained, the low sperm motility appears to be a limiting
 factor.  The information on variables that correlate with semen quality
 and ultimate pregnancy success or failure is far from complete.  We have
 proposed to develop a standardized protocol to be used in a large multi--
 institutional study of electroejaculation (EEJ) of SCI men to correlate
 patient evaluation, treatment and data collection.  We will evaluate the
 epididymal secretion of specific proteins, as well as the hormonal and
 spermatogenic characteristics of a large population of spinal cord
 injured men.  Changes in genital tract protein secretion after neurologic
 injury will provide the potential to develop a useful prognostic assay
 for damage to the proximal ductal system.  Most significantly we have
 proposed to devise a new approach for the multivariate analysis of the
 data using an artificial neural network Traditional statistical analysis
 of fertility has proven very unsatisfactory, with life table analysis
 commonly employed as a means of approximating the evaluation of fertility
 potential.  By ""learning"" and ""generalizing"", the neural network model
 provides an ideal method of addressing such a complex issue.  Upon
 completion of these aims we will have performed the largest study to date
 on the effect of SCI on male reproductive function and will have used
 this information to develop a powerful new system for the diagnosis of
 fertility potential in this unique patient population.
",2202497,R01HD030155,['R01HD030155'],HD,https://reporter.nih.gov/project-details/2202497,R01,1995,282707,-0.09572259230934133
"     The amino acid sequence of a protein uniquely determines its tertiary
 structure.  Deciphering this relationship, the protein folding problem, has
 become increasingly important to molecular biologists.  DNA sequencing has
 become routine, but structural experiments remain very difficult.
 Computational strategies are needed to help address this problem.
 
      This proposal describes a strategy to identify the location of
 alpha-helices and beta-strands throughout the sequence.  A rationale is
 offered for employing neural networks and pattern based algorithms to
 address the secondary structure prediction problem.  Once secondary
 structure is located, computational methods exist for generating plausible
 tertiary structures.  However, these combinatorial strategies give rise to
 a large number of alternative structures which are difficult to distinguish
 from the correct fold.  Simplified potential functions are proposed as a
 method for overcoming this structure evaluation problem.  The properties of
 a non-lattice based simplified representation of a polypeptide chain will
 be explored to aid in the construction of an appropriate simplified
 potential function. Collaborative ventures are planned to experimentally
 test the merits of existing algorithms for predicting protein structure.
 
      In collaboration with Dr. Bunn at Harvard, the relationship of the
 erythropoietin sequence to its structure and function will be explored.  In
 collaboration with Dr.  Wang at UCSF, the merits of a proposed structure of
 hypoxanthine guanine phosphoribosyl transferase will be studied using site
 directed mutagenesis.  An exploration of the possibility of grafting the
 active site of one enzyme onto the structural scaffold provided by another
 protein will be studied in collaboration with Dr. Craik at UCSF and Dr.
 Wells at Genentech.
 active sites; artificial intelligence; computer graphics /printing; computer program /software; computer simulation; conformation; enzyme structure; erythropoietin; globular protein; hypoxanthine phosphoribosyltransferase; intermolecular interaction; molecular dynamics; parallel processing; physical model; protein engineering; protein folding; protein sequence; protein structure function; site directed mutagenesis COMPUTER ANALYSIS AND PREDICTION OF PROTEIN STRUCTURE","     The amino acid sequence of a protein uniquely determines its tertiary
 structure.  Deciphering this relationship, the protein folding problem, has
 become increasingly important to molecular biologists.  DNA sequencing has
 become routine, but structural experiments remain very difficult.
 Computational strategies are needed to help address this problem.
 
      This proposal describes a strategy to identify the location of
 alpha-helices and beta-strands throughout the sequence.  A rationale is
 offered for employing neural networks and pattern based algorithms to
 address the secondary structure prediction problem.  Once secondary
 structure is located, computational methods exist for generating plausible
 tertiary structures.  However, these combinatorial strategies give rise to
 a large number of alternative structures which are difficult to distinguish
 from the correct fold.  Simplified potential functions are proposed as a
 method for overcoming this structure evaluation problem.  The properties of
 a non-lattice based simplified representation of a polypeptide chain will
 be explored to aid in the construction of an appropriate simplified
 potential function. Collaborative ventures are planned to experimentally
 test the merits of existing algorithms for predicting protein structure.
 
      In collaboration with Dr. Bunn at Harvard, the relationship of the
 erythropoietin sequence to its structure and function will be explored.  In
 collaboration with Dr.  Wang at UCSF, the merits of a proposed structure of
 hypoxanthine guanine phosphoribosyl transferase will be studied using site
 directed mutagenesis.  An exploration of the possibility of grafting the
 active site of one enzyme onto the structural scaffold provided by another
 protein will be studied in collaboration with Dr. Craik at UCSF and Dr.
 Wells at Genentech.
",2180085,R01GM039900,['R01GM039900'],GM,https://reporter.nih.gov/project-details/2180085,R01,1995,254939,-0.06948859116191433
"Stuttering is a disorder of speech with a prevalence estimated to be 1 %
 of the world's population of school-age children. It is often a
 significant communicative problem for the individual, limiting educational
 and employment opportunities and social and psychological adjustment. The
 etiology of stuttering is unknown, and standardized, successful treatments
 for stuttering have not been developed. A major impediment to
 understanding the etiology of stuttering and to the development of
 successful therapeutic techniques is the lack of understanding of the
 physiological bases of the disorder. Stuttering manifests itself as a
 breakdown in speech motor processes. The complex variables known to affect
 the occurrence of stuttering, such as emotional state or linguistic
 complexity, must ultimately have an effect on the physiological events
 necessary for the production of speech. Therefore, to understand
 stuttering it is essential to understand the physiological mechanisms
 underlying disruptions of speech motor processes in stuttering.
 
 The research proposed in the present application addresses this general
 question: What is the nature of the movement disorder associated with
 stuttering? The specific aims are (1) to determine whether motor processes
 show evidence of continuous, underlying disturbances in stutterers'
 speech, (2) to assess whether failures in speech movement control in
 stuttering are related to autonomic nervous system activity and/or to
 metabolic respiratory control, (3) to develop new metrics for the analysis
 of physiological signals related to speech and to apply these new metrics
 to the assessment of stuttering, and (4) to develop pattern recognition
 algorithms to determine if there is a consistent set of physiological
 events associated with stuttering. The results of the proposed studies and
 those completed in the past years of this project should help us to
 understand the complex human behavior that is stuttering. In addition,
 work on this project has significant implications for the study of normal
 speech production and a variety of motor speech disorders that occur in
 neurologically impaired individuals.
 artificial intelligence; biomechanics; blood volume; electromyography; facial muscles; galvanic skin response; heart rate; human subject; jaw movement; lip; mathematical model; muscle function; neuromuscular disorder; neuromuscular function; plethysmography; psychological stressor; psychomotor function; pulmonary respiration; reading; speech; speech disorder diagnosis; stress; stuttering; sympathetic nervous system; tremor PHYSIOLOGICAL CORRELATES OF STUTTERING","Stuttering is a disorder of speech with a prevalence estimated to be 1 %
 of the world's population of school-age children. It is often a
 significant communicative problem for the individual, limiting educational
 and employment opportunities and social and psychological adjustment. The
 etiology of stuttering is unknown, and standardized, successful treatments
 for stuttering have not been developed. A major impediment to
 understanding the etiology of stuttering and to the development of
 successful therapeutic techniques is the lack of understanding of the
 physiological bases of the disorder. Stuttering manifests itself as a
 breakdown in speech motor processes. The complex variables known to affect
 the occurrence of stuttering, such as emotional state or linguistic
 complexity, must ultimately have an effect on the physiological events
 necessary for the production of speech. Therefore, to understand
 stuttering it is essential to understand the physiological mechanisms
 underlying disruptions of speech motor processes in stuttering.
 
 The research proposed in the present application addresses this general
 question: What is the nature of the movement disorder associated with
 stuttering? The specific aims are (1) to determine whether motor processes
 show evidence of continuous, underlying disturbances in stutterers'
 speech, (2) to assess whether failures in speech movement control in
 stuttering are related to autonomic nervous system activity and/or to
 metabolic respiratory control, (3) to develop new metrics for the analysis
 of physiological signals related to speech and to apply these new metrics
 to the assessment of stuttering, and (4) to develop pattern recognition
 algorithms to determine if there is a consistent set of physiological
 events associated with stuttering. The results of the proposed studies and
 those completed in the past years of this project should help us to
 understand the complex human behavior that is stuttering. In addition,
 work on this project has significant implications for the study of normal
 speech production and a variety of motor speech disorders that occur in
 neurologically impaired individuals.
",2125802,R01DC000559,['R01DC000559'],DC,https://reporter.nih.gov/project-details/2125802,R01,1995,197085,-0.00011633611339756546
"Changes in soft tissue elasticity are usually related to pathological
 processes.  Because of this, palpation is still widely used for
 diagnosis.  Its efficacy, however, is limited to abnormalities located
 relatively close to the skin surface.  The goal of quantitative
 elasticity imaging is to develop surrogate, remote palpation, thus
 expanding its range to include deep lying lesions.  The elastic
 properties of any continuous medium such as tissue can be assessed
 through precise measurement of mechanical deformations throughout that
 medium induced by forces applied at the surface.  Using modern medical
 imaging devices to precisely measure internal motion, it should be
 possible to estimate and even image elastic properties of internal
 organs.  In competition with other imaging modalities, ultrasound has two
 major advantages for elasticity imaging; it is inherently real-time and
 speckle artifacts limiting the quality of conventional images provide
 excellent markers for accurate tracking of tissue motion.  Elasticity can
 be imaged, therefore, by measuring motion with an ultrasound speckle
 tracking algorithm, followed by reconstruction of the elasticity
 distribution.  Although some other imaging systems, particularly real-
 time ultrasound, must be used to monitor tissue motion, elasticity
 imaging represents a fundamentally new diagnostic modality.  To
 investigate quantitative elasticity imaging for medical diagnosis, a
 research plan addressing the important clinical problem of renal
 inflammation and scarring has been formulated.  Preliminary data support
 the hypothesis that kidney elasticity changes with renal damage and
 concomitant scarring before renal problems are detectable by traditional
 diagnostic techniques such as laboratory measurements of renal function.
 Therefore, quantitative elasticity imaging may be valuable in detecting
 and quantifying scar for conditions such as kidney transplant rejection
 where rejection is difficult to quantify from functional measurements
 alone.  Based on the results of these studies, it is the long range goal
 of this research program to develop a sensitive diagnostic technique
 based on quantitative elasticity imaging permitting surrogate palpation
 of deep lying lesions.
 animal tissue; artificial intelligence; computer program /software; elasticity; glomerular filtration rate; guinea pigs; histopathology; hydroxyproline; kidney disorder diagnosis; laboratory rabbit; mechanical stress; nephritis; phantom model ELASTICITY IMAGING FOR EARLY RENAL PATHOLOGY DETECTION","Changes in soft tissue elasticity are usually related to pathological
 processes.  Because of this, palpation is still widely used for
 diagnosis.  Its efficacy, however, is limited to abnormalities located
 relatively close to the skin surface.  The goal of quantitative
 elasticity imaging is to develop surrogate, remote palpation, thus
 expanding its range to include deep lying lesions.  The elastic
 properties of any continuous medium such as tissue can be assessed
 through precise measurement of mechanical deformations throughout that
 medium induced by forces applied at the surface.  Using modern medical
 imaging devices to precisely measure internal motion, it should be
 possible to estimate and even image elastic properties of internal
 organs.  In competition with other imaging modalities, ultrasound has two
 major advantages for elasticity imaging; it is inherently real-time and
 speckle artifacts limiting the quality of conventional images provide
 excellent markers for accurate tracking of tissue motion.  Elasticity can
 be imaged, therefore, by measuring motion with an ultrasound speckle
 tracking algorithm, followed by reconstruction of the elasticity
 distribution.  Although some other imaging systems, particularly real-
 time ultrasound, must be used to monitor tissue motion, elasticity
 imaging represents a fundamentally new diagnostic modality.  To
 investigate quantitative elasticity imaging for medical diagnosis, a
 research plan addressing the important clinical problem of renal
 inflammation and scarring has been formulated.  Preliminary data support
 the hypothesis that kidney elasticity changes with renal damage and
 concomitant scarring before renal problems are detectable by traditional
 diagnostic techniques such as laboratory measurements of renal function.
 Therefore, quantitative elasticity imaging may be valuable in detecting
 and quantifying scar for conditions such as kidney transplant rejection
 where rejection is difficult to quantify from functional measurements
 alone.  Based on the results of these studies, it is the long range goal
 of this research program to develop a sensitive diagnostic technique
 based on quantitative elasticity imaging permitting surrogate palpation
 of deep lying lesions.
",2146817,R01DK047324,['R01DK047324'],DK,https://reporter.nih.gov/project-details/2146817,R01,1995,191426,0.019498607242339434
"The ability to elucidate RNA structure has broad significance in the study
 of RNA.  Comparative sequence analysis is one method, complementary to
 experimental procedures, that has yielded important RNA structural
 information.
 
 The sequencing revolution is helping to create larger comparative sequence
 databases, which in turn creates an opportunity to decipher more RNA
 structural information.  Computationally intensive methods are required to
 find this information.
 
 The purpose of this project, is to develop and refine our correlation
 analysis, and apply these analysis tools to the question of RNA structure
 determination.  Initial studies are very encouraging, suggesting that
 continued efforts will result in new structural findings.
 
 The results of this analysis have widespread applications, including but no
 limited to antisense research, overall structural considerations of
 ribosomal RNAs, and other important RNA molecules, and general RNA structur
 prediction.
 RNA; artificial intelligence; biochemical evolution; computer data analysis; computer graphics /printing; computer human interaction; computer program /software; computer simulation; nucleic acid sequence; nucleic acid structure; ribosomal RNA; transfer RNA RNA STRUCTURE DETERMINATION WITH COMPARATIVE METHODS","The ability to elucidate RNA structure has broad significance in the study
 of RNA.  Comparative sequence analysis is one method, complementary to
 experimental procedures, that has yielded important RNA structural
 information.
 
 The sequencing revolution is helping to create larger comparative sequence
 databases, which in turn creates an opportunity to decipher more RNA
 structural information.  Computationally intensive methods are required to
 find this information.
 
 The purpose of this project, is to develop and refine our correlation
 analysis, and apply these analysis tools to the question of RNA structure
 determination.  Initial studies are very encouraging, suggesting that
 continued efforts will result in new structural findings.
 
 The results of this analysis have widespread applications, including but no
 limited to antisense research, overall structural considerations of
 ribosomal RNAs, and other important RNA molecules, and general RNA structur
 prediction.
",2185696,R01GM048207,['R01GM048207'],GM,https://reporter.nih.gov/project-details/2185696,R01,1995,136167,-0.021923818412103124
"The Human Genome Project faces a number of formidable challenges.  Among
 these are the development of highly sensitive and accurate automated
 sequencing techniques, the development of optimal strategies for gene
 recognition in sequence data, and an improved understanding of gene
 function and regulation.  My intent in this Special Emphasis Research
 Career Award project is to address these and other related problems
 within the context of state-of-the-art genome research.  I believe that
 my background in theoretical and experimental particle physics provides
 me with a unique set of skills relevant to the solution of these and
 other problems.  The goals of this training program are to: 1) obtain a
 firm grounding in modern molecular biology and genetics with
 specialization in genome research, 2) develop a set of skills in
 laboratory-based genome research through a first year project in
 physical mapping and DNA sequencing, and 3) develop a long term project
 focusing on DNA sequence acquisition and analysis which will utilize the
 analytical, computational and model building skills which I have
 developed as a physicist.  In particular, I would hope to develop
 optimum protocols for gene mapping and sequence assembly applicable to
 large scale genome analysis and to develop new and more efficient
 algorithms for gene sequence identification and interpretation.  In
 addition, I intend to pursue the development of advanced automated
 sequencing technology based on highly sensitive detectors and techniques
 developed for particle physics.  Achievement of either or both of these
 goals will greatly facilitate the formidable task faced by the
 researchers involved in the Human Genome Project in accumulating and
 analyzing vast amounts of genomic DNA sequence.  This project will begin
 by immersion in the work currently being done on the physical mapping of
 human chromosome 11, and the sequencing of selected reference markers
 and cDNA clones, as part of the Salk Institute large scale physical
 mapping project.  The formal training I will receive during the tenure
 of the NCHGR/SERCA provides an ideal environment in which I can gain the
 necessary grounding in current techniques so that I can effectively work
 on the development of new techniques and strategies and contribute to
 the genome research of the future.
 artificial chromosomes; artificial intelligence; chromosomes; complementary DNA; computer assisted sequence analysis; flow cytometry; genetic mapping; genetic markers; genetic techniques; genome; human genetic material tag; in situ hybridization; molecular biology; molecular genetics; nucleic acid sequence; technology /technique development; training MOLECULAR ANALYSIS OF THE HUMAN GENOME","The Human Genome Project faces a number of formidable challenges.  Among
 these are the development of highly sensitive and accurate automated
 sequencing techniques, the development of optimal strategies for gene
 recognition in sequence data, and an improved understanding of gene
 function and regulation.  My intent in this Special Emphasis Research
 Career Award project is to address these and other related problems
 within the context of state-of-the-art genome research.  I believe that
 my background in theoretical and experimental particle physics provides
 me with a unique set of skills relevant to the solution of these and
 other problems.  The goals of this training program are to: 1) obtain a
 firm grounding in modern molecular biology and genetics with
 specialization in genome research, 2) develop a set of skills in
 laboratory-based genome research through a first year project in
 physical mapping and DNA sequencing, and 3) develop a long term project
 focusing on DNA sequence acquisition and analysis which will utilize the
 analytical, computational and model building skills which I have
 developed as a physicist.  In particular, I would hope to develop
 optimum protocols for gene mapping and sequence assembly applicable to
 large scale genome analysis and to develop new and more efficient
 algorithms for gene sequence identification and interpretation.  In
 addition, I intend to pursue the development of advanced automated
 sequencing technology based on highly sensitive detectors and techniques
 developed for particle physics.  Achievement of either or both of these
 goals will greatly facilitate the formidable task faced by the
 researchers involved in the Human Genome Project in accumulating and
 analyzing vast amounts of genomic DNA sequence.  This project will begin
 by immersion in the work currently being done on the physical mapping of
 human chromosome 11, and the sequencing of selected reference markers
 and cDNA clones, as part of the Salk Institute large scale physical
 mapping project.  The formal training I will receive during the tenure
 of the NCHGR/SERCA provides an ideal environment in which I can gain the
 necessary grounding in current techniques so that I can effectively work
 on the development of new techniques and strategies and contribute to
 the genome research of the future.
",2208285,K01HG000007,['K01HG000007'],HG,https://reporter.nih.gov/project-details/2208285,K01,1995,91126,-0.08057279765057224
"The primary objective of this research is to improve automated analysis of
 gel-based DNA sequencing ladders, through pattern recognition-based
 translation of raw instrument data to DNA sequences. We emphasize neural
 networks, adapted to particular sequencing conditions and instruments. The
 performances of pattern recognition and conventional basecalling software
 will be evaluated: (1) as experimental errors challenge description of the
 natural allelic diversity of human adenoviral genomes; (2) for detection
 and specification of heterozygous loci in diploid template experiments;
 (3) for primer selection and assembly operations of large scale sequencing
 projects. Distributions of basecalling errors will be analyzed in the
 contexts of neighboring nucleotide identities and as results of different
 sequencing strategies.
 
 Three principal advantages are expected from pattern recognition
 basecalling software: (1) analysis of contextual arrays of oligomer traces
 improves basecalling accuracy; (2) specifically tasked, neural network and
 algorithmic processors support on-line signal conditioning and basecalling
 in real time; and (3) the signal conditioning and pattern recognition
 modules support objective measures of confidence for each basecall.
 
 This project will significantly and positively impact progress towards the
 stated goals of the human genome initiative. No incremental costs for
 hardware or strategic modifications are required. Cost savings can be
 realized through automation of labor intensive review and editing of
 primary data. Real-time basecalling supports higher throughput
 instruments, exploiting faster separation of larger parallel arrays of
 sequencing ladders. Objective basecall confidence parameters support
 overlap assignment during sequence assembly, and should facilitate
 sequence - match searches through expanding databases.
 DNA; artificial intelligence; automated data processing; bioengineering /biomedical engineering; computer assisted sequence analysis; computer program /software; computer system design /evaluation; genome; human genetic material tag; image processing; nucleic acid sequence TRANSLATION OF AUTOMATED SEUENCER DATA TO DNA SEQUENCES","The primary objective of this research is to improve automated analysis of
 gel-based DNA sequencing ladders, through pattern recognition-based
 translation of raw instrument data to DNA sequences. We emphasize neural
 networks, adapted to particular sequencing conditions and instruments. The
 performances of pattern recognition and conventional basecalling software
 will be evaluated: (1) as experimental errors challenge description of the
 natural allelic diversity of human adenoviral genomes; (2) for detection
 and specification of heterozygous loci in diploid template experiments;
 (3) for primer selection and assembly operations of large scale sequencing
 projects. Distributions of basecalling errors will be analyzed in the
 contexts of neighboring nucleotide identities and as results of different
 sequencing strategies.
 
 Three principal advantages are expected from pattern recognition
 basecalling software: (1) analysis of contextual arrays of oligomer traces
 improves basecalling accuracy; (2) specifically tasked, neural network and
 algorithmic processors support on-line signal conditioning and basecalling
 in real time; and (3) the signal conditioning and pattern recognition
 modules support objective measures of confidence for each basecall.
 
 This project will significantly and positively impact progress towards the
 stated goals of the human genome initiative. No incremental costs for
 hardware or strategic modifications are required. Cost savings can be
 realized through automation of labor intensive review and editing of
 primary data. Real-time basecalling supports higher throughput
 instruments, exploiting faster separation of larger parallel arrays of
 sequencing ladders. Objective basecall confidence parameters support
 overlap assignment during sequence assembly, and should facilitate
 sequence - match searches through expanding databases.
",2208900,R01HG000562,['R01HG000562'],HG,https://reporter.nih.gov/project-details/2208900,R01,1995,267443,-0.214883952088995
"We propose to design efficient computer algorithms providing novel and/or
 improved methods and software for a number of computational problems in
 molecular biology.  the emphasis will b eon blending theoretical results
 with practical concerns.  All software emerging from the project will be
 made available free of charge.  The proposal and the principal
 investigator's current research efforts are divided into three projects.
 
 The first project centers on computational problems in the physical mapping
 and sequencing of DNA.  We propose to continue refining a software system
 for the fragment assembly problem, i.e., determine the most likely complete
 DNA sequence consistent with electrophoresis data gathered from cloned
 fragments.  The refinements consist of improved algorithms for a number of
 the phases of the computation, expanding the functionality to support user-
 interaction, and developing a complete environment to support megabase
 sequencing projects.  The methods we developed for fragment assembly also
 apply in large part to the problem of determining physical maps via various
 fingerprinting techniques.  We have formulated a generalized assembly
 problem and plan to build a system that is capable of solving such problems
 for any combination of restriction map, digest, and hybridization
 information about the clones.
 
 The second project is to design algorithms for a number of computational
 problems arising in molecular biology.  Progress in this arena tends to be
 inspired rather than calculated.  We demonstrate our track record of
 producing interesting results and then describe the following problems for
 which we have a number of ideas and preliminary results:  sublinear
 similarity searches, restriction map comparison, super pattern matching
 (gene recognition), determining restriction maps from digest data,
 designing oligonucleotide probes, and RNA secondary structure prediction.
 
 The objective of the last project is to develop a pattern matching system
 permitting the expression of complex patterns and their reduction to
 efficient search strategies.  The pattern specification language is simple
 yet powerful enough to succinctly express the most complex patterns of
 biological interest.  An ""expert system"" compiler for the language will
 examine a pattern and will choose a search strategy or combination of
 strategies from a built-in library of basic search techniques.  The build-
 in library will contain implementations of the best available search
 algorithms for exact and approximate matches to keywords, repeats, and
 regular expressions.  Using a dynamic-programming style calculation the
 expert compiler chooses the optimal backtracking strategy over the basic
 library searches.  We have proven the efficacy of this approach on a small
 prototype for a subset of the language that is useful for specifying
 protein motifs.  We now propose to embark upon the construction of a
 complete system.
 artificial intelligence; computer assisted sequence analysis; computer data analysis; computer program /software; information retrieval; nucleic acid sequence EFFICIENT SOFTWARE FOR THE ANALYSIS OF BIOSEQUENCES","We propose to design efficient computer algorithms providing novel and/or
 improved methods and software for a number of computational problems in
 molecular biology.  the emphasis will b eon blending theoretical results
 with practical concerns.  All software emerging from the project will be
 made available free of charge.  The proposal and the principal
 investigator's current research efforts are divided into three projects.
 
 The first project centers on computational problems in the physical mapping
 and sequencing of DNA.  We propose to continue refining a software system
 for the fragment assembly problem, i.e., determine the most likely complete
 DNA sequence consistent with electrophoresis data gathered from cloned
 fragments.  The refinements consist of improved algorithms for a number of
 the phases of the computation, expanding the functionality to support user-
 interaction, and developing a complete environment to support megabase
 sequencing projects.  The methods we developed for fragment assembly also
 apply in large part to the problem of determining physical maps via various
 fingerprinting techniques.  We have formulated a generalized assembly
 problem and plan to build a system that is capable of solving such problems
 for any combination of restriction map, digest, and hybridization
 information about the clones.
 
 The second project is to design algorithms for a number of computational
 problems arising in molecular biology.  Progress in this arena tends to be
 inspired rather than calculated.  We demonstrate our track record of
 producing interesting results and then describe the following problems for
 which we have a number of ideas and preliminary results:  sublinear
 similarity searches, restriction map comparison, super pattern matching
 (gene recognition), determining restriction maps from digest data,
 designing oligonucleotide probes, and RNA secondary structure prediction.
 
 The objective of the last project is to develop a pattern matching system
 permitting the expression of complex patterns and their reduction to
 efficient search strategies.  The pattern specification language is simple
 yet powerful enough to succinctly express the most complex patterns of
 biological interest.  An ""expert system"" compiler for the language will
 examine a pattern and will choose a search strategy or combination of
 strategies from a built-in library of basic search techniques.  The build-
 in library will contain implementations of the best available search
 algorithms for exact and approximate matches to keywords, repeats, and
 regular expressions.  Using a dynamic-programming style calculation the
 expert compiler chooses the optimal backtracking strategy over the basic
 library searches.  We have proven the efficacy of this approach on a small
 prototype for a subset of the language that is useful for specifying
 protein motifs.  We now propose to embark upon the construction of a
 complete system.
",2237672,R01LM004960,['R01LM004960'],LM,https://reporter.nih.gov/project-details/2237672,R01,1995,143074,-0.01681662655529709
"The research described in this proposal begins with completion and formal
 evaluation of MIDAS ""a computer program designed to automatically
 construct decision models from an underlying medical knowledge base"". The
 capabilities of MIDAS will be extended to knowledge-based construction
 of Markov decision models. A second project will develop a comprehensive
 knowledge management scheme for the problem of pulmonary disease in AIDS.
 This scheme will use a knowledge base structured according to knowledge
 needed to perform a decision analysis. It will incorporate summaries of
 relevant data, sources and quality of data and links to the original
 sources. This knowledge management scheme will be deployed in the
 hospital and evaluated in a group of medical residents at Robert Wood
 Johnson University Hospital.
 HIV infections; artificial intelligence; computer assisted medical decision making; computer human interaction; health care model; human data; information system analysis; lung disorder; model design /development; opportunistic infections; physicians; pneumonia KNOWLEDGE MANAGEMENT FOR CLINICAL DECISION ANALYSIS","The research described in this proposal begins with completion and formal
 evaluation of MIDAS ""a computer program designed to automatically
 construct decision models from an underlying medical knowledge base"". The
 capabilities of MIDAS will be extended to knowledge-based construction
 of Markov decision models. A second project will develop a comprehensive
 knowledge management scheme for the problem of pulmonary disease in AIDS.
 This scheme will use a knowledge base structured according to knowledge
 needed to perform a decision analysis. It will incorporate summaries of
 relevant data, sources and quality of data and links to the original
 sources. This knowledge management scheme will be deployed in the
 hospital and evaluated in a group of medical residents at Robert Wood
 Johnson University Hospital.
",2237610,K04LM000096,['K04LM000096'],LM,https://reporter.nih.gov/project-details/2237610,K04,1995,72090,-0.1677465384223487
"Health care costs are rapidly outstripping society's ability to pay.
 Practice guidelines have been proposed as a way to reduce costs while
 improving health care quality, and many kinds of organizations (e.g.,
 American College of Physicians, AHCPR, Mayo Clinics, etc.) are developing
 such guidelines.  Some (Lamas 1992, Lamas 1989, Kosecoff 1987), however,
 argue that guidelines will have little effect on practice patterns if
 they are merely published without employing reminders or incentives of
 some kind.  We have shown that computer reminders based on simple
 guidelines can change patterns (McDonald 1976, McDonald 1984, Tierney
 1986, Litzelman 1993).  However, we have not dealt with the rich practice
 guidelines being developed by AHCPR and others because the advice of such
 guidelines depends upon 'first order' data (collected from patients about
 their history, physical, and current symptoms) as well as the 'second
 order' data (from hospital services such as the pharmacy, laboratory,
 radiology, etc.) that the Regenstrief system now contains.  In the
 proposed work, we will rigorously measure the real effect of automated
 guidelines on physician practice problems and patient outcome.
 
 Specifically, using our current network of ordering workstations as the
 platform, we propose to:  1) Define, test and refine detailed computer
 executable guidelines for the management (and/or prevention) of a number
 of medical problems including congestive heart failure, pneumonia, and
 urinary tract infection.  2) To define, test and refine instruments for
 capturing the historical, physical and symptom (first order) data needed
 by these guidelines.  3)  To determine the reliability of these
 instruments when used by research assistants and  4) to refine and
 perfect mechanisms for delivering reminders produced by the computer
 executed guidelines to care providers.  5)  To assess provider agreement
 with the guideline logic and their attitudes about the computer system.
 6)  To build statistical models that predict adverse effects and extended
 hospital stays based on the collected 'first order' data and incorporate
 the model's predictions into reminders.  7)  To perform a randomized
 controlled trial of the effect of automated guideline reminders on a
 number of outcomes including provider compliance with the guideline
 advise, patients health status, length of stay, problem related costs,
 30 and 90 day re-admission rates and adverse events.
 
 As American medicine attempts to halt the reckless growth of health care
 costs, proposed solutions should be rigorously studied.  This project
 will shed light on the practicality, costs, and benefits of both practice
 guidelines automatically applied by a CBPR.
 anticoagulants; artificial intelligence; automated medical record system; blood transfusion; computer assisted diagnosis; computer assisted medical decision making; computer assisted patient care; congestive heart failure; data collection methodology /evaluation; disease /disorder prevention /control; health care cost /financing; health care personnel performance; health care professional practice; health care quality; health care service; health care service evaluation; hospital analysis; human subject; mathematical model; patient care management; pneumonia; urinary tract infection COMPUTER RECORDS, GUIDELINES QUALITY AND EFFICIENT CARE","Health care costs are rapidly outstripping society's ability to pay.
 Practice guidelines have been proposed as a way to reduce costs while
 improving health care quality, and many kinds of organizations (e.g.,
 American College of Physicians, AHCPR, Mayo Clinics, etc.) are developing
 such guidelines.  Some (Lamas 1992, Lamas 1989, Kosecoff 1987), however,
 argue that guidelines will have little effect on practice patterns if
 they are merely published without employing reminders or incentives of
 some kind.  We have shown that computer reminders based on simple
 guidelines can change patterns (McDonald 1976, McDonald 1984, Tierney
 1986, Litzelman 1993).  However, we have not dealt with the rich practice
 guidelines being developed by AHCPR and others because the advice of such
 guidelines depends upon 'first order' data (collected from patients about
 their history, physical, and current symptoms) as well as the 'second
 order' data (from hospital services such as the pharmacy, laboratory,
 radiology, etc.) that the Regenstrief system now contains.  In the
 proposed work, we will rigorously measure the real effect of automated
 guidelines on physician practice problems and patient outcome.
 
 Specifically, using our current network of ordering workstations as the
 platform, we propose to:  1) Define, test and refine detailed computer
 executable guidelines for the management (and/or prevention) of a number
 of medical problems including congestive heart failure, pneumonia, and
 urinary tract infection.  2) To define, test and refine instruments for
 capturing the historical, physical and symptom (first order) data needed
 by these guidelines.  3)  To determine the reliability of these
 instruments when used by research assistants and  4) to refine and
 perfect mechanisms for delivering reminders produced by the computer
 executed guidelines to care providers.  5)  To assess provider agreement
 with the guideline logic and their attitudes about the computer system.
 6)  To build statistical models that predict adverse effects and extended
 hospital stays based on the collected 'first order' data and incorporate
 the model's predictions into reminders.  7)  To perform a randomized
 controlled trial of the effect of automated guideline reminders on a
 number of outcomes including provider compliance with the guideline
 advise, patients health status, length of stay, problem related costs,
 30 and 90 day re-admission rates and adverse events.
 
 As American medicine attempts to halt the reckless growth of health care
 costs, proposed solutions should be rigorously studied.  This project
 will shed light on the practicality, costs, and benefits of both practice
 guidelines automatically applied by a CBPR.
",2236204,R01HS007719,['R01HS007719'],HS,https://reporter.nih.gov/project-details/2236204,R01,1995,583446,-0.03869576667870716
"In this FIRST award, using dentistry as a model, methods developed in a
 University of Washington pilot study will be extended to a larger sample
 to explore acute and chronic pain perceptions among patients and dentists
 in Scandinavia, China, and United States.  Specifically, we seek to
 determine how verbal descriptors are cognitively organized to reveal
 cultural influences on pain and coping remedies through a stepwise
 combination of ethnographic interviews and validations with participant
 observation and quantitative survey methods. 700 subjects will describe
 kinds of pains and ways of coping with them, especially acute and chronic
 orofacial pain.  Subject groups will be matched for age, socioeconomic
 status, gender, and education.
 
 In a first interview, questions like 'What kinds of pain are there?' will
 be asked.  Subjects' statements will be recorded verbatim in their native
 languages with the aim to reveal differences in perceptual context and
 detail their relevance.  From this database, pain and coping remedy terms
 will be selected by specific criteria and used to construct crosscultural
 pain survey instruments.  In a second interview, subjects will use a card
 sort instrument to judge the similarity of pain and remedy concepts.
 Reasons for sorting choices will be recorded to reveal perceptual
 categories.  Grid matrices will also be used to match pains with
 subjects' descriptors and pain coping strategies.
 
 All open-ended data will be analyzed by content and detailed pain
 narratives written for each culture.  Instrument data will be analyzed by
 multidimensional scaling and hierarchical clustering analyses.  Results
 will be validated by behavioral observations.  Reliability of the methods
 will be assessed by checking specified outcomes of quantitative indices;
 validity by how these indices relate to the findings of the qualitative
 phase.
 
 Our long-term objective is to understand how cultural influences such as
 ethnicity and professional socialization shape the perceived meanings of
 pain and pain coping remedies, with the intent to improve health-care
 communication in diagnosis and treatment.
 China; Scandinavian country; United States; coping; culture; ethnic group; human subject; interview; pain threshold; perception; psychometrics; questionnaires CROSSCULTURAL STUDY OF PAIN--PATIENTS AND DENTISTS","In this FIRST award, using dentistry as a model, methods developed in a
 University of Washington pilot study will be extended to a larger sample
 to explore acute and chronic pain perceptions among patients and dentists
 in Scandinavia, China, and United States.  Specifically, we seek to
 determine how verbal descriptors are cognitively organized to reveal
 cultural influences on pain and coping remedies through a stepwise
 combination of ethnographic interviews and validations with participant
 observation and quantitative survey methods. 700 subjects will describe
 kinds of pains and ways of coping with them, especially acute and chronic
 orofacial pain.  Subject groups will be matched for age, socioeconomic
 status, gender, and education.
 
 In a first interview, questions like 'What kinds of pain are there?' will
 be asked.  Subjects' statements will be recorded verbatim in their native
 languages with the aim to reveal differences in perceptual context and
 detail their relevance.  From this database, pain and coping remedy terms
 will be selected by specific criteria and used to construct crosscultural
 pain survey instruments.  In a second interview, subjects will use a card
 sort instrument to judge the similarity of pain and remedy concepts.
 Reasons for sorting choices will be recorded to reveal perceptual
 categories.  Grid matrices will also be used to match pains with
 subjects' descriptors and pain coping strategies.
 
 All open-ended data will be analyzed by content and detailed pain
 narratives written for each culture.  Instrument data will be analyzed by
 multidimensional scaling and hierarchical clustering analyses.  Results
 will be validated by behavioral observations.  Reliability of the methods
 will be assessed by checking specified outcomes of quantitative indices;
 validity by how these indices relate to the findings of the qualitative
 phase.
 
 Our long-term objective is to understand how cultural influences such as
 ethnicity and professional socialization shape the perceived meanings of
 pain and pain coping remedies, with the intent to improve health-care
 communication in diagnosis and treatment.
",2130956,R29DE009945,['R29DE009945'],DE,https://reporter.nih.gov/project-details/2130956,R29,1995,90126,-0.004550265508730355
"Despite the capability to collect sophisticated multiparameter listmode
 data, both rectilinear or bit-map cell sorting boundaries still are
 usually chosen manually and in a rather arbitrary fashion. Usually the
 experimenter performs visual clustering prior to drawing boundaries which
 have no statistical prediction of successful classification.
 Visualization of complex multiparameter data is also difficult. One way
 to deal with the visualization problem is to view the first three
 principal components of the data and use this information to estimate the
 number and approximate centroids for ""guided"" cluster analysis. Cluster
 membership probabilities will then be used to make sort decisions.
 
 Another way to deal with the problem of placing sort boundaries on the
 basis of arbitrary ""visual classifications"" is to apply statistical
 methods of classifying cells, e.g. discriminant analysis with Bayes
 decision boundaries. Discriminant functions will be calculated and Bayes
 decision boundaries will be used to sort cells on the basis of
 discriminant function scores which will be calculated in real-time by
 hardware and/or software lookup tables. A cost of misclassification will
 also be included in the cell sorting decision.
 
 For all classifier systems developed, classifier performance will be
 measured through ROC (""receiver operating characteristics"") analyses of
 true-positives and false-positives. To accomplish this we will use a
 well-defined system of data and model cell systems whereby all
 classifiers can be checked for correctness against ""tagged"" parameters.
 All sorted model cells can be unequivocally identified by PCR (polymerase
 chain reaction) or by FISH (fluorescence in-situ hybridization).
 
 While the main focus of the proposal is to develop real-time cell
 classifiers useful for cell sorting, many of the techniques can also be
 used by other researchers for off-line analysis of conventional listmode
 flow cytometry data.  Hence many of these techniques should prove
 important to other researchers even if they are unable to perform the
 sophisticated cell sorting described in this proposal.
 
 To demonstrate the importance of these new techniques to many problems
 in biology and medicine we will attempt to apply these new techniques to
 several important applications including: (1) high-resolution sorting of
 single fetal cells from human maternal blood for prenatal diagnosis; (2)
 molecular characterizations of oncogene, tumor suppresser, metastatic,
 and multi-drug resistance genes in rare human metastatic breast cancer
 cells isolated from peripheral blood and bone marrow by high-speed
 enrichment or high-resolution cell sorting; and (3) bone marrow purging
 of metastatic cells to allow for autologous transplantations in breast
 cancer patients undergoing high-dose chemotherapy.
 artificial intelligence; bone marrow purging; breast neoplasms; cell sorting; classification; computer system design /evaluation; confocal scanning microscopy; female; human subject; in situ hybridization; metastasis; multidrug resistance; neoplasm /cancer genetics; oncogenes; polymerase chain reaction; pregnancy circulation; statistics /biometry; tumor suppressor genes; women's health CLASSIFIERS FOR HIGH RESOLUTION CELL SORTING","Despite the capability to collect sophisticated multiparameter listmode
 data, both rectilinear or bit-map cell sorting boundaries still are
 usually chosen manually and in a rather arbitrary fashion. Usually the
 experimenter performs visual clustering prior to drawing boundaries which
 have no statistical prediction of successful classification.
 Visualization of complex multiparameter data is also difficult. One way
 to deal with the visualization problem is to view the first three
 principal components of the data and use this information to estimate the
 number and approximate centroids for ""guided"" cluster analysis. Cluster
 membership probabilities will then be used to make sort decisions.
 
 Another way to deal with the problem of placing sort boundaries on the
 basis of arbitrary ""visual classifications"" is to apply statistical
 methods of classifying cells, e.g. discriminant analysis with Bayes
 decision boundaries. Discriminant functions will be calculated and Bayes
 decision boundaries will be used to sort cells on the basis of
 discriminant function scores which will be calculated in real-time by
 hardware and/or software lookup tables. A cost of misclassification will
 also be included in the cell sorting decision.
 
 For all classifier systems developed, classifier performance will be
 measured through ROC (""receiver operating characteristics"") analyses of
 true-positives and false-positives. To accomplish this we will use a
 well-defined system of data and model cell systems whereby all
 classifiers can be checked for correctness against ""tagged"" parameters.
 All sorted model cells can be unequivocally identified by PCR (polymerase
 chain reaction) or by FISH (fluorescence in-situ hybridization).
 
 While the main focus of the proposal is to develop real-time cell
 classifiers useful for cell sorting, many of the techniques can also be
 used by other researchers for off-line analysis of conventional listmode
 flow cytometry data.  Hence many of these techniques should prove
 important to other researchers even if they are unable to perform the
 sophisticated cell sorting described in this proposal.
 
 To demonstrate the importance of these new techniques to many problems
 in biology and medicine we will attempt to apply these new techniques to
 several important applications including: (1) high-resolution sorting of
 single fetal cells from human maternal blood for prenatal diagnosis; (2)
 molecular characterizations of oncogene, tumor suppresser, metastatic,
 and multi-drug resistance genes in rare human metastatic breast cancer
 cells isolated from peripheral blood and bone marrow by high-speed
 enrichment or high-resolution cell sorting; and (3) bone marrow purging
 of metastatic cells to allow for autologous transplantations in breast
 cancer patients undergoing high-dose chemotherapy.
",2179439,R01GM038645,['R01GM038645'],GM,https://reporter.nih.gov/project-details/2179439,R01,1995,223845,-0.014774193341259812
  MACHINE LEARNING FOR PROGNOSTIC PREDICTION,,2112703,F32CA068690,['F32CA068690'],CA,https://reporter.nih.gov/project-details/2112703,F32,1995,16800,-0.016892098860929405
"The Heart Failure Program project is a resource-related research program to
 develop new reasoning methods for the application of Artifical Intelligence
 techniques to medicine for the effort of the SUMEX-AIM community.  The
 context and driving force for this research is the management of heart
 failure in the intensive care setting.  We will:  1) Develop a
 representational methodology capable of supporting the clinically relevant
 distinctions of patient state.  This representation will utilize the
 clinically significant qualitative parameter values and will include causal
 relationships, time dependencies and relations about change.  2) Build a
 qualitative physiological model of the cardiovascular system using this
 methodology to act as a store for evolving knowledge of patient state.  3)
 Explore and develop strategies for determining the appropriate parameter
 values in the model from input data, reasoning support methods and
 heuristics for carrying out the diagnostic reasoning with the model, and
 reasoning support methods and heuristics for carrying out the diagnostic
 reasoning with the model, and reasoning support methods and heuristics for
 finding possible therapies and determining their potential consequences.
 4) Build around this core a program to assist the physician in exploring
 his or her understanding of the implications in an individual case.  The
 physician and program will reason together about the case with the
 physician providing ideas and the program assuring consistent consideration
 of the implications.  5) Generalize the techniques for use in other medical
 domains.
 artificial intelligence; cardiovascular disorder chemotherapy; cardiovascular disorder diagnosis; computer assisted diagnosis; computer assisted patient care; computer human interaction; computer system design /evaluation; diagnosis design /evaluation; human subject; managed care; patient care management ARTIFICIAL INTELLIGENCE CARDIOVASCULAR REASONING","The Heart Failure Program project is a resource-related research program to
 develop new reasoning methods for the application of Artifical Intelligence
 techniques to medicine for the effort of the SUMEX-AIM community.  The
 context and driving force for this research is the management of heart
 failure in the intensive care setting.  We will:  1) Develop a
 representational methodology capable of supporting the clinically relevant
 distinctions of patient state.  This representation will utilize the
 clinically significant qualitative parameter values and will include causal
 relationships, time dependencies and relations about change.  2) Build a
 qualitative physiological model of the cardiovascular system using this
 methodology to act as a store for evolving knowledge of patient state.  3)
 Explore and develop strategies for determining the appropriate parameter
 values in the model from input data, reasoning support methods and
 heuristics for carrying out the diagnostic reasoning with the model, and
 reasoning support methods and heuristics for carrying out the diagnostic
 reasoning with the model, and reasoning support methods and heuristics for
 finding possible therapies and determining their potential consequences.
 4) Build around this core a program to assist the physician in exploring
 his or her understanding of the implications in an individual case.  The
 physician and program will reason together about the case with the
 physician providing ideas and the program assuring consistent consideration
 of the implications.  5) Generalize the techniques for use in other medical
 domains.
",2217162,R01HL033041,['R01HL033041'],HL,https://reporter.nih.gov/project-details/2217162,R01,1995,430068,-0.04136418740837679
"The goal of this proposal is to generate method and tools to link
 knowledge-based systems (KBSs) to real clinical databases.  The linking
 is done via quires that map conceptual entities in the KBS to actual
 entries in the database. They may be used when a KBS is first created,
 when an existing KBS is linked to a database or when a KBS is transferred
 to another institution.  The primary purpose of the queries is to apply
 the KBS to individual patients for direct patient care, rather than to
 extend the knowledge base itself.  Several of the tools (Aims 3 and 4)
 are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All
 the proposed tools will be applied to CPMC's MLM knowledge base, which
 is actively used for patient care.
 
 AIM 1 to increase the accuracy while reducing the writing time and
 technical skills required to author clinical database queries.  Query by
 Review will provide a familiar interface to novice query authors,
 enabling them to write queries by traversing the same type of result
 review screens that they use every day in clinical care. When data are
 selected from the screen, the tools will generate an appropriate query
 (in Arden Syntax and HL7) that can be inserted into a KBS or used for
 clinical research.
 
 AIM 2 to facilitate the testing of queries, and to improve the match
 between a query's result and the needs to a KBS.  The Clinical Database
 Brower will allow author to characterize the data returned by a query in
 order to determine if they query is appropriate.  It will also allow the
 author to determine whether additional logic is necessary to convert the
 raw data into form expected by the KBS.  The Brower's design is unique
 in its use of a semantic network to aggregate complex categorical data.
 
 AIM 3 to provide an environment for inserting queries and additional
 logic into the KBS.  The Advanced MLM Editor will include a mechanism for
 inserting queries generated by the Query by Review tool into MLMs.  It
 will also allow authors to reuse queries and logic employed in existing
 MLMs.
 
 AIM 4 to facilitate the testing of queries within the environment of the
 KBS.  The Event Playback tool will allow the MLM author to run an MLM
 against the clinical database to see whether the MLM performs as
 expected.  Rather than presenting a snapshot of the database, the tool
 will run the MLM as if medical events (e.g, clinical database
 transactions) were occurring in real time, better simulating actual use.
 The Interactive MLM Interpreter will allow an author to debug an MLM by
 running it line-by line; the author will be able to respond to each query
 manually.  It will also support the batch testing of MLMs using a test
 data set.
 
 AIM 5 to evaluate and disseminate the proposed tools.  The impact of the
 Query by Review tool on query authoring time and query accuracy will be
 measured.  To assess the usefulness of the tools for non-Arden Syntax
 KBS, findings from the QMR vocabulary will be studied.  Usage of the
 tools will be measured for CPMC's production KBS.  Tools, components, and
 methods will be disseminated.
 abstracting; artificial intelligence; computer assisted patient care; computer human interaction; health care facility information system; human subject; information retrieval; physicians; vocabulary development for information system LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES","The goal of this proposal is to generate method and tools to link
 knowledge-based systems (KBSs) to real clinical databases.  The linking
 is done via quires that map conceptual entities in the KBS to actual
 entries in the database. They may be used when a KBS is first created,
 when an existing KBS is linked to a database or when a KBS is transferred
 to another institution.  The primary purpose of the queries is to apply
 the KBS to individual patients for direct patient care, rather than to
 extend the knowledge base itself.  Several of the tools (Aims 3 and 4)
 are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All
 the proposed tools will be applied to CPMC's MLM knowledge base, which
 is actively used for patient care.
 
 AIM 1 to increase the accuracy while reducing the writing time and
 technical skills required to author clinical database queries.  Query by
 Review will provide a familiar interface to novice query authors,
 enabling them to write queries by traversing the same type of result
 review screens that they use every day in clinical care. When data are
 selected from the screen, the tools will generate an appropriate query
 (in Arden Syntax and HL7) that can be inserted into a KBS or used for
 clinical research.
 
 AIM 2 to facilitate the testing of queries, and to improve the match
 between a query's result and the needs to a KBS.  The Clinical Database
 Brower will allow author to characterize the data returned by a query in
 order to determine if they query is appropriate.  It will also allow the
 author to determine whether additional logic is necessary to convert the
 raw data into form expected by the KBS.  The Brower's design is unique
 in its use of a semantic network to aggregate complex categorical data.
 
 AIM 3 to provide an environment for inserting queries and additional
 logic into the KBS.  The Advanced MLM Editor will include a mechanism for
 inserting queries generated by the Query by Review tool into MLMs.  It
 will also allow authors to reuse queries and logic employed in existing
 MLMs.
 
 AIM 4 to facilitate the testing of queries within the environment of the
 KBS.  The Event Playback tool will allow the MLM author to run an MLM
 against the clinical database to see whether the MLM performs as
 expected.  Rather than presenting a snapshot of the database, the tool
 will run the MLM as if medical events (e.g, clinical database
 transactions) were occurring in real time, better simulating actual use.
 The Interactive MLM Interpreter will allow an author to debug an MLM by
 running it line-by line; the author will be able to respond to each query
 manually.  It will also support the batch testing of MLMs using a test
 data set.
 
 AIM 5 to evaluate and disseminate the proposed tools.  The impact of the
 Query by Review tool on query authoring time and query accuracy will be
 measured.  To assess the usefulness of the tools for non-Arden Syntax
 KBS, findings from the QMR vocabulary will be studied.  Usage of the
 tools will be measured for CPMC's production KBS.  Tools, components, and
 methods will be disseminated.
",2237960,R29LM005627,['R29LM005627'],LM,https://reporter.nih.gov/project-details/2237960,R29,1995,102119,-0.05224983468671817
"Therapeutic management based on protocols, and clinical algorithms is
 increasing in significance as medicine tackles more serious illnesses and
 must do so in cost-efficient manner. Clinical algorithms are appealing as
 they provide a good overview of the clinical state of a patient, identify
 key decisions and the outcomes associated with those decisions. They
 provide guidance and improve the quality of care without taking the
 control away from the physicians. Despite these advantages clinical
 algorithm design, specification, and delivery remain problematic. This
 research proposal seeks to overcome these difficulties, while retaining
 the advantages of clinical algorithms.
 
 We propose to develop a general framework for the representation for
 clinical algorithms and supporting medical knowledge, to communicate these
 algorithms effectively by extracting and presenting protocols specific
 individual patient's condition, and to provide decision aids for protocol-
 based patient care. This framework will use knowledge about therapy in the
 context of specific conditions to create patient-specific algorithms
 refined from the general-purpose algorithms. Aspects not pertinent to the
 case will be removed. The derivation of the specialized algorithms from
 the general algorithm will be recorded and used to explain the logic of
 the specialized algorithms. Because the algorithms will be tailored to the
 patient context, they will be detailed, yet manageable. This will avoid
 the over-simplifications of current ""one-size-fits-all"" algorithm. Mixed
 graphic and textual presentation techniques will be used to develop a
 user-friendly interface with explanation facilities integrated with
 patient information systems. We will test the framework by developing
 operational systems for hemodynamic resuscitation and management of
 circulatory shock in critically ill postoperative patients.
 
 We have chosen the domain of the management of circulatory failure in
 critically ill postoperative patients because: (a) circulatory failure is
 the significant medical problem affecting many postoperative patients
 where proper management can mean the difference between life and death,
 (b) we have a long track record of on-going activities in development of
 objective treatment methodologies in this area, (c) we have developed and
 tested a general clinical algorithm for management of high risk
 postoperative surgical patients, (d) our recent attempts to extend and
 refine the initial algorithm using traditional approaches have been
 frustrated by the complexity of the task. The proposed framework will
 support the creation of detailed therapeutic management algorithms and
 decision support systems to implement these algorithms in daily practice.
 artificial intelligence; computer assisted medical decision making; computer assisted patient care; computer graphics /printing; computer program /software; critical care; hemodynamics; hemorrhagic shock; human data; patient care management; postoperative complications CLINICAL MANAGEMENT OF CRITICAL ILLNESS USING AI","Therapeutic management based on protocols, and clinical algorithms is
 increasing in significance as medicine tackles more serious illnesses and
 must do so in cost-efficient manner. Clinical algorithms are appealing as
 they provide a good overview of the clinical state of a patient, identify
 key decisions and the outcomes associated with those decisions. They
 provide guidance and improve the quality of care without taking the
 control away from the physicians. Despite these advantages clinical
 algorithm design, specification, and delivery remain problematic. This
 research proposal seeks to overcome these difficulties, while retaining
 the advantages of clinical algorithms.
 
 We propose to develop a general framework for the representation for
 clinical algorithms and supporting medical knowledge, to communicate these
 algorithms effectively by extracting and presenting protocols specific
 individual patient's condition, and to provide decision aids for protocol-
 based patient care. This framework will use knowledge about therapy in the
 context of specific conditions to create patient-specific algorithms
 refined from the general-purpose algorithms. Aspects not pertinent to the
 case will be removed. The derivation of the specialized algorithms from
 the general algorithm will be recorded and used to explain the logic of
 the specialized algorithms. Because the algorithms will be tailored to the
 patient context, they will be detailed, yet manageable. This will avoid
 the over-simplifications of current ""one-size-fits-all"" algorithm. Mixed
 graphic and textual presentation techniques will be used to develop a
 user-friendly interface with explanation facilities integrated with
 patient information systems. We will test the framework by developing
 operational systems for hemodynamic resuscitation and management of
 circulatory shock in critically ill postoperative patients.
 
 We have chosen the domain of the management of circulatory failure in
 critically ill postoperative patients because: (a) circulatory failure is
 the significant medical problem affecting many postoperative patients
 where proper management can mean the difference between life and death,
 (b) we have a long track record of on-going activities in development of
 objective treatment methodologies in this area, (c) we have developed and
 tested a general clinical algorithm for management of high risk
 postoperative surgical patients, (d) our recent attempts to extend and
 refine the initial algorithm using traditional approaches have been
 frustrated by the complexity of the task. The proposed framework will
 support the creation of detailed therapeutic management algorithms and
 decision support systems to implement these algorithms in daily practice.
",2237763,R01LM005324,['R01LM005324'],LM,https://reporter.nih.gov/project-details/2237763,R01,1995,252773,-0.029356469708609
"This proposal has four major divisions:
 
 1. The continuation of computational support for the molecular biology
 research community.
 
 2. The dissemination of newly developed computational technologies and
 application examples.
 
 3.The continuation of a multi-level training program with a new emphasis on
 the training of graduate students with strong mathematical and physical
 science backgrounds.
 
 4.The continuation of the research and development program in computational
 molecular biology closely coupled experimental research laboratories, with
 a new emphasis on the integration of protein structural information into
 sequence functional analyses.
 
 A number of direction changes from the previous project periods are planned
 due to the changing nature and sophistication of molecular biology
 computational needs:
 
 l. a. Training in computational methods: To develop and provide training in
 the use and limitations of computational methods applicable to molecular
 biology, with a new emphasis on accessing, evaluating and using the wealth
 of network accessible computational and database services.
 
 b. Training in molecular biological applications: To expand the training of
 physical and computational scientists with strong analytical background in
 the analyses of molecular biological problems.
 
 2. a. The development and evaluation of methods for function identification
 that integrate the available information obtained from various diagnostic
 pattern and database searches with other information such as protein
 structure, homolog family membership, genetic regulation signals, and
 enzymatic function(s) associations.
 
 b. The development and automation of new and existing methods for the
 exploitation of determined protein structure information in experimental
 design and analyses by non structural experts.
 artificial intelligence; biomedical equipment resource; computer assisted sequence analysis; computer center; computer network; computer program /software; computer simulation; information dissemination; information retrieval; molecular biology; protein engineering; protein structure function; training; workshop BIOMOLECULAR ENGINEERING RESEARCH CENTER","This proposal has four major divisions:
 
 1. The continuation of computational support for the molecular biology
 research community.
 
 2. The dissemination of newly developed computational technologies and
 application examples.
 
 3.The continuation of a multi-level training program with a new emphasis on
 the training of graduate students with strong mathematical and physical
 science backgrounds.
 
 4.The continuation of the research and development program in computational
 molecular biology closely coupled experimental research laboratories, with
 a new emphasis on the integration of protein structural information into
 sequence functional analyses.
 
 A number of direction changes from the previous project periods are planned
 due to the changing nature and sophistication of molecular biology
 computational needs:
 
 l. a. Training in computational methods: To develop and provide training in
 the use and limitations of computational methods applicable to molecular
 biology, with a new emphasis on accessing, evaluating and using the wealth
 of network accessible computational and database services.
 
 b. Training in molecular biological applications: To expand the training of
 physical and computational scientists with strong analytical background in
 the analyses of molecular biological problems.
 
 2. a. The development and evaluation of methods for function identification
 that integrate the available information obtained from various diagnostic
 pattern and database searches with other information such as protein
 structure, homolog family membership, genetic regulation signals, and
 enzymatic function(s) associations.
 
 b. The development and automation of new and existing methods for the
 exploitation of determined protein structure information in experimental
 design and analyses by non structural experts.
",2237714,P41LM005205,['P41LM005205'],LM,https://reporter.nih.gov/project-details/2237714,P41,1995,897627,-0.002890067195314146
  A CANCER RADIOTHERAPY EXPERT SYSTEM USING SIMULATION,,2237618,R01LM004174,['R01LM004174'],LM,https://reporter.nih.gov/project-details/2237618,R01,1995,197188,-0.08720628914183401
"The public's perception of the risk of HIV infection by blood transfusion
 has greatly intensified the concern regarding safety of the blood supply,
 thus, making transfusion safety a matter of national priority. A major
 constraint in transfusion safety is that there is no means for the
 systematic collection and analysis of indicents of transfusion medicine
 errors. However, incident reporting systems have been developed in other
 error-critical fields including aviation, nuclear power, and
 anesthesiology. These systems can be used as guiding models for the
 development of an similar reporting system in transfusion medicine.
 Therefore, the specific aims of this project are to: (l) design a
 prototype reporting system for the collection and classification of
 incidents with the potential for compromising the safety of the blood
 supply, (2) develop and construct an operational prototype reporting
 system based upon the design criteria, (3) demonstrate the effectiveness
 of the reporting system in collecting, storing, and classifying
 information related to safety and human error at multiple sites through
 implementation testing, (4) derive rational strategies for enhancement of
 human performance and safety based upon the analysis of the classification
 of error types contained in the prototype data system, (5) evaluate the
 prototype reporting system's effectiveness, document the development
 process, and report project outcomes. This project will be carried out as
 an interdisciplinary effort involving experts from the fields of
 transfusion medicine, education and training, cognitive psychology,
 artificial intelligence, aviation safety, and nuclear power. This will be
 achieved using consensus development. The system will be implemented and
 tested in three blood centers (Blood Care of Dallas, Dallas, Texas; New
 York Blood Center, New York City, NY; Oklahoma Blood Institute, Oklahoma
 City, OK) and three hospital transfusion services (Parkland Memorial
 Hospital, Dallas, TX; New York University Medical Center, New York City,
 NY; University of Southern California Medical Center, Los Angeles, CA).
 This prototype system may well serve as a national model for improving
 safety of the nation's blood supply.
 blood banks; data collection methodology /evaluation; information system analysis; method development REPORTING SYSTEM TO IMPROVE SAFETY OF THE BLOOD SUPPLY","The public's perception of the risk of HIV infection by blood transfusion
 has greatly intensified the concern regarding safety of the blood supply,
 thus, making transfusion safety a matter of national priority. A major
 constraint in transfusion safety is that there is no means for the
 systematic collection and analysis of indicents of transfusion medicine
 errors. However, incident reporting systems have been developed in other
 error-critical fields including aviation, nuclear power, and
 anesthesiology. These systems can be used as guiding models for the
 development of an similar reporting system in transfusion medicine.
 Therefore, the specific aims of this project are to: (l) design a
 prototype reporting system for the collection and classification of
 incidents with the potential for compromising the safety of the blood
 supply, (2) develop and construct an operational prototype reporting
 system based upon the design criteria, (3) demonstrate the effectiveness
 of the reporting system in collecting, storing, and classifying
 information related to safety and human error at multiple sites through
 implementation testing, (4) derive rational strategies for enhancement of
 human performance and safety based upon the analysis of the classification
 of error types contained in the prototype data system, (5) evaluate the
 prototype reporting system's effectiveness, document the development
 process, and report project outcomes. This project will be carried out as
 an interdisciplinary effort involving experts from the fields of
 transfusion medicine, education and training, cognitive psychology,
 artificial intelligence, aviation safety, and nuclear power. This will be
 achieved using consensus development. The system will be implemented and
 tested in three blood centers (Blood Care of Dallas, Dallas, Texas; New
 York Blood Center, New York City, NY; Oklahoma Blood Institute, Oklahoma
 City, OK) and three hospital transfusion services (Parkland Memorial
 Hospital, Dallas, TX; New York University Medical Center, New York City,
 NY; University of Southern California Medical Center, Los Angeles, CA).
 This prototype system may well serve as a national model for improving
 safety of the nation's blood supply.
",2231869,R01HL053772,['R01HL053772'],HL,https://reporter.nih.gov/project-details/2231869,R01,1995,300451,-0.03795697602007247
  EVALUATION OF AN EXPERT SYSTEM FOR SUBJECT INDEXING,,2320015,01LM053516,['N01LM053516'],LM,https://reporter.nih.gov/project-details/2320015,N01,1995,52877,-0.01814431109068372
"The Family Planning Council of Southeastern Pennsylvania, in collaboration
 with the Cancer Prevention Research Center of the University of Rhode
 Island, is proposing an innovative study designed to address the risk
 behaviors associated with a recent alarming increase in the incidence of
 cervical neoplasias among young women. The proposed study is designed to
 develop, implement and evaluate interventions that increase consistent
 condom use and decrease cigarette smoking among 1,800 low income female
 youth aged 14-17 years who obtain family planning services at four diverse
 federally-funded family planning clinics. Smoking and unprotected sexual
 intercourse have been found to be independently associated with increased
 risks of cervical cancer in this population The proposed intervention is
 based on the Transtheoretical or Stages of Change model combined with one
 of the most promising modalities for reaching youth, an interactive
 computer based expert system whose efficacy will be evaluated alone and in
 combination with an adaptation of the anticipatory counseling model. Data
 will be collected at four points during a nine-month intervention period
 and at six-month intervals for 18 months post.intervention to assess
 effects over time. In addition, the intervention is aimed at increasing
 utilization of comprehensive, gynecologic health care including routine
 Pap smear screening, follow.up colposcopic examination and treatment of
 cervical dysplasia, the precursor of cervical cancers, when indicated.
 The proposed study represents the combined expertise of family planning
 researchers and service providers with considerable experience working
 with economically disadvantaged females and behavioral scientists with
 extensive research expertise in high risk behavior change.
 adolescence (12-20); cancer prevention; cervix neoplasms; computer human interaction; condoms; female; high risk behavior /lifestyle; human subject; human therapy evaluation; longitudinal human study; tobacco abuse CHANGING TEEN BEHAVIORS--CERVICAL CANCER PREVENTION","The Family Planning Council of Southeastern Pennsylvania, in collaboration
 with the Cancer Prevention Research Center of the University of Rhode
 Island, is proposing an innovative study designed to address the risk
 behaviors associated with a recent alarming increase in the incidence of
 cervical neoplasias among young women. The proposed study is designed to
 develop, implement and evaluate interventions that increase consistent
 condom use and decrease cigarette smoking among 1,800 low income female
 youth aged 14-17 years who obtain family planning services at four diverse
 federally-funded family planning clinics. Smoking and unprotected sexual
 intercourse have been found to be independently associated with increased
 risks of cervical cancer in this population The proposed intervention is
 based on the Transtheoretical or Stages of Change model combined with one
 of the most promising modalities for reaching youth, an interactive
 computer based expert system whose efficacy will be evaluated alone and in
 combination with an adaptation of the anticipatory counseling model. Data
 will be collected at four points during a nine-month intervention period
 and at six-month intervals for 18 months post.intervention to assess
 effects over time. In addition, the intervention is aimed at increasing
 utilization of comprehensive, gynecologic health care including routine
 Pap smear screening, follow.up colposcopic examination and treatment of
 cervical dysplasia, the precursor of cervical cancers, when indicated.
 The proposed study represents the combined expertise of family planning
 researchers and service providers with considerable experience working
 with economically disadvantaged females and behavioral scientists with
 extensive research expertise in high risk behavior change.
",2105791,R01CA063745,['R01CA063745'],CA,https://reporter.nih.gov/project-details/2105791,R01,1995,609482,-0.17035510151251182
"Skin cancer is a major public health problem with a major avoidable
 causative factor (solar ultraviolet radiation) and substantial potential
 for minimization of morbidity and mortality by early detection.
 Individuals can minimize exposure to solar ultraviolet by simple behavior
 changes but many do not, and primary care physicians can play an
 effective role in counseling and screening patients for skin cancer, but
 also typically do not.  This project will develop, implement, and
 evaluate in a randomize controlled trial an innovative multicomponent
 educational intervention initiated in a high exposure setting (among sun
 bathers at the beach) and followed by a personalized expert system
 intervention delivered by mail (the ""beach/expert"" component).  This
 project will also develop, implement, and evaluate, in a controlled
 trial, an educational intervention for primary care physicians to
 increase their awareness and knowledge about skin cancer and their skills
 in counseling about and screening for skin cancer (the ""physician""
 component).  Both components are based on the transtheoretical (""stage
 of change"") model of behavior change.  The intervention package delivered
 at the beach includes elements appropriate to each stage.  The expert
 system is an individualized stage-specific intervention.  The physician
 component will include training in stage-appropriate counseling of
 patients as well as in early detection skills.  Ultimately, the
 beach/expert component seeks to demonstrate the potential effectiveness
 of beach-based intervention programs for future use in the high risk
 beach-going population, and in the broader context of community-based,
 public health interventions; and to better define the processes and
 determinants leading to behavior change for sun exposure, with consequent
 generalization of the transtheoretical model of behavior change for
 cancer prevention.  We also seek to demonstrate how a stage-based
 approach may be integrated with comprehensive educational programs
 developed to enhanced the effectiveness of physician-based cancer
 prevention.  The overall project is designed to be disseminable so that
 the results obtained may be used in public health efforts to decrease the
 incidence, morbidity, and mortality from skin cancer.
 attitude; behavior modification; cancer prevention; early diagnosis; education evaluation /planning; health behavior; health care personnel education; human subject; neoplasm /cancer education; primary care physician; skin neoplasms; sunscreens SKIN CANCER PREVENTION\CONTROL--EDUCATION RESEARCH","Skin cancer is a major public health problem with a major avoidable
 causative factor (solar ultraviolet radiation) and substantial potential
 for minimization of morbidity and mortality by early detection.
 Individuals can minimize exposure to solar ultraviolet by simple behavior
 changes but many do not, and primary care physicians can play an
 effective role in counseling and screening patients for skin cancer, but
 also typically do not.  This project will develop, implement, and
 evaluate in a randomize controlled trial an innovative multicomponent
 educational intervention initiated in a high exposure setting (among sun
 bathers at the beach) and followed by a personalized expert system
 intervention delivered by mail (the ""beach/expert"" component).  This
 project will also develop, implement, and evaluate, in a controlled
 trial, an educational intervention for primary care physicians to
 increase their awareness and knowledge about skin cancer and their skills
 in counseling about and screening for skin cancer (the ""physician""
 component).  Both components are based on the transtheoretical (""stage
 of change"") model of behavior change.  The intervention package delivered
 at the beach includes elements appropriate to each stage.  The expert
 system is an individualized stage-specific intervention.  The physician
 component will include training in stage-appropriate counseling of
 patients as well as in early detection skills.  Ultimately, the
 beach/expert component seeks to demonstrate the potential effectiveness
 of beach-based intervention programs for future use in the high risk
 beach-going population, and in the broader context of community-based,
 public health interventions; and to better define the processes and
 determinants leading to behavior change for sun exposure, with consequent
 generalization of the transtheoretical model of behavior change for
 cancer prevention.  We also seek to demonstrate how a stage-based
 approach may be integrated with comprehensive educational programs
 developed to enhanced the effectiveness of physician-based cancer
 prevention.  The overall project is designed to be disseminable so that
 the results obtained may be used in public health efforts to decrease the
 incidence, morbidity, and mortality from skin cancer.
",2082647,R01AR043051,['R01AR043051'],AR,https://reporter.nih.gov/project-details/2082647,R01,1995,376345,-0.024706993530646083
"The long-term objective of this research is to gain an understanding of
 neural mechanisms and information processing principles involved in sensory
 acquisition in vertebrate sensory systems.
 The research is focused on those aspects of sensory processing in which
 the nervous system actively influences the content and quality of
 incoming sensory information.  In general, the neural mechanisms under
 investigation fall into two categories, those involved in the control of
 filtering properties in sensory processing pathways (adaptive signal
 processing aspects) and those involved in the active positioning of
 peripheral sensory structures (motor control aspects).  The research
 proposed here will investigate both adaptive signal processing and motor
 control aspects of sensory acquisition in the electrosensory system of
 the weakly electric fish, Apteronotus leptorhyncus (brown ghost knife
 fish).  Adaptive signal processing studies will explore the neural
 mechanisms involved in the descending modulation of stimulus filtering
 properties in the first-order electrosensory nucleus (the electrosensory
 lateral line lobe, or ELL).  Motor control studies will quantify the
 behavioral strategy use by these fish when carrying out electrosensory
 discrimination tasks.  The research approach relies heavily on computer
 modeling and simulation techniques to elucidate underlying neural
 mechanisms.  Biologically-detailed computer simulations of electrosensory
 processing will be constructed, including:  (1) finite-element
 simulations of peripheral electrical image formation, (2) time-domain
 models of primary afferent response dynamics, (3) compartmental models of
 ELL pyramidal cells, and (4) network models of electrosensory processing
 in the ELL.  In vivo single-unit recordings from primary afferents and
 first-order electrosensory neurons will be carried out to guide and
 constrain the development of the computer models.  These experiments will
 involve quantifying the threshold, gain, and time-domain response
 properties of primary afferents and the gain and spatiotemporal tuning
 properties of pyramidal cells in the ELL.  The behavior of weakly
 electric fish performing electrosensory discrimination tasks will be
 recorded on videotape and subsequently analyzed to extract information
 about the control of body position and velocity during sensory
 acquisition.  Spatiotemporal patterns of afferent activation will be
 reconstructed and compared with the spatiotemporal tuning properties of
 first-order electrosensory neurons.  Stochastic optimal estimation theory
 will be used to test the hypothesis that the stimulus filtering
 accomplished by the primary afferents and first-order electrosensory
 neurons is near-optimal for extracting estimates of the size and location
 of nearby objects from the electrical images generated on the fish's body
 surface.
 Osteichthyes; action potentials; alternatives to animals in research; artificial intelligence; biophysics; computational neuroscience; computer program /software; computer simulation; electric field; electrophysiology; electrostimulus; fish electric organ; histology; image processing; ion transport; lateral line; mathematical model; neural conduction; neural information processing; neural transmission; predation; pyramidal cells; sensorimotor system; sensory discrimination; sensory mechanism; sensory signal detection; sensory thresholds; somatic afferent nerve; somesthesis; videotape /videodisc CONTROL OF SENSORY ACQUISITION IN WEAKLY ELECTRIC FISH","The long-term objective of this research is to gain an understanding of
 neural mechanisms and information processing principles involved in sensory
 acquisition in vertebrate sensory systems.
 The research is focused on those aspects of sensory processing in which
 the nervous system actively influences the content and quality of
 incoming sensory information.  In general, the neural mechanisms under
 investigation fall into two categories, those involved in the control of
 filtering properties in sensory processing pathways (adaptive signal
 processing aspects) and those involved in the active positioning of
 peripheral sensory structures (motor control aspects).  The research
 proposed here will investigate both adaptive signal processing and motor
 control aspects of sensory acquisition in the electrosensory system of
 the weakly electric fish, Apteronotus leptorhyncus (brown ghost knife
 fish).  Adaptive signal processing studies will explore the neural
 mechanisms involved in the descending modulation of stimulus filtering
 properties in the first-order electrosensory nucleus (the electrosensory
 lateral line lobe, or ELL).  Motor control studies will quantify the
 behavioral strategy use by these fish when carrying out electrosensory
 discrimination tasks.  The research approach relies heavily on computer
 modeling and simulation techniques to elucidate underlying neural
 mechanisms.  Biologically-detailed computer simulations of electrosensory
 processing will be constructed, including:  (1) finite-element
 simulations of peripheral electrical image formation, (2) time-domain
 models of primary afferent response dynamics, (3) compartmental models of
 ELL pyramidal cells, and (4) network models of electrosensory processing
 in the ELL.  In vivo single-unit recordings from primary afferents and
 first-order electrosensory neurons will be carried out to guide and
 constrain the development of the computer models.  These experiments will
 involve quantifying the threshold, gain, and time-domain response
 properties of primary afferents and the gain and spatiotemporal tuning
 properties of pyramidal cells in the ELL.  The behavior of weakly
 electric fish performing electrosensory discrimination tasks will be
 recorded on videotape and subsequently analyzed to extract information
 about the control of body position and velocity during sensory
 acquisition.  Spatiotemporal patterns of afferent activation will be
 reconstructed and compared with the spatiotemporal tuning properties of
 first-order electrosensory neurons.  Stochastic optimal estimation theory
 will be used to test the hypothesis that the stimulus filtering
 accomplished by the primary afferents and first-order electrosensory
 neurons is near-optimal for extracting estimates of the size and location
 of nearby objects from the electrical images generated on the fish's body
 surface.
",2248766,R29MH049242,['R29MH049242'],MH,https://reporter.nih.gov/project-details/2248766,R29,1995,94791,-0.017713939730010106
  PRECISE BREAST CANCER STAGING WITH AN EXPERT SYSTEM,,2108250,R43CA065314,['R43CA065314'],CA,https://reporter.nih.gov/project-details/2108250,R43,1995,70361,-0.011566087842029909
"A significant fraction of all courses of external-beam radiotherapy have
 unplanned interruptions, i.e. scheduled treatments are missed. Because of
 the kinetics of cellular damage repair and tumor repopulation, designing
 extra or augmented treatments to compensate for the missed fractions is
 non-trivial. Since some individualized compensation regimen must be
 chosen, it should be rationalized using state-of-the-art radiobiology.  In
 Phase I we wrote prototype software (RIC, the Radiotherapy Interruption
 Compensator) which produces a range of practical options for the
 radiotherapist to compensate for unplanned interruptions during an
 extended radiotherapy regimen. It calculates what doses are needed to
 produce equal tumor control (or, alternately, equal late normal-tissue
 complications) when a proposed interruption-compensated regimen is
 substituted for the originally planned protocol. The program is designed
 as an ""expert system"", making available to any practicing radiotherapist
 the expertise of leading practitioners, in a personal-computer based,
 user-friendly package.
 
 Having demonstrated feasibility, we propose, in this Phase II application,
 to continue the research effort initiated in Phase I. The goal is to
 produce, at the end of the Phase II period, a marketable product which is
 a) scientifically state-of-the-art, b) designed and written with
 appropriate quality control, c) robust to use, d) credible to practicing
 clinicians.
 
 PROPOSED COMMERCIAL APPLICATION: Potential market consists of more than
 2,500 Radiation Oncology facilities worldwide.  Potentially about 1-4
 copies of the software purchased by each facility, at about $900 per copy.
 artificial intelligence; computer assisted patient care; computer human interaction; computer system design /evaluation; mathematical model; neoplasm /cancer radiation therapy; personal computers; radiation therapy dosage; radiobiology SOFTWARE TO COMPENSATE FOR INTERRUPTIONS IN RADIOTHERAPY","A significant fraction of all courses of external-beam radiotherapy have
 unplanned interruptions, i.e. scheduled treatments are missed. Because of
 the kinetics of cellular damage repair and tumor repopulation, designing
 extra or augmented treatments to compensate for the missed fractions is
 non-trivial. Since some individualized compensation regimen must be
 chosen, it should be rationalized using state-of-the-art radiobiology.  In
 Phase I we wrote prototype software (RIC, the Radiotherapy Interruption
 Compensator) which produces a range of practical options for the
 radiotherapist to compensate for unplanned interruptions during an
 extended radiotherapy regimen. It calculates what doses are needed to
 produce equal tumor control (or, alternately, equal late normal-tissue
 complications) when a proposed interruption-compensated regimen is
 substituted for the originally planned protocol. The program is designed
 as an ""expert system"", making available to any practicing radiotherapist
 the expertise of leading practitioners, in a personal-computer based,
 user-friendly package.
 
 Having demonstrated feasibility, we propose, in this Phase II application,
 to continue the research effort initiated in Phase I. The goal is to
 produce, at the end of the Phase II period, a marketable product which is
 a) scientifically state-of-the-art, b) designed and written with
 appropriate quality control, c) robust to use, d) credible to practicing
 clinicians.
 
 PROPOSED COMMERCIAL APPLICATION: Potential market consists of more than
 2,500 Radiation Oncology facilities worldwide.  Potentially about 1-4
 copies of the software purchased by each facility, at about $900 per copy.
",2106039,R44CA063897,['R44CA063897'],CA,https://reporter.nih.gov/project-details/2106039,R44,1995,346512,-0.2679461933733747
"DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the
 ability to understand conversation  under difficult listening conditions,
 such  as  in highly reverberant rooms  or in  gatherings where several
 persons are  talking simultaneously,  affects a substantial portion  of
 elderly individuals.  This impairment may vary in severity, but only in
 very few cases can it be overcome  by the use  of currently available
 prosthetic devices.  Attempts to alleviate  this  impairment  have been
 impeded by  the fact that  neither the precise  characteristics of  the
 intact  process in  the young, nor the causes  of its  breakdown in the
 old, are currently well understood.
 
 The proposed research represents a continuation of work aimed at
 investigating  the ability of  both elderly and young individuals to
 understand speech under  non-optimal  listening conditions,  i.e.,
 perceptual separation  of a  speech  target from simultaneously ongoing
 irrelevant ""noise"".  The research  has two  main  objectives: (1) to
 investigate,  in elderly and  in young listeners, the  perceptual
 processes (in  particular,  spatial resolution and resolution of temporal
 fluctuations) which  play a role  in the separation of  simultaneously
 presented relevant and irrelevant  auditory signals; and (2) to study
 a group  of elderly individuals over a  five year period, in order to
 detect initial or  progressive deterioration of the ability to separate
 simultaneous signals and  to determine the correlates of this
 deterioration.
 
 These objectives  will be achieved by  testing selected groups of elderly
 and  young individuals on standard  and non-standard audiological tests
 as  well as  psychophysical  tests.  Spatial hearing  will be assessed
 in a simulated  free field.  Multidimensional  auditory performance
 profiles of subjects  will  be  defined  through  principal  component
 analysis  and other  multivariate  statistical methods.
 
 The  major scientific  significance of the  proposed  study is  that it
 will  provide a more precise  definition of auditory temporal and
 spatial processes  that allow  for the perceptual separation  of speech
 and  background noise and  will also identify precise auditory processes
 affected by aging.  The clinical  significance of  the study is that  it
 will establish a  multidimensional data  base of auditory  capabilities
 in  elderly individuals with  mild to  moderate  sensorineural  hearing
 loss,  and  may identify  auditory processes which, if  impaired, will
 help predict  impending deterioration of  speech understanding  under
 non-optimal  listening conditions.  This  work will have an impact on a
 wide-spread impairment of verbal communication in the elderly.
 adolescence (12-20); aging; audiometry; auditory discrimination; binaural hearing; human middle age (35-64); human old age (65+); human subject; longitudinal human study; noise; perception; psychoacoustics; sensorineural hearing loss; space perception; speech; speech recognition; young adult human (21-34) SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING","DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the
 ability to understand conversation  under difficult listening conditions,
 such  as  in highly reverberant rooms  or in  gatherings where several
 persons are  talking simultaneously,  affects a substantial portion  of
 elderly individuals.  This impairment may vary in severity, but only in
 very few cases can it be overcome  by the use  of currently available
 prosthetic devices.  Attempts to alleviate  this  impairment  have been
 impeded by  the fact that  neither the precise  characteristics of  the
 intact  process in  the young, nor the causes  of its  breakdown in the
 old, are currently well understood.
 
 The proposed research represents a continuation of work aimed at
 investigating  the ability of  both elderly and young individuals to
 understand speech under  non-optimal  listening conditions,  i.e.,
 perceptual separation  of a  speech  target from simultaneously ongoing
 irrelevant ""noise"".  The research  has two  main  objectives: (1) to
 investigate,  in elderly and  in young listeners, the  perceptual
 processes (in  particular,  spatial resolution and resolution of temporal
 fluctuations) which  play a role  in the separation of  simultaneously
 presented relevant and irrelevant  auditory signals; and (2) to study
 a group  of elderly individuals over a  five year period, in order to
 detect initial or  progressive deterioration of the ability to separate
 simultaneous signals and  to determine the correlates of this
 deterioration.
 
 These objectives  will be achieved by  testing selected groups of elderly
 and  young individuals on standard  and non-standard audiological tests
 as  well as  psychophysical  tests.  Spatial hearing  will be assessed
 in a simulated  free field.  Multidimensional  auditory performance
 profiles of subjects  will  be  defined  through  principal  component
 analysis  and other  multivariate  statistical methods.
 
 The  major scientific  significance of the  proposed  study is  that it
 will  provide a more precise  definition of auditory temporal and
 spatial processes  that allow  for the perceptual separation  of speech
 and  background noise and  will also identify precise auditory processes
 affected by aging.  The clinical  significance of  the study is that  it
 will establish a  multidimensional data  base of auditory  capabilities
 in  elderly individuals with  mild to  moderate  sensorineural  hearing
 loss,  and  may identify  auditory processes which, if  impaired, will
 help predict  impending deterioration of  speech understanding  under
 non-optimal  listening conditions.  This  work will have an impact on a
 wide-spread impairment of verbal communication in the elderly.
",2049983,R01AG007998,['R01AG007998'],AG,https://reporter.nih.gov/project-details/2049983,R01,1995,145134,-0.015905248758203504
"This proposal is a response to RFA #CA/HD-93-33, ""Rehabilitation and
 Psychosocial Research in Younger Women with Breast Cancer."" The objectives
 of this research are: 1) to test the benefits of a computer-based support
 system (CHESS, the Comprehensive Health Enhancement Support System) on the
 quality of life and disability of younger women with breast cancer; and,
 2) to begin to understand how and for what kind of people CHESS has that
 effect. CHESS uses expert systems, computer-mediated communication, data
 bases, and an easy-to-use interface to provide information, decision
 analysis, and social support services to women with breast cancer.
 
 Three hundred women with breast cancer under age 50 (including 100
 minority women) will be recruited from hospitals in Madison and Chicago
 and will be randomized into control and experimental groups. Subjects will
 include women with breast cancer at Stages I to IV, including recurrent
 breast cancer. A CHESS computer will be placed into the homes of the
 experimental group members for six months, while the control group will
 have access to standard medical practice for patient education and
 support, supplemented by a consistent set of written information on breast
 cancer. Both groups will be surveyed at pre-test, and with 2, 4 and 8
 month post-tests.
 
 The primary aim of this research is to understand the effect of CHESS on
 quality of -life for women with breast cancer. Specifically, we will
 examine whether access to CHESS produces more rapid and greater
 improvements in specific quality of life dimensions: functional,
 emotional, social-family, physical, and disease-specific complaints. We
 will also investigate impact of global quality of life as an exploratory
 analysis. The secondary aim is to begin to explore: l) whether CHESS has
 an effect on detailed functional performance and level of disability; and,
 2) how and for what kinds of patients (e.g., minorities, more or less
 educated, stage of disease, etc.) does CHESS have its effects on quality
 of life.
 artificial intelligence; breast neoplasm /cancer diagnosis; breast neoplasms; cancer information system; computer assisted medical decision making; data collection; early diagnosis; emotions; family; female; functional ability; health surveys; human subject; neoplasm /cancer classification /staging; neoplasm /cancer education; neoplasm /cancer relapse /recurrence; psychosocial service; quality of life; questionnaires; social support network; telecommunications PSYCHOSOCIAL SUPPORT FOR YOUNG WOMEN WITH BREAST CANCER","This proposal is a response to RFA #CA/HD-93-33, ""Rehabilitation and
 Psychosocial Research in Younger Women with Breast Cancer."" The objectives
 of this research are: 1) to test the benefits of a computer-based support
 system (CHESS, the Comprehensive Health Enhancement Support System) on the
 quality of life and disability of younger women with breast cancer; and,
 2) to begin to understand how and for what kind of people CHESS has that
 effect. CHESS uses expert systems, computer-mediated communication, data
 bases, and an easy-to-use interface to provide information, decision
 analysis, and social support services to women with breast cancer.
 
 Three hundred women with breast cancer under age 50 (including 100
 minority women) will be recruited from hospitals in Madison and Chicago
 and will be randomized into control and experimental groups. Subjects will
 include women with breast cancer at Stages I to IV, including recurrent
 breast cancer. A CHESS computer will be placed into the homes of the
 experimental group members for six months, while the control group will
 have access to standard medical practice for patient education and
 support, supplemented by a consistent set of written information on breast
 cancer. Both groups will be surveyed at pre-test, and with 2, 4 and 8
 month post-tests.
 
 The primary aim of this research is to understand the effect of CHESS on
 quality of -life for women with breast cancer. Specifically, we will
 examine whether access to CHESS produces more rapid and greater
 improvements in specific quality of life dimensions: functional,
 emotional, social-family, physical, and disease-specific complaints. We
 will also investigate impact of global quality of life as an exploratory
 analysis. The secondary aim is to begin to explore: l) whether CHESS has
 an effect on detailed functional performance and level of disability; and,
 2) how and for what kinds of patients (e.g., minorities, more or less
 educated, stage of disease, etc.) does CHESS have its effects on quality
 of life.
",2206219,R01HD032922,['R01HD032922'],HD,https://reporter.nih.gov/project-details/2206219,R01,1995,345521,-0.04355516736949109
"The solution to the protein folding problem is critical to an
 understanding of the naturally occurring proteins and their deliberately
 engineered counterparts.  Our principal goal is to elucidate the
 stereochemical code that governs protein folding and use it to formulate
 a practical folding algorithm.
 
 Of particular interest is the alpha-helix, first proposed by Pauling and
 coworkers.  Helices are a hallmark of protein structure, and much effort
 has been directed towards understanding which sequences can form stable
 helices.  Two converging lines of investigation in our lab have focused
 separately on the central and terminal residues within the helix.  For
 central residues, it is hypothesized that sidechain entropy is the
 principal factor that governs helix propensity.  For terminal residues,
 it is hypothesized that ""capping"" H-bonds between sidechain polar groups
 and the otherwise unsatisfied initial four amide hydrogens or final four
 carbonyl oxygens in the mainchain are the major factor in helix
 specificity.  Results from this work can be used predictively, and,
 unlike empirical strategies, such predictions are grounded in plausible
 thermodynamics.
 
 This work, which is well underway, will be extended to include other
 categories of secondary structure as well.  Following the approach used
 for helices, residues in beta-sheet can be modelled in each
 microenvironment, in either middle or edge strands and at the center or
 terminus of the strand.  Nonrepetitive structure can also be included.
 
 Preliminary results suggest that many residues have clearly
 understandable, sharply defined secondary structure preferences.  For
 example, valine in the unfolded state can populate all three sidechain
 conformers almost equivalently, but essentially only one conformer when
 in a helix.  The corresponding energy loss (T-delta-S) due solely to the
 reduction in sidechain entropy is approximately RTln3, of order
 physiological kT.  These energy terms, which cause residues to favor one
 type of secondary structure over others, though individually modest, are
 significant in the aggregate, when summed over an entire helix or sheet.
 The effect of a given class of secondary structure on the sidechain
 entropy and/or intra-molecular hydrogen bonding of a residue is
 tantamount to a stereochemical code.
 artificial intelligence; biochemical evolution; chemical models; computer program /software; computer simulation; conformation; globular protein; hydrogen bond; intermolecular interaction; physical model; protein folding; protein sequence; protein structure; stereochemistry; thermodynamics SELF-RECOGNITION IN GLOBULAR PROTEINS","The solution to the protein folding problem is critical to an
 understanding of the naturally occurring proteins and their deliberately
 engineered counterparts.  Our principal goal is to elucidate the
 stereochemical code that governs protein folding and use it to formulate
 a practical folding algorithm.
 
 Of particular interest is the alpha-helix, first proposed by Pauling and
 coworkers.  Helices are a hallmark of protein structure, and much effort
 has been directed towards understanding which sequences can form stable
 helices.  Two converging lines of investigation in our lab have focused
 separately on the central and terminal residues within the helix.  For
 central residues, it is hypothesized that sidechain entropy is the
 principal factor that governs helix propensity.  For terminal residues,
 it is hypothesized that ""capping"" H-bonds between sidechain polar groups
 and the otherwise unsatisfied initial four amide hydrogens or final four
 carbonyl oxygens in the mainchain are the major factor in helix
 specificity.  Results from this work can be used predictively, and,
 unlike empirical strategies, such predictions are grounded in plausible
 thermodynamics.
 
 This work, which is well underway, will be extended to include other
 categories of secondary structure as well.  Following the approach used
 for helices, residues in beta-sheet can be modelled in each
 microenvironment, in either middle or edge strands and at the center or
 terminus of the strand.  Nonrepetitive structure can also be included.
 
 Preliminary results suggest that many residues have clearly
 understandable, sharply defined secondary structure preferences.  For
 example, valine in the unfolded state can populate all three sidechain
 conformers almost equivalently, but essentially only one conformer when
 in a helix.  The corresponding energy loss (T-delta-S) due solely to the
 reduction in sidechain entropy is approximately RTln3, of order
 physiological kT.  These energy terms, which cause residues to favor one
 type of secondary structure over others, though individually modest, are
 significant in the aggregate, when summed over an entire helix or sheet.
 The effect of a given class of secondary structure on the sidechain
 entropy and/or intra-molecular hydrogen bonding of a residue is
 tantamount to a stereochemical code.
",2175511,R01GM029458,['R01GM029458'],GM,https://reporter.nih.gov/project-details/2175511,R01,1995,279526,-0.029855055518916304
"Our overall objective is to solve the problem of how proteins fold into
 their native conformations.  For this purpose, we are using the
 methodology of protein chemistry and developing and applying experimental
 and theoretical techniques to provide an understanding of the internal
 interactions that stabilize native proteins in aqueous solution.  This
 project is concerned with the theoretical work which involves the use of
 empirical potentials (including the effects of hydration and entropy) in
 various computational approaches to study the interactions in protein
 folding.  Emphasis is currently being placed on solving the multiple-
 minima problem arising from the existence of numerous local minima in the
 potential energy surface of the protein, the objective being to locate the
 global minimum on this surface.  An understanding of the interactions in
 proteins is of potential applicability to the elucidation of the role of
 conformation in biological processes, e.g. the undesirable association of
 sickle-cell hemoglobin, or the induction of an oncogene product whose
 properties involve a conformational change when only one amino acid
 residue in the sequence is changed.
 artificial intelligence; chemical bond; chemical hydration; chemical stability; computer program /software; computer simulation; computer system design /evaluation; conformation; diffusion; intermolecular interaction; mathematical model; method development; protein folding; protein sequence; protein structure; quantum chemistry; structural biology; thermodynamics; water solution INTERNAL BONDING IN PROTEINS","Our overall objective is to solve the problem of how proteins fold into
 their native conformations.  For this purpose, we are using the
 methodology of protein chemistry and developing and applying experimental
 and theoretical techniques to provide an understanding of the internal
 interactions that stabilize native proteins in aqueous solution.  This
 project is concerned with the theoretical work which involves the use of
 empirical potentials (including the effects of hydration and entropy) in
 various computational approaches to study the interactions in protein
 folding.  Emphasis is currently being placed on solving the multiple-
 minima problem arising from the existence of numerous local minima in the
 potential energy surface of the protein, the objective being to locate the
 global minimum on this surface.  An understanding of the interactions in
 proteins is of potential applicability to the elucidation of the role of
 conformation in biological processes, e.g. the undesirable association of
 sickle-cell hemoglobin, or the induction of an oncogene product whose
 properties involve a conformational change when only one amino acid
 residue in the sequence is changed.
",2168973,R01GM014312,['R01GM014312'],GM,https://reporter.nih.gov/project-details/2168973,R01,1995,162206,-0.037999647845973795
"DESCRIPTION: Over  the past decade  a variety of  alternative computer
 based  modeling  techniques  have  been   introduced  which  show
 promise  for   the  construction of clinical decision aids. These
 techniques include statistical  regression  approaches such as
 generalized additive  modeling, classification  tree induction such as
 ID3 or CART, and multi-layer neural  networks. Logistic  regression
 models (LR) are currently central to most probabilistic predictive
 clinical  decision aids and are fundamental to comparative analyses of
 medical  care based  risk adjusted events. These newer techniques
 have been applied on  a larger scale in the last few years. They
 appear to have unique advantages in  selected circumstances. The
 successful use  of these methods, however, depends  on understanding
 their accuracy, performance, and model transportability.
 
 A  formal assessment of  these new techniques with four specific  aims
 is  proposed:  (1) to assess  and compare the  performance of
 different  models to  determine  the  factors which affect
 performance; (2)  to  develop automated  computer  based procedures
 for exploratory model  development for each method;  (3)  to develop
 hybrid models incorporating the strengths of each of the existing
 techniques, and  (4) to  determine the situations  that restrict  the
 transportability of these models.
 
 These specific aims will be  achieved in a three stage project. In
 the first  stage four approaches will be pursued:  (1) the
 mathematical properties of the  different  computational  algorithms
 for the  modeling  techniques will be studied;  (2) automated
 modeling procedures will be developed and utilized; (3)  the  factors
 that  affect  performance for  each  modeling technique  will  be
 explored and(4) new hybrid techniques will be developed and assessed.
 In the  second stage the methods  developed in the first stage will
 be used to create  and  test  models  that predict  cardiovascular
 events  on  data from  15,000  patients in  a prospective clinical
 trial. In the third stage the factors that  affect  the
 generalizability and transportability of models to  new datasets  will
 be explored  by repeated sampling and  model construction  on
 different  subsets of the cardiovascular database  including
 separating the database into  subsets from each of ten different
 hospitals.
 
 This   work  will  broaden  the  understanding of  these  important
 modeling  techniques  and their  potential contributions for
 clinical decision making,  health policy research,  and medical
 informatics.   New modeling  techniques  might be developed which
 incorporate elements from different techniques.
 artificial intelligence; cardiovascular disorder epidemiology; cardiovascular function; computer assisted medical decision making; computer simulation; disease /disorder proneness /risk; health care facility information system; human data; mathematical model; model design /development NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS","DESCRIPTION: Over  the past decade  a variety of  alternative computer
 based  modeling  techniques  have  been   introduced  which  show
 promise  for   the  construction of clinical decision aids. These
 techniques include statistical  regression  approaches such as
 generalized additive  modeling, classification  tree induction such as
 ID3 or CART, and multi-layer neural  networks. Logistic  regression
 models (LR) are currently central to most probabilistic predictive
 clinical  decision aids and are fundamental to comparative analyses of
 medical  care based  risk adjusted events. These newer techniques
 have been applied on  a larger scale in the last few years. They
 appear to have unique advantages in  selected circumstances. The
 successful use  of these methods, however, depends  on understanding
 their accuracy, performance, and model transportability.
 
 A  formal assessment of  these new techniques with four specific  aims
 is  proposed:  (1) to assess  and compare the  performance of
 different  models to  determine  the  factors which affect
 performance; (2)  to  develop automated  computer  based procedures
 for exploratory model  development for each method;  (3)  to develop
 hybrid models incorporating the strengths of each of the existing
 techniques, and  (4) to  determine the situations  that restrict  the
 transportability of these models.
 
 These specific aims will be  achieved in a three stage project. In
 the first  stage four approaches will be pursued:  (1) the
 mathematical properties of the  different  computational  algorithms
 for the  modeling  techniques will be studied;  (2) automated
 modeling procedures will be developed and utilized; (3)  the  factors
 that  affect  performance for  each  modeling technique  will  be
 explored and(4) new hybrid techniques will be developed and assessed.
 In the  second stage the methods  developed in the first stage will
 be used to create  and  test  models  that predict  cardiovascular
 events  on  data from  15,000  patients in  a prospective clinical
 trial. In the third stage the factors that  affect  the
 generalizability and transportability of models to  new datasets  will
 be explored  by repeated sampling and  model construction  on
 different  subsets of the cardiovascular database  including
 separating the database into  subsets from each of ten different
 hospitals.
 
 This   work  will  broaden  the  understanding of  these  important
 modeling  techniques  and their  potential contributions for
 clinical decision making,  health policy research,  and medical
 informatics.   New modeling  techniques  might be developed which
 incorporate elements from different techniques.
",2237926,R01LM005607,['R01LM005607'],LM,https://reporter.nih.gov/project-details/2237926,R01,1995,251015,0.019498607242339434
"Biomedical researchers are generating vast amounts of data and knowledge,
 at an accelerating pace.  Neither existing database systems nor existing
 frame knowledge representation systems have the capabilities required to
 support the development of large, shared repositories of biological
 information.  Therefore, SRI International proposes to integrate database
 and knowledge-representation technology to develop a biological
 knowledge-base management system (KBMS) with unprecedented power to en-
 code biological knowledge.  This KBMS will provide the expressive power
 needed to represent biological knowledge in all its complexity, will
 support the evolution of complex knowledge-base schemas, will enable
 multiple users to access large amounts of reliably stored information in
 a shared fashion, and will support inference over this knowledge.
 
 Our implementation efforts will build on an existing frame representation
 system called THEO.  We will extend THEO so that its underlying storage
 system utilizes a database management system to facilitate the
 construction of frame knowledge bases containing large numbers (millions)
 of persistent frames.  In addition, we will implement a collaboration
 subsystem that coordinates concurrent distributed development of large
 knowledge bases by multiple users.
 
 To exercise and validate our development efforts, this KBMS will be used
 in an application to construct a large biological knowledge base of
 intermediary metabolism.
 artificial intelligence; computer program /software; enzyme activity; information system analysis; information systems; metabolism BIOLOGICAL KNOWLEDGE-BASE MANAGEMENT SYSTEM","Biomedical researchers are generating vast amounts of data and knowledge,
 at an accelerating pace.  Neither existing database systems nor existing
 frame knowledge representation systems have the capabilities required to
 support the development of large, shared repositories of biological
 information.  Therefore, SRI International proposes to integrate database
 and knowledge-representation technology to develop a biological
 knowledge-base management system (KBMS) with unprecedented power to en-
 code biological knowledge.  This KBMS will provide the expressive power
 needed to represent biological knowledge in all its complexity, will
 support the evolution of complex knowledge-base schemas, will enable
 multiple users to access large amounts of reliably stored information in
 a shared fashion, and will support inference over this knowledge.
 
 Our implementation efforts will build on an existing frame representation
 system called THEO.  We will extend THEO so that its underlying storage
 system utilizes a database management system to facilitate the
 construction of frame knowledge bases containing large numbers (millions)
 of persistent frames.  In addition, we will implement a collaboration
 subsystem that coordinates concurrent distributed development of large
 knowledge bases by multiple users.
 
 To exercise and validate our development efforts, this KBMS will be used
 in an application to construct a large biological knowledge base of
 intermediary metabolism.
",2237800,R29LM005413,['R29LM005413'],LM,https://reporter.nih.gov/project-details/2237800,R29,1995,136815,-0.03183286684945493
"The Human Genome Project faces a number of formidable challenges.  Among        
these are the development of highly sensitive and accurate automated            
sequencing techniques, the development of optimal strategies for gene           
recognition in sequence data, and an improved understanding of gene             
function and regulation.  My intent in this Special Emphasis Research           
Career Award project is to address these and other related problems             
within the context of state-of-the-art genome research.  I believe that         
my background in theoretical and experimental particle physics provides         
me with a unique set of skills relevant to the solution of these and            
other problems.  The goals of this training program are to: 1) obtain a         
firm grounding in modern molecular biology and genetics with                    
specialization in genome research, 2) develop a set of skills in                
laboratory-based genome research through a first year project in                
physical mapping and DNA sequencing, and 3) develop a long term project         
focusing on DNA sequence acquisition and analysis which will utilize the        
analytical, computational and model building skills which I have                
developed as a physicist.  In particular, I would hope to develop               
optimum protocols for gene mapping and sequence assembly applicable to          
large scale genome analysis and to develop new and more efficient               
algorithms for gene sequence identification and interpretation.  In             
addition, I intend to pursue the development of advanced automated              
sequencing technology based on highly sensitive detectors and techniques        
developed for particle physics.  Achievement of either or both of these         
goals will greatly facilitate the formidable task faced by the                  
researchers involved in the Human Genome Project in accumulating and            
analyzing vast amounts of genomic DNA sequence.  This project will begin        
by immersion in the work currently being done on the physical mapping of        
human chromosome 11, and the sequencing of selected reference markers           
and cDNA clones, as part of the Salk Institute large scale physical             
mapping project.  The formal training I will receive during the tenure          
of the NCHGR/SERCA provides an ideal environment in which I can gain the        
necessary grounding in current techniques so that I can effectively work        
on the development of new techniques and strategies and contribute to           
the genome research of the future.                                              
 artificial chromosomes; artificial intelligence; chromosomes; complementary DNA; computer assisted sequence analysis; flow cytometry; genetic mapping; genetic markers; genetic techniques; genome; human genetic material tag; in situ hybridization; molecular biology; molecular genetics; nucleic acid sequence; technology /technique development; training MOLECULAR ANALYSIS OF THE HUMAN GENOME","The Human Genome Project faces a number of formidable challenges.  Among        
these are the development of highly sensitive and accurate automated            
sequencing techniques, the development of optimal strategies for gene           
recognition in sequence data, and an improved understanding of gene             
function and regulation.  My intent in this Special Emphasis Research           
Career Award project is to address these and other related problems             
within the context of state-of-the-art genome research.  I believe that         
my background in theoretical and experimental particle physics provides         
me with a unique set of skills relevant to the solution of these and            
other problems.  The goals of this training program are to: 1) obtain a         
firm grounding in modern molecular biology and genetics with                    
specialization in genome research, 2) develop a set of skills in                
laboratory-based genome research through a first year project in                
physical mapping and DNA sequencing, and 3) develop a long term project         
focusing on DNA sequence acquisition and analysis which will utilize the        
analytical, computational and model building skills which I have                
developed as a physicist.  In particular, I would hope to develop               
optimum protocols for gene mapping and sequence assembly applicable to          
large scale genome analysis and to develop new and more efficient               
algorithms for gene sequence identification and interpretation.  In             
addition, I intend to pursue the development of advanced automated              
sequencing technology based on highly sensitive detectors and techniques        
developed for particle physics.  Achievement of either or both of these         
goals will greatly facilitate the formidable task faced by the                  
researchers involved in the Human Genome Project in accumulating and            
analyzing vast amounts of genomic DNA sequence.  This project will begin        
by immersion in the work currently being done on the physical mapping of        
human chromosome 11, and the sequencing of selected reference markers           
and cDNA clones, as part of the Salk Institute large scale physical             
mapping project.  The formal training I will receive during the tenure          
of the NCHGR/SERCA provides an ideal environment in which I can gain the        
necessary grounding in current techniques so that I can effectively work        
on the development of new techniques and strategies and contribute to           
the genome research of the future.                                              
",2440343,K01HG000007,['K01HG000007'],HG,https://reporter.nih.gov/project-details/2440343,K01,1996,78818,-0.0993259542455452
  EXPERT SYSTEM FOR RECOMBINANT DNA INVENTION DISCLOSURE,,2235471,R44HL058327,['R44HL058327'],HL,https://reporter.nih.gov/project-details/2235471,R44,1996,319846,-0.033997430628076884
"In this FIRST award, using dentistry as a model, methods developed in a         
University of Washington pilot study will be extended to a larger sample        
to explore acute and chronic pain perceptions among patients and dentists       
in Scandinavia, China, and United States.  Specifically, we seek to             
determine how verbal descriptors are cognitively organized to reveal            
cultural influences on pain and coping remedies through a stepwise              
combination of ethnographic interviews and validations with participant         
observation and quantitative survey methods. 700 subjects will describe         
kinds of pains and ways of coping with them, especially acute and chronic       
orofacial pain.  Subject groups will be matched for age, socioeconomic          
status, gender, and education.                                                  
                                                                                
In a first interview, questions like 'What kinds of pain are there?' will       
be asked.  Subjects' statements will be recorded verbatim in their native       
languages with the aim to reveal differences in perceptual context and          
detail their relevance.  From this database, pain and coping remedy terms       
will be selected by specific criteria and used to construct crosscultural       
pain survey instruments.  In a second interview, subjects will use a card       
sort instrument to judge the similarity of pain and remedy concepts.            
Reasons for sorting choices will be recorded to reveal perceptual               
categories.  Grid matrices will also be used to match pains with                
subjects' descriptors and pain coping strategies.                               
                                                                                
All open-ended data will be analyzed by content and detailed pain               
narratives written for each culture.  Instrument data will be analyzed by       
multidimensional scaling and hierarchical clustering analyses.  Results         
will be validated by behavioral observations.  Reliability of the methods       
will be assessed by checking specified outcomes of quantitative indices;        
validity by how these indices relate to the findings of the qualitative         
phase.                                                                          
                                                                                
Our long-term objective is to understand how cultural influences such as        
ethnicity and professional socialization shape the perceived meanings of        
pain and pain coping remedies, with the intent to improve health-care           
communication in diagnosis and treatment.                                       
 China; Scandinavian country; United States; coping; culture; ethnic group; human subject; interview; pain threshold; perception; psychometrics; questionnaires CROSSCULTURAL STUDY OF PAIN--PATIENTS AND DENTISTS","In this FIRST award, using dentistry as a model, methods developed in a         
University of Washington pilot study will be extended to a larger sample        
to explore acute and chronic pain perceptions among patients and dentists       
in Scandinavia, China, and United States.  Specifically, we seek to             
determine how verbal descriptors are cognitively organized to reveal            
cultural influences on pain and coping remedies through a stepwise              
combination of ethnographic interviews and validations with participant         
observation and quantitative survey methods. 700 subjects will describe         
kinds of pains and ways of coping with them, especially acute and chronic       
orofacial pain.  Subject groups will be matched for age, socioeconomic          
status, gender, and education.                                                  
                                                                                
In a first interview, questions like 'What kinds of pain are there?' will       
be asked.  Subjects' statements will be recorded verbatim in their native       
languages with the aim to reveal differences in perceptual context and          
detail their relevance.  From this database, pain and coping remedy terms       
will be selected by specific criteria and used to construct crosscultural       
pain survey instruments.  In a second interview, subjects will use a card       
sort instrument to judge the similarity of pain and remedy concepts.            
Reasons for sorting choices will be recorded to reveal perceptual               
categories.  Grid matrices will also be used to match pains with                
subjects' descriptors and pain coping strategies.                               
                                                                                
All open-ended data will be analyzed by content and detailed pain               
narratives written for each culture.  Instrument data will be analyzed by       
multidimensional scaling and hierarchical clustering analyses.  Results         
will be validated by behavioral observations.  Reliability of the methods       
will be assessed by checking specified outcomes of quantitative indices;        
validity by how these indices relate to the findings of the qualitative         
phase.                                                                          
                                                                                
Our long-term objective is to understand how cultural influences such as        
ethnicity and professional socialization shape the perceived meanings of        
pain and pain coping remedies, with the intent to improve health-care           
communication in diagnosis and treatment.                                       
",2130957,R29DE009945,['R29DE009945'],DE,https://reporter.nih.gov/project-details/2130957,R29,1996,64581,-0.214883952088995
  EXPERT SYSTEM FOR AOD UTILIZATION REVIEW,,2047654,R43AA011044,['R43AA011044'],AA,https://reporter.nih.gov/project-details/2047654,R43,1996,96376,-0.04136418740837679
 artificial intelligence; computer system design /evaluation; health care facility information system; information system analysis IAIMS PLANNING AT IHC,,2238629,G08LM006237,['G08LM006237'],LM,https://reporter.nih.gov/project-details/2238629,G08,1996,89668,-0.08056485212065131
"Non-small cell lung cancer (NSCLC) is the leading cause of cancer morality      
in men and women in the United States, and the overall long-term survial is     
less than 15%.  Pathologic stage I makes up  25-35% of NSCLC cases and has      
a good prognosis.  However, cancer relapse and death rate in this subset is     
35 to 50% by 5 years.  Chemotherapy is beneficial for the treatment of          
several localized solid tumors after resection and may prove to be useful       
in the treatment of patients with stage I NSCLC.  Thr purpose of this           
project is to define tissue and serum tumor markers in patients with stage      
I NSCLC which predict for early cancer recurrence.  Pathologic stage I          
NSCLC was chosen for study to eliminate the significant influence of            
positive lymph nodes and distant metastases on survival.                        
Immunohistochemical staining will identify potential tissue tumor markers       
and radioimmunoassay (RIA) or enzyme-link immunosorbent assay (ELISA) will      
identify potential serum tumor markers.                                         
                                                                                
Specific aim #1 will examine a set of twelve tissue tumor markers in a          
retrospective cohort of 275 stage I NSCLC patients.  Markers are                
categorized by hypothetical method of action: molecular genetic markers         
(Kras, erbB-1, erbB-2, rb, p53, bcl-2), markers of metastatic propensity        
(angiogenesis factor viii), proliferation markers (K1-67) and markers of        
cellular differentiation (Blood group A, H/LeV/LeB, NCAM, CD44).  Results       
will be used to develop a prediction rule for recurrence in stage I NSCLC       
using Cox proportional hazards regression analysis and an artificial neural     
network.                                                                        
                                                                                
Specific aim #2 will examine a set of eight serum tumor markers in a            
retrospective cohort of 250 patients with stage I to IV NSCLC.  These           
markers are categorized as molecular genetic markers (anti-p53), markers of     
metastatic propensity (angiogenesis bFGF), somatamedins (growth factor IGF-     
1) and markers of cellular differentiation (CEA, CA-125, CA 15-3, CYFRA21-      
1, CD44).  The purpose of this aim is to identify any correlations between      
titers of serum markers and tumor histology, stage or mass.  One hundred        
patients in this cohort had a second serum collection after tumor               
resection.  This subgroup of serum will allow analyses of titters before        
and after cyto-reduction.  Significant correlates with tumor stage and mass     
will be evaluated in a prospective cohort of patients with stage I NSCLC.       
                                                                                
In specific aim #3, paraffin-embedded and fresh-frozen tumor tissue will be     
collected from a prospective cohort of 330 patients with stage I NSCLC to       
validate the prediction rule developed in specific aim #1.  In these same       
patients, serial serum specimens will be collected for a minimum of 2.0         
years after resection (specific aim #4).  The significant markers               
identified in specific aim #2 will be analyzed in this cohort to describe       
correlations with tumor recurrence.  Tissue and serum markers identified by     
the model can be used to select high risk patients for a prospective,           
multi-institutional chemotherapy trial for stage I NSCLC.                       
 angiogenesis factor; artificial intelligence; biomarker; cell differentiation; clinical research; enzyme linked immunosorbent assay; genetic markers; histopathology; human subject; immunocytochemistry; insulinlike growth factor; longitudinal human study; mathematical model; metastasis; neoplasm /cancer classification /staging; neoplasm /cancer diagnosis; neoplasm /cancer epidemiology; neoplasm /cancer relapse /recurrence; nonsmall cell lung cancer; prognosis; respiratory surgery; serology /serodiagnosis TISSUE AND SERUM INDICATORS OF LUNG CANCER RECURRENCE","Non-small cell lung cancer (NSCLC) is the leading cause of cancer morality      
in men and women in the United States, and the overall long-term survial is     
less than 15%.  Pathologic stage I makes up  25-35% of NSCLC cases and has      
a good prognosis.  However, cancer relapse and death rate in this subset is     
35 to 50% by 5 years.  Chemotherapy is beneficial for the treatment of          
several localized solid tumors after resection and may prove to be useful       
in the treatment of patients with stage I NSCLC.  Thr purpose of this           
project is to define tissue and serum tumor markers in patients with stage      
I NSCLC which predict for early cancer recurrence.  Pathologic stage I          
NSCLC was chosen for study to eliminate the significant influence of            
positive lymph nodes and distant metastases on survival.                        
Immunohistochemical staining will identify potential tissue tumor markers       
and radioimmunoassay (RIA) or enzyme-link immunosorbent assay (ELISA) will      
identify potential serum tumor markers.                                         
                                                                                
Specific aim #1 will examine a set of twelve tissue tumor markers in a          
retrospective cohort of 275 stage I NSCLC patients.  Markers are                
categorized by hypothetical method of action: molecular genetic markers         
(Kras, erbB-1, erbB-2, rb, p53, bcl-2), markers of metastatic propensity        
(angiogenesis factor viii), proliferation markers (K1-67) and markers of        
cellular differentiation (Blood group A, H/LeV/LeB, NCAM, CD44).  Results       
will be used to develop a prediction rule for recurrence in stage I NSCLC       
using Cox proportional hazards regression analysis and an artificial neural     
network.                                                                        
                                                                                
Specific aim #2 will examine a set of eight serum tumor markers in a            
retrospective cohort of 250 patients with stage I to IV NSCLC.  These           
markers are categorized as molecular genetic markers (anti-p53), markers of     
metastatic propensity (angiogenesis bFGF), somatamedins (growth factor IGF-     
1) and markers of cellular differentiation (CEA, CA-125, CA 15-3, CYFRA21-      
1, CD44).  The purpose of this aim is to identify any correlations between      
titers of serum markers and tumor histology, stage or mass.  One hundred        
patients in this cohort had a second serum collection after tumor               
resection.  This subgroup of serum will allow analyses of titters before        
and after cyto-reduction.  Significant correlates with tumor stage and mass     
will be evaluated in a prospective cohort of patients with stage I NSCLC.       
                                                                                
In specific aim #3, paraffin-embedded and fresh-frozen tumor tissue will be     
collected from a prospective cohort of 330 patients with stage I NSCLC to       
validate the prediction rule developed in specific aim #1.  In these same       
patients, serial serum specimens will be collected for a minimum of 2.0         
years after resection (specific aim #4).  The significant markers               
identified in specific aim #2 will be analyzed in this cohort to describe       
correlations with tumor recurrence.  Tissue and serum markers identified by     
the model can be used to select high risk patients for a prospective,           
multi-institutional chemotherapy trial for stage I NSCLC.                       
",2517794,R29CA073980,['R29CA073980'],CA,https://reporter.nih.gov/project-details/2517794,R29,1997,62148,-0.06634009470812134
"Clinically significant delays in the acquisition of language are the most       
prevalent developmental problem in preschool children.  Early                   
intervention is crucial since children with language delays are at greatly      
increased risk for learning disabilities and behavioral disorders, but          
intervention resources are scarce and not sufficient to provide the             
intensive individualized instruction that effective intervention requires.      
The objective of this project is to develop a computer-based Intelligent        
Early Language Intervention System that will supplement the efforts of          
those who work with language impaired children.  The system will                
combine traditional intervention methods with strategies derived from           
contemporary linguistic theory, and will incorporate an intelligent             
computer-aided training (ICAT) system that uses artificial intelligence         
to generate customized language intervention strategies based on an             
individual~s performance.  In Phase I we developed and field tested a           
prototype system.  Our Phase II objectives are to (a) further develop           
the prototype activities, (b) develop additional activities for children at     
later stages of language development, ' field test and validate the             
effectiveness of program components, and (d) integrate all components           
into the ICAT framework.  We anticipate this system will be an asset            
to clinical service providers, teachers, and parents who work with              
children who have language impairments.                                         
 artificial intelligence; behavioral /social science research tag; clinical research; computer assisted instruction; computer program /software; human subject; language development; language disorders; method development; preschool child (1-5) INTELLIGENT EARLY LANGUAGE INTERVENTION SYSTEM","Clinically significant delays in the acquisition of language are the most       
prevalent developmental problem in preschool children.  Early                   
intervention is crucial since children with language delays are at greatly      
increased risk for learning disabilities and behavioral disorders, but          
intervention resources are scarce and not sufficient to provide the             
intensive individualized instruction that effective intervention requires.      
The objective of this project is to develop a computer-based Intelligent        
Early Language Intervention System that will supplement the efforts of          
those who work with language impaired children.  The system will                
combine traditional intervention methods with strategies derived from           
contemporary linguistic theory, and will incorporate an intelligent             
computer-aided training (ICAT) system that uses artificial intelligence         
to generate customized language intervention strategies based on an             
individual~s performance.  In Phase I we developed and field tested a           
prototype system.  Our Phase II objectives are to (a) further develop           
the prototype activities, (b) develop additional activities for children at     
later stages of language development, ' field test and validate the             
effectiveness of program components, and (d) integrate all components           
into the ICAT framework.  We anticipate this system will be an asset            
to clinical service providers, teachers, and parents who work with              
children who have language impairments.                                         
",2014568,R44DC002601,['R44DC002601'],DC,https://reporter.nih.gov/project-details/2014568,R44,1997,408989,-0.01681662655529709
"Knowledge discovery and Data Mining (Knowledge Discovery in                     
Databases-KDD) is a new research field which incorporates                       
methodologies from artificial intelligence, data bases, and statistics to       
address the problem of discovering novel, interesting and useful                
patterns (knowledge) hidden in large databases.  A prototype KDD                
surveillance system for epidemiology named Hawkeye has been                     
developed at UAB.  The goal of this research project is to further              
develop Hawkeye into a useful, general-purpose KDD surveillance                 
system for epidemiology.  This will be accomplished in two ways.  First,        
experiments with the prototype system will be conducted using hospital          
infection control data and public health data (CDC).  These                     
experiments will enlist infection control experts, and epidemiologists.         
Objective results and subjective feedback will be obtained.  These              
application-based experiments will further efforts in addressing                
fundamental research issues.  Second, specific methods improving the            
presentation of discovered knowledge to the user will be defined and            
implemented.  These methods make use of a novel idea called a                   
phenomenon cluster.                                                             
 artificial intelligence; computer data analysis; data collection; data collection methodology /evaluation; data management; epidemiology; health surveys; nosocomial infections; statistics /biometry KNOWLEDGE DISCOVERY/DATA MINING IN EPIDEMIOLOGY SURVEYS","Knowledge discovery and Data Mining (Knowledge Discovery in                     
Databases-KDD) is a new research field which incorporates                       
methodologies from artificial intelligence, data bases, and statistics to       
address the problem of discovering novel, interesting and useful                
patterns (knowledge) hidden in large databases.  A prototype KDD                
surveillance system for epidemiology named Hawkeye has been                     
developed at UAB.  The goal of this research project is to further              
develop Hawkeye into a useful, general-purpose KDD surveillance                 
system for epidemiology.  This will be accomplished in two ways.  First,        
experiments with the prototype system will be conducted using hospital          
infection control data and public health data (CDC).  These                     
experiments will enlist infection control experts, and epidemiologists.         
Objective results and subjective feedback will be obtained.  These              
application-based experiments will further efforts in addressing                
fundamental research issues.  Second, specific methods improving the            
presentation of discovered knowledge to the user will be defined and            
implemented.  These methods make use of a novel idea called a                   
phenomenon cluster.                                                             
",2411189,F37LM000057,['F37LM000057'],LM,https://reporter.nih.gov/project-details/2411189,F37,1997,20996,-0.023297369767514765
"DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
 artificial intelligence; behavioral /social science research tag; computer program /software; computer system design /evaluation; digital imaging; emotions; face expression; human subject; image processing; interpersonal relations; statistics /biometry; videotape /videodisc; visual tracking FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING","DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
",2397736,R01MH051435,['R01MH051435'],MH,https://reporter.nih.gov/project-details/2397736,R01,1997,234369,-0.12998730458869884
"The Family Planning Council of Southeastern Pennsylvania, in collaboration      
with the Cancer Prevention Research Center of the University of Rhode           
Island, is proposing an innovative study designed to address the risk           
behaviors associated with a recent alarming increase in the incidence of        
cervical neoplasias among young women. The proposed study is designed to        
develop, implement and evaluate interventions that increase consistent          
condom use and decrease cigarette smoking among 1,800 low income female         
youth aged 14-17 years who obtain family planning services at four diverse      
federally-funded family planning clinics. Smoking and unprotected sexual        
intercourse have been found to be independently associated with increased       
risks of cervical cancer in this population The proposed intervention is        
based on the Transtheoretical or Stages of Change model combined with one       
of the most promising modalities for reaching youth, an interactive             
computer based expert system whose efficacy will be evaluated alone and in      
combination with an adaptation of the anticipatory counseling model. Data       
will be collected at four points during a nine-month intervention period        
and at six-month intervals for 18 months post.intervention to assess            
effects over time. In addition, the intervention is aimed at increasing         
utilization of comprehensive, gynecologic health care including routine         
Pap smear screening, follow.up colposcopic examination and treatment of         
cervical dysplasia, the precursor of cervical cancers, when indicated.          
The proposed study represents the combined expertise of family planning         
researchers and service providers with considerable experience working          
with economically disadvantaged females and behavioral scientists with          
extensive research expertise in high risk behavior change.                      
 adolescence (12-20); behavioral /social science research tag; cancer prevention; cervix neoplasms; computer human interaction; condoms; female; high risk behavior /lifestyle; human subject; human therapy evaluation; longitudinal human study; smoking; tobacco abuse CHANGING TEEN BEHAVIORS--CERVICAL CANCER PREVENTION","The Family Planning Council of Southeastern Pennsylvania, in collaboration      
with the Cancer Prevention Research Center of the University of Rhode           
Island, is proposing an innovative study designed to address the risk           
behaviors associated with a recent alarming increase in the incidence of        
cervical neoplasias among young women. The proposed study is designed to        
develop, implement and evaluate interventions that increase consistent          
condom use and decrease cigarette smoking among 1,800 low income female         
youth aged 14-17 years who obtain family planning services at four diverse      
federally-funded family planning clinics. Smoking and unprotected sexual        
intercourse have been found to be independently associated with increased       
risks of cervical cancer in this population The proposed intervention is        
based on the Transtheoretical or Stages of Change model combined with one       
of the most promising modalities for reaching youth, an interactive             
computer based expert system whose efficacy will be evaluated alone and in      
combination with an adaptation of the anticipatory counseling model. Data       
will be collected at four points during a nine-month intervention period        
and at six-month intervals for 18 months post.intervention to assess            
effects over time. In addition, the intervention is aimed at increasing         
utilization of comprehensive, gynecologic health care including routine         
Pap smear screening, follow.up colposcopic examination and treatment of         
cervical dysplasia, the precursor of cervical cancers, when indicated.          
The proposed study represents the combined expertise of family planning         
researchers and service providers with considerable experience working          
with economically disadvantaged females and behavioral scientists with          
extensive research expertise in high risk behavior change.                      
",2390828,R01CA063745,['R01CA063745'],CA,https://reporter.nih.gov/project-details/2390828,R01,1997,377734,-0.07768352122422495
"This application seeks funding for Phase II of a three-phase program of         
research to identify, classify, and test nursing sensitive patient              
outcomes and their indicators for use in standardized language                  
development, practice, research, and education.  The purposes of the            
research are to:  1) identify, label, validate, and classify nursing            
sensitive patient outcomes and indicators, 2) evaluate the validity and         
usefulness of the classification in clinical field testing, and 3) define       
and test measurement procedures for the outcomes and indicators.  The           
classification is expected to contain patient outcomes, indicators, and         
measurement activities at three to four levels of abstraction and to            
identify those patient outcomes most influenced by nursing.  The research       
uses both inductive and deductive approaches.  An inductive approach will       
be used to extract outcomes, indicators, and measures from current              
nursing literature and instruments.  A combined inductive and deductive         
approach will be used to label outcomes, specify indicators for the             
outcomes, and group the outcomes in broad categories based on the Medical       
Outcomes framework and categories identified by nurses.  Delphi                 
techniques and surveys of random samples of masters prepared nurses will        
be used to validate the outcomes and indicators prior to field testing          
the outcomes and indicators in four sites, a tertiary care hospital, a          
community hospital, a nursing home, and a community agency.  Hierarchical       
clustering techniques and nonmetric scaling analysis will be used to            
develop the classification of nursing sensitive patient outcomes and a          
survey of nurse experts will be used for initial validation of the              
classification.                                                                 
 data collection; method development; nursing care evaluation; nursing research; outcomes research; prognosis CLASSIFICATION OF NURSING-SENSITIVE PATIENT OUTCOMES","This application seeks funding for Phase II of a three-phase program of         
research to identify, classify, and test nursing sensitive patient              
outcomes and their indicators for use in standardized language                  
development, practice, research, and education.  The purposes of the            
research are to:  1) identify, label, validate, and classify nursing            
sensitive patient outcomes and indicators, 2) evaluate the validity and         
usefulness of the classification in clinical field testing, and 3) define       
and test measurement procedures for the outcomes and indicators.  The           
classification is expected to contain patient outcomes, indicators, and         
measurement activities at three to four levels of abstraction and to            
identify those patient outcomes most influenced by nursing.  The research       
uses both inductive and deductive approaches.  An inductive approach will       
be used to extract outcomes, indicators, and measures from current              
nursing literature and instruments.  A combined inductive and deductive         
approach will be used to label outcomes, specify indicators for the             
outcomes, and group the outcomes in broad categories based on the Medical       
Outcomes framework and categories identified by nurses.  Delphi                 
techniques and surveys of random samples of masters prepared nurses will        
be used to validate the outcomes and indicators prior to field testing          
the outcomes and indicators in four sites, a tertiary care hospital, a          
community hospital, a nursing home, and a community agency.  Hierarchical       
clustering techniques and nonmetric scaling analysis will be used to            
develop the classification of nursing sensitive patient outcomes and a          
survey of nurse experts will be used for initial validation of the              
classification.                                                                 
",2035886,R01NR003437,['R01NR003437'],NR,https://reporter.nih.gov/project-details/2035886,R01,1997,222811,-0.08791180656101946
"This research develops quantitative image analysis software tools for           
detecting and precisely measuring anatomic changes in sequences of 3D           
medical image data, such as MRI or CT scans taken over time. Accurate           
structural change detection and identification will improve several             
medical applications, such as the experimental evaluation of drugs and          
treatments, precise monitoring of disease progression, and early disease        
diagnosis. The key objective of this research is to demonstrate that            
practical automated registration tools can be developed for handling the        
problems encountered in medical change detection applications: structures       
of complex 3D shape, tissue deformation, varying image resolution, and          
sensor distortion.                                                              
                                                                                
This Phase I research program proposes to demonstrate the feasibility of        
automated image registration tools for detecting anatomic changes by: l)        
establishing bounds on attainable performance of the image-based anatomic       
change detection technique as a function of sensor resolution and other         
image acquisition parameters; 2) assessing the performance of our approach      
through extensive experimentation with phantom models and controlled            
imagery; and 3) identifying impact on specific applications.  Following a       
successful validation study in Phase I, a proposed Phase II effort will         
continue with animal studies and increased interaction with clinicians to       
refine the change detection capabilities.                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We propose to develop a 3D image analysis tool for quantifying structural       
changes in MR or CT images taken in subjects over time. By developing           
accurate and robust products whose performance is well-understood we aim        
to initially develop end-products that may be directly used by clinical         
researchers for drug efficacy trials, such as for multiple sclerosis and        
stroke. Such tools may also form the backbone of a commercial service in        
which disease diagnosis and treatment monitoring tests are performed.           
 artificial intelligence; automated data processing; automated medical record system; computer assisted diagnosis; computer assisted medical decision making; computer program /software; image processing; method development; morphology; phantom model QUANTITATIVE 3D IMAGE AIDED ANATOMIC CHANGE DETECTION","This research develops quantitative image analysis software tools for           
detecting and precisely measuring anatomic changes in sequences of 3D           
medical image data, such as MRI or CT scans taken over time. Accurate           
structural change detection and identification will improve several             
medical applications, such as the experimental evaluation of drugs and          
treatments, precise monitoring of disease progression, and early disease        
diagnosis. The key objective of this research is to demonstrate that            
practical automated registration tools can be developed for handling the        
problems encountered in medical change detection applications: structures       
of complex 3D shape, tissue deformation, varying image resolution, and          
sensor distortion.                                                              
                                                                                
This Phase I research program proposes to demonstrate the feasibility of        
automated image registration tools for detecting anatomic changes by: l)        
establishing bounds on attainable performance of the image-based anatomic       
change detection technique as a function of sensor resolution and other         
image acquisition parameters; 2) assessing the performance of our approach      
through extensive experimentation with phantom models and controlled            
imagery; and 3) identifying impact on specific applications.  Following a       
successful validation study in Phase I, a proposed Phase II effort will         
continue with animal studies and increased interaction with clinicians to       
refine the change detection capabilities.                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We propose to develop a 3D image analysis tool for quantifying structural       
changes in MR or CT images taken in subjects over time. By developing           
accurate and robust products whose performance is well-understood we aim        
to initially develop end-products that may be directly used by clinical         
researchers for drug efficacy trials, such as for multiple sclerosis and        
stroke. Such tools may also form the backbone of a commercial service in        
which disease diagnosis and treatment monitoring tests are performed.           
",2035827,R43MH057200,['R43MH057200'],MH,https://reporter.nih.gov/project-details/2035827,R43,1997,93226,-0.04386970145405733
"Nuclear Magnetic Resonance Spectroscopy (MRS) offers unique chemical            
data, complementary to the anatomic information provided by the Magnetic        
Resonance Imaging (MRI), that adds a new dimension to the in vivo               
diagnosis of neurological disease. MRS is now emerging from the                 
laboratory and exploratory clinical studies that have shown a wide range        
of possible clinical applications, ranging from critically ill neonates         
to dementia in the elderly. The widespread clinical application of this         
important new diagnostic will require development of fast and reliable          
automated tools for interpretation of the spectra in terms of metabolite        
concentrations and medical diagnosis. ORINCON proposes to develop MRS           
interpretation software based on a neural network model employing a             
database of spectra of single metabolites. In Phase I of this project,          
ORINCON will design and implement prototype software to estimate, from          
simulated, one-dimensional proton spectra of the brain, the                     
concentrations of compounds that are important for diagnosis of                 
neurological disorders. The ultimate goal, to be realized in Phase II of        
this program, is to implement the MRS analysis software that will,              
without intervention of the operator, interpret the brain MRS data in           
terms of the metabolite concentrations and probability of relevant              
diseases. The interpretation software will be compatible with presently         
used spectroscopy packages and will also be portable to a personal              
computer (PC) platform.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
Software for automated analysis of NMR spectra of the brain will be a           
commercially viable product that can be distributed as a part of                
existing MRS packages. We anticipate that the interpretation software           
will be implemented within SageIDL - the MRS software package developed         
by GE Medical Systems for nuclear magnetic resonance image (MRI)                
scanners. The methods developed here may also find commercial                   
application in medical diagnostic tools based on MRS of urine,                  
cerebrospinal fluid, plasma, bile, semen, and amniotic fluid, as well as        
in atomic and molecular spectroscopy.                                           
 artificial intelligence; computational neuroscience; computer assisted diagnosis; computer program /software; computer system design /evaluation; diagnosis design /evaluation; electronic spectra; magnetic resonance imaging; nervous system disorder; nuclear magnetic resonance spectroscopy AUTOMATED INTERPRETATION OF PROTON MAGNETIC RESONANCE","Nuclear Magnetic Resonance Spectroscopy (MRS) offers unique chemical            
data, complementary to the anatomic information provided by the Magnetic        
Resonance Imaging (MRI), that adds a new dimension to the in vivo               
diagnosis of neurological disease. MRS is now emerging from the                 
laboratory and exploratory clinical studies that have shown a wide range        
of possible clinical applications, ranging from critically ill neonates         
to dementia in the elderly. The widespread clinical application of this         
important new diagnostic will require development of fast and reliable          
automated tools for interpretation of the spectra in terms of metabolite        
concentrations and medical diagnosis. ORINCON proposes to develop MRS           
interpretation software based on a neural network model employing a             
database of spectra of single metabolites. In Phase I of this project,          
ORINCON will design and implement prototype software to estimate, from          
simulated, one-dimensional proton spectra of the brain, the                     
concentrations of compounds that are important for diagnosis of                 
neurological disorders. The ultimate goal, to be realized in Phase II of        
this program, is to implement the MRS analysis software that will,              
without intervention of the operator, interpret the brain MRS data in           
terms of the metabolite concentrations and probability of relevant              
diseases. The interpretation software will be compatible with presently         
used spectroscopy packages and will also be portable to a personal              
computer (PC) platform.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
Software for automated analysis of NMR spectra of the brain will be a           
commercially viable product that can be distributed as a part of                
existing MRS packages. We anticipate that the interpretation software           
will be implemented within SageIDL - the MRS software package developed         
by GE Medical Systems for nuclear magnetic resonance image (MRI)                
scanners. The methods developed here may also find commercial                   
application in medical diagnostic tools based on MRS of urine,                  
cerebrospinal fluid, plasma, bile, semen, and amniotic fluid, as well as        
in atomic and molecular spectroscopy.                                           
",2035465,R43MH056818,['R43MH056818'],MH,https://reporter.nih.gov/project-details/2035465,R43,1997,93475,-0.19601508726274847
"Catheter ablation is a medical procedure that involves the destruction of       
small volumes of heart tissue with radiofrequency energy.  To be                
successful, catheter ablation requires precise localization of the tissue       
to be destroyed. To accomplish this in a typical ablation procedure, five       
catheters containing a total of 19 electrode pairs are utilized to record       
potentials (called electrograms) from spatially distinct locations within       
the heart throughout the cardiac cycle. Experienced cardiologists               
interpret the electrograms to locate the conduction pathways responsible        
for the arrhythmia. The pathways are destroyed by applications of               
radiofrequency energy, thus treating the arrhythmia.                            
                                                                                
Ablation procedures are performed by highly trained and experienced             
cardiology sub-specialists yet the massive amount of data produced during       
these procedures creates a data overload problem that can impede the            
performance of even the best practitioners. This may be evidenced by (1)        
overlooking important signal features, (2) misinterpreting the signals,         
and (3) misinterpreting catheter locations in the heart, all of which may       
lead to increased procedure duration and/or applications of radiofrequency      
energy to the wrong part of the heart.                                          
                                                                                
The purpose of this project is to develop a model-based system for              
interpreting intracardiac electrograms in near real-time. The system is         
intended to assist physicians in ""making sense"" of the enormous amounts of      
data recorded during a cardiac electrophysiology study. New computer            
algorithms for reasoning about the time- and space-varying nature of            
intracardiac electrograms from underlying causal models of the heart will       
be developed. The models can be represented as a graph of nodes connected       
by arcs. The nodes represent specific an atomic regions of the heart while      
the arcs represent the connections between the regions.                         
                                                                                
The analytic approach will be a variation of the hypothesize-and-test           
paradigm. The control loop will be based on the ""tracking"" concept whereby      
models will be tracked as long as the data supports the model. Rule-based       
knowledge will be utilized to generate models based on the observed data.       
The output of the system will be a series of ladder diagrams describing         
the data. In the event that the data admits more than a single explanatory      
model, ladder diagrams will be generated for all created models.                
                                                                                
This proposal is an extension of the applicant's graduate and post-             
doctoral project, which used the same approach to the simpler domain of         
the interpretation of the body-surface electrocardiogram. The domain of         
this proposal is more complex because it adds reasoning with a more             
detailed three dimensional cardiac model to the temporal reasoning              
required for previous work. Also, this domain requires the ability to           
account for simultaneous activation of the heart at multiple locations,         
which was performed at a rudimentary level in the previous work.                
                                                                                
This project is important for three reasons: (1) it offers new knowledge-       
based algorithms for reasoning about time- and space-varying data, (2) a        
comprehensive and extensible model of the cardiac conduction will be            
created that incorporates the spatial resolution necessary for                  
interpreting intracardiac electrograms, and (3) a software tool such the        
one proposed may help clinicians improve the quality of health care.            
 arrhythmia; artificial intelligence; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; electrocardiography; heart catheterization; heart conduction system; human data; interactive multimedia; mathematical model; model design /development MODEL BASED INTERPRETATION OF INTRACARDIAC ELECTROGRAMS","Catheter ablation is a medical procedure that involves the destruction of       
small volumes of heart tissue with radiofrequency energy.  To be                
successful, catheter ablation requires precise localization of the tissue       
to be destroyed. To accomplish this in a typical ablation procedure, five       
catheters containing a total of 19 electrode pairs are utilized to record       
potentials (called electrograms) from spatially distinct locations within       
the heart throughout the cardiac cycle. Experienced cardiologists               
interpret the electrograms to locate the conduction pathways responsible        
for the arrhythmia. The pathways are destroyed by applications of               
radiofrequency energy, thus treating the arrhythmia.                            
                                                                                
Ablation procedures are performed by highly trained and experienced             
cardiology sub-specialists yet the massive amount of data produced during       
these procedures creates a data overload problem that can impede the            
performance of even the best practitioners. This may be evidenced by (1)        
overlooking important signal features, (2) misinterpreting the signals,         
and (3) misinterpreting catheter locations in the heart, all of which may       
lead to increased procedure duration and/or applications of radiofrequency      
energy to the wrong part of the heart.                                          
                                                                                
The purpose of this project is to develop a model-based system for              
interpreting intracardiac electrograms in near real-time. The system is         
intended to assist physicians in ""making sense"" of the enormous amounts of      
data recorded during a cardiac electrophysiology study. New computer            
algorithms for reasoning about the time- and space-varying nature of            
intracardiac electrograms from underlying causal models of the heart will       
be developed. The models can be represented as a graph of nodes connected       
by arcs. The nodes represent specific an atomic regions of the heart while      
the arcs represent the connections between the regions.                         
                                                                                
The analytic approach will be a variation of the hypothesize-and-test           
paradigm. The control loop will be based on the ""tracking"" concept whereby      
models will be tracked as long as the data supports the model. Rule-based       
knowledge will be utilized to generate models based on the observed data.       
The output of the system will be a series of ladder diagrams describing         
the data. In the event that the data admits more than a single explanatory      
model, ladder diagrams will be generated for all created models.                
                                                                                
This proposal is an extension of the applicant's graduate and post-             
doctoral project, which used the same approach to the simpler domain of         
the interpretation of the body-surface electrocardiogram. The domain of         
this proposal is more complex because it adds reasoning with a more             
detailed three dimensional cardiac model to the temporal reasoning              
required for previous work. Also, this domain requires the ability to           
account for simultaneous activation of the heart at multiple locations,         
which was performed at a rudimentary level in the previous work.                
                                                                                
This project is important for three reasons: (1) it offers new knowledge-       
based algorithms for reasoning about time- and space-varying data, (2) a        
comprehensive and extensible model of the cardiac conduction will be            
created that incorporates the spatial resolution necessary for                  
interpreting intracardiac electrograms, and (3) a software tool such the        
one proposed may help clinicians improve the quality of health care.            
",2032386,R29LM006004,['R29LM006004'],LM,https://reporter.nih.gov/project-details/2032386,R29,1997,83300,-0.18192386257050658
"This is an application for an NCHGR/SERCA grant (K01) to support                
interdisciplinary research in genomic science.  The immediate objective of      
this research project is to develop a fully automated/robust algorithm for      
computer assembly of high density cosmid contigs to serve as templates for      
large-scale sequence-based mapping.  This includes integrating the cosmid       
contigs with chromosome-wide YAC based maps and incorporating diverse           
sources of data (e.g., DNA sequence from cosmid ends, STSs and other            
markers, or results of specific gap-filling efforts) to resolve                 
ambiguities and verify self-consistency.  Initial efforts will be focused       
on the high resolution mapping and sampled sequencing of human chromosome       
11.  The mapping strategy employs high density cosmid contig assembly over      
individual 200 kb to 1 Mb regions of the chromosome coupled with DNA            
sequencing of the cosmid ends.  The relative order and spacing of the           
sequence fragments is determined from the template contig resulting in a        
physical map of 1 to 5 kb resolution which contains up to 40% of the            
entire sequence at one-pass accuracy.  A simple restriction-site based          
approach to cosmid fingerprinting will be effective, provided that the          
contig-building algorithm correlates fragment data from multiple                
restriction digests and includes labeled vector/insert end fragments to         
determine the orientation of individual cosmids, i.e., the orientation of       
the genomic insert relative to the cloning vector.                              
                                                                                
The candidate is a physicist with nearly a ten year record of achievement       
and publication in the field of plasma fusion energy--theory/computation.       
The research advisor, Prof. Glen Evans, is the Principal Investigator of        
the Genome Science and Technology Center (GESTEC) at the University of          
Texas Southwestern Medical Center at Dallas, one of the top academic            
medical centers in the nation and the performance site of this grant.           
Prof. Evans is an accomplished biologist with an established record of          
coordinating multi-disciplinary research to develop informatics and             
automation for the Human Genome Project (HGP).  The candidate's skills          
also compliment those of Prof. Harold Garner, physicist and co-Principal        
Investigator of GESTEC.  The candidate will receive explicit training           
through an intensive 18 months program that includes formal course-work in      
the Division of Cell and Molecular Biology (at UT Southwestern), and            
laboratory rotations in the mapping, sequencing, and informatics units of       
GESTEC.                                                                         
 DNA; artificial chromosomes; artificial intelligence; chemical fingerprinting; chromosomes; computer assisted sequence analysis; computer program /software; genetic mapping; human genetic material tag; molecular cloning; nucleic acid sequence; plasmids; restriction fragment length polymorphism ADVANCED ALGORITHMS AND TOOLS FOR CONTIG BUILDING","This is an application for an NCHGR/SERCA grant (K01) to support                
interdisciplinary research in genomic science.  The immediate objective of      
this research project is to develop a fully automated/robust algorithm for      
computer assembly of high density cosmid contigs to serve as templates for      
large-scale sequence-based mapping.  This includes integrating the cosmid       
contigs with chromosome-wide YAC based maps and incorporating diverse           
sources of data (e.g., DNA sequence from cosmid ends, STSs and other            
markers, or results of specific gap-filling efforts) to resolve                 
ambiguities and verify self-consistency.  Initial efforts will be focused       
on the high resolution mapping and sampled sequencing of human chromosome       
11.  The mapping strategy employs high density cosmid contig assembly over      
individual 200 kb to 1 Mb regions of the chromosome coupled with DNA            
sequencing of the cosmid ends.  The relative order and spacing of the           
sequence fragments is determined from the template contig resulting in a        
physical map of 1 to 5 kb resolution which contains up to 40% of the            
entire sequence at one-pass accuracy.  A simple restriction-site based          
approach to cosmid fingerprinting will be effective, provided that the          
contig-building algorithm correlates fragment data from multiple                
restriction digests and includes labeled vector/insert end fragments to         
determine the orientation of individual cosmids, i.e., the orientation of       
the genomic insert relative to the cloning vector.                              
                                                                                
The candidate is a physicist with nearly a ten year record of achievement       
and publication in the field of plasma fusion energy--theory/computation.       
The research advisor, Prof. Glen Evans, is the Principal Investigator of        
the Genome Science and Technology Center (GESTEC) at the University of          
Texas Southwestern Medical Center at Dallas, one of the top academic            
medical centers in the nation and the performance site of this grant.           
Prof. Evans is an accomplished biologist with an established record of          
coordinating multi-disciplinary research to develop informatics and             
automation for the Human Genome Project (HGP).  The candidate's skills          
also compliment those of Prof. Harold Garner, physicist and co-Principal        
Investigator of GESTEC.  The candidate will receive explicit training           
through an intensive 18 months program that includes formal course-work in      
the Division of Cell and Molecular Biology (at UT Southwestern), and            
laboratory rotations in the mapping, sequencing, and informatics units of       
GESTEC.                                                                         
",2430543,K01HG000018,['K01HG000018'],HG,https://reporter.nih.gov/project-details/2430543,K01,1997,97527,-0.13213551341279017
"DESCRIPTION: The goal is to develop commercially viable, comprehensive,         
and state-of-the-art computer software for performing meta-analyses of          
health care evidence.  It will feature ""Analytic Wizards"" to guide less         
experienced users through the analytic process and the interpretation           
of results.  The software will also capitalize on innovations developed         
by the P.I.  to provide a flexible and powerful platform for storing,           
retrieving, visualizing, and maintaining clinical studies data for              
meta-analyses.  The software will employ these techniques to seamlessly         
integrate data management, analytical procedures and graphical                  
functions.                                                                      
                                                                                
The goal in this STTR Phase I project is to investigate the feasibility         
of implementing ""Analytic Wizards"" in the conduct of meta-analysis by           
performing analyses on a large number of data sets to derive rules for          
guiding the selection of appropriate analytic methods.  These rules will        
then be integrated and tested in a prototype.                                   
                                                                                
Integrated meta-analysis software with guidance on the use of                   
statistical procedures will simplify and standardize the production of          
meta-analyses, making it more efficient and reliable, and allowing the          
user to concentrate on the content rather than the mechanics of meta-           
analysis.                                                                       
 artificial intelligence; computer data analysis; computer graphics /printing; computer human interaction; computer program /software; computer system design /evaluation; data management; information systems; meta analysis; method development INTEGRATED HEURISTICS-GUIDED META-ANALYSIS SYSTEM","DESCRIPTION: The goal is to develop commercially viable, comprehensive,         
and state-of-the-art computer software for performing meta-analyses of          
health care evidence.  It will feature ""Analytic Wizards"" to guide less         
experienced users through the analytic process and the interpretation           
of results.  The software will also capitalize on innovations developed         
by the P.I.  to provide a flexible and powerful platform for storing,           
retrieving, visualizing, and maintaining clinical studies data for              
meta-analyses.  The software will employ these techniques to seamlessly         
integrate data management, analytical procedures and graphical                  
functions.                                                                      
                                                                                
The goal in this STTR Phase I project is to investigate the feasibility         
of implementing ""Analytic Wizards"" in the conduct of meta-analysis by           
performing analyses on a large number of data sets to derive rules for          
guiding the selection of appropriate analytic methods.  These rules will        
then be integrated and tested in a prototype.                                   
                                                                                
Integrated meta-analysis software with guidance on the use of                   
statistical procedures will simplify and standardize the production of          
meta-analyses, making it more efficient and reliable, and allowing the          
user to concentrate on the content rather than the mechanics of meta-           
analysis.                                                                       
",2418266,R41RR012416,['R41RR012416'],RR,https://reporter.nih.gov/project-details/2418266,R41,1997,100000,-0.036394980283091
"The goal of this proposal is the development of Computer-Aided                  
Instrument Design (CAID) tools and resources for the creation of                
embedded biomedical instruments. The Phase I work has demonstrated              
the conceptual feasibility of our approach through the development              
of a programmable, portable, hardware platform and a reusable set               
of software libraries. These tools and libraries were used to create            
a prototype of a Cardiac Monitor instrument. The Phase II work is               
aimed at expanding and improving the existing rudimentary CAID tools            
and resources to create a complete, turnkey environment for the                 
design of many different physiological instruments. The Phase II                
work will additionally involve creating and testing two                         
representative instruments: one will be an advanced ECG research                
device suggested and defined by our collaborators at JHU (Johns                 
Hopkins University); the other will be an Airway Monitor, suggested             
and defined by our collaborators at EVMS (Eastern Virginia Medical              
School).                                                                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The potential commercial                       
opportunities of our proposed technology are compelling. In addition            
to our selected Phase II experimental projects, various potential               
collaborators suggested applications including: an advanced Holter              
monitor with real-time detection of specific arrhythmias, fetal                 
heart monitors (using acoustic-based sensors of different                       
technologies), vital signs acquisition and logging,                             
ambulatory/portable EEG devices for sleep studies, seizure, and                 
brain injury detection, and many others.                                        
 artificial intelligence; biomedical equipment development; clinical biomedical equipment; computer program /software; computer simulation; computer system design /evaluation; electrical measurement; electrocardiography PHYSIOLOGICAL SIGNAL PROCESSOR","The goal of this proposal is the development of Computer-Aided                  
Instrument Design (CAID) tools and resources for the creation of                
embedded biomedical instruments. The Phase I work has demonstrated              
the conceptual feasibility of our approach through the development              
of a programmable, portable, hardware platform and a reusable set               
of software libraries. These tools and libraries were used to create            
a prototype of a Cardiac Monitor instrument. The Phase II work is               
aimed at expanding and improving the existing rudimentary CAID tools            
and resources to create a complete, turnkey environment for the                 
design of many different physiological instruments. The Phase II                
work will additionally involve creating and testing two                         
representative instruments: one will be an advanced ECG research                
device suggested and defined by our collaborators at JHU (Johns                 
Hopkins University); the other will be an Airway Monitor, suggested             
and defined by our collaborators at EVMS (Eastern Virginia Medical              
School).                                                                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The potential commercial                       
opportunities of our proposed technology are compelling. In addition            
to our selected Phase II experimental projects, various potential               
collaborators suggested applications including: an advanced Holter              
monitor with real-time detection of specific arrhythmias, fetal                 
heart monitors (using acoustic-based sensors of different                       
technologies), vital signs acquisition and logging,                             
ambulatory/portable EEG devices for sleep studies, seizure, and                 
brain injury detection, and many others.                                        
",2421505,R44HL056545,['R44HL056545'],HL,https://reporter.nih.gov/project-details/2421505,R44,1997,348950,-0.09572259230934133
"Since their introduction in the mid-198Os, commercial automated                 
karyotyping systems have come into widespread usage. Commercial                 
instruments have improved to the point where the major factor limiting          
throughput the time required for operator correction of chromosome              
classification errors. An improvement in chromosome classification              
accuracy would significantly increase the value of these instruments in         
the laboratory. The goal of this project is to develop improved chromosome      
measurement and classification techniques to bake the automated                 
karyotyping of human and mammalian cells significantly faster and more          
accurate, thereby enhancing the utility of these instruments in clinical        
diagnosis and in genetics and cancer research. Currently, the chromosome        
classification technique most widely used in commercial systems is based        
on weighted density distributions (WDDs [11]). The features used are inner      
products between the banding profile and six heuristically-defined WDDs.        
While WDDs have been shown to outperform the only two other weighting           
function sets that have been tested (Gaussians and sinusoids), there is no      
reason to believe they are optimal for chromosome classification accuracy.      
In light of recent developments in wavelet theory we are now able to            
design hundreds of new linear transformations having specified properties.      
Using parameter optimization algorithms we an test their basis functions        
for chromosome classification accuracy. Recently developed wavelet              
transform theory now permits a much more directed study of the chromosome       
classification problem, with the possibility of significant improvement in      
classification accuracy by using better-suited weighting functions than         
the WDDs. In phase 1 we will evaluate the use of wavelet-transform-derived      
features in automatic chromosome classification. We will use a genetic          
algorithm to evaluate a large number of wavelet-based features using            
large, published data sets for training and testing. If wavelet-based           
features can effect an improvement, this instrument will produce more           
accurate karyotypes, significantly increasing its throughput rate. In           
Phase 2 the wavelet-based features that prove the most effective will be        
incorporated into a PSI PowerGene automated karyotyping instrument. We          
will test the system on prenatal, postnatal and cancer specimens from           
amniotic fluid, bone marrow and peripheral blood, and on mammalian cells.       
When fully developed, the new technology will be integrated into PSI's          
existing line of cytogenetics automation products. This will result in          
commercial instruments that are far more effective in automated                 
karyotyping.                                                                    
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
As soon as the new techniques are developed and qualified for routine           
application, they will be incorporated into PSI's PowerGene product line        
of cytogenetics automation equipment, both in new systems sold and as an        
upgrade to existing systems already in use in cytogenetics labs, thus           
commercializing the technology quickly.                                         
 artificial intelligence; biomedical automation; biomedical equipment development; chromosomes; computer program /software; computer system design /evaluation; cytogenetics; digital imaging; genetic mapping; human genetic material tag; karyotype; method development WAVELET BASED AUTOMATED CHROMOSOME IDENTIFICATION","Since their introduction in the mid-198Os, commercial automated                 
karyotyping systems have come into widespread usage. Commercial                 
instruments have improved to the point where the major factor limiting          
throughput the time required for operator correction of chromosome              
classification errors. An improvement in chromosome classification              
accuracy would significantly increase the value of these instruments in         
the laboratory. The goal of this project is to develop improved chromosome      
measurement and classification techniques to bake the automated                 
karyotyping of human and mammalian cells significantly faster and more          
accurate, thereby enhancing the utility of these instruments in clinical        
diagnosis and in genetics and cancer research. Currently, the chromosome        
classification technique most widely used in commercial systems is based        
on weighted density distributions (WDDs [11]). The features used are inner      
products between the banding profile and six heuristically-defined WDDs.        
While WDDs have been shown to outperform the only two other weighting           
function sets that have been tested (Gaussians and sinusoids), there is no      
reason to believe they are optimal for chromosome classification accuracy.      
In light of recent developments in wavelet theory we are now able to            
design hundreds of new linear transformations having specified properties.      
Using parameter optimization algorithms we an test their basis functions        
for chromosome classification accuracy. Recently developed wavelet              
transform theory now permits a much more directed study of the chromosome       
classification problem, with the possibility of significant improvement in      
classification accuracy by using better-suited weighting functions than         
the WDDs. In phase 1 we will evaluate the use of wavelet-transform-derived      
features in automatic chromosome classification. We will use a genetic          
algorithm to evaluate a large number of wavelet-based features using            
large, published data sets for training and testing. If wavelet-based           
features can effect an improvement, this instrument will produce more           
accurate karyotypes, significantly increasing its throughput rate. In           
Phase 2 the wavelet-based features that prove the most effective will be        
incorporated into a PSI PowerGene automated karyotyping instrument. We          
will test the system on prenatal, postnatal and cancer specimens from           
amniotic fluid, bone marrow and peripheral blood, and on mammalian cells.       
When fully developed, the new technology will be integrated into PSI's          
existing line of cytogenetics automation products. This will result in          
commercial instruments that are far more effective in automated                 
karyotyping.                                                                    
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
As soon as the new techniques are developed and qualified for routine           
application, they will be incorporated into PSI's PowerGene product line        
of cytogenetics automation equipment, both in new systems sold and as an        
upgrade to existing systems already in use in cytogenetics labs, thus           
commercializing the technology quickly.                                         
",2539644,R43CA076896,['R43CA076896'],CA,https://reporter.nih.gov/project-details/2539644,R43,1997,91070,-0.10524742381387149
 artificial intelligence; behavioral /social science research tag; biomedical automation; biomedical equipment development; clinical biomedical equipment; clinical research; communication disorder aid; computer program /software; computer system design /evaluation; human subject; language; statistics /biometry; vocabulary ABBREVIATION EXPANSION FOR AUGMENTATIVE COMMUNICATION,,2609735,R43RR013220,['R43RR013220'],RR,https://reporter.nih.gov/project-details/2609735,R43,1997,92198,-0.06042685736542608
"The goal of this project is to refine and evaluate techniques that              
automatically construct, from clinical databases, Bayesian belief networks      
that can be used as diagnostic and prognostic aids.  The amount of              
clinical information stored in databases has increased markedly in the          
last two decades, and it seems likely that this trend will continue.            
Belief networks are able to represent the probabilistic dependencies among      
clinical variables in a relatively general manner.  Researchers have            
developed algorithms for performing probabilistic inference using belief        
networks, and they have applied these algorithms to perform medical             
diagnosis and prognosis.  Although advances have been made in developing        
the theory and application of belief networks, the manual construction of       
these networks often remains a difficult, time-consuming task.  The             
automated generation of belief networks from high-quality databases may         
facilitate significantly the construction of diagnostic and prognostic          
systems, which can serve as clinical decision aids, after their accuracy        
and usefulness are validated.                                                   
                                                                                
The long-range goal of this research is to advance our understanding and        
development of probabilistic systems that can serve as useful diagnostic        
and prognostic tools for physicians.  Such systems can serve as one method      
for disseminating the clinical knowledge captured in high-quality               
databases, such as those developed from PORT studies.  Within this              
context, the specific aims of the current, proposed research project are        
to:                                                                             
                                                                                
* refine and extend current methods for automatically constructing belief       
networks from large databases;                                                  
                                                                                
* test the diagnostic and prognostic accuracy of systems that are based on      
belief networks constructed automatically from high quality databases,          
compared to several standard statistical techniques;                            
                                                                                
* test whether a combination of automated and expert-based methods for          
constructing belief networks will yield diagnostic and prognostic systems       
that are more accurate than systems that are based on belief networks that      
are constructed automatically.                                                  
                                                                                
These three aims will be pursued using large, high-quality clinical-            
research databases at the University of Pittsburgh that contain                 
information on patients with syncope and patients in a PORT study with          
community-acquired-pneumonia.                                                   
 artificial intelligence; computer assisted diagnosis; computer assisted medical decision making; diagnosis quality /standard; human data; information systems; pneumonia; prognosis; statistics /biometry; syncope STRUCTURING MEDICAL KNOWLEDGE--PROBABILISTIC INFERENCE","The goal of this project is to refine and evaluate techniques that              
automatically construct, from clinical databases, Bayesian belief networks      
that can be used as diagnostic and prognostic aids.  The amount of              
clinical information stored in databases has increased markedly in the          
last two decades, and it seems likely that this trend will continue.            
Belief networks are able to represent the probabilistic dependencies among      
clinical variables in a relatively general manner.  Researchers have            
developed algorithms for performing probabilistic inference using belief        
networks, and they have applied these algorithms to perform medical             
diagnosis and prognosis.  Although advances have been made in developing        
the theory and application of belief networks, the manual construction of       
these networks often remains a difficult, time-consuming task.  The             
automated generation of belief networks from high-quality databases may         
facilitate significantly the construction of diagnostic and prognostic          
systems, which can serve as clinical decision aids, after their accuracy        
and usefulness are validated.                                                   
                                                                                
The long-range goal of this research is to advance our understanding and        
development of probabilistic systems that can serve as useful diagnostic        
and prognostic tools for physicians.  Such systems can serve as one method      
for disseminating the clinical knowledge captured in high-quality               
databases, such as those developed from PORT studies.  Within this              
context, the specific aims of the current, proposed research project are        
to:                                                                             
                                                                                
* refine and extend current methods for automatically constructing belief       
networks from large databases;                                                  
                                                                                
* test the diagnostic and prognostic accuracy of systems that are based on      
belief networks constructed automatically from high quality databases,          
compared to several standard statistical techniques;                            
                                                                                
* test whether a combination of automated and expert-based methods for          
constructing belief networks will yield diagnostic and prognostic systems       
that are more accurate than systems that are based on belief networks that      
are constructed automatically.                                                  
                                                                                
These three aims will be pursued using large, high-quality clinical-            
research databases at the University of Pittsburgh that contain                 
information on patients with syncope and patients in a PORT study with          
community-acquired-pneumonia.                                                   
",2460259,R29LM005291,['R29LM005291'],LM,https://reporter.nih.gov/project-details/2460259,R29,1997,102782,-0.06948859116191433
"1. Bayesian Learning in ME and HME Architectures - This Specific Aim will       
be to develop and apply Markov Chain Monte Carlo methodology to two             
specific types of neural networks: Mixtures-of-Experts (ME) and                 
Hierarchical Mixtures-of-Experts (HME) Architectures. Recently, Peng,           
Jacobs and Tanner (1994) developed a Bayesian learning scheme for such          
architectures. The overall goal will be to further study and extend this        
methodology. The first specific subaim is to extend and investigate the ME      
and HME architectures in the binary response case. The second subaim will       
be to extend this ME and HME methodology to handle censored response data.      
The third subaim will be to compare this ME and HME methodology with            
competing nonparametric methods such as CART, MARS, generalized additive        
models (GAM's), and projection pursuit regression (PPR). The fourth             
specific subaim will be to develop methods to prune the ME and HME              
architectures.                                                                  
                                                                                
2. Strategies for Computing the Marginal Likelihood - While algorithms          
such as data augmentation, the Gibbs sampler, the Metropolis algorithm and      
the Metropolis-Hastings algorithm have facilitated computations regarding       
estimation and prediction, the problem of computing the marginal                
likelihood remains an open problem. Such Markov Chain Monte Carlo               
algorithms yield a sample from the posterior- the marginal likelihood (or       
marginal density of the data) is obtained by integrating the likelihood         
function with respect to the prior density. The first subaim will be to         
develop and assess a new approach to this problem. The second subaim will       
be to validate this methodology using real data sets. The third subaim          
will be to compare this approach to those developed by other researchers.       
                                                                                
3. Development of Software to Accompany Methodology - This Specific Aim is      
to develop transportable, documented, efficient code to. support the            
methodology developed under this grant. In particular, software will be         
written to apply the mixtures-of-experts and hierarchical mixtures-of-          
experts architectures to regression problems, binary response problems,         
categorical response problems and censored regression problems. Code will       
also be written to evaluate the marginal density of the sample data, thus       
allowing the comparison of competing models.                                    
 artificial intelligence; computer system design /evaluation; human data; mathematical model; model design /development; statistics /biometry NONPARAMETRIC ANALYSIS OF CENSORED DATA","1. Bayesian Learning in ME and HME Architectures - This Specific Aim will       
be to develop and apply Markov Chain Monte Carlo methodology to two             
specific types of neural networks: Mixtures-of-Experts (ME) and                 
Hierarchical Mixtures-of-Experts (HME) Architectures. Recently, Peng,           
Jacobs and Tanner (1994) developed a Bayesian learning scheme for such          
architectures. The overall goal will be to further study and extend this        
methodology. The first specific subaim is to extend and investigate the ME      
and HME architectures in the binary response case. The second subaim will       
be to extend this ME and HME methodology to handle censored response data.      
The third subaim will be to compare this ME and HME methodology with            
competing nonparametric methods such as CART, MARS, generalized additive        
models (GAM's), and projection pursuit regression (PPR). The fourth             
specific subaim will be to develop methods to prune the ME and HME              
architectures.                                                                  
                                                                                
2. Strategies for Computing the Marginal Likelihood - While algorithms          
such as data augmentation, the Gibbs sampler, the Metropolis algorithm and      
the Metropolis-Hastings algorithm have facilitated computations regarding       
estimation and prediction, the problem of computing the marginal                
likelihood remains an open problem. Such Markov Chain Monte Carlo               
algorithms yield a sample from the posterior- the marginal likelihood (or       
marginal density of the data) is obtained by integrating the likelihood         
function with respect to the prior density. The first subaim will be to         
develop and assess a new approach to this problem. The second subaim will       
be to validate this methodology using real data sets. The third subaim          
will be to compare this approach to those developed by other researchers.       
                                                                                
3. Development of Software to Accompany Methodology - This Specific Aim is      
to develop transportable, documented, efficient code to. support the            
methodology developed under this grant. In particular, software will be         
written to apply the mixtures-of-experts and hierarchical mixtures-of-          
experts architectures to regression problems, binary response problems,         
categorical response problems and censored regression problems. Code will       
also be written to evaluate the marginal density of the sample data, thus       
allowing the comparison of competing models.                                    
",2458033,R01CA035464,['R01CA035464'],CA,https://reporter.nih.gov/project-details/2458033,R01,1997,99777,-0.0010472595316998407
"     We propose to develop a machine-vision system for the diagnostic           
interpretation of histopathologic sections and cytologic preparations. in       
continuation of an ongoing research project grant.  The system will have        
""image understanding capability."" that is, it will follow in its reasoning      
a model of the histology of a given application.  The interpretive              
expert-system module will integrate concepts from human diagnostic              
knowledge with machine-computable histometric features.  Systems of this        
kind can provide objective. quantitative, consistent evaluation of lesions,     
yielding more reliable interpretation and diagnostic, as well as                
prognostic, assessment.  Using a combination of large data bases of             
digitized imagery for given diagnostic situations. and relational data          
bases (including patient history. treatment, and outcome), it is hoped that     
the objective assessment eventually will be related reliably to truth in        
diagnosis.                                                                      
     The requirements for representative sampling are in the multimegapixel     
range, and require fully automatic scene segmentation and histometric           
feature extraction.  This difficult problem has been largely resolved           
through ongoing support, employing an AI-based, adaptive segmentation           
approach.                                                                       
     The major challenges for the proposed research are: 1) to gain an          
understanding of the necessary and sufficient human diagnostic clues and        
corresponding histometric analogs, that is, to determine the library of         
transforms to be used by the interpretive expert-system module; 2) to           
develop learning capability of the system for conceptual data; and 3) to        
gain an understanding of the functional requirements. dependence structure,     
and decision control capabilities of such a system.                             
 artificial intelligence; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; diagnosis quality /standard; histopathology; information system analysis; prognosis KNOWLEDGE BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY","     We propose to develop a machine-vision system for the diagnostic           
interpretation of histopathologic sections and cytologic preparations. in       
continuation of an ongoing research project grant.  The system will have        
""image understanding capability."" that is, it will follow in its reasoning      
a model of the histology of a given application.  The interpretive              
expert-system module will integrate concepts from human diagnostic              
knowledge with machine-computable histometric features.  Systems of this        
kind can provide objective. quantitative, consistent evaluation of lesions,     
yielding more reliable interpretation and diagnostic, as well as                
prognostic, assessment.  Using a combination of large data bases of             
digitized imagery for given diagnostic situations. and relational data          
bases (including patient history. treatment, and outcome), it is hoped that     
the objective assessment eventually will be related reliably to truth in        
diagnosis.                                                                      
     The requirements for representative sampling are in the multimegapixel     
range, and require fully automatic scene segmentation and histometric           
feature extraction.  This difficult problem has been largely resolved           
through ongoing support, employing an AI-based, adaptive segmentation           
approach.                                                                       
     The major challenges for the proposed research are: 1) to gain an          
understanding of the necessary and sufficient human diagnostic clues and        
corresponding histometric analogs, that is, to determine the library of         
transforms to be used by the interpretive expert-system module; 2) to           
develop learning capability of the system for conceptual data; and 3) to        
gain an understanding of the functional requirements. dependence structure,     
and decision control capabilities of such a system.                             
",2007904,R35CA053877,['R35CA053877'],CA,https://reporter.nih.gov/project-details/2007904,R35,1997,505855,-0.044691315813121704
"Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
 HIV infections; Internet; artificial intelligence; computer graphics /printing; computer human interaction; computer program /software; computer system design /evaluation; flow cytometry; information retrieval; information systems FACS-PENGUIN--KNOWLEDGE BASE SUPPORT FOR FLOW CYTOMETRY","Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
",2407366,R01LM004836,['R01LM004836'],LM,https://reporter.nih.gov/project-details/2407366,R01,1997,220818,-0.029667342213398094
"DESCRIPTION (Taken from application abstract):  The purpose of this proposal    
is to design and evaluate a novel graphical system for the presentation of      
data obtained from routine bedside monitoring of patients undergoing            
surgical intensive care (SICU).  The system, designated the V/Q-P/Q             
Assistant, is intended to enable expert and trainee physicians, nurses and      
other personnel to arrive at therapeutic decisions concerning pulmonary gas     
exchange and hemodynamics more efficiently.  This will be achieved at a         
human-computer interface by providing easier visualization of measured data,    
improved representation of functional abnormalities and the ability to          
simulate the particular patient so that trials of virtual therapy may guide     
the choice of treatment.  Analysis suggests that the task of a SICU             
physician is to translate the dense and unrelenting data stream of the SICU     
environment into a mental representation of the pathophysiological state.       
The data sources have been analyzed and software designed to collect data       
from typical classes of patients, at preselected intervals, from each           
monitoring device for storage in an Access database.  These data will form      
the basis for the experimental evaluation and design of the V/Q-P/Q             
Assistant.  Previous work on pulmonary pathophysiology has been synthesized     
into a model that unites the causes of impaired oxygenation                     
(ventilation/perfusion maldistribution) with a complete, and actively           
regulated pulmonary vasculature.  This model, combined with traditional and     
non-traditional clinical measurements, provides the computational basis for     
the system.  The human-computer interface will be developed under a             
cognitive system design framework, using an iterative process of rapid          
prototyping based on formative evaluations.  A series of computer display       
screens is proposed organized in five levels of increasing information.         
Screen designs use configural representations that are evolved to enhance       
information transfer, understanding of basic pathophysiology and evaluation     
of functional trajectories.  The ability of the V/Q-P/Q Assistant to improve    
medical care, enhance learning and contribute economically will be finally      
tested in a series of summative studies leading to the derivation of a multi    
attribute utility index.                                                        
 artificial intelligence; clinical research; computer assisted medical decision making; computer graphics /printing; computer human interaction; computer system design /evaluation; hemodynamics; human subject; information display; intensive care; patient monitoring device; respiratory airway volume; respiratory function; respiratory gas; respiratory hypoxia INFORMATION INTEGRATION AND VIRTUAL THERAPY IN THE SICU","DESCRIPTION (Taken from application abstract):  The purpose of this proposal    
is to design and evaluate a novel graphical system for the presentation of      
data obtained from routine bedside monitoring of patients undergoing            
surgical intensive care (SICU).  The system, designated the V/Q-P/Q             
Assistant, is intended to enable expert and trainee physicians, nurses and      
other personnel to arrive at therapeutic decisions concerning pulmonary gas     
exchange and hemodynamics more efficiently.  This will be achieved at a         
human-computer interface by providing easier visualization of measured data,    
improved representation of functional abnormalities and the ability to          
simulate the particular patient so that trials of virtual therapy may guide     
the choice of treatment.  Analysis suggests that the task of a SICU             
physician is to translate the dense and unrelenting data stream of the SICU     
environment into a mental representation of the pathophysiological state.       
The data sources have been analyzed and software designed to collect data       
from typical classes of patients, at preselected intervals, from each           
monitoring device for storage in an Access database.  These data will form      
the basis for the experimental evaluation and design of the V/Q-P/Q             
Assistant.  Previous work on pulmonary pathophysiology has been synthesized     
into a model that unites the causes of impaired oxygenation                     
(ventilation/perfusion maldistribution) with a complete, and actively           
regulated pulmonary vasculature.  This model, combined with traditional and     
non-traditional clinical measurements, provides the computational basis for     
the system.  The human-computer interface will be developed under a             
cognitive system design framework, using an iterative process of rapid          
prototyping based on formative evaluations.  A series of computer display       
screens is proposed organized in five levels of increasing information.         
Screen designs use configural representations that are evolved to enhance       
information transfer, understanding of basic pathophysiology and evaluation     
of functional trajectories.  The ability of the V/Q-P/Q Assistant to improve    
medical care, enhance learning and contribute economically will be finally      
tested in a series of summative studies leading to the derivation of a multi    
attribute utility index.                                                        
",2393913,R01LM005997,['R01LM005997'],LM,https://reporter.nih.gov/project-details/2393913,R01,1997,240944,-0.012317182149802666
"The long-term objective of our research group is to facilitate automatic        
or semi-automatic classification and retrieval of natural language texts,       
in support of reducing the cost and improving the quality of computerized       
medical information. This proposal develops further and applies a novel         
approach, the Linear Least Squares Fit (LLSF) mapping, to document              
indexing and document retrieval of the MEDLINE database. LLSF mapping is        
a statistical method developed by the PI for learning human knowledge           
about matching queries, documents, and canonical concepts. The goal is to       
improve the quality (recall and precision) of automatic document indexing       
and retrieval, which cannot be achieved by surface-based matching without       
using human knowledge or thesaurus-based matching dependent on manually         
developed synonyms. This project applies LLSF to MEDLINE, the world's           
largest and most frequently used on-line database, to evaluate the              
effectiveness of this method and to explore the practical potential on          
large scale databases. The specific aims and methods are:                       
                                                                                
l. To collect data needed for the training and evaluation of the LLSF           
method. A collaboration with another research institute is planned for          
utilizing and refining a large collection of MEDLINE retrieval data. A          
sampling of MEDLINE searches at the Mayo Clinic will be employed for            
obtaining additional tasks.                                                     
                                                                                
2. To develop automatic noise reduction techniques for improving both the       
accuracy of the LLSF mapping and the efficiency of the computation. A           
multi-step noise reduction in the training process of LLSF will be              
investigated, including a statistical term weighting for the removal of         
non-informative terms, a truncated singular value decomposition (SVD) for       
reducing the noise at the semantic structure level, and the truncation of       
insignificant elements in the LLSF solution matrix for noise-reduction at       
the level of term-to-concept mapping.                                           
                                                                                
3. To scale-up the training capacity for enabling the LLSF to accommodate       
the large size of MEDLINE data. A split-merge approach decomposes a large       
training sample into tractable subsets, computes an LLSF mapping function       
for each subset, and then merges the lcal mapping functions into a global       
one.                                                                            
                                                                                
4. To improve the computational efficiency by employing algorithms              
optimized for sparse matrices and for noise reduction. The potential            
solutions include the Block Lanczos truncated SVD algorithm which can           
reduce the cubic time complexity of standard SVD (on dense matrices) to a       
quadratic complexity, a QR decomposition which solves the LLSF without          
SVD, a sparse matrix algorithm which has shown a speed-up in matrix             
multiplication and cosine computation by a factor of l to 4 magnitudes,         
and parallel computing.                                                         
                                                                                
5. To evaluate the effectiveness of LLSF on large MEDLINE document sets         
and compare with the performance of alternate indexing/retrieval systems.       
 computer program /software; data collection methodology /evaluation; indexing; information retrieval; information systems; semantics; statistics /biometry; vocabulary development for information system LLSF MAPPING FOR INDEXING AND RETRIEVAL OF MEDLINE","The long-term objective of our research group is to facilitate automatic        
or semi-automatic classification and retrieval of natural language texts,       
in support of reducing the cost and improving the quality of computerized       
medical information. This proposal develops further and applies a novel         
approach, the Linear Least Squares Fit (LLSF) mapping, to document              
indexing and document retrieval of the MEDLINE database. LLSF mapping is        
a statistical method developed by the PI for learning human knowledge           
about matching queries, documents, and canonical concepts. The goal is to       
improve the quality (recall and precision) of automatic document indexing       
and retrieval, which cannot be achieved by surface-based matching without       
using human knowledge or thesaurus-based matching dependent on manually         
developed synonyms. This project applies LLSF to MEDLINE, the world's           
largest and most frequently used on-line database, to evaluate the              
effectiveness of this method and to explore the practical potential on          
large scale databases. The specific aims and methods are:                       
                                                                                
l. To collect data needed for the training and evaluation of the LLSF           
method. A collaboration with another research institute is planned for          
utilizing and refining a large collection of MEDLINE retrieval data. A          
sampling of MEDLINE searches at the Mayo Clinic will be employed for            
obtaining additional tasks.                                                     
                                                                                
2. To develop automatic noise reduction techniques for improving both the       
accuracy of the LLSF mapping and the efficiency of the computation. A           
multi-step noise reduction in the training process of LLSF will be              
investigated, including a statistical term weighting for the removal of         
non-informative terms, a truncated singular value decomposition (SVD) for       
reducing the noise at the semantic structure level, and the truncation of       
insignificant elements in the LLSF solution matrix for noise-reduction at       
the level of term-to-concept mapping.                                           
                                                                                
3. To scale-up the training capacity for enabling the LLSF to accommodate       
the large size of MEDLINE data. A split-merge approach decomposes a large       
training sample into tractable subsets, computes an LLSF mapping function       
for each subset, and then merges the lcal mapping functions into a global       
one.                                                                            
                                                                                
4. To improve the computational efficiency by employing algorithms              
optimized for sparse matrices and for noise reduction. The potential            
solutions include the Block Lanczos truncated SVD algorithm which can           
reduce the cubic time complexity of standard SVD (on dense matrices) to a       
quadratic complexity, a QR decomposition which solves the LLSF without          
SVD, a sparse matrix algorithm which has shown a speed-up in matrix             
multiplication and cosine computation by a factor of l to 4 magnitudes,         
and parallel computing.                                                         
                                                                                
5. To evaluate the effectiveness of LLSF on large MEDLINE document sets         
and compare with the performance of alternate indexing/retrieval systems.       
",2392815,R29LM005714,['R29LM005714'],LM,https://reporter.nih.gov/project-details/2392815,R29,1997,95259,-0.042278395236715165
"Predicting survival in cancer is important because survival, to a large         
extent, determines the selection of a particular therapy. Most cancer data      
sets contain large numbers of cases with missing data, and the usual            
approach is to remove such cases. But this reduction in data set size,          
combined with the further reduction caused by splitting the data set into       
training and testing subsets, can significantly reduce the accuracy of          
statistical models. This research seeks to validate at least one of two         
promising approaches for missing data, based upon Mixture Networks and          
Iterative Relaxation. Both methods avoid throwing away the cases that           
contain missing data, and both use all the available data to estimate the       
missing values. Phase II will build a full ""Missing Data Handler"" software      
package for general application to deal with missing data in three              
contexts; as a standalone package, integrated into Belmont's CrossGraphs        
visualization software, and integrated into Belmont's ClinTrans prototype       
for database transformations. The major technical innovation in Phase I         
will be a general method for working with data that contains missing            
values. The major health-related contribution will be improved accuracy         
for predicting cancer survival. resulting in better therapy selection and       
improved survival.                                                              
                                                                                
Proposed commercial application:                                                
A successful project will result in new tools for handling missing data.        
Tools will be incorporated into existing applications software packages         
developed and licensed by Belmont Research, Inc.                                
 artificial intelligence; cancer information system; computer program /software; computer system design /evaluation; data collection methodology /evaluation; mathematical model; statistics /biometry SOLUTIONS TO MISSING DATA IN CANCER","Predicting survival in cancer is important because survival, to a large         
extent, determines the selection of a particular therapy. Most cancer data      
sets contain large numbers of cases with missing data, and the usual            
approach is to remove such cases. But this reduction in data set size,          
combined with the further reduction caused by splitting the data set into       
training and testing subsets, can significantly reduce the accuracy of          
statistical models. This research seeks to validate at least one of two         
promising approaches for missing data, based upon Mixture Networks and          
Iterative Relaxation. Both methods avoid throwing away the cases that           
contain missing data, and both use all the available data to estimate the       
missing values. Phase II will build a full ""Missing Data Handler"" software      
package for general application to deal with missing data in three              
contexts; as a standalone package, integrated into Belmont's CrossGraphs        
visualization software, and integrated into Belmont's ClinTrans prototype       
for database transformations. The major technical innovation in Phase I         
will be a general method for working with data that contains missing            
values. The major health-related contribution will be improved accuracy         
for predicting cancer survival. resulting in better therapy selection and       
improved survival.                                                              
                                                                                
Proposed commercial application:                                                
A successful project will result in new tools for handling missing data.        
Tools will be incorporated into existing applications software packages         
developed and licensed by Belmont Research, Inc.                                
",2597596,R43CA077531,['R43CA077531'],CA,https://reporter.nih.gov/project-details/2597596,R43,1997,92558,-0.17861136454347087
"This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
 adenosine; anesthetics; artificial intelligence; blood pressure; cardiovascular agents; computer simulation; drug tolerance; heart rate; model design /development; neurotransmitter metabolism; nicotine; pharmacokinetics; pulmonary respiration MODELS FOR BIOLOGICAL DATA RELEVANT TO HEALTH CARE","This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
",2519015,R29GM051197,['R29GM051197'],GM,https://reporter.nih.gov/project-details/2519015,R29,1997,98970,-0.0305898437699804
"The long term goal of our research is to understand the flow of                 
information from the genome to the phenotype of organisms. In this              
proposal, we will attempt to use Bayesian networks and near-optimal             
sequence alignments to represent protein secondary structures and motifs.       
A Bayesian network describes the likelihood of amino acids at each              
position in a motif as well as the dependence of amino acids in one             
position on the amino acids at other position. Hence, Bayesian networks         
can describe both the conservation of amino acids at single positions and       
the conservation of correlations between two positions simultaneously.          
                                                                                
Conserved amino acids result from evolutionary selection for a specific         
amino acid or type of amino acid at one position in a protein structure.        
These positions often have important functional or structural                   
requirements. Correlated changes between amino acids generally result from      
side-chain side-chain interactions between pairs of amino acids in a            
protein's structure. The types of correlations we have represented with         
Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-      
bond donor and acceptor and inversely correlated packing volumes among          
others. These Bayesian networks can be used to 1) discover side-chain           
side--chain interactions within protei motifs and 2) to search sequence         
databases for motifs showing both correlations and conserved amino acids.       
                                                                                
Near-optimal alignments between two sequences can display regions that          
have been more highly conserved or less highly conserved using the              
information contained in only two sequences. The most highly conserved          
region correspond to the most highly structured regions and the most            
highly variable regions correspond to loops and coils and other                 
hypervariable regions. We propose to use near-optimal alignments to             
display conserved secondary structures of proteins and hypervariable            
regions. We will use secondary-structure specific amino acid substitution       
matrices to provide specificity.                                                
                                                                                
The goals of this proposal are to 1) build a database of Bayesian networks      
that represent protein motifs, 2) test these networks for their ability to      
detect motifs using test sets and crossvalidation methods, 3) compare           
these networks with other methods for searching protein databases , 4)          
build an integrated set of Bayesian networks to predict protein secondary       
structure, 5) compare the prediction of protein secondary structure with        
existing method 6) build a near-optimal sequence alignment workbench, and       
7) predict structured and unstructured regions in proteins from near-           
optimal alignments.                                                             
 artificial intelligence; biochemical evolution; chemical information system; computer assisted sequence analysis; hydrogen bond; hydropathy; ionic bond; model design /development; physical model; protein sequence; protein structure function; structural biology MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES","The long term goal of our research is to understand the flow of                 
information from the genome to the phenotype of organisms. In this              
proposal, we will attempt to use Bayesian networks and near-optimal             
sequence alignments to represent protein secondary structures and motifs.       
A Bayesian network describes the likelihood of amino acids at each              
position in a motif as well as the dependence of amino acids in one             
position on the amino acids at other position. Hence, Bayesian networks         
can describe both the conservation of amino acids at single positions and       
the conservation of correlations between two positions simultaneously.          
                                                                                
Conserved amino acids result from evolutionary selection for a specific         
amino acid or type of amino acid at one position in a protein structure.        
These positions often have important functional or structural                   
requirements. Correlated changes between amino acids generally result from      
side-chain side-chain interactions between pairs of amino acids in a            
protein's structure. The types of correlations we have represented with         
Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-      
bond donor and acceptor and inversely correlated packing volumes among          
others. These Bayesian networks can be used to 1) discover side-chain           
side--chain interactions within protei motifs and 2) to search sequence         
databases for motifs showing both correlations and conserved amino acids.       
                                                                                
Near-optimal alignments between two sequences can display regions that          
have been more highly conserved or less highly conserved using the              
information contained in only two sequences. The most highly conserved          
region correspond to the most highly structured regions and the most            
highly variable regions correspond to loops and coils and other                 
hypervariable regions. We propose to use near-optimal alignments to             
display conserved secondary structures of proteins and hypervariable            
regions. We will use secondary-structure specific amino acid substitution       
matrices to provide specificity.                                                
                                                                                
The goals of this proposal are to 1) build a database of Bayesian networks      
that represent protein motifs, 2) test these networks for their ability to      
detect motifs using test sets and crossvalidation methods, 3) compare           
these networks with other methods for searching protein databases , 4)          
build an integrated set of Bayesian networks to predict protein secondary       
structure, 5) compare the prediction of protein secondary structure with        
existing method 6) build a near-optimal sequence alignment workbench, and       
7) predict structured and unstructured regions in proteins from near-           
optimal alignments.                                                             
",2519669,R01LM005716,['R01LM005716'],LM,https://reporter.nih.gov/project-details/2519669,R01,1997,214055,-0.09770154842228262
"DESCRIPTION:  (Applicant's Abstract) In the field of computational biology,     
an important emerging specialty is the use of intelligent systems in support    
of molecular biology.  The Intelligent Systems for Molecular Biology (ISMB)     
conference provides a unique venue for the dissemination of recent              
developments in this field and for the interchange of ideas between             
experimental molecular biologists and computer scientists, mathematicians,      
and statisticians.  The ISMB-97 conference will be held in Halkidiki, Greece    
and the ISMB-98 conference will be held at the University of Montreal,          
Canada.  These locales continue the tradition of alternating between North      
America and Europe.                                                             
                                                                                
This grant request is for travel funds to enable students and young             
scientists to attend ISMB-97 and ISMB-98.  We are requesting $20,900 over       
two years to fund partial reimbursement of expenses for up to 20 students or    
postdocs each year.                                                             
 artificial intelligence; international cooperation; meeting /conference /symposium; molecular biology; travel 5TH AND 6TH INTERNATIONAL CONFERENCES ON ISMB","DESCRIPTION:  (Applicant's Abstract) In the field of computational biology,     
an important emerging specialty is the use of intelligent systems in support    
of molecular biology.  The Intelligent Systems for Molecular Biology (ISMB)     
conference provides a unique venue for the dissemination of recent              
developments in this field and for the interchange of ideas between             
experimental molecular biologists and computer scientists, mathematicians,      
and statisticians.  The ISMB-97 conference will be held in Halkidiki, Greece    
and the ISMB-98 conference will be held at the University of Montreal,          
Canada.  These locales continue the tradition of alternating between North      
America and Europe.                                                             
                                                                                
This grant request is for travel funds to enable students and young             
scientists to attend ISMB-97 and ISMB-98.  We are requesting $20,900 over       
two years to fund partial reimbursement of expenses for up to 20 students or    
postdocs each year.                                                             
",2447522,R13HG001683,['R13HG001683'],HG,https://reporter.nih.gov/project-details/2447522,R13,1997,10400,-0.0744797721085124
"This application is a request for a NIH Research Scientist Development          
Award, Level II RSDA-II) to extend work supported under the RSDA-I              
previously awarded to the applicant (DA00139).  (The RSDA-I granted in 1989     
was titled ""Innovative Statistical Approaches to Drug Abuse Data""; this         
application for the RSDA-II is titled ""Drug Abuse: Epidemiology, Treatment      
Processes, and Outcomes."") The RSDA-Il will continue to ensure financial        
stability and release time from the pursuit of funding for actual research      
work.  The major focus for the applicant during this five-year award is         
continuation of her professional work examining drug use epidemiology and       
treatment interventions for problematic drug abuse. Examining the               
implications of research findings for treatment strategies and developing       
the necessary social policy changes to support the implementation of            
improved treatment strategies is of further interest. The applicant will        
continue her professional work applying innovative statistical                  
methodologies to drug abuse data. To this end, three convergent lines of        
current research will be continued. The first examines drug use and             
treatment utilization among subjects recruited through hospital emergency       
rooms sexually transmitted disease clinics, and jails.  The second project      
is to improve the efficacy and efficiency of matching drug users' treatment     
needs to services. The third involves examining and evaluating the process      
of treatment service delivery with a special focus on the roles and             
functions of drug treatment counselors.                                         
                                                                                
The applicant's supporting institution is a research unit, the                  
Neuropsychiatric Institute (NPI), organized within the Department of            
Psychiatry, School of Medicine, UCLA. Affiliated with the NPI is the UCLA       
Drug Abuse Research Center, which has been conducting research in drug          
abuse epidemiology natural history of narcotics addiction, treatment            
evaluation, and social policy over the past 20 years.  In this setting, the     
applicant will conduct the proposed research and will receive additional        
training in psychiatric aspects of drug abuse treatment and in the              
implementation of treatment services.  Furthermore. the applicant's             
considerable psychosocial research knowledge and skills in drug abuse           
issues will contribute to the NPI's general program in drug abuse research      
by complementing the Institute's biobehavioral perspective.                     
                                                                                
During the award period, the applicant also expects to grow professionally      
as Associate Director of the UCLA Drug Abuse Research Center. in addition       
to pursuing the aforementioned research. activities will include the career     
development of new investigators from various disciplines and the mentoring     
of graduate and undergraduate students in related fields.                       
 artificial intelligence; computer simulation; computer system design /evaluation; drug abuse therapy; drug addiction; health care service utilization; human subject; human therapy evaluation; interview; longitudinal human study; mental health counseling; mental health epidemiology; outcomes research; prognosis; questionnaires DRUG ABUSE--EPIDEMIOLOGY TREATMENT PROCESS AND OUTCOMES","This application is a request for a NIH Research Scientist Development          
Award, Level II RSDA-II) to extend work supported under the RSDA-I              
previously awarded to the applicant (DA00139).  (The RSDA-I granted in 1989     
was titled ""Innovative Statistical Approaches to Drug Abuse Data""; this         
application for the RSDA-II is titled ""Drug Abuse: Epidemiology, Treatment      
Processes, and Outcomes."") The RSDA-Il will continue to ensure financial        
stability and release time from the pursuit of funding for actual research      
work.  The major focus for the applicant during this five-year award is         
continuation of her professional work examining drug use epidemiology and       
treatment interventions for problematic drug abuse. Examining the               
implications of research findings for treatment strategies and developing       
the necessary social policy changes to support the implementation of            
improved treatment strategies is of further interest. The applicant will        
continue her professional work applying innovative statistical                  
methodologies to drug abuse data. To this end, three convergent lines of        
current research will be continued. The first examines drug use and             
treatment utilization among subjects recruited through hospital emergency       
rooms sexually transmitted disease clinics, and jails.  The second project      
is to improve the efficacy and efficiency of matching drug users' treatment     
needs to services. The third involves examining and evaluating the process      
of treatment service delivery with a special focus on the roles and             
functions of drug treatment counselors.                                         
                                                                                
The applicant's supporting institution is a research unit, the                  
Neuropsychiatric Institute (NPI), organized within the Department of            
Psychiatry, School of Medicine, UCLA. Affiliated with the NPI is the UCLA       
Drug Abuse Research Center, which has been conducting research in drug          
abuse epidemiology natural history of narcotics addiction, treatment            
evaluation, and social policy over the past 20 years.  In this setting, the     
applicant will conduct the proposed research and will receive additional        
training in psychiatric aspects of drug abuse treatment and in the              
implementation of treatment services.  Furthermore. the applicant's             
considerable psychosocial research knowledge and skills in drug abuse           
issues will contribute to the NPI's general program in drug abuse research      
by complementing the Institute's biobehavioral perspective.                     
                                                                                
During the award period, the applicant also expects to grow professionally      
as Associate Director of the UCLA Drug Abuse Research Center. in addition       
to pursuing the aforementioned research. activities will include the career     
development of new investigators from various disciplines and the mentoring     
of graduate and undergraduate students in related fields.                       
",2458329,K02DA000139,['K02DA000139'],DA,https://reporter.nih.gov/project-details/2458329,K02,1997,94782,-0.021241869843107043
"The three-dimensional structure of DNA is quite dependent on sequence.          
This should be intuitively obvious from the sequence specificity required       
by many proteins for recognition.  The few available x-ray crystal              
structures and NMR solution structures attest to this structural                
flexibility as well.  The location and orientation of potential ligand          
binding functions on the DNA such as charges, hydrogen bonding and              
hydrophobic sites can be modified substantially from that which one might       
expect on the basis of assuming a canonical B-DNA structure.                    
Consequently, we intend to continue development of the methodology for          
determination of high-resolution nucleic acid structures in solution and        
apply this methodology to some oligonucleotides and oligonucleotide             
complexes.  This entails methods designed to improve the accuracy and           
resolution of the structures determined.  Improved structures can be            
obtained with more accurate and more numerous experimental distance and         
torsion angle constraints, as well as improvements in calculating               
structure from these constraints.  Enhancements will result from                
improvements in our iterative complete relaxation matrix program                
MARDIGRAS, development of a more encompassing density matrix approach for       
analysis of spectra derived from any pulse sequences (even those not yet        
invented), development of tailored excitation pulses, inclusion of              
experimental molecular motion information, and development of alternative       
methods of reducing experimental structural constraint data to                  
structures.  The latter includes (a) for restrained molecular dynamics          
simulations, use of improved force fields, empirical development of             
improved force fields, and use of constraint terms permitting a more            
realistic picture of conformational flexibility, and (b) development of         
an alternative restrained Monte Carlo method in torsion angle and helical       
parameter space, which is quite promising especially for structure              
refinement directly against NOE intensities.  Applications will include         
oligonucleotides of interest, in particular sequences recognized by             
transcription factors or regulators, genome targets, antisense                  
oligonucleotides, and a DNA microcircle duplex.  Structures of proteins         
(including nucleic acid complexes) which are important for initiation or        
regulation of transcription will be determined.  In particular, the             
72-residue protein GerE which is a regulatory protein that binds                
specifically to a target site in promoter DNA will be the subject of            
study.  Other proteins will be evaluated as possible candidates for             
study.                                                                          
 DNA; antisense nucleic acid; artificial intelligence; biophysics; computer program /software; computer simulation; computer system design /evaluation; conformation; genetic promoter element; method development; molecular dynamics; nuclear magnetic resonance spectroscopy; nucleic acid sequence; nucleic acid structure; oligonucleotides; protein structure; solutions; stereoisomer; structural biology; supercomputer; transcription factor SOLUTION STRUCTURE OF DNA AND DNA-PROTEIN COMPLEXES","The three-dimensional structure of DNA is quite dependent on sequence.          
This should be intuitively obvious from the sequence specificity required       
by many proteins for recognition.  The few available x-ray crystal              
structures and NMR solution structures attest to this structural                
flexibility as well.  The location and orientation of potential ligand          
binding functions on the DNA such as charges, hydrogen bonding and              
hydrophobic sites can be modified substantially from that which one might       
expect on the basis of assuming a canonical B-DNA structure.                    
Consequently, we intend to continue development of the methodology for          
determination of high-resolution nucleic acid structures in solution and        
apply this methodology to some oligonucleotides and oligonucleotide             
complexes.  This entails methods designed to improve the accuracy and           
resolution of the structures determined.  Improved structures can be            
obtained with more accurate and more numerous experimental distance and         
torsion angle constraints, as well as improvements in calculating               
structure from these constraints.  Enhancements will result from                
improvements in our iterative complete relaxation matrix program                
MARDIGRAS, development of a more encompassing density matrix approach for       
analysis of spectra derived from any pulse sequences (even those not yet        
invented), development of tailored excitation pulses, inclusion of              
experimental molecular motion information, and development of alternative       
methods of reducing experimental structural constraint data to                  
structures.  The latter includes (a) for restrained molecular dynamics          
simulations, use of improved force fields, empirical development of             
improved force fields, and use of constraint terms permitting a more            
realistic picture of conformational flexibility, and (b) development of         
an alternative restrained Monte Carlo method in torsion angle and helical       
parameter space, which is quite promising especially for structure              
refinement directly against NOE intensities.  Applications will include         
oligonucleotides of interest, in particular sequences recognized by             
transcription factors or regulators, genome targets, antisense                  
oligonucleotides, and a DNA microcircle duplex.  Structures of proteins         
(including nucleic acid complexes) which are important for initiation or        
regulation of transcription will be determined.  In particular, the             
72-residue protein GerE which is a regulatory protein that binds                
specifically to a target site in promoter DNA will be the subject of            
study.  Other proteins will be evaluated as possible candidates for             
study.                                                                          
",2605283,R01GM039247,['R01GM039247'],GM,https://reporter.nih.gov/project-details/2605283,R01,1997,64549,-0.004550265508730355
 artificial intelligence; biomedical equipment development; clinical biomedical equipment; clinical research; communication disorder aid; computer program /software; computer system design /evaluation; computer system hardware; human subject; vocabulary COMMUNICATION AID UTILIZING WORD-LEVEL DISAMBIGUATION,,2595715,R43RR013191,['R43RR013191'],RR,https://reporter.nih.gov/project-details/2595715,R43,1997,100000,-0.09420353544267186
"We propose to extend the successful work we have achieved with                  
statistically based indexing and retrieval systems, by incorporating            
semantic structures which accommodate the modifying attributes of clinical      
conCepts. Patient data is rarely limited to a single axis of meaning or         
detail, and retrieval for application in quality improvement, decision          
support, or epidemiologic research, demands Consistent information              
struCture. This proposal will invoke the knowledge and tool suites of the       
UMLS Specialist Lexicon, the SGML markup and recognition capabilities of        
the TextMachine application, extensions to our locally developed CliniCal       
Query Language, and layer these enhancements upon our core techniques for       
statistically based indexing and retrieval of patient data. We commit           
these activities to remain compliant with emerging standards for medical        
concept representation arising from the Canon efforts and the                   
standardization processes at ANSI-HISPP, CEN TC251 and the CPRI                 
initiatives.                                                                    
 artificial intelligence; automated medical record system; behavioral /social science research tag; computer assisted medical decision making; human data; indexing; information retrieval; semantics; statistics /biometry; vocabulary development for information system LATENT SEMANTIC INDEXING IN SUPPORT OF DATA RETRIEVAL","We propose to extend the successful work we have achieved with                  
statistically based indexing and retrieval systems, by incorporating            
semantic structures which accommodate the modifying attributes of clinical      
conCepts. Patient data is rarely limited to a single axis of meaning or         
detail, and retrieval for application in quality improvement, decision          
support, or epidemiologic research, demands Consistent information              
struCture. This proposal will invoke the knowledge and tool suites of the       
UMLS Specialist Lexicon, the SGML markup and recognition capabilities of        
the TextMachine application, extensions to our locally developed CliniCal       
Query Language, and layer these enhancements upon our core techniques for       
statistically based indexing and retrieval of patient data. We commit           
these activities to remain compliant with emerging standards for medical        
concept representation arising from the Canon efforts and the                   
standardization processes at ANSI-HISPP, CEN TC251 and the CPRI                 
initiatives.                                                                    
",2032342,R01LM005416,['R01LM005416'],LM,https://reporter.nih.gov/project-details/2032342,R01,1997,172085,-0.006200551819820932
"This project will develop and evaluate a multimedia exercise                    
promotion program to meet adult learning characteristics by: 1)                 
structuring the program into modules that can be used individually              
or sequentially; 2) allowing users to select their own path through             
the program, based on time, need, and learning style; 3) ensuring               
that the focus of the program is on the application of skills and               
concepts through use of scenario-based learning experiences; and                
4) ensuring that the feedback component of the program is a                     
constructive learning experience providing users with response-                 
specific feedback, remedial branching when needed, and a                        
complete, printed, personal exercise prescription.  Video models of             
basically healthy but inactive adults overcoming barriers to exercise           
will be available as part of the program.  Exercise activities and              
programs will be selected by the computer to match user                         
characteristics entered at the beginning of the program.  For users             
with modems, an on-line connection will be made possible with                   
online fitness support groups.  In the Phase I prototype we will                
design and evaluate a prototype of one module of the envisioned                 
product.  All efforts will be directed toward development of an                 
evaluated and commercially feasible multimedia product in Phase                 
II and for its distribution thereafter.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
This marketed to :1) adults in the US and the English-language                  
world market, with its rapidly expanding multimedia home market;                
and 2) medical clinics, prepaid health plans, worksites, health                 
clubs, etc.  Multimedia sales were projected to reach 1.45 billion              
dollars in 1995, up 85 percent from 1994.  With 84 percent of 1995              
sales, CD-ROM is the current platform of choice, although this                  
may be replaced by the on-line, or other digital distribution                   
mechanisms in the near future.                                                  
 DVD /CD ROM; adult human (21+); artificial intelligence; cardiovascular disorder prevention; choice; computer assisted instruction; computer human interaction; computer program /software; computer system design /evaluation; exercise; health education; human subject; information display; interactive multimedia; lifestyle; online computer; physical fitness MULTIMEDIA FITNESS PROMOTION FOR SEDENTARY ADULTS","This project will develop and evaluate a multimedia exercise                    
promotion program to meet adult learning characteristics by: 1)                 
structuring the program into modules that can be used individually              
or sequentially; 2) allowing users to select their own path through             
the program, based on time, need, and learning style; 3) ensuring               
that the focus of the program is on the application of skills and               
concepts through use of scenario-based learning experiences; and                
4) ensuring that the feedback component of the program is a                     
constructive learning experience providing users with response-                 
specific feedback, remedial branching when needed, and a                        
complete, printed, personal exercise prescription.  Video models of             
basically healthy but inactive adults overcoming barriers to exercise           
will be available as part of the program.  Exercise activities and              
programs will be selected by the computer to match user                         
characteristics entered at the beginning of the program.  For users             
with modems, an on-line connection will be made possible with                   
online fitness support groups.  In the Phase I prototype we will                
design and evaluate a prototype of one module of the envisioned                 
product.  All efforts will be directed toward development of an                 
evaluated and commercially feasible multimedia product in Phase                 
II and for its distribution thereafter.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
This marketed to :1) adults in the US and the English-language                  
world market, with its rapidly expanding multimedia home market;                
and 2) medical clinics, prepaid health plans, worksites, health                 
clubs, etc.  Multimedia sales were projected to reach 1.45 billion              
dollars in 1995, up 85 percent from 1994.  With 84 percent of 1995              
sales, CD-ROM is the current platform of choice, although this                  
may be replaced by the on-line, or other digital distribution                   
mechanisms in the near future.                                                  
",2031539,R43HL058338,['R43HL058338'],HL,https://reporter.nih.gov/project-details/2031539,R43,1997,93500,-0.12297610233900969
"The electronic medical record (EMR) holds great allure to both the              
medical informatics and health services research communities. In this           
project, we propose to enhance the capability of electronic medical             
record (EMR) systems by creating and evaluating tools to extract                
clinical vocabularies as well as patient data from narrative text               
reports. We will apply advanced natural language processing tools from          
the CLARIT system to both of the above problems. We contend that fast           
and robust automated text processing methods are the only way that the          
problems of vocabulary construction and narrative text extraction can           
be solved.                                                                      
                                                                                
We will address the clinical vocabulary problem by utilizing the                
thesaurus extraction techniques already present in the CLARIT system.           
Using several gigabytes of narrative text, including discharge                  
summaries, progress notes, radiology reports, and other clinical text,          
we plan to:                                                                     
l. Identify empirically the terminology used in medicine.                       
2. Compare the coverage of that terminology in several existing large           
medical vocabularies: UMLS, SNOMED, and the Medical Entities                    
Dictionary.                                                                     
3. Discern the semantic characteristics of that terminology to allow            
other structured vocabularies a richer substrate of terms as well as            
providing us the opportunity to implement a clinical vocabulary schema          
based on the methods of the MedSORT-II Project.                                 
4. Evaluate how well our tools assist the vocabulary building efforts           
of ourselves and others.                                                        
                                                                                
The narrative extraction problem will be approached differently than            
in the past, building on the efforts of previous investigators who              
have tackled this problem before but changing the perspective by                
focusing on the development of tools specific to researchers and                
others with a need to extract data from narrative text. This approach           
will be applied in two domains:                                                 
l.Consortium-based research in the use of esophogastroduodenoscopy              
(EGD).                                                                          
2.Practice guidelines implementation in blood product transfusion.              
 abstracting; automated medical record system; blood transfusion; cooperative study; endoscopy; vocabulary development for information system VOCABULARY AND TEXT DATA EXTRACTION FROM THE EMR","The electronic medical record (EMR) holds great allure to both the              
medical informatics and health services research communities. In this           
project, we propose to enhance the capability of electronic medical             
record (EMR) systems by creating and evaluating tools to extract                
clinical vocabularies as well as patient data from narrative text               
reports. We will apply advanced natural language processing tools from          
the CLARIT system to both of the above problems. We contend that fast           
and robust automated text processing methods are the only way that the          
problems of vocabulary construction and narrative text extraction can           
be solved.                                                                      
                                                                                
We will address the clinical vocabulary problem by utilizing the                
thesaurus extraction techniques already present in the CLARIT system.           
Using several gigabytes of narrative text, including discharge                  
summaries, progress notes, radiology reports, and other clinical text,          
we plan to:                                                                     
l. Identify empirically the terminology used in medicine.                       
2. Compare the coverage of that terminology in several existing large           
medical vocabularies: UMLS, SNOMED, and the Medical Entities                    
Dictionary.                                                                     
3. Discern the semantic characteristics of that terminology to allow            
other structured vocabularies a richer substrate of terms as well as            
providing us the opportunity to implement a clinical vocabulary schema          
based on the methods of the MedSORT-II Project.                                 
4. Evaluate how well our tools assist the vocabulary building efforts           
of ourselves and others.                                                        
                                                                                
The narrative extraction problem will be approached differently than            
in the past, building on the efforts of previous investigators who              
have tackled this problem before but changing the perspective by                
focusing on the development of tools specific to researchers and                
others with a need to extract data from narrative text. This approach           
will be applied in two domains:                                                 
l.Consortium-based research in the use of esophogastroduodenoscopy              
(EGD).                                                                          
2.Practice guidelines implementation in blood product transfusion.              
",2332614,U01LM005879,['U01LM005879'],LM,https://reporter.nih.gov/project-details/2332614,U01,1997,294900,-0.0001339163026085335
"DESCRIPTION (Taken from application abstract):  Recent advances in              
understanding neuronal functions have led to an expanded scientific interest    
in defining the organization of the nervous system so the spatial correlates    
of these functions can be identified.  This interest has been formalized as     
the Human Brain Project, an ambitious multi disciplinary effort to map the      
nervous system from the organismal to the macromolecular levels.  One of the    
greatest challenges of this effort is to preserve the complex                   
three-dimensional relationships that occur between neuronal structures.         
This problem will require new methods for data acquisition as well as data      
visualization.                                                                  
                                                                                
The project described here is an interdisciplinary effort to derive             
three-dimensional reconstructions of synaptic architecture from stereo          
electron micrographs acquired from multiple viewpoints.  The collaboration      
combines advanced ultrastructural visualization techniques with massively       
parallel computational methods and an innovative set of pattern recognition,    
stereo correspondence and depth mapping algorithms.  Our goal is to             
integrate structural information from numerous images into a single,            
high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.    
The immediate result of this collaboration will be an improved understanding    
of the spatial relationships between synaptic macromolecules.  More             
importantly, the project will produce a set of computational tools that can     
be applied to stereo image data sets of various areas of the nervous system,    
from the macroscopic to the molecular level.  Finally, our studies will         
advance the state-of-the-art of parallel computation and interactive            
reconstruction methods that can provide novel solutions to difficult            
problems of neuroscience visualization.                                         
 artificial intelligence; brain mapping; cell cell interaction; computational neuroscience; computer program /software; computer system design /evaluation; cytoskeleton; data collection; electron microscopy; image processing; imaging /visualization /scanning; macromolecule; method development; nerve endings; neurons; parallel processing; stereophotography; structural biology; synapses THREE DIMENSIONAL RECONSTRUCTION OF SYNAPSES","DESCRIPTION (Taken from application abstract):  Recent advances in              
understanding neuronal functions have led to an expanded scientific interest    
in defining the organization of the nervous system so the spatial correlates    
of these functions can be identified.  This interest has been formalized as     
the Human Brain Project, an ambitious multi disciplinary effort to map the      
nervous system from the organismal to the macromolecular levels.  One of the    
greatest challenges of this effort is to preserve the complex                   
three-dimensional relationships that occur between neuronal structures.         
This problem will require new methods for data acquisition as well as data      
visualization.                                                                  
                                                                                
The project described here is an interdisciplinary effort to derive             
three-dimensional reconstructions of synaptic architecture from stereo          
electron micrographs acquired from multiple viewpoints.  The collaboration      
combines advanced ultrastructural visualization techniques with massively       
parallel computational methods and an innovative set of pattern recognition,    
stereo correspondence and depth mapping algorithms.  Our goal is to             
integrate structural information from numerous images into a single,            
high-accuracy three-dimensional reconstruction of the synaptic cytoskeleton.    
The immediate result of this collaboration will be an improved understanding    
of the spatial relationships between synaptic macromolecules.  More             
importantly, the project will produce a set of computational tools that can     
be applied to stereo image data sets of various areas of the nervous system,    
from the macroscopic to the molecular level.  Finally, our studies will         
advance the state-of-the-art of parallel computation and interactive            
reconstruction methods that can provide novel solutions to difficult            
problems of neuroscience visualization.                                         
",2453331,R01LM006326,['R01LM006326'],LM,https://reporter.nih.gov/project-details/2453331,R01,1997,153381,-0.07187245250318569
"During Phase I, Flint Hills Scientific developed an algorithm for real          
time quantitative seizure detection which performs with sensitivity and         
specificity equal to expert visual analysis. Of even greater value is the       
capability of this algorithm to predict seizure onset by 13.6 seconds           
(mean), in its generic mode. Preliminary studies indicate that with             
automated individualized adaptation, prediction time can be increased to        
180 seconds or longer. To the best of our knowledge no other system in          
existence has achieved this level of success. These results lay the ground      
for the fulfillment of ""seizure prediction, early recognition and blockage      
of seizures,"" the number one AES research priority.                             
                                                                                
The main goal of Phase II will be to advance, further refine, and validate      
this technology for implementation into a portable or implantable device        
with diagnostic, warning, and therapeutic capabilities. We are confident        
that this technology, by decreasing or eliminating unpredictability, will       
minimize the potentially devastating effect of seizures on quality of life      
while decreasing morbidity, the cost of health care, and the reliance on        
the welfare system. These unique advantages will ensure widespread              
acceptance of this technology by those directly and indirectly affected by      
epilepsy and by the health care system.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
1. Software package for real time seizure prediction, detection,                
localization, imaging, and quantitative analysis. 2. Software package for       
automated, selective noise reduction 3. Portable device for the automated       
early warning of impending seizures. 4. Portable or implantable devices         
for automated early therapeutic intervention.                                   
 artificial intelligence; bioengineering /biomedical engineering; brain electrical activity; clinical biomedical equipment; clinical research; computer program /software; digital imaging; electroencephalography; epilepsy; health care cost /financing; human subject; monitoring device SOFTWARE AND DEVICES FOR REAL TIME SEIZURE DETECTION","During Phase I, Flint Hills Scientific developed an algorithm for real          
time quantitative seizure detection which performs with sensitivity and         
specificity equal to expert visual analysis. Of even greater value is the       
capability of this algorithm to predict seizure onset by 13.6 seconds           
(mean), in its generic mode. Preliminary studies indicate that with             
automated individualized adaptation, prediction time can be increased to        
180 seconds or longer. To the best of our knowledge no other system in          
existence has achieved this level of success. These results lay the ground      
for the fulfillment of ""seizure prediction, early recognition and blockage      
of seizures,"" the number one AES research priority.                             
                                                                                
The main goal of Phase II will be to advance, further refine, and validate      
this technology for implementation into a portable or implantable device        
with diagnostic, warning, and therapeutic capabilities. We are confident        
that this technology, by decreasing or eliminating unpredictability, will       
minimize the potentially devastating effect of seizures on quality of life      
while decreasing morbidity, the cost of health care, and the reliance on        
the welfare system. These unique advantages will ensure widespread              
acceptance of this technology by those directly and indirectly affected by      
epilepsy and by the health care system.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
1. Software package for real time seizure prediction, detection,                
localization, imaging, and quantitative analysis. 2. Software package for       
automated, selective noise reduction 3. Portable device for the automated       
early warning of impending seizures. 4. Portable or implantable devices         
for automated early therapeutic intervention.                                   
",2422022,R44NS034630,['R44NS034630'],NS,https://reporter.nih.gov/project-details/2422022,R44,1997,426936,-0.05751229603601687
"DESCRIPTION (From Applicant's Abstract): Interval and doubly censored data      
and doubly censored current status data are fundamental incomplete data         
types in HIV/AIDS studies, where the exact time of the disease event is         
rarely known and the event is usually first detected during a clinical          
visit. For these types of data the nonparametric maximum likelihood             
estimator (NPMLE) is a more accurate estimator of the survival function         
and possesses other nice statistical properties. Recently the applicants        
have developed an efficient algorithm for computing the NPMLE in interval       
and double censored data. They propose to develop commercial software           
incorporating this algorithm and other recent advances in this area into        
INDUCE, a new product based upon S-PLUS data analysis and graphical             
environment. Using profile likelihood approach INDUCE will, for the             
first time, provide for routine joint estimation of the survival function       
and regression coefficients in a Cox proportional hazards model for             
interval and doubly censored data, and for doubly censored status data.         
The efficiency of the algorithm will also make it feasible to put               
bootstrap confidence bands around the NPMLE and perform significance            
tests.                                                                          
 artificial intelligence; computer data analysis; computer program /software; computer system design /evaluation; epidemiology; human data; human immunodeficiency virus; prognosis; statistics /biometry INDUCE--A NONPARAMETRIC MLE SURVIVAL ANALYSIS MODULE","DESCRIPTION (From Applicant's Abstract): Interval and doubly censored data      
and doubly censored current status data are fundamental incomplete data         
types in HIV/AIDS studies, where the exact time of the disease event is         
rarely known and the event is usually first detected during a clinical          
visit. For these types of data the nonparametric maximum likelihood             
estimator (NPMLE) is a more accurate estimator of the survival function         
and possesses other nice statistical properties. Recently the applicants        
have developed an efficient algorithm for computing the NPMLE in interval       
and double censored data. They propose to develop commercial software           
incorporating this algorithm and other recent advances in this area into        
INDUCE, a new product based upon S-PLUS data analysis and graphical             
environment. Using profile likelihood approach INDUCE will, for the             
first time, provide for routine joint estimation of the survival function       
and regression coefficients in a Cox proportional hazards model for             
interval and doubly censored data, and for doubly censored status data.         
The efficiency of the algorithm will also make it feasible to put               
bootstrap confidence bands around the NPMLE and perform significance            
tests.                                                                          
",2419963,R43AI041789,['R43AI041789'],AI,https://reporter.nih.gov/project-details/2419963,R43,1997,93476,-0.20983318566325157
"The development of an emergency vehicle alert system for hearing                
impaired drivers is proposed for NIA Subject 3 A, 1996. The proposed            
system will display a visual warning signal for hearing impaired (most          
of whom are elderly) or deaf drivers if an emergency vehicle is                 
approaching. Through the display, the driver can quickly determine the          
presence, distance, and direction of the approaching emergency vehicle.         
The Phase I objective will be to verify the feasibility of the proposed         
acoustic alert system concept. The system will consist of microphones,          
a signal processor, and display. Several products for the same purpose          
are currently or were previously marketed. These devices use banks of           
filters and level detection, and they provide only a simple warning             
sign. False detection rate of these devices is very high and the                
performance of these devices is far from satisfactory. The key goal             
addressed by this project is to drastically increase the accuracy and           
reliability of such systems while maintaining low cost. Recent strides          
in signal processing and in the performance of low cost microprocessors         
make this a practical goal.                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION: The proposed alert system for hearing          
impaired drivers also has a potential application for drivers with              
normal hearing. With all windows up and the radio on, drivers with              
normal hearing may have the same difficulties as the hearing impaired           
in detecting warning sirens. Therefore, this system should have a high          
commercial potential.                                                           
 artificial intelligence; biomedical equipment development; computer system design /evaluation; deaf aid EMERGENCY VEHICLE ALERT SYSTEM--HEARING IMPAIRED DRIVERS","The development of an emergency vehicle alert system for hearing                
impaired drivers is proposed for NIA Subject 3 A, 1996. The proposed            
system will display a visual warning signal for hearing impaired (most          
of whom are elderly) or deaf drivers if an emergency vehicle is                 
approaching. Through the display, the driver can quickly determine the          
presence, distance, and direction of the approaching emergency vehicle.         
The Phase I objective will be to verify the feasibility of the proposed         
acoustic alert system concept. The system will consist of microphones,          
a signal processor, and display. Several products for the same purpose          
are currently or were previously marketed. These devices use banks of           
filters and level detection, and they provide only a simple warning             
sign. False detection rate of these devices is very high and the                
performance of these devices is far from satisfactory. The key goal             
addressed by this project is to drastically increase the accuracy and           
reliability of such systems while maintaining low cost. Recent strides          
in signal processing and in the performance of low cost microprocessors         
make this a practical goal.                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION: The proposed alert system for hearing          
impaired drivers also has a potential application for drivers with              
normal hearing. With all windows up and the radio on, drivers with              
normal hearing may have the same difficulties as the hearing impaired           
in detecting warning sirens. Therefore, this system should have a high          
commercial potential.                                                           
",2014678,R43DC003017,['R43DC003017'],DC,https://reporter.nih.gov/project-details/2014678,R43,1997,98815,-0.025009605770888066
"DESCRIPTION (adapted from investigator's abstract): This project develops       
and tests a Resident-Centered Information (RCI) system for Assisted             
Living: a psycho-socially oriented assessment protocol based on the best        
available instruments; a service planning format using an expert decision       
system; an integrated resident-services logging system; and a complete          
set of management modules. The system supports assisted living providers        
serving frail elders and other dependent groups and includes approaches         
to meet the needs of those with cognitive impairment. The system, to be         
developed in both a paper and pencil and computerized form, is easy to          
use, so that it is supportive to staff and creates reliable data for            
continuous improvement and outcomes research.                                   
                                                                                
Nursing homes currently use the Minimum Data Set (MDS) as an industry-wide      
system for assessment, service planning, reporting. However, the MDS, with      
its health care orientation, does not meet the needs of assisted living         
providers. The RCI system will use many of the same data points to allow        
for comparability across settings, and ease of transfer for individual          
users of long-term care services. The new system will build on                  
Hearthstone's experience in assisted living operations and research,            
particularly its current SBIR/NIA project is using assessment and outcome       
measures developed under NIA-funded collaborative studies.                      
 artificial intelligence; behavioral /social science research tag; caregivers; clinical research; cognition disorders; comprehensive care; comprehensive medical planning; computer program /software; computer system design /evaluation; data collection methodology /evaluation; frail elderly; functional ability; health care model; health care service; health services research tag; home health care; human data; information systems; outcomes research; public opinion RESIDENT CENTERED INFORMATION SYSTEM FOR ASSISTED LIVING","DESCRIPTION (adapted from investigator's abstract): This project develops       
and tests a Resident-Centered Information (RCI) system for Assisted             
Living: a psycho-socially oriented assessment protocol based on the best        
available instruments; a service planning format using an expert decision       
system; an integrated resident-services logging system; and a complete          
set of management modules. The system supports assisted living providers        
serving frail elders and other dependent groups and includes approaches         
to meet the needs of those with cognitive impairment. The system, to be         
developed in both a paper and pencil and computerized form, is easy to          
use, so that it is supportive to staff and creates reliable data for            
continuous improvement and outcomes research.                                   
                                                                                
Nursing homes currently use the Minimum Data Set (MDS) as an industry-wide      
system for assessment, service planning, reporting. However, the MDS, with      
its health care orientation, does not meet the needs of assisted living         
providers. The RCI system will use many of the same data points to allow        
for comparability across settings, and ease of transfer for individual          
users of long-term care services. The new system will build on                  
Hearthstone's experience in assisted living operations and research,            
particularly its current SBIR/NIA project is using assessment and outcome       
measures developed under NIA-funded collaborative studies.                      
",2002517,R43AG014617,['R43AG014617'],AG,https://reporter.nih.gov/project-details/2002517,R43,1997,96375,-0.02533313677470351
"DESCRIPTION:  The overall objective of the proposed research is to              
determine  the mechanisms responsible for the thermally induced changes         
that occur in the  coefficients of thermal expansion of dental porcelains       
as a result of multiple firings, slow cooling and post-soldering                
operations.  These changes in  porcelain thermal expansion are                  
detrimental and produce cracks in porcelain  fused to metal restorations        
(PFM) either shortly after firing or sometime  later.  Either event             
causes additional costs and possibly additional trauma to  the patient.         
There are six Specific Aims.  The first is to explain the  thermal              
expansion behavior of porcelains on the basis of leucite and sanidine           
volume fractions, microcrack densities, an leucite particle surface             
areas.  The second is to determine the effect of localized cooling rate         
differences on  the thermal instability of dental porcelain during              
multiple firing, cooling,  and isothermal anneals.  The third is to             
determine the role of H2O as a glass  modifier in dental porcelain and          
specifically in the crystallization of  leucite and sanidine.  The fourth       
is to develop firing schedules for various  porcelain-metal systems to          
minimize the thermally induced microstructural  changes that lead to            
thermal expansion changes.The fifth is to modify porcelains to render           
them more resistant to thermal expansion changes.  The  sixth to develop        
a prototype expert system computer program for solving  thermal expansion       
mismatch problems. Techniques used in pursuit of these aims  include            
quantitative X-Ray diffraction, scanning electron microscopy with  energy       
dispersive X-ray spectroscopy, quantitative stereology, and conventional        
and laser dilatometry.                                                          
 X ray crystallography; biomaterial development /preparation; biomaterial evaluation; computer program /software; lasers; metal dental material; physical property; porcelain; scanning electron microscopy; temperature; thermostability; water solution THERMALLY INDUCED CHANGES IN DENTAL PORCELAIN EXPANSION","DESCRIPTION:  The overall objective of the proposed research is to              
determine  the mechanisms responsible for the thermally induced changes         
that occur in the  coefficients of thermal expansion of dental porcelains       
as a result of multiple firings, slow cooling and post-soldering                
operations.  These changes in  porcelain thermal expansion are                  
detrimental and produce cracks in porcelain  fused to metal restorations        
(PFM) either shortly after firing or sometime  later.  Either event             
causes additional costs and possibly additional trauma to  the patient.         
There are six Specific Aims.  The first is to explain the  thermal              
expansion behavior of porcelains on the basis of leucite and sanidine           
volume fractions, microcrack densities, an leucite particle surface             
areas.  The second is to determine the effect of localized cooling rate         
differences on  the thermal instability of dental porcelain during              
multiple firing, cooling,  and isothermal anneals.  The third is to             
determine the role of H2O as a glass  modifier in dental porcelain and          
specifically in the crystallization of  leucite and sanidine.  The fourth       
is to develop firing schedules for various  porcelain-metal systems to          
minimize the thermally induced microstructural  changes that lead to            
thermal expansion changes.The fifth is to modify porcelains to render           
them more resistant to thermal expansion changes.  The  sixth to develop        
a prototype expert system computer program for solving  thermal expansion       
mismatch problems. Techniques used in pursuit of these aims  include            
quantitative X-Ray diffraction, scanning electron microscopy with  energy       
dispersive X-ray spectroscopy, quantitative stereology, and conventional        
and laser dilatometry.                                                          
",2458588,R01DE007806,['R01DE007806'],DE,https://reporter.nih.gov/project-details/2458588,R01,1997,147757,-0.051227764049953925
"The proposed research seeks to understand the process by which an older         
adult who is experiencing an episode of illness decides whether to seek         
the care of a physician. Two complementary methodological approaches are        
taken.  (l) Detailed illness episode data from a seven wave longitudinal        
panel study of 1009 Medicare recipients enrolled in a Health Maintenance        
Organization (HMO) will be used to develop a series of event history            
models of physician contact.  The effects of various explanatory variables      
(sociodemographic, prior health history) and time.varying covariates            
(other illness response strategies) on the risk of seeking medical care         
will be estimated in proportional hazards models that control for specific      
illness types and respondent categories. (2) A sample of 150 ethnographic       
informants will be selected from the original group of respondents to           
represent theoretically relevant categories of the population (e.g., men,       
women, married, not married, U.S. born, immigrants, health-status).             
Detailed ethnographic data on illnesses experienced in later life, their        
causes and symptoms, and the range of available and appropriate treatment       
alternatives for each illness will be collected systematically using free       
lists, card sorts, paired comparisons, and sentence frames.  These data         
will be analyzed using techniques such as consensus analysis,                   
multidimensional scaling and hierarchical clustering. Expressed rules for       
deciding among treatment alternatives, particularly whether to contact a        
physician, will be explored using ethnographic decision tree modeling           
techniques. Both approaches should yield information useful to planners         
and evaluators of health education programs aimed at increasing the match       
between need for services as defined by the medical profession, and actual      
use of medical services by this population.                                     
 Medicare /Medicaid; age difference; behavioral /social science research tag; decision making; gender difference; health behavior; health care model; health care service utilization; human data; human old age (65+); human subject; human very old age (85+); interview; longitudinal human study; managed care; mathematical model; population survey; racial /ethnic difference; religion; self care; socioeconomics DECISIONS ABOUT HMO SERVICE USE FOR LATE LIFE ILLNESS","The proposed research seeks to understand the process by which an older         
adult who is experiencing an episode of illness decides whether to seek         
the care of a physician. Two complementary methodological approaches are        
taken.  (l) Detailed illness episode data from a seven wave longitudinal        
panel study of 1009 Medicare recipients enrolled in a Health Maintenance        
Organization (HMO) will be used to develop a series of event history            
models of physician contact.  The effects of various explanatory variables      
(sociodemographic, prior health history) and time.varying covariates            
(other illness response strategies) on the risk of seeking medical care         
will be estimated in proportional hazards models that control for specific      
illness types and respondent categories. (2) A sample of 150 ethnographic       
informants will be selected from the original group of respondents to           
represent theoretically relevant categories of the population (e.g., men,       
women, married, not married, U.S. born, immigrants, health-status).             
Detailed ethnographic data on illnesses experienced in later life, their        
causes and symptoms, and the range of available and appropriate treatment       
alternatives for each illness will be collected systematically using free       
lists, card sorts, paired comparisons, and sentence frames.  These data         
will be analyzed using techniques such as consensus analysis,                   
multidimensional scaling and hierarchical clustering. Expressed rules for       
deciding among treatment alternatives, particularly whether to contact a        
physician, will be explored using ethnographic decision tree modeling           
techniques. Both approaches should yield information useful to planners         
and evaluators of health education programs aimed at increasing the match       
between need for services as defined by the medical profession, and actual      
use of medical services by this population.                                     
",2457545,R29AG010887,['R29AG010887'],AG,https://reporter.nih.gov/project-details/2457545,R29,1997,98501,-0.05224983468671817
"The long-term objective is to develop computer technology needed to             
accomplish the objectives of the Human Genome Project and to apply the          
technology to the analysis and management of sequencing data.  Currently,       
a database search for sequence similarities represents the most direct          
computational approach to the analysis of genomic information.  However,        
the search is becoming ever more forbidding due to the accelerating             
growth of sequencing data.  The goal of the proposed research is to             
further develop and enhance a software tool for speedy classification of        
unknown sequences, and make it available to the genome community.  The          
research will build upon a pilot system designed and developed by the           
principal investigator that has shown great promise.  The specific aims         
are (1) to enhance the tool for speedy identification of PIR                    
superfamilies and ProSite patterns, (2) to develop a pilot DNA/RNA              
classification system, (3) to distribute the tool, and (4) to aid PIR           
protein database and RDP ribosomal RNA database organization.  In               
contrast to other search methods whose search time grows linearly with          
the number of entries in the database, the time of the proposed tool            
grows with the number of families, which is likely to remain low.  The          
tool would automate family assignment which is especially important for         
managing the influx of new data in a timely manner.                             
                                                                                
The proposed research applies neural network technology to solving the          
database search/organization problem.  The major design principles              
involve an encoding schema to extract sequence information and a modular        
architecture to scale up backpropagation networks.  The encoding                
algorithm is a hashing function similar to the k-tuple method.  A pilot         
system has been implemented on a Cray supercomputer to classify electron        
transfer proteins and enzymes.  The system achieves about 90% accuracy          
and 50 times speed of other search methods.  The speed may be 1000 times        
faster than others in a decade if the database continues to grow at the         
current rate.  In the proposed research, the sensitivity of the tool            
would be improved and a full-scale system would be developed.  The              
automated software tool would be portable at the source code, user              
interface, and hardware levels.  The system would be updated in                 
accordance with database releases, and distributed to the research              
community via anonymous ftp.  The tool would be used to classify PIR            
sequences according to superfamilies and to classify ribosomal RNA              
sequences according to phylogenetic relations.                                  
 artificial intelligence; computer assisted sequence analysis; computer program /software; computer system design /evaluation; electron transport; genome; information systems; nucleic acid sequence; protein sequence CLASSIFICATION NEURAL NETWORKS FOR GENOME RESEARCH","The long-term objective is to develop computer technology needed to             
accomplish the objectives of the Human Genome Project and to apply the          
technology to the analysis and management of sequencing data.  Currently,       
a database search for sequence similarities represents the most direct          
computational approach to the analysis of genomic information.  However,        
the search is becoming ever more forbidding due to the accelerating             
growth of sequencing data.  The goal of the proposed research is to             
further develop and enhance a software tool for speedy classification of        
unknown sequences, and make it available to the genome community.  The          
research will build upon a pilot system designed and developed by the           
principal investigator that has shown great promise.  The specific aims         
are (1) to enhance the tool for speedy identification of PIR                    
superfamilies and ProSite patterns, (2) to develop a pilot DNA/RNA              
classification system, (3) to distribute the tool, and (4) to aid PIR           
protein database and RDP ribosomal RNA database organization.  In               
contrast to other search methods whose search time grows linearly with          
the number of entries in the database, the time of the proposed tool            
grows with the number of families, which is likely to remain low.  The          
tool would automate family assignment which is especially important for         
managing the influx of new data in a timely manner.                             
                                                                                
The proposed research applies neural network technology to solving the          
database search/organization problem.  The major design principles              
involve an encoding schema to extract sequence information and a modular        
architecture to scale up backpropagation networks.  The encoding                
algorithm is a hashing function similar to the k-tuple method.  A pilot         
system has been implemented on a Cray supercomputer to classify electron        
transfer proteins and enzymes.  The system achieves about 90% accuracy          
and 50 times speed of other search methods.  The speed may be 1000 times        
faster than others in a decade if the database continues to grow at the         
current rate.  In the proposed research, the sensitivity of the tool            
would be improved and a full-scale system would be developed.  The              
automated software tool would be portable at the source code, user              
interface, and hardware levels.  The system would be updated in                 
accordance with database releases, and distributed to the research              
community via anonymous ftp.  The tool would be used to classify PIR            
sequences according to superfamilies and to classify ribosomal RNA              
sequences according to phylogenetic relations.                                  
",2445394,R29LM005524,['R29LM005524'],LM,https://reporter.nih.gov/project-details/2445394,R29,1997,98580,-0.06634009470812134
"The goal of this proposal is to generate method and tools to link               
knowledge-based systems (KBSs) to real clinical databases.  The linking         
is done via quires that map conceptual entities in the KBS to actual            
entries in the database. They may be used when a KBS is first created,          
when an existing KBS is linked to a database or when a KBS is transferred       
to another institution.  The primary purpose of the queries is to apply         
the KBS to individual patients for direct patient care, rather than to          
extend the knowledge base itself.  Several of the tools (Aims 3 and 4)          
are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All         
the proposed tools will be applied to CPMC's MLM knowledge base, which          
is actively used for patient care.                                              
                                                                                
AIM 1 to increase the accuracy while reducing the writing time and              
technical skills required to author clinical database queries.  Query by        
Review will provide a familiar interface to novice query authors,               
enabling them to write queries by traversing the same type of result            
review screens that they use every day in clinical care. When data are          
selected from the screen, the tools will generate an appropriate query          
(in Arden Syntax and HL7) that can be inserted into a KBS or used for           
clinical research.                                                              
                                                                                
AIM 2 to facilitate the testing of queries, and to improve the match            
between a query's result and the needs to a KBS.  The Clinical Database         
Brower will allow author to characterize the data returned by a query in        
order to determine if they query is appropriate.  It will also allow the        
author to determine whether additional logic is necessary to convert the        
raw data into form expected by the KBS.  The Brower's design is unique          
in its use of a semantic network to aggregate complex categorical data.         
                                                                                
AIM 3 to provide an environment for inserting queries and additional            
logic into the KBS.  The Advanced MLM Editor will include a mechanism for       
inserting queries generated by the Query by Review tool into MLMs.  It          
will also allow authors to reuse queries and logic employed in existing         
MLMs.                                                                           
                                                                                
AIM 4 to facilitate the testing of queries within the environment of the        
KBS.  The Event Playback tool will allow the MLM author to run an MLM           
against the clinical database to see whether the MLM performs as                
expected.  Rather than presenting a snapshot of the database, the tool          
will run the MLM as if medical events (e.g, clinical database                   
transactions) were occurring in real time, better simulating actual use.        
The Interactive MLM Interpreter will allow an author to debug an MLM by         
running it line-by line; the author will be able to respond to each query       
manually.  It will also support the batch testing of MLMs using a test          
data set.                                                                       
                                                                                
AIM 5 to evaluate and disseminate the proposed tools.  The impact of the        
Query by Review tool on query authoring time and query accuracy will be         
measured.  To assess the usefulness of the tools for non-Arden Syntax           
KBS, findings from the QMR vocabulary will be studied.  Usage of the            
tools will be measured for CPMC's production KBS.  Tools, components, and       
methods will be disseminated.                                                   
 abstracting; artificial intelligence; computer assisted patient care; computer human interaction; health care facility information system; human subject; information retrieval; physicians; vocabulary development for information system LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES","The goal of this proposal is to generate method and tools to link               
knowledge-based systems (KBSs) to real clinical databases.  The linking         
is done via quires that map conceptual entities in the KBS to actual            
entries in the database. They may be used when a KBS is first created,          
when an existing KBS is linked to a database or when a KBS is transferred       
to another institution.  The primary purpose of the queries is to apply         
the KBS to individual patients for direct patient care, rather than to          
extend the knowledge base itself.  Several of the tools (Aims 3 and 4)          
are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All         
the proposed tools will be applied to CPMC's MLM knowledge base, which          
is actively used for patient care.                                              
                                                                                
AIM 1 to increase the accuracy while reducing the writing time and              
technical skills required to author clinical database queries.  Query by        
Review will provide a familiar interface to novice query authors,               
enabling them to write queries by traversing the same type of result            
review screens that they use every day in clinical care. When data are          
selected from the screen, the tools will generate an appropriate query          
(in Arden Syntax and HL7) that can be inserted into a KBS or used for           
clinical research.                                                              
                                                                                
AIM 2 to facilitate the testing of queries, and to improve the match            
between a query's result and the needs to a KBS.  The Clinical Database         
Brower will allow author to characterize the data returned by a query in        
order to determine if they query is appropriate.  It will also allow the        
author to determine whether additional logic is necessary to convert the        
raw data into form expected by the KBS.  The Brower's design is unique          
in its use of a semantic network to aggregate complex categorical data.         
                                                                                
AIM 3 to provide an environment for inserting queries and additional            
logic into the KBS.  The Advanced MLM Editor will include a mechanism for       
inserting queries generated by the Query by Review tool into MLMs.  It          
will also allow authors to reuse queries and logic employed in existing         
MLMs.                                                                           
                                                                                
AIM 4 to facilitate the testing of queries within the environment of the        
KBS.  The Event Playback tool will allow the MLM author to run an MLM           
against the clinical database to see whether the MLM performs as                
expected.  Rather than presenting a snapshot of the database, the tool          
will run the MLM as if medical events (e.g, clinical database                   
transactions) were occurring in real time, better simulating actual use.        
The Interactive MLM Interpreter will allow an author to debug an MLM by         
running it line-by line; the author will be able to respond to each query       
manually.  It will also support the batch testing of MLMs using a test          
data set.                                                                       
                                                                                
AIM 5 to evaluate and disseminate the proposed tools.  The impact of the        
Query by Review tool on query authoring time and query accuracy will be         
measured.  To assess the usefulness of the tools for non-Arden Syntax           
KBS, findings from the QMR vocabulary will be studied.  Usage of the            
tools will be measured for CPMC's production KBS.  Tools, components, and       
methods will be disseminated.                                                   
",2430871,R29LM005627,['R29LM005627'],LM,https://reporter.nih.gov/project-details/2430871,R29,1997,113161,-0.05192527914054365
 artificial intelligence; biomedical equipment development; computer program /software; computer system design /evaluation; confocal scanning microscopy; image enhancement; microscopy; neurosciences BRIGHTFIELD MICROSCOPY 3D IMAGE RECONSTRUCTION,,2423759,R44MH053691,['R44MH053691'],MH,https://reporter.nih.gov/project-details/2423759,R44,1997,342962,-0.06948859116191433
"Imaging detectors for photons in the 10 to 150 keV energy range have            
many uses in medical technology including tumor imaging, SPECT, and             
radiography.  Digital output is particularly useful since it allows             
image enhancement, analysis, transmission and storage.  An approach is          
proposed for developing a new technology, using CdZnTe detector                 
material, which would allow images to be obtained with 100 u spatial            
resolution and energy resolution of 2% or better. This capability would         
facilitate entirely new classes of medical diagnostic procedures. For           
example, ""dual energy"" angiography could now be done using a                    
polychromatic x-ray source.                                                     
                                                                                
The present effort will demonstrate the feasibility of the conceptual           
approach and create the technological foundation upon which later,              
application specific instruments can be built.  Detectors will be               
modeled, using Monte Carlo and electron transport, to predict their             
signal outputs. These outputs will be compared to measured signals to           
validate the models. The models will then be employed to develop signal         
processing algorithms which achieve the desired energy and spatial              
resolutions.  Finally, electronics will be designed to implement the            
algorithms.  In Phase II a working model would be constructed and               
tested.                                                                         
PROPOSED COMMERCIAL APPLICATION                                                 
As an energy resolved digital detector with 100 u spatial resolution,           
the proposed detector technology could find many medical applications,          
including SPECT, energy resolved angiography for small mammals, bone            
densitometry on rodent bones, and small, hand-held gamma cameras.  Non-         
medical applications would include non-destructive testing,                     
astrophysical gamma imaging, nuclear cleanup uses, and scientific               
instruments.                                                                    
 X ray; artificial intelligence; biomedical equipment development; cadmium; clinical biomedical equipment; digital imaging; gamma radiation; imaging /visualization /scanning; tellurium; zinc DIGITAL CZT X FOR GAMMA RAY IMAGING DETECTOR","Imaging detectors for photons in the 10 to 150 keV energy range have            
many uses in medical technology including tumor imaging, SPECT, and             
radiography.  Digital output is particularly useful since it allows             
image enhancement, analysis, transmission and storage.  An approach is          
proposed for developing a new technology, using CdZnTe detector                 
material, which would allow images to be obtained with 100 u spatial            
resolution and energy resolution of 2% or better. This capability would         
facilitate entirely new classes of medical diagnostic procedures. For           
example, ""dual energy"" angiography could now be done using a                    
polychromatic x-ray source.                                                     
                                                                                
The present effort will demonstrate the feasibility of the conceptual           
approach and create the technological foundation upon which later,              
application specific instruments can be built.  Detectors will be               
modeled, using Monte Carlo and electron transport, to predict their             
signal outputs. These outputs will be compared to measured signals to           
validate the models. The models will then be employed to develop signal         
processing algorithms which achieve the desired energy and spatial              
resolutions.  Finally, electronics will be designed to implement the            
algorithms.  In Phase II a working model would be constructed and               
tested.                                                                         
PROPOSED COMMERCIAL APPLICATION                                                 
As an energy resolved digital detector with 100 u spatial resolution,           
the proposed detector technology could find many medical applications,          
including SPECT, energy resolved angiography for small mammals, bone            
densitometry on rodent bones, and small, hand-held gamma cameras.  Non-         
medical applications would include non-destructive testing,                     
astrophysical gamma imaging, nuclear cleanup uses, and scientific               
instruments.                                                                    
",2423599,R43CA075844,['R43CA075844'],CA,https://reporter.nih.gov/project-details/2423599,R43,1997,94340,-0.07041976447498796
 artificial intelligence; biomedical automation; computer simulation; digital imaging; evaluation /testing; human subject; method development; model design /development; phantom model; volunteer SYSTEM FOR TESTING RESOLUTION OF ULTRASOUND SCANNERS,,2422178,R42GM054377,['R42GM054377'],GM,https://reporter.nih.gov/project-details/2422178,R42,1997,230870,-0.09572259230934133
 artificial intelligence; computer program /software; disease /disorder etiology; mathematical model; meta analysis; method development; statistics /biometry TRINOMIAL BIVARIATE NEURAL NETWORKS FOR GIS ANALYSIS,,2416847,R41CA073131,['R41CA073131'],CA,https://reporter.nih.gov/project-details/2416847,R41,1997,94854,-0.004550265508730355
"DESCRIPTION:  The proposed research will focus on the mechanisms of electron    
transfer and proton pumping in respiration.  Special emphasis will be on        
cytochrome oxidase, the primary site of coupling between the two processes.     
Our goals include the following:  1) The mechanism of the reduction of          
dixoygen to water by cytochrome oxidase will be studied under a variety of      
conditions with the flow-flash method, which uses the photolability of the      
CO complex to initiate the redox activity with O2.  Time-resolved               
multichannel optical absorption spectroscopy will be used to follow the         
kinetics of electron and proton transfer on time scales of nanoseconds to       
milliseconds.  Singular value decomposition and global exponential fitting      
methods will be applied to analyze the kinetics and determine the UV-Vis        
spectra of the transient intermediates.  These studies should provide new       
insight into the mechanism of the dioxygen reduction by cytochrome oxidase.     
2) Alternatives to CO photodissociation will be used to investigate the         
reaction of O2 with cytochrome oxidase.  The reaction of unliganded reduced     
cytochrome oxidase with oxygen will be studied using a superfast direct         
mixing method, pulsed-accelerated-flow (PAF).  The reaction of oxygen with      
the unliganded enzyme will also be investigated using O2 which is produced      
in situ on any relevant time scale by photodissociating synthetic dioxygen      
carriers such as dicobalt u-peroxo polyaine complexes.  Both the PAF method     
and the photodissociation of the dicobalt u-peroxo and u-superoxo polyamine     
complexes represent new approaches to study the fast dioxygen reactions of      
cytochrome oxidase and both avoid the mechanistic ambiguities associated        
with the fate of photodissociated CO in transitional flow-flash experiments.    
3) The mechanism of the redox-linked proton pump in cytochrome oxidase will     
be investigated.  The kinetics of electron transfer and proton pumping upon     
flash-induced oxidation of cytochrome oxidase reconstituted into                
phospholipid vesicles will be monitored using time-resolved optical             
absorption spectroscopy.  The proton pumping reactions will be probed by pH     
indicators located int he extra-vesicular space, trapped inside the             
vesicles, or covalently bound to the lipid or protein.  These studies will      
allow us to correlate proton pumping events with individual steps in the        
dioxygen/cytochrome oxidase redox cycle and will provide a foundation for a     
structural model of the energy transduction mechanism in cytochrome oxidase.    
 bioenergetics; carbon monoxide; cytochrome oxidase; electron transport; enzyme mechanism; enzyme reconstitution; flash photolysis; membrane model; membrane transport proteins; oxidation reduction reaction; oxygen; photochemistry; spectrometry; time resolved data ELECTRON TRANSFER/PROTON PUMPING IN CYTOCHROME OXIDATION","DESCRIPTION:  The proposed research will focus on the mechanisms of electron    
transfer and proton pumping in respiration.  Special emphasis will be on        
cytochrome oxidase, the primary site of coupling between the two processes.     
Our goals include the following:  1) The mechanism of the reduction of          
dixoygen to water by cytochrome oxidase will be studied under a variety of      
conditions with the flow-flash method, which uses the photolability of the      
CO complex to initiate the redox activity with O2.  Time-resolved               
multichannel optical absorption spectroscopy will be used to follow the         
kinetics of electron and proton transfer on time scales of nanoseconds to       
milliseconds.  Singular value decomposition and global exponential fitting      
methods will be applied to analyze the kinetics and determine the UV-Vis        
spectra of the transient intermediates.  These studies should provide new       
insight into the mechanism of the dioxygen reduction by cytochrome oxidase.     
2) Alternatives to CO photodissociation will be used to investigate the         
reaction of O2 with cytochrome oxidase.  The reaction of unliganded reduced     
cytochrome oxidase with oxygen will be studied using a superfast direct         
mixing method, pulsed-accelerated-flow (PAF).  The reaction of oxygen with      
the unliganded enzyme will also be investigated using O2 which is produced      
in situ on any relevant time scale by photodissociating synthetic dioxygen      
carriers such as dicobalt u-peroxo polyaine complexes.  Both the PAF method     
and the photodissociation of the dicobalt u-peroxo and u-superoxo polyamine     
complexes represent new approaches to study the fast dioxygen reactions of      
cytochrome oxidase and both avoid the mechanistic ambiguities associated        
with the fate of photodissociated CO in transitional flow-flash experiments.    
3) The mechanism of the redox-linked proton pump in cytochrome oxidase will     
be investigated.  The kinetics of electron transfer and proton pumping upon     
flash-induced oxidation of cytochrome oxidase reconstituted into                
phospholipid vesicles will be monitored using time-resolved optical             
absorption spectroscopy.  The proton pumping reactions will be probed by pH     
indicators located int he extra-vesicular space, trapped inside the             
vesicles, or covalently bound to the lipid or protein.  These studies will      
allow us to correlate proton pumping events with individual steps in the        
dioxygen/cytochrome oxidase redox cycle and will provide a foundation for a     
structural model of the energy transduction mechanism in cytochrome oxidase.    
",2023200,R01GM053788,['R01GM053788'],GM,https://reporter.nih.gov/project-details/2023200,R01,1997,155636,-0.024871539071547696
"We propose to work in the development and application of mathematical,          
statistical, and computational methods for the analysis of nucleic acid         
and amino acid sequence data. The long range goals can be placed into           
three categories. (1) Computational analysis is essential to our                
approaches to sequence data. Algorithms are being developed for shotgun         
sequence assembly, to search for tandem repeats of length up to 32              
basepairs, to find the consensus local alignment of an unknown region           
common to an unknown subset of sequences, to study the                          
thermodynamic/statistical behavior of experiments that repeatedly select        
and amplify DNA molecules, and to weight multiple and suboptimal sequence       
alignment paths. (2) Physical mapping of DNA is important in genome             
analysis. Studies include the PEP procedure to amplify single chromosomes,      
PCR is a branching process including both amplification errors and              
efficiency less than 1, the mathematical analysis of physical mapping           
using end characterized clones, and classification of multiple solutions        
of the double digest problem. (3) As sequence data increase, estimating         
statistical significance becomes more central. We will develop methods for      
estimating statistical significance of scores of tandem repeats, Poisson        
distributional results for sequence alignment in certain cases where the        
Chen-Stein method fails, the statistical distribution of correctly              
inferred sequence in shotgun sequencing projects as a function of depth         
and accuracy, and the growth of minimum free energy of secondary                
structures of a random RNA.                                                     
 RNA; RNA splicing; artificial intelligence; computer assisted sequence analysis; computer program /software; computer simulation; computer system design /evaluation; gene mutation; genetic mapping; genetic models; mathematical model; model design /development; molecular genetics; nucleic acid sequence; polymerase chain reaction; protein sequence; ribosomal RNA; statistics /biometry PATTERN RECOGNITION FOR ANALYSIS OF MOLECULAR SEQUENCES","We propose to work in the development and application of mathematical,          
statistical, and computational methods for the analysis of nucleic acid         
and amino acid sequence data. The long range goals can be placed into           
three categories. (1) Computational analysis is essential to our                
approaches to sequence data. Algorithms are being developed for shotgun         
sequence assembly, to search for tandem repeats of length up to 32              
basepairs, to find the consensus local alignment of an unknown region           
common to an unknown subset of sequences, to study the                          
thermodynamic/statistical behavior of experiments that repeatedly select        
and amplify DNA molecules, and to weight multiple and suboptimal sequence       
alignment paths. (2) Physical mapping of DNA is important in genome             
analysis. Studies include the PEP procedure to amplify single chromosomes,      
PCR is a branching process including both amplification errors and              
efficiency less than 1, the mathematical analysis of physical mapping           
using end characterized clones, and classification of multiple solutions        
of the double digest problem. (3) As sequence data increase, estimating         
statistical significance becomes more central. We will develop methods for      
estimating statistical significance of scores of tandem repeats, Poisson        
distributional results for sequence alignment in certain cases where the        
Chen-Stein method fails, the statistical distribution of correctly              
inferred sequence in shotgun sequencing projects as a function of depth         
and accuracy, and the growth of minimum free energy of secondary                
structures of a random RNA.                                                     
",2022091,R01GM036230,['R01GM036230'],GM,https://reporter.nih.gov/project-details/2022091,R01,1997,417552,-0.024187173717371408
"DESCRIPTION:  (Adapted from investigator's abstract) This project will          
examine new methodology for making inference about the regression parameters    
in the presence of missing covariate data for two commonly used classes of      
regression models.  In particular, we examine the class of generalized          
linear models for general types of response data and the Cox model for          
survival data.  The methodology addresses problems occurring frequently in      
clinical investigations for chronic disease, including cancer and AIDS.  The    
specific objectives of the project are to:  1) Develop and study classical      
and Bayesian methods of inference for the class of generalized linear models    
(GLM's) in the presence of missing covariate data.  In particular, we will      
i) examine methods for estimating the regression parameters when the missing    
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Also, parametric models for the covariate              
distribution will be examined.  The methods of estimation will focus on the     
Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other        
related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990)      
along with the adaptive rejection algorithm of Gilks and Wild (1992) will be    
used to sample from the conditional distribution of the missing covariates      
given the observed data.  ii) examine estimating the regression parameters      
when the missing covariates are either categorical or continuous and the        
missing data mechanism is nonignorable.  Models for the missing data            
mechanism will be studied.  iii) develop and study Bayesian methods of          
inference in the presence of missing covariate data when the missing            
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Parametric prior distributions for the regression      
coefficients are proposed.  Properties of the posterior distributions of the    
regression coefficients will be studied.  The methodology will be               
implemented using Markov Chain Monte Carlo methods similar to those of          
Tanner and Wong (1987).  iv) investigate Bayesian methods when the              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Multinomial models for the missing data             
mechanism will be studied.  Dirichlet prior distributions for the               
multinomial parameters will be investigated.                                    
                                                                                
2) Develop and study classical and Bayesian methods of inference for the Cox    
model for survival outcomes in the presence of missing covariates.              
Specifically, we will i) develop and study estimation methods for the Cox       
model for survival outcomes in the presence of missing covariates.  Methods     
for estimating the regression parameters when the missing covariates are        
either categorical or continuous will be studied.  The methods of estimation    
will focus on an EM type algorithm similar to that of Wei and Tanner (1990).    
ii) study estimation of the regression parameters when the missing              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Models for the missing data mechanism will be       
studied.  Bayesian methods similar to those of 1-iii) and iv) will be           
investigated.  Computational techniques using the Monte Carlo methods           
described in 1-iii) will be implemented.                                        
 artificial intelligence; computer data analysis; data collection methodology /evaluation; human data; mathematical model; method development; model design /development; statistics /biometry INFERENCE IN REGRESSION MODELS WITH MISSING COVARIATES","DESCRIPTION:  (Adapted from investigator's abstract) This project will          
examine new methodology for making inference about the regression parameters    
in the presence of missing covariate data for two commonly used classes of      
regression models.  In particular, we examine the class of generalized          
linear models for general types of response data and the Cox model for          
survival data.  The methodology addresses problems occurring frequently in      
clinical investigations for chronic disease, including cancer and AIDS.  The    
specific objectives of the project are to:  1) Develop and study classical      
and Bayesian methods of inference for the class of generalized linear models    
(GLM's) in the presence of missing covariate data.  In particular, we will      
i) examine methods for estimating the regression parameters when the missing    
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Also, parametric models for the covariate              
distribution will be examined.  The methods of estimation will focus on the     
Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other        
related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990)      
along with the adaptive rejection algorithm of Gilks and Wild (1992) will be    
used to sample from the conditional distribution of the missing covariates      
given the observed data.  ii) examine estimating the regression parameters      
when the missing covariates are either categorical or continuous and the        
missing data mechanism is nonignorable.  Models for the missing data            
mechanism will be studied.  iii) develop and study Bayesian methods of          
inference in the presence of missing covariate data when the missing            
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Parametric prior distributions for the regression      
coefficients are proposed.  Properties of the posterior distributions of the    
regression coefficients will be studied.  The methodology will be               
implemented using Markov Chain Monte Carlo methods similar to those of          
Tanner and Wong (1987).  iv) investigate Bayesian methods when the              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Multinomial models for the missing data             
mechanism will be studied.  Dirichlet prior distributions for the               
multinomial parameters will be investigated.                                    
                                                                                
2) Develop and study classical and Bayesian methods of inference for the Cox    
model for survival outcomes in the presence of missing covariates.              
Specifically, we will i) develop and study estimation methods for the Cox       
model for survival outcomes in the presence of missing covariates.  Methods     
for estimating the regression parameters when the missing covariates are        
either categorical or continuous will be studied.  The methods of estimation    
will focus on an EM type algorithm similar to that of Wei and Tanner (1990).    
ii) study estimation of the regression parameters when the missing              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Models for the missing data mechanism will be       
studied.  Bayesian methods similar to those of 1-iii) and iv) will be           
investigated.  Computational techniques using the Monte Carlo methods           
described in 1-iii) will be implemented.                                        
",2012012,R01CA074015,['R01CA074015'],CA,https://reporter.nih.gov/project-details/2012012,R01,1997,170141,-0.17035510151251182
"The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: i) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
i) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotherapeutically induced mutants         
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary' relationships and events, provide                
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
 DNA directed DNA polymerase; DNA directed RNA polymerase; DNA topoisomerases; Paramyxovirus; RNA directed DNA polymerase; RNA virus; Rhabdoviridae; bacterial proteins; biochemical evolution; computer assisted sequence analysis; genetic recombination; integrase; method development; nuclease; protein sequence; ribonuclease III; virus DNA; virus RNA; virus protein COMPUTER-BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION","The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: i) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
i) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotherapeutically induced mutants         
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary' relationships and events, provide                
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
",2517115,K04AI001277,['K04AI001277'],AI,https://reporter.nih.gov/project-details/2517115,K04,1997,63885,-0.08056485212065131
"The proposed mentored research award aims to develop the applicant's            
basic research skills and to prepare her for independent                        
investigations directed at exploring and dissecting the neural bases of         
motor and related dysfunctions in neurological disorders and normal             
aging using imaging techniques, psychophysical studies and brain                
network modeling. The primary mentor for this plan is Dr. David                 
Eidelberg, a neurologist with vast experience in the field of PET               
imaging and movement disorders. Dr. Claude Ghez, a neuroscientist               
who has worked in the field of motor control for over 25 years will             
serve as co-mentor. Dr. Glyn Johnson, experienced physicist in the              
field of magnetic resonance, will serve as consultant to instruct the           
applicant in functional MRI. Drs. Jim Moeller and Eva Petkova will              
further the applicant's education in advanced statistics. Dr. Vijay             
Dhawan will be consultant for PET biophysics and quantification. Dr.            
Ken Perrine, neuropsychologist, will also be available as consultant.           
                                                                                
Research plan: The goal of this study is to characterize changes in             
neural processing underlying implicit and explicit forms of motor               
learning in patients with Parkinson's disease and in a control normal           
aging population. A new family of motor tasks developed in Dr. Ghez's           
lab will be used during 150-H2O positron emission tomography. A new             
analytical methodology based on principal component analysis will be            
applied on measures of regional blood flow to determine how inter-              
subject differences in motor learning are reflected in modulations of           
brain network expression. These same methods will be used to                    
determine the neural bases of two therapeutic inventions in                     
Parkinson's disease, ventral pallidotomy and pallidal stimulation, and          
to assess their effectiveness in reversing alterations in motor                 
learning.                                                                       
                                                                                
Educational plan: The educational plan will focus on the following              
objectives: 1) Completion of the research plan. 2) Learning imaging             
techniques, including image acquisition and analysis using PET and              
fMRI. 3) Development of testing techniques to be used in fMRI. 4)               
Acquisition of skills in brain network analysis (including SSM) to be           
applied to both PET and NMR imaging. 5) Furthering the applicants               
statistical and computational skills through courses taken at Columbia          
University School of Public Health. 6) Developing the applicant's               
laboratory management, supervision, research communication and                  
mentoring skills. 7) Receiving instruction in the responsible conduct           
of research.                                                                    
 Parkinson's disease; blood flow measurement; brain electronic stimulator; brain imaging /visualization /scanning; clinical research; computational neuroscience; functional magnetic resonance imaging; human subject; learning; neural information processing; positron emission tomography; psychosurgery MOTOR LEARNING IN PARKINSONS DISEASE","The proposed mentored research award aims to develop the applicant's            
basic research skills and to prepare her for independent                        
investigations directed at exploring and dissecting the neural bases of         
motor and related dysfunctions in neurological disorders and normal             
aging using imaging techniques, psychophysical studies and brain                
network modeling. The primary mentor for this plan is Dr. David                 
Eidelberg, a neurologist with vast experience in the field of PET               
imaging and movement disorders. Dr. Claude Ghez, a neuroscientist               
who has worked in the field of motor control for over 25 years will             
serve as co-mentor. Dr. Glyn Johnson, experienced physicist in the              
field of magnetic resonance, will serve as consultant to instruct the           
applicant in functional MRI. Drs. Jim Moeller and Eva Petkova will              
further the applicant's education in advanced statistics. Dr. Vijay             
Dhawan will be consultant for PET biophysics and quantification. Dr.            
Ken Perrine, neuropsychologist, will also be available as consultant.           
                                                                                
Research plan: The goal of this study is to characterize changes in             
neural processing underlying implicit and explicit forms of motor               
learning in patients with Parkinson's disease and in a control normal           
aging population. A new family of motor tasks developed in Dr. Ghez's           
lab will be used during 150-H2O positron emission tomography. A new             
analytical methodology based on principal component analysis will be            
applied on measures of regional blood flow to determine how inter-              
subject differences in motor learning are reflected in modulations of           
brain network expression. These same methods will be used to                    
determine the neural bases of two therapeutic inventions in                     
Parkinson's disease, ventral pallidotomy and pallidal stimulation, and          
to assess their effectiveness in reversing alterations in motor                 
learning.                                                                       
                                                                                
Educational plan: The educational plan will focus on the following              
objectives: 1) Completion of the research plan. 2) Learning imaging             
techniques, including image acquisition and analysis using PET and              
fMRI. 3) Development of testing techniques to be used in fMRI. 4)               
Acquisition of skills in brain network analysis (including SSM) to be           
applied to both PET and NMR imaging. 5) Furthering the applicants               
statistical and computational skills through courses taken at Columbia          
University School of Public Health. 6) Developing the applicant's               
laboratory management, supervision, research communication and                  
mentoring skills. 7) Receiving instruction in the responsible conduct           
of research.                                                                    
",2471729,K08NS001961,['K08NS001961'],NS,https://reporter.nih.gov/project-details/2471729,K08,1997,80136,-0.012317182149802666
"The research described in this proposal begins with completion and formal       
evaluation of MIDAS ""a computer program designed to automatically               
construct decision models from an underlying medical knowledge base"". The       
capabilities of MIDAS will be extended to knowledge-based construction          
of Markov decision models. A second project will develop a comprehensive        
knowledge management scheme for the problem of pulmonary disease in AIDS.       
This scheme will use a knowledge base structured according to knowledge         
needed to perform a decision analysis. It will incorporate summaries of         
relevant data, sources and quality of data and links to the original            
sources. This knowledge management scheme will be deployed in the               
hospital and evaluated in a group of medical residents at Robert Wood           
Johnson University Hospital.                                                    
 HIV infections; artificial intelligence; behavioral /social science research tag; computer assisted medical decision making; computer human interaction; health care model; human data; information system analysis; lung disorder; model design /development; opportunistic infections; physicians; pneumonia KNOWLEDGE MANAGEMENT FOR CLINICAL DECISION ANALYSIS","The research described in this proposal begins with completion and formal       
evaluation of MIDAS ""a computer program designed to automatically               
construct decision models from an underlying medical knowledge base"". The       
capabilities of MIDAS will be extended to knowledge-based construction          
of Markov decision models. A second project will develop a comprehensive        
knowledge management scheme for the problem of pulmonary disease in AIDS.       
This scheme will use a knowledge base structured according to knowledge         
needed to perform a decision analysis. It will incorporate summaries of         
relevant data, sources and quality of data and links to the original            
sources. This knowledge management scheme will be deployed in the               
hospital and evaluated in a group of medical residents at Robert Wood           
Johnson University Hospital.                                                    
",2546277,K04LM000096,['K04LM000096'],LM,https://reporter.nih.gov/project-details/2546277,K04,1997,72630,-0.0001339163026085335
"Low tissue oxygen concentration or hypoxia occurs with cancer and               
cardiovascular disease and plays a role in diabetes and alcohol-induced         
liver disease. Hypoxic tumor cells are important because they resist both       
radiation and drug treatment. Hyperbaric oxygen, radiosensitizers,              
hypoxic cell cytotoxins and neutron irradiation are used to get around          
this resistance but hypoxia is rarely monitored so patients who might           
benefit are not selected and the reason for success or failure of these         
costly therapies is not known. The immunostaining, marker method is ideal       
for measuring tumor hypoxia.  The marker is injected into cervix, head          
and neck and breast cancer patients and biopsy samples taken 24 hours           
later. Immunostaining of formalin-fixed, tissue sections reveals hypoxia        
and standard image analysis methods measure its extent. The marker method       
could be of enormous value in treatment planning for cancer patients and        
could also be used in animal studies of liver damage, stroke, cardiac           
arrest, diabetes, etc.                                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Potential commercial applications             
include routine use in conventional radiation therapy (potential up to          
300,000 patients/year in US); in experimental therapies based on hypoxia        
in human tumors; in experimental models of tumor hypoxia, stroke, cardiac       
failure, liver disease, wound healing, angiogenesis and normal tissue           
development.                                                                    
 artificial intelligence; azoles; biomarker; carcinoma; cervix neoplasms; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; enzyme linked immunosorbent assay; human subject; hypoxia; image processing; immunocytochemistry; monoclonal antibody; neoplasm /cancer immunodiagnosis IMMUNOCHEMICAL MARKER FOR TUMOR HYPOXIA","Low tissue oxygen concentration or hypoxia occurs with cancer and               
cardiovascular disease and plays a role in diabetes and alcohol-induced         
liver disease. Hypoxic tumor cells are important because they resist both       
radiation and drug treatment. Hyperbaric oxygen, radiosensitizers,              
hypoxic cell cytotoxins and neutron irradiation are used to get around          
this resistance but hypoxia is rarely monitored so patients who might           
benefit are not selected and the reason for success or failure of these         
costly therapies is not known. The immunostaining, marker method is ideal       
for measuring tumor hypoxia.  The marker is injected into cervix, head          
and neck and breast cancer patients and biopsy samples taken 24 hours           
later. Immunostaining of formalin-fixed, tissue sections reveals hypoxia        
and standard image analysis methods measure its extent. The marker method       
could be of enormous value in treatment planning for cancer patients and        
could also be used in animal studies of liver damage, stroke, cardiac           
arrest, diabetes, etc.                                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Potential commercial applications             
include routine use in conventional radiation therapy (potential up to          
300,000 patients/year in US); in experimental therapies based on hypoxia        
in human tumors; in experimental models of tumor hypoxia, stroke, cardiac       
failure, liver disease, wound healing, angiogenesis and normal tissue           
development.                                                                    
",2545398,R42CA068826,['R42CA068826'],CA,https://reporter.nih.gov/project-details/2545398,R42,1997,198579,0.019498607242339434
 artificial intelligence; cell differentiation; computer assisted sequence analysis; computer system design /evaluation; data collection methodology /evaluation; erythropoiesis; gene expression; genetic regulatory element; information retrieval; information system analysis; information systems; nucleic acid sequence; vocabulary development for information system KNOWLEDGE BASED BIOLOGICAL MODELING INFORMATION SYSTEM,,2546574,R01RR004026,['R01RR004026'],RR,https://reporter.nih.gov/project-details/2546574,R01,1997,360958,-0.051227764049953925
"The goal of the proposed research is to develop computer-aided diagnosis        
(CAD) schemes in order to improve the diagnostic accuracy of breast cancer      
in mammography. Four specific aims are included: (l) development of             
computer programs for the detection and characterization of                     
microcalcifications, (2) development of computer programs for detection         
and characterization of masses, (3) implementation of the CAD algorithms        
in a dedicated workstation to perform a pilot preclinical testing of the        
accuracy of the CAD programs, and (4) evaluation of the effects of the CAD      
schemes on radiologists' performance. The proposed CAD schemes will aid         
radiologists in screening mammograms for suspicious lesions and provide         
estimate of the likelihood of malignancy for the detected lesions. The          
information is expected to reduce the miss rate and to improve the              
positive predictive value of the mammographic findings.                         
                                                                                
A data base of clinical mammograms which include malignant and benign           
microcalcifications and masses will be established. Physical measures           
which characterize the significant image features of the lesions will be        
developed. Based on these measures, linear discriminant classifiers or          
neural network classifiers will be optimized using a genetic algorithm          
approach to classify true and false signals and to estimate the likelihood      
of malignancy for each type of lesions.                                         
                                                                                
For automated detection and classification of microcalcifications, we will      
investigate the usefulness of multiresolution analysis for enhancement of       
the signal-to-noise ratio of the microcalcifications and for improvement        
of feature extraction techniques. Physical characteristics such as size,        
shape, frequency spectrum, spatial distribution, clustering properties,         
and texture features will be extracted and analyzed with the classifiers.       
                                                                                
For automated detection and classification of masses, we will improve the       
background correction and signal segmentation techniques, and develop           
effective false-positive reduction methods. Adaptive filtering, edge            
enhancement, and clustering segmentation methods will be developed for          
extraction of the mass margins. Physical characteristics such as size,          
density, edge sharpness, calcifications, shape, lobulation, spiculation,        
and multiresolution wavelet texture features will be extracted from the         
masses and analyzed with the classifiers.                                       
                                                                                
The algorithms will be implemented in a dedicated CAD workstation and           
preclinical testing will be conducted. The performance of the programs in       
a clinical setting will be assessed. The algorithms will be revised and         
improved based on the information obtained with the preclinical testing.        
The study is a vital step for the development of a clinically reliable CAD      
scheme.                                                                         
                                                                                
Observer performance studies using receiver operating characteristic (ROC)      
methodology will be conducted to evaluate the effects of the CAD schemes        
on radiologists'performance.                                                    
 artificial intelligence; breast neoplasm /cancer diagnosis; breast neoplasms; computational neuroscience; computer assisted diagnosis; computer assisted medical decision making; computer program /software; diagnosis design /evaluation; diagnosis quality /standard; digital imaging; image processing; mammography; mathematical model DEVELOPMENT OF COMPUTER BASED TECHNIQUES IN MAMMOGRAPHY","The goal of the proposed research is to develop computer-aided diagnosis        
(CAD) schemes in order to improve the diagnostic accuracy of breast cancer      
in mammography. Four specific aims are included: (l) development of             
computer programs for the detection and characterization of                     
microcalcifications, (2) development of computer programs for detection         
and characterization of masses, (3) implementation of the CAD algorithms        
in a dedicated workstation to perform a pilot preclinical testing of the        
accuracy of the CAD programs, and (4) evaluation of the effects of the CAD      
schemes on radiologists' performance. The proposed CAD schemes will aid         
radiologists in screening mammograms for suspicious lesions and provide         
estimate of the likelihood of malignancy for the detected lesions. The          
information is expected to reduce the miss rate and to improve the              
positive predictive value of the mammographic findings.                         
                                                                                
A data base of clinical mammograms which include malignant and benign           
microcalcifications and masses will be established. Physical measures           
which characterize the significant image features of the lesions will be        
developed. Based on these measures, linear discriminant classifiers or          
neural network classifiers will be optimized using a genetic algorithm          
approach to classify true and false signals and to estimate the likelihood      
of malignancy for each type of lesions.                                         
                                                                                
For automated detection and classification of microcalcifications, we will      
investigate the usefulness of multiresolution analysis for enhancement of       
the signal-to-noise ratio of the microcalcifications and for improvement        
of feature extraction techniques. Physical characteristics such as size,        
shape, frequency spectrum, spatial distribution, clustering properties,         
and texture features will be extracted and analyzed with the classifiers.       
                                                                                
For automated detection and classification of masses, we will improve the       
background correction and signal segmentation techniques, and develop           
effective false-positive reduction methods. Adaptive filtering, edge            
enhancement, and clustering segmentation methods will be developed for          
extraction of the mass margins. Physical characteristics such as size,          
density, edge sharpness, calcifications, shape, lobulation, spiculation,        
and multiresolution wavelet texture features will be extracted from the         
masses and analyzed with the classifiers.                                       
                                                                                
The algorithms will be implemented in a dedicated CAD workstation and           
preclinical testing will be conducted. The performance of the programs in       
a clinical setting will be assessed. The algorithms will be revised and         
improved based on the information obtained with the preclinical testing.        
The study is a vital step for the development of a clinically reliable CAD      
scheme.                                                                         
                                                                                
Observer performance studies using receiver operating characteristic (ROC)      
methodology will be conducted to evaluate the effects of the CAD schemes        
on radiologists'performance.                                                    
",2376830,R01CA048129,['R01CA048129'],CA,https://reporter.nih.gov/project-details/2376830,R01,1997,388428,-0.17861136454347087
 X ray spectrometry; analytical method; artificial intelligence; biomedical equipment development; electron probe spectrometry; image processing PULSE PILEUP RECOVERY FOR ENERGY DISPERSIVE SPECTROSCOPY,,2040081,R44RR008828,['R44RR008828'],RR,https://reporter.nih.gov/project-details/2040081,R44,1997,223782,-0.011566087842029909
"THIS IS A SHANNON AWARD PROVIDING PARTIAL SUPPORT FOR THE RESEARCH              
PROJECTS THAT FALL SHORT OF THE ASSIGNED INSTITUTE'S FUNDING RANGE BUT          
ARE IN THE MARGIN OF EXCELLENCE. THE SHANNON AWARD IS INTENDED TO PROVIDE       
SUPPORT TO TEST THE FEASIBILITY OF THE APPROACH; DEVELOP FURTHER TESTS          
AND REFINE RESEARCH TECHNIQUES; PERFORM SECONDARY ANALYSIS OF AVAILABLE         
DATA SETS; OR CONDUCT DISCRETE PROJECTS THAT CAN DEMONSTRATE THE PI'S           
RESEARCH CAPABILITIES OR LEAD ADDITIONAL WEIGHT TO AN ALREADY MERITORIOUS       
APPLICATION. THE APPLICATION BELOW IS TAKEN FROM THE ORIGINAL DOCUMENT          
SUBMITTED BY THE PRINCIPAL INVESTIGATOR.                                        
                                                                                
This proposal describes research intended to support informatics                
requirements for the development, implementation, and evaluation of             
clinical practice guidelines. The work is expected to result both in the        
development of improved guidelines and, concomitantly, to produce               
structured, computer-accessible repositories of medical knowledge that          
can be used for decision support. A research program with the following         
4 specific aims is proposed:                                                    
                                                                                
(l) To devise and extend fundamental techniques for modeling and                
processing guideline knowledge using augmented decision tables. Methods         
will be defined to deal with guideline knowledge complexity and                 
uncertainty.                                                                    
                                                                                
(2) To refine and disseminate techniques by which knowledge can be              
acquired for clinical guidelines and verified to assure its consistency         
and comprehensiveness. Working with the American Academy of Pediatrics,         
newly developed guidelines will be logically analyzed and verified prior        
to publication.                                                                 
                                                                                
(3) To evaluate the effectiveness of a guideline-based, decision support        
device to improve compliance with a national guideline for outpatient           
management of asthma in children. A randomized, controlled trial will be        
conducted to measure the effect of the AsthMonitor system on guideline          
adherence, encounter documentation, patient outcomes, and costs of care.        
                                                                                
(4) To develop a suite of interactive computer applications that support        
decision making and clinical workflow for the management of a broad range       
of common pediatric problems. Knowledge gained from the clinical trial          
will be combined with workflow analysis, sound application design               
principles, and advanced digital technologies to create useful and usable       
tools for guideline implementation.                                             
                                                                                
Individual providers will benefit from the availability of decision             
support applications based on guidelines knowledge. From a public health        
perspective, effective implementation of evidence-based guidelines for          
the outpatient management of asthma and other common pediatric disorders        
should contribute to a reduction in inappropriate practice and to               
improved quality of care.                                                       
 adolescence (12-20); artificial intelligence; asthma; behavioral /social science research tag; clinical research; computer assisted medical decision making; computer human interaction; decision making; health care professional practice; health care quality; health care service evaluation; human subject; middle childhood (6-11); outpatient care; patient care management; pediatrics; primary care physician KNOWLEDGE PROCESSING FOR CLINICAL PRACTICE GUIDELINES","THIS IS A SHANNON AWARD PROVIDING PARTIAL SUPPORT FOR THE RESEARCH              
PROJECTS THAT FALL SHORT OF THE ASSIGNED INSTITUTE'S FUNDING RANGE BUT          
ARE IN THE MARGIN OF EXCELLENCE. THE SHANNON AWARD IS INTENDED TO PROVIDE       
SUPPORT TO TEST THE FEASIBILITY OF THE APPROACH; DEVELOP FURTHER TESTS          
AND REFINE RESEARCH TECHNIQUES; PERFORM SECONDARY ANALYSIS OF AVAILABLE         
DATA SETS; OR CONDUCT DISCRETE PROJECTS THAT CAN DEMONSTRATE THE PI'S           
RESEARCH CAPABILITIES OR LEAD ADDITIONAL WEIGHT TO AN ALREADY MERITORIOUS       
APPLICATION. THE APPLICATION BELOW IS TAKEN FROM THE ORIGINAL DOCUMENT          
SUBMITTED BY THE PRINCIPAL INVESTIGATOR.                                        
                                                                                
This proposal describes research intended to support informatics                
requirements for the development, implementation, and evaluation of             
clinical practice guidelines. The work is expected to result both in the        
development of improved guidelines and, concomitantly, to produce               
structured, computer-accessible repositories of medical knowledge that          
can be used for decision support. A research program with the following         
4 specific aims is proposed:                                                    
                                                                                
(l) To devise and extend fundamental techniques for modeling and                
processing guideline knowledge using augmented decision tables. Methods         
will be defined to deal with guideline knowledge complexity and                 
uncertainty.                                                                    
                                                                                
(2) To refine and disseminate techniques by which knowledge can be              
acquired for clinical guidelines and verified to assure its consistency         
and comprehensiveness. Working with the American Academy of Pediatrics,         
newly developed guidelines will be logically analyzed and verified prior        
to publication.                                                                 
                                                                                
(3) To evaluate the effectiveness of a guideline-based, decision support        
device to improve compliance with a national guideline for outpatient           
management of asthma in children. A randomized, controlled trial will be        
conducted to measure the effect of the AsthMonitor system on guideline          
adherence, encounter documentation, patient outcomes, and costs of care.        
                                                                                
(4) To develop a suite of interactive computer applications that support        
decision making and clinical workflow for the management of a broad range       
of common pediatric problems. Knowledge gained from the clinical trial          
will be combined with workflow analysis, sound application design               
principles, and advanced digital technologies to create useful and usable       
tools for guideline implementation.                                             
                                                                                
Individual providers will benefit from the availability of decision             
support applications based on guidelines knowledge. From a public health        
perspective, effective implementation of evidence-based guidelines for          
the outpatient management of asthma and other common pediatric disorders        
should contribute to a reduction in inappropriate practice and to               
improved quality of care.                                                       
",2032350,R29LM005552,['R29LM005552'],LM,https://reporter.nih.gov/project-details/2032350,R29,1997,112831,-0.007178963642176563
"Biomedical researchers are generating vast amounts of data and knowledge,       
at an accelerating pace.  Neither existing database systems nor existing        
frame knowledge representation systems have the capabilities required to        
support the development of large, shared repositories of biological             
information.  Therefore, SRI International proposes to integrate database       
and knowledge-representation technology to develop a biological                 
knowledge-base management system (KBMS) with unprecedented power to en-         
code biological knowledge.  This KBMS will provide the expressive power         
needed to represent biological knowledge in all its complexity, will            
support the evolution of complex knowledge-base schemas, will enable            
multiple users to access large amounts of reliably stored information in        
a shared fashion, and will support inference over this knowledge.               
                                                                                
Our implementation efforts will build on an existing frame representation       
system called THEO.  We will extend THEO so that its underlying storage         
system utilizes a database management system to facilitate the                  
construction of frame knowledge bases containing large numbers (millions)       
of persistent frames.  In addition, we will implement a collaboration           
subsystem that coordinates concurrent distributed development of large          
knowledge bases by multiple users.                                              
                                                                                
To exercise and validate our development efforts, this KBMS will be used        
in an application to construct a large biological knowledge base of             
intermediary metabolism.                                                        
 artificial intelligence; computer program /software; enzyme activity; information system analysis; information systems; metabolism BIOLOGICAL KNOWLEDGE-BASE MANAGEMENT SYSTEM","Biomedical researchers are generating vast amounts of data and knowledge,       
at an accelerating pace.  Neither existing database systems nor existing        
frame knowledge representation systems have the capabilities required to        
support the development of large, shared repositories of biological             
information.  Therefore, SRI International proposes to integrate database       
and knowledge-representation technology to develop a biological                 
knowledge-base management system (KBMS) with unprecedented power to en-         
code biological knowledge.  This KBMS will provide the expressive power         
needed to represent biological knowledge in all its complexity, will            
support the evolution of complex knowledge-base schemas, will enable            
multiple users to access large amounts of reliably stored information in        
a shared fashion, and will support inference over this knowledge.               
                                                                                
Our implementation efforts will build on an existing frame representation       
system called THEO.  We will extend THEO so that its underlying storage         
system utilizes a database management system to facilitate the                  
construction of frame knowledge bases containing large numbers (millions)       
of persistent frames.  In addition, we will implement a collaboration           
subsystem that coordinates concurrent distributed development of large          
knowledge bases by multiple users.                                              
                                                                                
To exercise and validate our development efforts, this KBMS will be used        
in an application to construct a large biological knowledge base of             
intermediary metabolism.                                                        
",2032340,R29LM005413,['R29LM005413'],LM,https://reporter.nih.gov/project-details/2032340,R29,1997,113809,-0.12030002364461878
"Our overall objective is to solve the problem of how proteins fold into         
their native conformations.  For this purpose, we are using the                 
methodology of protein chemistry and developing and applying experimental       
and theoretical techniques to provide an understanding of the internal          
interactions that stabilize native proteins in aqueous solution.  This          
project is concerned with the theoretical work which involves the use of        
empirical potentials (including the effects of hydration and entropy) in        
various computational approaches to study the interactions in protein           
folding.  Emphasis is currently being placed on solving the multiple-           
minima problem arising from the existence of numerous local minima in the       
potential energy surface of the protein, the objective being to locate the      
global minimum on this surface.  An understanding of the interactions in        
proteins is of potential applicability to the elucidation of the role of        
conformation in biological processes, e.g. the undesirable association of       
sickle-cell hemoglobin, or the induction of an oncogene product whose           
properties involve a conformational change when only one amino acid             
residue in the sequence is changed.                                             
 artificial intelligence; chemical bond; chemical hydration; chemical stability; computer program /software; computer simulation; computer system design /evaluation; conformation; diffusion; intermolecular interaction; mathematical model; method development; protein folding; protein sequence; protein structure; quantum chemistry; structural biology; thermodynamics; water solution INTERNAL BONDING IN PROTEINS","Our overall objective is to solve the problem of how proteins fold into         
their native conformations.  For this purpose, we are using the                 
methodology of protein chemistry and developing and applying experimental       
and theoretical techniques to provide an understanding of the internal          
interactions that stabilize native proteins in aqueous solution.  This          
project is concerned with the theoretical work which involves the use of        
empirical potentials (including the effects of hydration and entropy) in        
various computational approaches to study the interactions in protein           
folding.  Emphasis is currently being placed on solving the multiple-           
minima problem arising from the existence of numerous local minima in the       
potential energy surface of the protein, the objective being to locate the      
global minimum on this surface.  An understanding of the interactions in        
proteins is of potential applicability to the elucidation of the role of        
conformation in biological processes, e.g. the undesirable association of       
sickle-cell hemoglobin, or the induction of an oncogene product whose           
properties involve a conformational change when only one amino acid             
residue in the sequence is changed.                                             
",2391778,R01GM014312,['R01GM014312'],GM,https://reporter.nih.gov/project-details/2391778,R01,1997,172959,-0.04059576706968617
"DESCRIPTION:  The proposed research is designed to develop more effective       
self-help interventions for smoking cessation.  Smoking is the most             
preventable cause of cancer mortality, but 47 million Americans continue to     
smoke.  Smoking cessation clinics have reached only about 1 percent of          
eligible smokers.  Home based programs that are action-oriented reach only      
about 1-5 percent.  In the current study using stage-matched interventions      
and proactive recruitment 85 percent of a defined population of smokers in      
an HMO were reached.  It is proposed that this marked increase in               
participation rates needs to be complimented by dramatic increases in           
cessation rates of the most promising self-help interventions developed to      
date.  The proposed research is a continuation of 15 years of research on       
self-change approaches to smoking cessation already funded by NIH.  This        
research program has produced models, measures and intervention methods that    
are having major impacts on the field.  The current project has accomplished    
all of the aims proposed including unprecedented participation rates,           
individualized and interactive interventions outperforming self-help manuals    
at 1, 2 3 and 6 series of contacts; enhanced proactive counselor protocols      
producing high abstinence rates at 12 months, and a low cost single contact     
expert system producing relatively high abstinence rates.  The proposed         
extension is a population based clinical trial with 5500 smokers proactively    
recruited from an HMO and randomly assigned to one of nine conditions           
varying from low to high contact intensity.  These conditions include:  1)      
no treatment; 2) proactive assessment every six months; 3) single expert        
system intervention; 4) a matched modality intervention combining the best      
modality for smokers in each of three stages of change; 5)the investigators     
most effective three intervention expert system; 6) and 7) enhanced personal    
choice and professional choice versions of the standard systems; 8) an          
enhanced proactive counseling condition with counselor fading; and 9) a         
stepped care population program progressing from expert systems, to             
counseling and nicotine replacement where indicated.  Data analyses will        
identify the most effective, cost-effective and generalizable elements of       
these smoking cessation interventions in preparation for dissemination to       
entire populations.  This research is designed to continue the                  
investigators' contributions to cancer control by developing and evaluating     
stage-matched, interactive and proactive self-help interventions that have      
the potential to produce unprecedented impacts on entire populations of         
smokers.                                                                        
 behavioral /social science research tag; cancer prevention; cancer risk; clinical research; computer data analysis; counseling; health behavior; health care cost /financing; human population study; human subject; information dissemination; longitudinal human study; mathematical model; method development; nicotine; questionnaires; relapse /recurrence; self help; smoking cessation; statistics /biometry ENHANCED SELF HELP INTERVENTIONS FOR SMOKING CES","DESCRIPTION:  The proposed research is designed to develop more effective       
self-help interventions for smoking cessation.  Smoking is the most             
preventable cause of cancer mortality, but 47 million Americans continue to     
smoke.  Smoking cessation clinics have reached only about 1 percent of          
eligible smokers.  Home based programs that are action-oriented reach only      
about 1-5 percent.  In the current study using stage-matched interventions      
and proactive recruitment 85 percent of a defined population of smokers in      
an HMO were reached.  It is proposed that this marked increase in               
participation rates needs to be complimented by dramatic increases in           
cessation rates of the most promising self-help interventions developed to      
date.  The proposed research is a continuation of 15 years of research on       
self-change approaches to smoking cessation already funded by NIH.  This        
research program has produced models, measures and intervention methods that    
are having major impacts on the field.  The current project has accomplished    
all of the aims proposed including unprecedented participation rates,           
individualized and interactive interventions outperforming self-help manuals    
at 1, 2 3 and 6 series of contacts; enhanced proactive counselor protocols      
producing high abstinence rates at 12 months, and a low cost single contact     
expert system producing relatively high abstinence rates.  The proposed         
extension is a population based clinical trial with 5500 smokers proactively    
recruited from an HMO and randomly assigned to one of nine conditions           
varying from low to high contact intensity.  These conditions include:  1)      
no treatment; 2) proactive assessment every six months; 3) single expert        
system intervention; 4) a matched modality intervention combining the best      
modality for smokers in each of three stages of change; 5)the investigators     
most effective three intervention expert system; 6) and 7) enhanced personal    
choice and professional choice versions of the standard systems; 8) an          
enhanced proactive counseling condition with counselor fading; and 9) a         
stepped care population program progressing from expert systems, to             
counseling and nicotine replacement where indicated.  Data analyses will        
identify the most effective, cost-effective and generalizable elements of       
these smoking cessation interventions in preparation for dissemination to       
entire populations.  This research is designed to continue the                  
investigators' contributions to cancer control by developing and evaluating     
stage-matched, interactive and proactive self-help interventions that have      
the potential to produce unprecedented impacts on entire populations of         
smokers.                                                                        
",2007291,R01CA027821,['R01CA027821'],CA,https://reporter.nih.gov/project-details/2007291,R01,1997,1181331,-0.13213551341279017
"Visual mental imagery helps people to perform a host of tasks, including        
remembering events, spatial reasoning, and language comprehension. The          
images produced in the service of these activities are the result of a          
complex information processing system. This system can be conceptualized        
as being composed of a set of distinct ""processing subsystems,"" each of         
which performs a specific cognitive operation (e.g., shifting the               
orientation of an imaged object, activating stored visual memories to           
create an image, encoding the relative location of part of the imaged           
object). The research described here makes use of a theory of these             
processing subsystems that draws on concepts from artificial intelligence       
and facts about the neurological substrate of high.level vision. The            
research will characterize the relative efficacy of eleven of these             
subsystems in the elderly, compared to younger adults. The primary aim          
of the research is to discover whether certain subsystems are, relative         
to other subsystems, selectively more effective in senescence. The              
research aims to disprove the idea that cognitive aging can be understood       
solely in terms of ""generalized slowing,"" showing that slowing with age         
differs for different subsystems. If so, then strategies that make use          
of relatively effective subsystems should be more useful for elderly            
people than strategies that rely on ineffective subsystems. The                 
experiments designed here are a step toward using contemporary theory           
from cognitive neuroscience and techniques of task analysis from                
cognitive science to design new tests for cognitive efficiency in the           
senescence.                                                                     
 adolescence (12-20); age difference; attention; cognition; human old age (65+); human subject; image processing; memory; neural information processing; neurosciences; occipital lobe /cortex; performance; questionnaires; retina; space perception; vision; visual perception; visual stimulus IMAGERY PROCESSING IN OLD AGE","Visual mental imagery helps people to perform a host of tasks, including        
remembering events, spatial reasoning, and language comprehension. The          
images produced in the service of these activities are the result of a          
complex information processing system. This system can be conceptualized        
as being composed of a set of distinct ""processing subsystems,"" each of         
which performs a specific cognitive operation (e.g., shifting the               
orientation of an imaged object, activating stored visual memories to           
create an image, encoding the relative location of part of the imaged           
object). The research described here makes use of a theory of these             
processing subsystems that draws on concepts from artificial intelligence       
and facts about the neurological substrate of high.level vision. The            
research will characterize the relative efficacy of eleven of these             
subsystems in the elderly, compared to younger adults. The primary aim          
of the research is to discover whether certain subsystems are, relative         
to other subsystems, selectively more effective in senescence. The              
research aims to disprove the idea that cognitive aging can be understood       
solely in terms of ""generalized slowing,"" showing that slowing with age         
differs for different subsystems. If so, then strategies that make use          
of relatively effective subsystems should be more useful for elderly            
people than strategies that rely on ineffective subsystems. The                 
experiments designed here are a step toward using contemporary theory           
from cognitive neuroscience and techniques of task analysis from                
cognitive science to design new tests for cognitive efficiency in the           
senescence.                                                                     
",2001589,R01AG012675,['R01AG012675'],AG,https://reporter.nih.gov/project-details/2001589,R01,1997,165380,-0.04136418740837679
"DESCRIPTION:(Adapted from applicant's abstract): Implementation and             
validation of a computerized expert system sleep scoring is proposed.           
Such a system would increase productivity of sleep laboratories, and            
increase the quality while reducing the cost of sleep diagnostic                
procedures. Phase I research will focus on the basic algorithms for             
automated sleep stage scoring. The algorithms are based on Bayesian             
decision theory and follow the steps of the human expert in the decision        
making process when scoring sleep by the Rechtschaffen-Kales standard.          
Likelihood estimates of the suggested and alternative decisions will be         
used to quantify confidence in suggested scores. The system will flag           
low confidence scores to be revised by human interaction. Initial tests         
show that the level of agreement between machine and human is  similar          
to that between two human scorers. Software tools will be developed to          
test the effect of the parameters of the algorithms on stage score              
decisions. An initial data base of training and test sets of normal and         
apneic sleep recordings will be established and the algorithms will be          
optimized and tested on this data. Phase II will optimize parameters and        
validate the system on larger data sets for other diagnostic and age            
groups, develop on-line, real-time capabilities, and extend the decision        
model to facilitate further research in sleep medicine.                         
 biomedical automation; biomedical equipment development; computer system design /evaluation; method development; sleep EXPERT SYSTEM FOR AUTOMATED SLEEP STAGE SCORING","DESCRIPTION:(Adapted from applicant's abstract): Implementation and             
validation of a computerized expert system sleep scoring is proposed.           
Such a system would increase productivity of sleep laboratories, and            
increase the quality while reducing the cost of sleep diagnostic                
procedures. Phase I research will focus on the basic algorithms for             
automated sleep stage scoring. The algorithms are based on Bayesian             
decision theory and follow the steps of the human expert in the decision        
making process when scoring sleep by the Rechtschaffen-Kales standard.          
Likelihood estimates of the suggested and alternative decisions will be         
used to quantify confidence in suggested scores. The system will flag           
low confidence scores to be revised by human interaction. Initial tests         
show that the level of agreement between machine and human is  similar          
to that between two human scorers. Software tools will be developed to          
test the effect of the parameters of the algorithms on stage score              
decisions. An initial data base of training and test sets of normal and         
apneic sleep recordings will be established and the algorithms will be          
optimized and tested on this data. Phase II will optimize parameters and        
validate the system on larger data sets for other diagnostic and age            
groups, develop on-line, real-time capabilities, and extend the decision        
model to facilitate further research in sleep medicine.                         
",2037847,R44NS033437,['R44NS033437'],NS,https://reporter.nih.gov/project-details/2037847,R44,1997,448180,-0.04007349086720499
"The objective of this research is the wide accessibility of 3D                  
deconvolution (computational deblurring and visual clarification of 3D          
image data) for the neuroscientific community and other life-science            
communities. Long-term aims are to provide the most reliable, robust and        
quantitatively accurate system for 3D visualization and morphometry, and        
to provide neuroscientists and other life-scientists with easy-to-use           
tools for studying the structure and function of normal and pathologic          
tissue. The focus of this project is on neuroscientific applications. The       
commercial objective is a profitable software product, which, owing to          
innovations, well outperforms competitive products in reliability,              
robustness, quantitative accuracy, speed and ease of usage. The specific        
aims of this project are: (1) To develop and test variations of the             
algorithm for a broader range of widely used confocal microscope types,         
including the slit-scan and array-detector geomeries; (2) To improve (and       
test) the robustness of the algorithm under a number of common adverse          
conditions; (3) To further validate the correctness of the image                
reconstructions by using fabricated test objects of known geometry; (4) To      
demonstrate the system on two of the most suitable computer platforms for       
wide usage. These are the Silicon Graphics unix-based platform and the          
IBM-PC compatible platform; (5) To beta test prototypes of the product at       
several potential customer sites. Innovations introduced by us in this          
research include: (1), a blind deconvolution approach, which obviates the       
need to measure the point spread function, (2), a maximum likelihood            
optimization approach, which makes the methodology robust against               
photodetector noise and other adverse conditions, and (3), a unified            
underlying mathematical model, which makes the algorithm easily adaptable       
among confocal geometries.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We estimate on the order of 800 biological pinhole confocal microscopes         
and 150 biological slit scan confocal microscopes to exist worldwide. We        
expect these numbers to increase to 8,000 confocal microscopes over 10          
years. We expect that 1/2 of these potentIal sites will purchase                
deconvolution software. We should secure at least 1/3rd of this potentIal       
market and thereby (conservatively) Bell at,least 100 units per year over       
10 years. Numerous nonblologlcal spin-off markets (e.g., pharmeceuticals,       
chemicals) exist as well.                                                       
 artificial intelligence; computer program /software; computer system design /evaluation; confocal scanning microscopy; dendrites; digital imaging; fluorescence microscopy; glia; image enhancement; laboratory rat; sectioning; tissue /cell preparation ROBUST 3D FLUORESCENCE CONFOCAL MICROSCOPY DEBLURRING","The objective of this research is the wide accessibility of 3D                  
deconvolution (computational deblurring and visual clarification of 3D          
image data) for the neuroscientific community and other life-science            
communities. Long-term aims are to provide the most reliable, robust and        
quantitatively accurate system for 3D visualization and morphometry, and        
to provide neuroscientists and other life-scientists with easy-to-use           
tools for studying the structure and function of normal and pathologic          
tissue. The focus of this project is on neuroscientific applications. The       
commercial objective is a profitable software product, which, owing to          
innovations, well outperforms competitive products in reliability,              
robustness, quantitative accuracy, speed and ease of usage. The specific        
aims of this project are: (1) To develop and test variations of the             
algorithm for a broader range of widely used confocal microscope types,         
including the slit-scan and array-detector geomeries; (2) To improve (and       
test) the robustness of the algorithm under a number of common adverse          
conditions; (3) To further validate the correctness of the image                
reconstructions by using fabricated test objects of known geometry; (4) To      
demonstrate the system on two of the most suitable computer platforms for       
wide usage. These are the Silicon Graphics unix-based platform and the          
IBM-PC compatible platform; (5) To beta test prototypes of the product at       
several potential customer sites. Innovations introduced by us in this          
research include: (1), a blind deconvolution approach, which obviates the       
need to measure the point spread function, (2), a maximum likelihood            
optimization approach, which makes the methodology robust against               
photodetector noise and other adverse conditions, and (3), a unified            
underlying mathematical model, which makes the algorithm easily adaptable       
among confocal geometries.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We estimate on the order of 800 biological pinhole confocal microscopes         
and 150 biological slit scan confocal microscopes to exist worldwide. We        
expect these numbers to increase to 8,000 confocal microscopes over 10          
years. We expect that 1/2 of these potentIal sites will purchase                
deconvolution software. We should secure at least 1/3rd of this potentIal       
market and thereby (conservatively) Bell at,least 100 units per year over       
10 years. Numerous nonblologlcal spin-off markets (e.g., pharmeceuticals,       
chemicals) exist as well.                                                       
",2034269,R44MH053692,['R44MH053692'],MH,https://reporter.nih.gov/project-details/2034269,R44,1997,378505,-0.0305898437699804
"DESCRIPTION (Taken from application abstract):  Over the last decade            
computational modeling has become central to neurobiology.  While much of       
this work has focused on cellular and sub-cellular processes, the last few      
years have seen increasing interest in systems level models and in              
integrative accounts that span data from the subcellular to behavioral          
levels.  Our proposal, in summary, is to extend existing work in parallel       
discrete event simulation (PDES) and integrate it with existing work on         
compartmental modeling environments, to produce a software environment which    
has comprehensive support for modeling large scale, highly structured           
networks of biophysically realistic cells; and which can efficiently exploit    
the full range of parallel platforms, including the largest parallel            
supercomputers, for simulation of these network models, which integrate         
information about the nervous system from sub-cellular to the whole-brain       
level.  Because of the scale of the models needed at this level of              
integration, advanced parallel computing is required.  The critical             
technical insight upon which this work rests is that neuronal modeling at       
the systems level can often be reduced to a form of discrete event              
simulation in which single cells are node functions and voltage spikes are      
events.                                                                         
                                                                                
Three neuroscience modeling projects, will mold, test, and utilize these new    
capabilities in investigations of system-level models of the nervous system     
which integrate behavioral, anatomical and physiological data on a scale        
that exceeds current simulation capabilities.  In collaboration with            
computer scientists at Pittsburgh Supercomputing Center and UCLA,               
neuroscientists at University of Virginia, the Born-Bunge Foundation,           
Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS       
packages, these tools will be developed and made available to the               
neuroscience community.  The software development aims include 1)               
investigation of a portable, PDES system capable of running efficiently on      
diverse parallel platforms, 2) development of interfaces to the PDES for        
NEURON and GENESIS allowing models developed in those packages to be scaled     
up, 3) investigation of a network specification language for neuronal           
models, and associated a visualization interface, to facilitate                 
investigation of systems-level models, 4) sufficiently robust and               
well-documented software for download and installation at other sites.  The     
three neuroscience projects will guide development of the software tools and    
use the tools for investigation of large-scale models of cerebellum,            
hippocampus and thalamocortical circuits.                                       
 artificial intelligence; biomedical automation; biotechnology; cerebellar cortex; computational neuroscience; computer network; computer program /software; computer simulation; computer system design /evaluation; hippocampus; mathematical model; neural information processing; neurotransmitters; parallel processing; supercomputer; thalamocortical tract; vocabulary development for information system PARALLEL SIMULATION OF LARGE SCALE NEURONAL MODELS","DESCRIPTION (Taken from application abstract):  Over the last decade            
computational modeling has become central to neurobiology.  While much of       
this work has focused on cellular and sub-cellular processes, the last few      
years have seen increasing interest in systems level models and in              
integrative accounts that span data from the subcellular to behavioral          
levels.  Our proposal, in summary, is to extend existing work in parallel       
discrete event simulation (PDES) and integrate it with existing work on         
compartmental modeling environments, to produce a software environment which    
has comprehensive support for modeling large scale, highly structured           
networks of biophysically realistic cells; and which can efficiently exploit    
the full range of parallel platforms, including the largest parallel            
supercomputers, for simulation of these network models, which integrate         
information about the nervous system from sub-cellular to the whole-brain       
level.  Because of the scale of the models needed at this level of              
integration, advanced parallel computing is required.  The critical             
technical insight upon which this work rests is that neuronal modeling at       
the systems level can often be reduced to a form of discrete event              
simulation in which single cells are node functions and voltage spikes are      
events.                                                                         
                                                                                
Three neuroscience modeling projects, will mold, test, and utilize these new    
capabilities in investigations of system-level models of the nervous system     
which integrate behavioral, anatomical and physiological data on a scale        
that exceeds current simulation capabilities.  In collaboration with            
computer scientists at Pittsburgh Supercomputing Center and UCLA,               
neuroscientists at University of Virginia, the Born-Bunge Foundation,           
Antwerp, and the Salk Institute, and developers of the NEURON and GENESIS       
packages, these tools will be developed and made available to the               
neuroscience community.  The software development aims include 1)               
investigation of a portable, PDES system capable of running efficiently on      
diverse parallel platforms, 2) development of interfaces to the PDES for        
NEURON and GENESIS allowing models developed in those packages to be scaled     
up, 3) investigation of a network specification language for neuronal           
models, and associated a visualization interface, to facilitate                 
investigation of systems-level models, 4) sufficiently robust and               
well-documented software for download and installation at other sites.  The     
three neuroscience projects will guide development of the software tools and    
use the tools for investigation of large-scale models of cerebellum,            
hippocampus and thalamocortical circuits.                                       
",2379407,R01MH057358,['R01MH057358'],MH,https://reporter.nih.gov/project-details/2379407,R01,1997,222238,-0.0744797721085124
"Investigating individual neurons has proven very useful in understanding        
their functionality.  However, all biological systems are composed of a         
multitude of neurons that interact-with a variety of sensory and skeletal       
muscular systems.  Researchers are anxious to have the ability to record        
a large number of neuron firing patterns to study how they communicate          
and interact in a biological system.  Tools such as the multi-channel           
electrode have been created to study the interactions of large numbers          
of neurons.  However, in order to record all the channels, the researcher       
must gang several recorders together.  This leads to difficulties in            
accurately matching all the channels during playback, because of the            
complexity of ganged multiple recorders. R.C - Electronics Inc. proposes        
to build a low cost, high speed digital recorder utilizing PC technology.       
This digital recorder will be able to record continuously to a mass             
storage device at an aggregate rate of 1 MHz.  It will also provide the         
researcher with the ability to acquire up to 96 channels, with each             
channel sampled at 10 KHz; higher speeds will be possible with fewer            
channels.                                                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION:  There is a market demand for the              
digital recorder in the multitude of life science applications engaged          
in complex biological system studies.   We also believe that this will          
be a good industrial recorder, capable of replacing the existing analog         
and DAT recorders for acoustic, noise. and vibration studies.  There will       
also be a market to replace some of the high-end work stations based on         
HP, Concurrent and Sun data acquisition systems.                                
 artificial intelligence; biomedical equipment development; computational neuroscience; computer data analysis; computer system design /evaluation; digital imaging; electrodes; electronic recording system; electrophysiology; personal computers HIGH SPEED DIGITAL RECORDER FOR ACQUIRING ELECTROPHYSIOL","Investigating individual neurons has proven very useful in understanding        
their functionality.  However, all biological systems are composed of a         
multitude of neurons that interact-with a variety of sensory and skeletal       
muscular systems.  Researchers are anxious to have the ability to record        
a large number of neuron firing patterns to study how they communicate          
and interact in a biological system.  Tools such as the multi-channel           
electrode have been created to study the interactions of large numbers          
of neurons.  However, in order to record all the channels, the researcher       
must gang several recorders together.  This leads to difficulties in            
accurately matching all the channels during playback, because of the            
complexity of ganged multiple recorders. R.C - Electronics Inc. proposes        
to build a low cost, high speed digital recorder utilizing PC technology.       
This digital recorder will be able to record continuously to a mass             
storage device at an aggregate rate of 1 MHz.  It will also provide the         
researcher with the ability to acquire up to 96 channels, with each             
channel sampled at 10 KHz; higher speeds will be possible with fewer            
channels.                                                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION:  There is a market demand for the              
digital recorder in the multitude of life science applications engaged          
in complex biological system studies.   We also believe that this will          
be a good industrial recorder, capable of replacing the existing analog         
and DAT recorders for acoustic, noise. and vibration studies.  There will       
also be a market to replace some of the high-end work stations based on         
HP, Concurrent and Sun data acquisition systems.                                
",2445836,R44NS033807,['R44NS033807'],NS,https://reporter.nih.gov/project-details/2445836,R44,1997,319438,-0.023297369767514765
"In the STARE (STructured Analysis of the REtina) project, we are                
developing a computerized image-interpreting system with hierarchical           
inferencing to measure, compare, and diagnose images of the ocular              
fundus. The system will have sufficient depth of imaging tools to be a          
resource for researchers or clinicians.                                         
                                                                                
The STARE system is designed to find objects of interest (normal                
anatomical structures and lesions) in digitized ocular fundus images and        
to use these objects to diagnose an image, to detect changes in                 
sequential images, and to make clinically useful measurements that are          
currently tedious or costly. To accomplish these difficult goals, we            
must segment and identify the objects of interest. Identified objects           
can be used to compare images, and the objects can be assembled to              
describe the image. Image interpretation incorporating expert systems           
and neural networks can provide the structure for cross-sectional               
epidemiological studies.                                                        
                                                                                
There was no established paradigm to follow to construct a system for           
image interpretation. We designed the overall structure of the process          
and determined how each task was to be accomplished. We have broken the         
project into steps, each of which has been accomplished. We are able to         
find objects of importance and correctly identify and localize them on          
a fundus coordinate system that we designed. We have created and tested         
a neural network and an expert system (INTELLEYE) to handle the                 
interpretation of an image and its contents.                                    
                                                                                
We will now improve the accuracy of each step, increase the number of           
lesions we can identify, and integrate the image analysis steps with the        
expert system to allow smooth progression from image to diagnosis and           
change detection. We will validate the usefulness and accuracy of the           
system by comparing its diagnosis and image comparison to trained               
readers of ophthalmic images.                                                   
                                                                                
The goal of this project is a system with multiple imaging tools and            
inferencing ability that can be adapted to a variety of imaging tasks.          
The outcome will be an image-interpreting system for use in clinical and        
research settings that will build annotated image databases, screen             
images of the ocular fundus for health care systems, furnish decision           
support for primary care providers, and extend the capability and               
productivity of the ophthalmologist.                                            
 artificial intelligence; computer system design /evaluation; diagnosis design /evaluation; digital imaging; eye fundus photography; human subject; image processing STRUCTURED ANALYSIS OF THE RETINA","In the STARE (STructured Analysis of the REtina) project, we are                
developing a computerized image-interpreting system with hierarchical           
inferencing to measure, compare, and diagnose images of the ocular              
fundus. The system will have sufficient depth of imaging tools to be a          
resource for researchers or clinicians.                                         
                                                                                
The STARE system is designed to find objects of interest (normal                
anatomical structures and lesions) in digitized ocular fundus images and        
to use these objects to diagnose an image, to detect changes in                 
sequential images, and to make clinically useful measurements that are          
currently tedious or costly. To accomplish these difficult goals, we            
must segment and identify the objects of interest. Identified objects           
can be used to compare images, and the objects can be assembled to              
describe the image. Image interpretation incorporating expert systems           
and neural networks can provide the structure for cross-sectional               
epidemiological studies.                                                        
                                                                                
There was no established paradigm to follow to construct a system for           
image interpretation. We designed the overall structure of the process          
and determined how each task was to be accomplished. We have broken the         
project into steps, each of which has been accomplished. We are able to         
find objects of importance and correctly identify and localize them on          
a fundus coordinate system that we designed. We have created and tested         
a neural network and an expert system (INTELLEYE) to handle the                 
interpretation of an image and its contents.                                    
                                                                                
We will now improve the accuracy of each step, increase the number of           
lesions we can identify, and integrate the image analysis steps with the        
expert system to allow smooth progression from image to diagnosis and           
change detection. We will validate the usefulness and accuracy of the           
system by comparing its diagnosis and image comparison to trained               
readers of ophthalmic images.                                                   
                                                                                
The goal of this project is a system with multiple imaging tools and            
inferencing ability that can be adapted to a variety of imaging tasks.          
The outcome will be an image-interpreting system for use in clinical and        
research settings that will build annotated image databases, screen             
images of the ocular fundus for health care systems, furnish decision           
support for primary care providers, and extend the capability and               
productivity of the ophthalmologist.                                            
",2415716,R01LM005759,['R01LM005759'],LM,https://reporter.nih.gov/project-details/2415716,R01,1997,265260,-0.08332251982492558
"The goal of this project is to develop rehabilitation guidelines for            
restoration of ambulation in patients following a stroke. Specifically          
the aims are to identify patterns of gait deviations in both the sound          
and paretic sides among patients with hemiplegia, form classifications          
of motor control strategies used during walking and relate these findings       
to the patient's potential for recovery of ambulation and response to           
intensive rehabilitation. Three gait training strategies will be                
compared: i) supported treadmill gait training, 2) intensified-use of the       
paretic leg and 3) functional independence and endurance training using         
motor learning principles. Variables that predict improvement in walking        
ability after rehabilitation will be identified. Three clinical tests           
(Functional Independence Measure, Fugl-meyer and Upright Motor Control)         
will be used to identify predictor-criteria. Testing will be done within        
one week of admission to rehabilitation and at the 6 month-post-stroke          
anniversary. Gait analysis will be performed to define outcome measures         
(level and community velocities) and associated gait impairment variables       
(muscle and motion patterns). The comprehensive baseline gait evaluation        
will be conducted within the first week the patient is able to walk six         
meters with assistance. Follow-up testing will be conducted at discharge        
from inpatient rehabilitation and at 6 months and i year post-stroke.           
Function of lower gluteus maximus, gluteus medius, long head of biceps          
femoris, semimembranosus, rectus femoris, adductor longus, vastus               
intermedius, soleus, anterior tibialis and peroneus brevis muscles will         
be recorded with dynamic EMG using intramuscular fine wire electrodes.          
Motion of the trunk, pelvis, hip, thigh, knee and ankle will be recorded        
for both the sound and paretic limbs with the Vicon Motion Analysis             
system. Stride characteristics and foot-floor contact patterns will be          
recorded with Stride Analyzer footswitch system. Subjects will ambulate         
on level surfaces and over a curb both in bare feet and in shoes with           
their customary orthoses. Patterned muscle strength will be documented          
with the Upright Motor Control test and selective strength of both lower        
extremities will be recorded with the LIDO isokinetic dynamometer (knee         
and ankle) and a strain gauge tensiometer (hip). Walking endurance will         
be measured in a fifteen minute test on an outdoor track. Clinical              
factors and gait impairment variables that best predict level and               
community walking velocities will be identified. Patterns of gait errors        
and substitutions will be formulated. Patients will be classified by            
their gait motor control strategy with a data analysis/expert system.           
Effectiveness of the three treatment programs will be discerned by              
comparing functional outcome measures (gait velocities) and gait                
impairment variables (EMG and motion). Patients in the three treatment          
groups will be subdivided based on initial motor control strategy to            
determine if initial severity affects the response to the rehabilitation        
strategies.                                                                     
 electromyography; gait; hemiplegia; human subject; human therapy evaluation; longitudinal human study; muscle function; muscle strength; rehabilitation; stroke RECOVERY AND REHABILITATION OF GAIT IN STROKE PATIENTS","The goal of this project is to develop rehabilitation guidelines for            
restoration of ambulation in patients following a stroke. Specifically          
the aims are to identify patterns of gait deviations in both the sound          
and paretic sides among patients with hemiplegia, form classifications          
of motor control strategies used during walking and relate these findings       
to the patient's potential for recovery of ambulation and response to           
intensive rehabilitation. Three gait training strategies will be                
compared: i) supported treadmill gait training, 2) intensified-use of the       
paretic leg and 3) functional independence and endurance training using         
motor learning principles. Variables that predict improvement in walking        
ability after rehabilitation will be identified. Three clinical tests           
(Functional Independence Measure, Fugl-meyer and Upright Motor Control)         
will be used to identify predictor-criteria. Testing will be done within        
one week of admission to rehabilitation and at the 6 month-post-stroke          
anniversary. Gait analysis will be performed to define outcome measures         
(level and community velocities) and associated gait impairment variables       
(muscle and motion patterns). The comprehensive baseline gait evaluation        
will be conducted within the first week the patient is able to walk six         
meters with assistance. Follow-up testing will be conducted at discharge        
from inpatient rehabilitation and at 6 months and i year post-stroke.           
Function of lower gluteus maximus, gluteus medius, long head of biceps          
femoris, semimembranosus, rectus femoris, adductor longus, vastus               
intermedius, soleus, anterior tibialis and peroneus brevis muscles will         
be recorded with dynamic EMG using intramuscular fine wire electrodes.          
Motion of the trunk, pelvis, hip, thigh, knee and ankle will be recorded        
for both the sound and paretic limbs with the Vicon Motion Analysis             
system. Stride characteristics and foot-floor contact patterns will be          
recorded with Stride Analyzer footswitch system. Subjects will ambulate         
on level surfaces and over a curb both in bare feet and in shoes with           
their customary orthoses. Patterned muscle strength will be documented          
with the Upright Motor Control test and selective strength of both lower        
extremities will be recorded with the LIDO isokinetic dynamometer (knee         
and ankle) and a strain gauge tensiometer (hip). Walking endurance will         
be measured in a fifteen minute test on an outdoor track. Clinical              
factors and gait impairment variables that best predict level and               
community walking velocities will be identified. Patterns of gait errors        
and substitutions will be formulated. Patients will be classified by            
their gait motor control strategy with a data analysis/expert system.           
Effectiveness of the three treatment programs will be discerned by              
comparing functional outcome measures (gait velocities) and gait                
impairment variables (EMG and motion). Patients in the three treatment          
groups will be subdivided based on initial motor control strategy to            
determine if initial severity affects the response to the rehabilitation        
strategies.                                                                     
",2403399,R01HD031931,['R01HD031931'],HD,https://reporter.nih.gov/project-details/2403399,R01,1997,185834,-0.2679461933733747
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The purpose of this 3 year randomized clinical trial project is to implement    
and test innovative social security representative payee strategies as they     
are integrated into long-term dual disorder treatment for substance abusing     
severely mentally ill outpatients in order to decrease substance abuse.  The    
project targets subjects who 1) receive SSI or SSDI disability benefits for     
severe mental illness, 2) have an alcohol/other duty disorder, and 3) have      
assigned representative payee status to their Community Mental Health Center    
(CMHC).  Two different CMHC's (urban/rural) will be used as study sites.        
After six months of baseline preintervention study, subjects will be            
randomized (stratified by baseline substance use) into either contingent or     
non-contingent payee management and compared over six months [randomized        
cohorts of subjects will be ""signed to one of two treatment orders, both Or     
which include 12 -month contingent and 6-month non contingent phases Of         
payee management].  In the contingent phase substance avoidance, regular        
treatment attendance, [clinical stability] and responsible money management     
will earn subjects increasing levels of autonomy in terms of the form (cash     
verses vouchers) and frequency (monthly/weekly/daily) of benefit                
disbursement by their representative payee case managers.  The case managers    
will use a carefully described payee log with built-in disbursement plan        
[and decision tree] to tighten or loosen form and frequency (not overall        
amount) of benefit disbursement according to the above [defined] subject        
behaviors.  In the non-contingent phase, disbursements are made once weekly     
without any behavioral contingencies.  While the primary outcome will be        
comparative substance abstinence, other key multi-dimensional outcomes will     
be assessed.  [By using a staggered phase, cross-over design,                   
multi-dimensional outcomes will be compared across and within subjects.] It     
is hypothesized that contingent management will be associated with              
significantly more abstinence from substance, better treatment adherence and    
money management; improved psychiatric symptomatology and quality of life,      
and decreased homelessness, hospitalization and incarcerations than             
noncontingent management in this high risk-high cost population.  [fewer        
adverse outcomes (hospitalization, incarceration, homelessness) and             
increased positive outcomes ([decreased substance use] greater treatment        
involvement, stability, quality Of life) than baseline or non-contingent        
management practices, in this high-risk/high-cost population.  The findings     
of this study should be immediately relevant for providers and applicable       
for the approximately half-million similarly affected individuals               
nationwide.                                                                     
 alcoholism /alcohol abuse; behavioral /social science research tag; bipolar depression; clinical research; clinical trials; comorbidity; disability insurance; functional ability; health services research tag; homeless; hospital utilization; human subject; human therapy evaluation; interview; longitudinal human study; major depression; mental disorders; outcomes research; patient care management; psychological tests; quality of life; reinforcer; schizophrenia; social service evaluation; socioeconomics; substance abuse related disorder CONTINGENT BENEFITS IN SUBSTANCE ABUSING MENTALLY ILL","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The purpose of this 3 year randomized clinical trial project is to implement    
and test innovative social security representative payee strategies as they     
are integrated into long-term dual disorder treatment for substance abusing     
severely mentally ill outpatients in order to decrease substance abuse.  The    
project targets subjects who 1) receive SSI or SSDI disability benefits for     
severe mental illness, 2) have an alcohol/other duty disorder, and 3) have      
assigned representative payee status to their Community Mental Health Center    
(CMHC).  Two different CMHC's (urban/rural) will be used as study sites.        
After six months of baseline preintervention study, subjects will be            
randomized (stratified by baseline substance use) into either contingent or     
non-contingent payee management and compared over six months [randomized        
cohorts of subjects will be ""signed to one of two treatment orders, both Or     
which include 12 -month contingent and 6-month non contingent phases Of         
payee management].  In the contingent phase substance avoidance, regular        
treatment attendance, [clinical stability] and responsible money management     
will earn subjects increasing levels of autonomy in terms of the form (cash     
verses vouchers) and frequency (monthly/weekly/daily) of benefit                
disbursement by their representative payee case managers.  The case managers    
will use a carefully described payee log with built-in disbursement plan        
[and decision tree] to tighten or loosen form and frequency (not overall        
amount) of benefit disbursement according to the above [defined] subject        
behaviors.  In the non-contingent phase, disbursements are made once weekly     
without any behavioral contingencies.  While the primary outcome will be        
comparative substance abstinence, other key multi-dimensional outcomes will     
be assessed.  [By using a staggered phase, cross-over design,                   
multi-dimensional outcomes will be compared across and within subjects.] It     
is hypothesized that contingent management will be associated with              
significantly more abstinence from substance, better treatment adherence and    
money management; improved psychiatric symptomatology and quality of life,      
and decreased homelessness, hospitalization and incarcerations than             
noncontingent management in this high risk-high cost population.  [fewer        
adverse outcomes (hospitalization, incarceration, homelessness) and             
increased positive outcomes ([decreased substance use] greater treatment        
involvement, stability, quality Of life) than baseline or non-contingent        
management practices, in this high-risk/high-cost population.  The findings     
of this study should be immediately relevant for providers and applicable       
for the approximately half-million similarly affected individuals               
nationwide.                                                                     
",2410923,R01DA010838,['R01DA010838'],DA,https://reporter.nih.gov/project-details/2410923,R01,1997,209275,-0.0577333040500775
"This proposal has four major divisions:                                         
                                                                                
1. The continuation of computational support for the molecular biology          
research community.                                                             
                                                                                
2. The dissemination of newly developed computational technologies and          
application examples.                                                           
                                                                                
3.The continuation of a multi-level training program with a new emphasis on     
the training of graduate students with strong mathematical and physical         
science backgrounds.                                                            
                                                                                
4.The continuation of the research and development program in computational     
molecular biology closely coupled experimental research laboratories, with      
a new emphasis on the integration of protein structural information into        
sequence functional analyses.                                                   
                                                                                
A number of direction changes from the previous project periods are planned     
due to the changing nature and sophistication of molecular biology              
computational needs:                                                            
                                                                                
l. a. Training in computational methods: To develop and provide training in     
the use and limitations of computational methods applicable to molecular        
biology, with a new emphasis on accessing, evaluating and using the wealth      
of network accessible computational and database services.                      
                                                                                
b. Training in molecular biological applications: To expand the training of     
physical and computational scientists with strong analytical background in      
the analyses of molecular biological problems.                                  
                                                                                
2. a. The development and evaluation of methods for function identification     
that integrate the available information obtained from various diagnostic       
pattern and database searches with other information such as protein            
structure, homolog family membership, genetic regulation signals, and           
enzymatic function(s) associations.                                             
                                                                                
b. The development and automation of new and existing methods for the           
exploitation of determined protein structure information in experimental        
design and analyses by non structural experts.                                  
 artificial intelligence; biomedical equipment resource; computer assisted sequence analysis; computer center; computer network; computer program /software; computer simulation; information dissemination; information retrieval; molecular biology; protein engineering; protein structure function; training; workshop BIOMOLECULAR ENGINEERING RESEARCH CENTER","This proposal has four major divisions:                                         
                                                                                
1. The continuation of computational support for the molecular biology          
research community.                                                             
                                                                                
2. The dissemination of newly developed computational technologies and          
application examples.                                                           
                                                                                
3.The continuation of a multi-level training program with a new emphasis on     
the training of graduate students with strong mathematical and physical         
science backgrounds.                                                            
                                                                                
4.The continuation of the research and development program in computational     
molecular biology closely coupled experimental research laboratories, with      
a new emphasis on the integration of protein structural information into        
sequence functional analyses.                                                   
                                                                                
A number of direction changes from the previous project periods are planned     
due to the changing nature and sophistication of molecular biology              
computational needs:                                                            
                                                                                
l. a. Training in computational methods: To develop and provide training in     
the use and limitations of computational methods applicable to molecular        
biology, with a new emphasis on accessing, evaluating and using the wealth      
of network accessible computational and database services.                      
                                                                                
b. Training in molecular biological applications: To expand the training of     
physical and computational scientists with strong analytical background in      
the analyses of molecular biological problems.                                  
                                                                                
2. a. The development and evaluation of methods for function identification     
that integrate the available information obtained from various diagnostic       
pattern and database searches with other information such as protein            
structure, homolog family membership, genetic regulation signals, and           
enzymatic function(s) associations.                                             
                                                                                
b. The development and automation of new and existing methods for the           
exploitation of determined protein structure information in experimental        
design and analyses by non structural experts.                                  
",2415713,P41LM005205,['P41LM005205'],LM,https://reporter.nih.gov/project-details/2415713,P41,1997,1197857,-0.18192386257050658
"The Minorities in Research and Science Achievement (MIRASA) Program at The      
University of Texas at San Antonio is intended to (1) strengthen the            
biomedical research capabilities of the University and faculty, (2)             
increase the number and quality of minority scientists by providing             
biomedical research training opportunities for students at the                  
undergraduate the graduate levels, and (3) expose minority science students     
to a variety of scientists and their research.  To accomplish these             
endeavors, nineteen research projects actively involving 23 Graduate, 23        
Undergraduate Students, 16 Principal Investigators, 3 Associate                 
Investigators, 3 co-principal investigators, 17 collaborators and 5             
consultants will direct biomedical research projects which will provide         
training opportunities for minority students.  These nineteen research          
projects include;  (1)  Isolation of regulatory kinase from the visual          
system of Drosophila (2)  Effects of neuronal activity on maturation of         
hippocampal neurons (3)  The role of the endoplasmic reticulum and other        
organelles in GAP-43 sorting (4)  3-D image processing for automated            
digital mammography (5) Biological and biochemical properties of                
glycosylated human growth hormone (6)  The role of versican in myelin and       
multiple sclerosis (7)  Molecular genetics of a nematode acetylcholine          
receptor (8)  Expression of HP chimeric genes in response to inflammatory       
factors and hormones (9)  New approaches tot he preparation of carcinogen-      
deoxynucleoside adducts (10)  Purple membrane proton pump (11)  Artificial      
neural network classification of fetal heart rate signals (12)  Optical and     
thermal characterization of ocular tissue (13)  Statistical analysis of         
long DNA sequences (14)  Muscle control using below-lesion                      
electromyographic signals: an innovative methodology (15)  Metal-mediated       
molecular and supramolecular aggregates of guanine and adenine nucleotides      
(16)  Regulation of myogenesis and motoneuron innervation by the                
neurotrophins (17) Ca+ homeostasis hormone sensitive Ca2+ channels (18)         
Augmented sympathoinhibition by area postrema noradrenergic neurons (19)        
11-cis retinyl ester hydrolase in the eye.  Moreover, student and faculty       
research potential will be enhanced by exposing them to sophisticated           
research instruments and modern research techniques employed in                 
biochemistry, bioengineering, chemistry, genetics, molecular biology,           
neuroanatomy, neurobiochemistry, neurophysiology, biophysics and                
biostatistics.  In addition, students and faculty will interact with            
regional and national scientists actively involved in biomedical research.      
Students and faculty will publish their results in prestigious refereed         
journals and  present their research at professional, national and              
international symposiums.  Institutional and class seminars will be             
continued featuring outstanding scientists and in cooperation with other        
institutions.                                                                   
 minority institution research support MINORITIES RESEARCH AND SCIENCE ACHIEVEMENT PROGRAM","The Minorities in Research and Science Achievement (MIRASA) Program at The      
University of Texas at San Antonio is intended to (1) strengthen the            
biomedical research capabilities of the University and faculty, (2)             
increase the number and quality of minority scientists by providing             
biomedical research training opportunities for students at the                  
undergraduate the graduate levels, and (3) expose minority science students     
to a variety of scientists and their research.  To accomplish these             
endeavors, nineteen research projects actively involving 23 Graduate, 23        
Undergraduate Students, 16 Principal Investigators, 3 Associate                 
Investigators, 3 co-principal investigators, 17 collaborators and 5             
consultants will direct biomedical research projects which will provide         
training opportunities for minority students.  These nineteen research          
projects include;  (1)  Isolation of regulatory kinase from the visual          
system of Drosophila (2)  Effects of neuronal activity on maturation of         
hippocampal neurons (3)  The role of the endoplasmic reticulum and other        
organelles in GAP-43 sorting (4)  3-D image processing for automated            
digital mammography (5) Biological and biochemical properties of                
glycosylated human growth hormone (6)  The role of versican in myelin and       
multiple sclerosis (7)  Molecular genetics of a nematode acetylcholine          
receptor (8)  Expression of HP chimeric genes in response to inflammatory       
factors and hormones (9)  New approaches tot he preparation of carcinogen-      
deoxynucleoside adducts (10)  Purple membrane proton pump (11)  Artificial      
neural network classification of fetal heart rate signals (12)  Optical and     
thermal characterization of ocular tissue (13)  Statistical analysis of         
long DNA sequences (14)  Muscle control using below-lesion                      
electromyographic signals: an innovative methodology (15)  Metal-mediated       
molecular and supramolecular aggregates of guanine and adenine nucleotides      
(16)  Regulation of myogenesis and motoneuron innervation by the                
neurotrophins (17) Ca+ homeostasis hormone sensitive Ca2+ channels (18)         
Augmented sympathoinhibition by area postrema noradrenergic neurons (19)        
11-cis retinyl ester hydrolase in the eye.  Moreover, student and faculty       
research potential will be enhanced by exposing them to sophisticated           
research instruments and modern research techniques employed in                 
biochemistry, bioengineering, chemistry, genetics, molecular biology,           
neuroanatomy, neurobiochemistry, neurophysiology, biophysics and                
biostatistics.  In addition, students and faculty will interact with            
regional and national scientists actively involved in biomedical research.      
Students and faculty will publish their results in prestigious refereed         
journals and  present their research at professional, national and              
international symposiums.  Institutional and class seminars will be             
continued featuring outstanding scientists and in cooperation with other        
institutions.                                                                   
",2459235,S06GM008194,['S06GM008194'],GM,https://reporter.nih.gov/project-details/2459235,S06,1997,1734659,-0.19601508726274847
"This proposal has four major divisions:                                         
                                                                                
1. The continuation of computational support for the molecular biology          
research community.                                                             
                                                                                
2. The dissemination of newly developed computational technologies and          
application examples.                                                           
                                                                                
3.The continuation of a multi-level training program with a new emphasis on     
the training of graduate students with strong mathematical and physical         
science backgrounds.                                                            
                                                                                
4.The continuation of the research and development program in computational     
molecular biology closely coupled experimental research laboratories, with      
a new emphasis on the integration of protein structural information into        
sequence functional analyses.                                                   
                                                                                
A number of direction changes from the previous project periods are planned     
due to the changing nature and sophistication of molecular biology              
computational needs:                                                            
                                                                                
l. a. Training in computational methods: To develop and provide training in     
the use and limitations of computational methods applicable to molecular        
biology, with a new emphasis on accessing, evaluating and using the wealth      
of network accessible computational and database services.                      
                                                                                
b. Training in molecular biological applications: To expand the training of     
physical and computational scientists with strong analytical background in      
the analyses of molecular biological problems.                                  
                                                                                
2. a. The development and evaluation of methods for function identification     
that integrate the available information obtained from various diagnostic       
pattern and database searches with other information such as protein            
structure, homolog family membership, genetic regulation signals, and           
enzymatic function(s) associations.                                             
                                                                                
b. The development and automation of new and existing methods for the           
exploitation of determined protein structure information in experimental        
design and analyses by non structural experts.                                  
 artificial intelligence; biomedical equipment resource; computer assisted sequence analysis; computer center; computer network; computer program /software; computer simulation; information dissemination; information retrieval; molecular biology information system; protein engineering; protein structure function; training; workshop BIOMOLECULAR ENGINEERING RESEARCH CENTER","This proposal has four major divisions:                                         
                                                                                
1. The continuation of computational support for the molecular biology          
research community.                                                             
                                                                                
2. The dissemination of newly developed computational technologies and          
application examples.                                                           
                                                                                
3.The continuation of a multi-level training program with a new emphasis on     
the training of graduate students with strong mathematical and physical         
science backgrounds.                                                            
                                                                                
4.The continuation of the research and development program in computational     
molecular biology closely coupled experimental research laboratories, with      
a new emphasis on the integration of protein structural information into        
sequence functional analyses.                                                   
                                                                                
A number of direction changes from the previous project periods are planned     
due to the changing nature and sophistication of molecular biology              
computational needs:                                                            
                                                                                
l. a. Training in computational methods: To develop and provide training in     
the use and limitations of computational methods applicable to molecular        
biology, with a new emphasis on accessing, evaluating and using the wealth      
of network accessible computational and database services.                      
                                                                                
b. Training in molecular biological applications: To expand the training of     
physical and computational scientists with strong analytical background in      
the analyses of molecular biological problems.                                  
                                                                                
2. a. The development and evaluation of methods for function identification     
that integrate the available information obtained from various diagnostic       
pattern and database searches with other information such as protein            
structure, homolog family membership, genetic regulation signals, and           
enzymatic function(s) associations.                                             
                                                                                
b. The development and automation of new and existing methods for the           
exploitation of determined protein structure information in experimental        
design and analyses by non structural experts.                                  
",2627517,P41LM005205,['P41LM005205'],LM,https://reporter.nih.gov/project-details/2627517,P41,1997,78678,-0.1553676206435571
"Recent advances in sensor technology for a broad class of applications          
have resulted in the evolution of sophisticated hardware for high               
resolution multispectral imaging.  Multispectral imaging and associated         
cutting edge multispectral image and data fusion processing software            
have led to the realization of techniques which add significantly to the        
ability of identifying and characterizing the nature of scenes on the           
earth from space, or the identification of other objects from their             
multispectral and spatial signatures.                                           
                                                                                
A multispectral image fusion system is proposed which will present              
additional diagnostic information to a clinical or research                     
ophthalmologist.  Using state-of-the-art multispectral image capture            
system, advanced multispectral image fusion techniques inspired by              
the human vision system, and neural network feature classification              
algorithms, a Multispectral Image fusion System for detection and               
identification of ocular pathological features is described.                    
 artificial intelligence; biomedical equipment development; charge coupled device camera; clinical biomedical equipment; computational neuroscience; diagnosis design /evaluation; eye disorder diagnosis; eye fundus photography; fundus oculi; hemoglobin; image processing MULTISPECTRAL FUNDUS IMAGING SYSTEM","Recent advances in sensor technology for a broad class of applications          
have resulted in the evolution of sophisticated hardware for high               
resolution multispectral imaging.  Multispectral imaging and associated         
cutting edge multispectral image and data fusion processing software            
have led to the realization of techniques which add significantly to the        
ability of identifying and characterizing the nature of scenes on the           
earth from space, or the identification of other objects from their             
multispectral and spatial signatures.                                           
                                                                                
A multispectral image fusion system is proposed which will present              
additional diagnostic information to a clinical or research                     
ophthalmologist.  Using state-of-the-art multispectral image capture            
system, advanced multispectral image fusion techniques inspired by              
the human vision system, and neural network feature classification              
algorithms, a Multispectral Image fusion System for detection and               
identification of ocular pathological features is described.                    
",2606924,R43EY011675,['R43EY011675'],EY,https://reporter.nih.gov/project-details/2606924,R43,1997,93406,-0.046781019031206836
"We propose creating a shared computing resource to support research             
activities in biomedical informatics at the Stanford University School of       
Medicine.  The School has established the Center for Advanced Medical           
Informatics at Stanford (CAMIS), which unites academic, administrative,         
and research computing.  The requested funds will support an active core        
research program and the infrastructure and day-to-day computing needs of       
a large number of researchers, each of whom has external support from NIH       
or other agencies through grants or contracts.  No single research group        
could support the complex set of infrastructural and system capabilities        
needed to conduct advanced medical informatics research in the 1990's.          
Our approach to designing, managing, and building the CAMIS resource will       
be guided by a number of overall goals: (1) to foster scientific                
communication, collaboration, and sharing within a distributed research         
environment; (2) to provide a superb setting for exploring biomedical           
informatics topics; (3) to pursue a strong basic replications systems; (4)      
to help define new ways to disseminate computing and                            
information-management technologies into real-world settings; and (5) to        
create a state-of-the-art computing and communication environment for our       
research work.                                                                  
                                                                                
The specific aims for this undertaking fall under four categories that          
define the functions of the CAMIS resources:                                    
                                                                                
Core Research and Development:  We are proposing core research projects in      
several areas: (1) Clinical Trials Workstation (CTW) Project-generalize         
and abstract system-design concepts and implementation solutions from our       
ONCOCIN and T-HELPER projects so that they can be applied to facilitate         
the most rapid development of integrated clinical-trials workstations in        
any domain of clinical-trials management; (2) Understanding the Code for        
Biological Molecules--development and generalize improved tools for             
discovering and for understanding the structure relationships so these can      
be used to understand the functions of genes identified in genome mapping       
investigation; (3) Advanced Computing Systems and Environments--build upon      
the extensive distributed-computing environment already in place and            
improve, extend, and adapt new computing tools as necessary to support the      
CAMIS community.  Specific systems research areas include gesture-based         
portable computing information resource navigation and retrieval tools,         
email management tools, high-speed networks, large-volume file storage,         
and new workstations technologics.                                              
                                                                                
Collaborative Research:  A large community of researchers will be               
supported by the CAMIS resource, doing work in areas such as clinical           
trial protocol management, methods for uncertain reasoning,                     
bioinformatics, health care outcomes and economics, basic artificial            
intelligence research, human-computer interfaces, multimedia authoring and      
delivery tools for  medical education programs, and distributed library         
resources. We will accordingly encourage collaborations through improved        
mechanisms for inter- and intra-group communications.                           
                                                                                
Service and Resource Operations: CAMIS will provide effective and widely        
accessible communication and computing facilities, consulting services,         
and managerial support and will serve in an advisory capacity to Medical        
School administration.                                                          
                                                                                
Training, Education, and Dissemination: CAMIS will emphasize a strong user      
orientation in its facilities management, will continue to run two formal       
graduate degree programs (one in medical informatics and one in health          
services research), and will actively disseminate research results through      
publications, software exports, video presentations, visiting scholar           
programs, and an annual CAMIS Symposium.                                        
 biomedical facility; computer center; information systems CENTER FOR ADVANCED MEDICAL INFORMATICS","We propose creating a shared computing resource to support research             
activities in biomedical informatics at the Stanford University School of       
Medicine.  The School has established the Center for Advanced Medical           
Informatics at Stanford (CAMIS), which unites academic, administrative,         
and research computing.  The requested funds will support an active core        
research program and the infrastructure and day-to-day computing needs of       
a large number of researchers, each of whom has external support from NIH       
or other agencies through grants or contracts.  No single research group        
could support the complex set of infrastructural and system capabilities        
needed to conduct advanced medical informatics research in the 1990's.          
Our approach to designing, managing, and building the CAMIS resource will       
be guided by a number of overall goals: (1) to foster scientific                
communication, collaboration, and sharing within a distributed research         
environment; (2) to provide a superb setting for exploring biomedical           
informatics topics; (3) to pursue a strong basic replications systems; (4)      
to help define new ways to disseminate computing and                            
information-management technologies into real-world settings; and (5) to        
create a state-of-the-art computing and communication environment for our       
research work.                                                                  
                                                                                
The specific aims for this undertaking fall under four categories that          
define the functions of the CAMIS resources:                                    
                                                                                
Core Research and Development:  We are proposing core research projects in      
several areas: (1) Clinical Trials Workstation (CTW) Project-generalize         
and abstract system-design concepts and implementation solutions from our       
ONCOCIN and T-HELPER projects so that they can be applied to facilitate         
the most rapid development of integrated clinical-trials workstations in        
any domain of clinical-trials management; (2) Understanding the Code for        
Biological Molecules--development and generalize improved tools for             
discovering and for understanding the structure relationships so these can      
be used to understand the functions of genes identified in genome mapping       
investigation; (3) Advanced Computing Systems and Environments--build upon      
the extensive distributed-computing environment already in place and            
improve, extend, and adapt new computing tools as necessary to support the      
CAMIS community.  Specific systems research areas include gesture-based         
portable computing information resource navigation and retrieval tools,         
email management tools, high-speed networks, large-volume file storage,         
and new workstations technologics.                                              
                                                                                
Collaborative Research:  A large community of researchers will be               
supported by the CAMIS resource, doing work in areas such as clinical           
trial protocol management, methods for uncertain reasoning,                     
bioinformatics, health care outcomes and economics, basic artificial            
intelligence research, human-computer interfaces, multimedia authoring and      
delivery tools for  medical education programs, and distributed library         
resources. We will accordingly encourage collaborations through improved        
mechanisms for inter- and intra-group communications.                           
                                                                                
Service and Resource Operations: CAMIS will provide effective and widely        
accessible communication and computing facilities, consulting services,         
and managerial support and will serve in an advisory capacity to Medical        
School administration.                                                          
                                                                                
Training, Education, and Dissemination: CAMIS will emphasize a strong user      
orientation in its facilities management, will continue to run two formal       
graduate degree programs (one in medical informatics and one in health          
services research), and will actively disseminate research results through      
publications, software exports, video presentations, visiting scholar           
programs, and an annual CAMIS Symposium.                                        
",2601716,P41LM005305,['P41LM005305'],LM,https://reporter.nih.gov/project-details/2601716,P41,1997,239365,-0.08827072543586326
"The information produced by the Human Genome Project and related research       
is expected to revolutionize medicine in general and recombinant DNA            
biotechnology in particular. At present, however, obtaining patent              
protection for novel, useful, and unobvious inventions involving the            
human genome is a recognized bottleneck in the development of a globally-       
competitive U.S. biotechnology industry. While much attention has been          
focused on problems that occur during prosecution of patent applications,       
little has centered on the quality of the input to this process: the            
invention disclosure. This research will investigate whether a knowledge-       
based expert system can help scientists/inventors produce better                
collaboratively-authored recombinant DNA invention disclosures in               
partnership with their technology managers and patent attorneys. The            
system will help scientists/inventors better understand how an                  
intellectual property technology portfolio is created and managed,              
resulting in better communication among the inventor, technology manager        
and patent attorney. The specific objectives of this research are to: (I)       
expand the project team's understanding of the communication and                
information needs of the scientist/inventor, technology manager and             
patent attorney system users, (2) iteratively develop a Phase 11 system         
prototype using an object-oriented programming approach, and (3) conduct        
a field test of the prototype to evaluates its impact on invention              
disclosure quality.                                                             
                                                                                
PROPOSED COMMERCIAL APPLICATIONS: Worldwide sales of biotechnology              
products are expected to reach $100 billion by the year 2000. The U.5           
continues to lead all other countries in international patent activity in       
this field. Preparation of biotechnology patent applications is a               
business which exceeds $70 million in the U.S. alone. The system proposed       
herein will enable preparation of higher-quality invention disclosures          
and more informed intellectual-property protection decisions.                   
 behavioral /social science research tag; computer program /software; computer system design /evaluation; genome; health related legal; recombinant DNA EXPERT SYSTEM FOR RECOMBINANT DNA INVENTION DISCLOSURE","The information produced by the Human Genome Project and related research       
is expected to revolutionize medicine in general and recombinant DNA            
biotechnology in particular. At present, however, obtaining patent              
protection for novel, useful, and unobvious inventions involving the            
human genome is a recognized bottleneck in the development of a globally-       
competitive U.S. biotechnology industry. While much attention has been          
focused on problems that occur during prosecution of patent applications,       
little has centered on the quality of the input to this process: the            
invention disclosure. This research will investigate whether a knowledge-       
based expert system can help scientists/inventors produce better                
collaboratively-authored recombinant DNA invention disclosures in               
partnership with their technology managers and patent attorneys. The            
system will help scientists/inventors better understand how an                  
intellectual property technology portfolio is created and managed,              
resulting in better communication among the inventor, technology manager        
and patent attorney. The specific objectives of this research are to: (I)       
expand the project team's understanding of the communication and                
information needs of the scientist/inventor, technology manager and             
patent attorney system users, (2) iteratively develop a Phase 11 system         
prototype using an object-oriented programming approach, and (3) conduct        
a field test of the prototype to evaluates its impact on invention              
disclosure quality.                                                             
                                                                                
PROPOSED COMMERCIAL APPLICATIONS: Worldwide sales of biotechnology              
products are expected to reach $100 billion by the year 2000. The U.5           
continues to lead all other countries in international patent activity in       
this field. Preparation of biotechnology patent applications is a               
business which exceeds $70 million in the U.S. alone. The system proposed       
herein will enable preparation of higher-quality invention disclosures          
and more informed intellectual-property protection decisions.                   
",2546198,R44HL058327,['R44HL058327'],HL,https://reporter.nih.gov/project-details/2546198,R44,1997,371572,-0.14633321187637705
"Electrophoresis can resolve thousands of proteins, but analysis of such         
data is extremely difficult. Automated methods for detection and                
recognition of proteins on two-dimensional (2D) electrophoretograms are         
needed. Image processing techniques and expert systems have been tried,         
but these are computationally intensive, and require heuristic rule bases.      
In Phase I and on a recent IR&D project, ORINCON has developed a protein        
identification system based on automated image spot correspondence              
processing and the use of a hierarchy of neural nets for recognition of         
protein distribution in 2D electrophoretograms. This system demonstrates        
the feasibility of differentiating and correctly classifying different          
clinical conditions in 2D electrophoretograms. In Phase II we will further      
develop these techniques to handle large numbers-of image spots for an          
expanded set of known proteins to enable analysis of entire 2D                  
electrophoresis biological samples. The technique will be tested on a           
large data set collected by Scripps Clinic and Large Scale Biology (LSB)        
Corporation. This would lead to development of a commercial package for         
automated protein analysis of 2D electrophoretograms. It could easily be        
modified to categorize other constituents (e.g., amino and nucleic acids)       
via analysis of appropriate data sets.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION: The research should lead to development        
of a commercial package for automated processing of 2D electrophoresis          
protein data. Applications of this technology would be widespread; in           
addition to laboratories, in which electrophoresis is common, the               
technology could be applied to clinical settings, in which patient protein      
analysis would be a very important diagnostic tool for differentiation          
between normal and diseased states. This technique can potentially be used      
as a cancer screening tool. Over one million cases of cancer are diagnosed      
in the U.S. every year.                                                         
 artificial intelligence; bioengineering /biomedical engineering; computational neuroscience; electrophoresis; image processing; method development; proteins NEURAL NET FOR 2D ELECTROPHORESIS PROTEIN IDENTIFICATION","Electrophoresis can resolve thousands of proteins, but analysis of such         
data is extremely difficult. Automated methods for detection and                
recognition of proteins on two-dimensional (2D) electrophoretograms are         
needed. Image processing techniques and expert systems have been tried,         
but these are computationally intensive, and require heuristic rule bases.      
In Phase I and on a recent IR&D project, ORINCON has developed a protein        
identification system based on automated image spot correspondence              
processing and the use of a hierarchy of neural nets for recognition of         
protein distribution in 2D electrophoretograms. This system demonstrates        
the feasibility of differentiating and correctly classifying different          
clinical conditions in 2D electrophoretograms. In Phase II we will further      
develop these techniques to handle large numbers-of image spots for an          
expanded set of known proteins to enable analysis of entire 2D                  
electrophoresis biological samples. The technique will be tested on a           
large data set collected by Scripps Clinic and Large Scale Biology (LSB)        
Corporation. This would lead to development of a commercial package for         
automated protein analysis of 2D electrophoretograms. It could easily be        
modified to categorize other constituents (e.g., amino and nucleic acids)       
via analysis of appropriate data sets.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION: The research should lead to development        
of a commercial package for automated processing of 2D electrophoresis          
protein data. Applications of this technology would be widespread; in           
addition to laboratories, in which electrophoresis is common, the               
technology could be applied to clinical settings, in which patient protein      
analysis would be a very important diagnostic tool for differentiation          
between normal and diseased states. This technique can potentially be used      
as a cancer screening tool. Over one million cases of cancer are diagnosed      
in the U.S. every year.                                                         
",2545355,R44CA062469,['R44CA062469'],CA,https://reporter.nih.gov/project-details/2545355,R44,1997,210839,-0.101065947749136
"This Small Business Innovation Research Phase I project will develop            
several software products dealing with the analysis of stochastic point         
processes for use in basic electrophysiology and behavioral neuroscience,       
as well as in fields dealing with higher aspects of human behavior such as      
mental health and alcoholism treatment relapse issues.  Although not            
specifically targeted, because of the ubiquity of the subject matter, we        
anticipate considerable usage by researchers in Epidemiology and other          
medical fields, and even in diverse fields such as electrical engineering,      
forensics and operations research.  The software will focus on the              
analysis of single and paired event data, i.e., single or multiple lists        
of the times that events of a particular type occur.  The algorithms            
selected for inclusion in the program will include some of the better-          
known existing algorithms for point processes, as well as selected,             
recently developed algorithms in this area.  The software will be for the       
PC platform and distributed in the form of a stand-alone windows                
application, as well as DLL's and scripts to allow inclusion of the             
functionality in existing software products.                                    
                                                                                
PROPOSED COMMERCIAL APPLICATION:  The proposed software will be a research      
tool that should have wide marketability within the target research             
communities.  In addition, because of the complete lack of comparable           
software this software has the potential to penetrate the much larger non-      
specialist market.                                                              
 artificial intelligence; computer data analysis; computer program /software; computer system design /evaluation; human data; mathematical model; model design /development; personal computers; statistics /biometry INITIATED EVENT MODELS OF STOCHASTIC POINT PROCESSES","This Small Business Innovation Research Phase I project will develop            
several software products dealing with the analysis of stochastic point         
processes for use in basic electrophysiology and behavioral neuroscience,       
as well as in fields dealing with higher aspects of human behavior such as      
mental health and alcoholism treatment relapse issues.  Although not            
specifically targeted, because of the ubiquity of the subject matter, we        
anticipate considerable usage by researchers in Epidemiology and other          
medical fields, and even in diverse fields such as electrical engineering,      
forensics and operations research.  The software will focus on the              
analysis of single and paired event data, i.e., single or multiple lists        
of the times that events of a particular type occur.  The algorithms            
selected for inclusion in the program will include some of the better-          
known existing algorithms for point processes, as well as selected,             
recently developed algorithms in this area.  The software will be for the       
PC platform and distributed in the form of a stand-alone windows                
application, as well as DLL's and scripts to allow inclusion of the             
functionality in existing software products.                                    
                                                                                
PROPOSED COMMERCIAL APPLICATION:  The proposed software will be a research      
tool that should have wide marketability within the target research             
communities.  In addition, because of the complete lack of comparable           
software this software has the potential to penetrate the much larger non-      
specialist market.                                                              
",2034104,R44MH052010,['R44MH052010'],MH,https://reporter.nih.gov/project-details/2034104,R44,1997,258057,-0.01809711021115384
"DESCRIPTION (Taken from application abstract):  Reminder systems are expert     
 artificial intelligence; automated medical record system; behavioral /social science research tag; belief; computer assisted medical decision making; computer assisted patient care; computer system design /evaluation; health care facility information system; health services research tag; human data; patient care management BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN","DESCRIPTION (Taken from application abstract):  Reminder systems are expert     
",2032395,R29LM006233,['R29LM006233'],LM,https://reporter.nih.gov/project-details/2032395,R29,1997,95423,-0.029667342213398094
"Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
 AIDS; Cryptococcus neoformans; antibody specificity; antifungal antibody; artificial intelligence; carbohydrate structure; cell wall; epitope mapping; fungal antigens; information systems; microorganism classification; nuclear magnetic resonance spectroscopy; opportunistic infections; polysaccharides CRYPTOCOCCUS NEOFORMANS--EPITOPE ANTIBODIES & STRUCTURE","Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
",2330365,R01AI031769,['R01AI031769'],AI,https://reporter.nih.gov/project-details/2330365,R01,1997,244811,-0.018010272736929858
"The goal of this project is to develop an Early Vocalization Analyzer           
(EVA), a computer program to automatically analyze recorded samples of          
infant vocalizations. This program will be the basis of a screening test,       
for use in hospitals and clinics, to evaluate which infants are at-risk         
for later communication or other developmental problems. The major purpose      
of this Phase-I project is to determine whether the landmark detection          
theory of Stevens, as implemented for the recognition of adult speech, can      
be adapted to the analysis of syllabic pre-speech vocalizations produced        
by typically developing infants. We will also merge this landmark-              
detection algorithm with a kernel version of EVA, developed by a team at        
Northeastern University, to produce a proof-of-concept prototype that will      
count the number of utterances, detect and count syllable-like                  
vocalizations, and classify them according to fundamental frequency and         
duration. The goal of Phase II is to develop a commercial software program      
to automatically analyze infant utterances from audio recordings, as a          
non-invasive research and clinical-screening instrument.                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The product would be useful for hospitals and infant-screening clinics          
assessing developmental risks, as well as hearing deficits. It could also       
be used by treatment centers to the efficacy of early intervention.             
Finally, it would provide a research tool for investigations of the             
earliest stages of infant speech and of age-specific control of the speech      
articulators.                                                                   
 artificial intelligence; biomedical equipment; child (0-11); clinical research; computer data analysis; computer program /software; computer system design /evaluation; developmental psychology; diagnosis design /evaluation; early diagnosis; human subject; infant human (0-1 year); vocalization EVA, AN EARLY VOCALIZATION ANALYZER","The goal of this project is to develop an Early Vocalization Analyzer           
(EVA), a computer program to automatically analyze recorded samples of          
infant vocalizations. This program will be the basis of a screening test,       
for use in hospitals and clinics, to evaluate which infants are at-risk         
for later communication or other developmental problems. The major purpose      
of this Phase-I project is to determine whether the landmark detection          
theory of Stevens, as implemented for the recognition of adult speech, can      
be adapted to the analysis of syllabic pre-speech vocalizations produced        
by typically developing infants. We will also merge this landmark-              
detection algorithm with a kernel version of EVA, developed by a team at        
Northeastern University, to produce a proof-of-concept prototype that will      
count the number of utterances, detect and count syllable-like                  
vocalizations, and classify them according to fundamental frequency and         
duration. The goal of Phase II is to develop a commercial software program      
to automatically analyze infant utterances from audio recordings, as a          
non-invasive research and clinical-screening instrument.                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The product would be useful for hospitals and infant-screening clinics          
assessing developmental risks, as well as hearing deficits. It could also       
be used by treatment centers to the efficacy of early intervention.             
Finally, it would provide a research tool for investigations of the             
earliest stages of infant speech and of age-specific control of the speech      
articulators.                                                                   
",2026043,R41HD034686,['R41HD034686'],HD,https://reporter.nih.gov/project-details/2026043,R41,1997,95896,-0.12998730458869884
"This proposal aims to develop an improved understanding of the mechanisms       
involved in functional MRI of the brain and to optimize imaging and data        
analysis strategies for the detection of neuronal activity.  Functional MRI     
relies on the ability to detect the changes in NMR signal that are produced     
in discrete regions of cortex in response to specific activating stimuli,       
and are believed to reflect changes in local blood flow, volume and             
oxygenation.  Functional MRI promises to be a major addition to the methods     
available for studying brain activation.  Despite the widespread claims for     
the power and successes of the method, there remain several unanswered          
questions regarding its optimal mode of use, the tissue and technical           
factors that are important in determining the signal changes detected, and      
the significance and interpretation of these signal changes.  The research      
proposed would systematically address such issues.  The underlying              
mechanism may include both susceptibility contrast effects, based on the        
BOLD effect, as well as wash-in effects, and these will be separately           
quantified.  The factors that affect each mechanism will be separately          
identified and measured.  For the BOLD effect, extensive computer modeling      
and measurements in phantoms and animals brains will be used to establish       
the relative sensitivity to vascular structures of different sizes,             
spacings and orientations, as well as other tissue properties such as the       
rate of water diffusion.  The separate sensitivities to s-called static         
field effects (T2*),  diffusive losses and other mechanisms will also be        
established.  The performance of different pulse sequences will be compared     
to devise optimal methods of scanning and detection at 1.5T.  Echo planar       
imaging, conventional gradient echo and fast spin echo imaging as well as       
more novel schemes will be compared in phantoms, animal brains and examples     
of human activation.  Human and animal activations will be produced in vivo     
using  visual and motor stimuli as well as by alteration of global blood        
flow by acetazolamide and hypercarbia.  A critical feature of current           
paradigms for detecting activation is the method of data analysis, which is     
interrelated with the nature of the task and imaging method used.  We will      
compare different methods of analyzing functional data sets, including          
statistical parameter mapping, time-correlation analyses, and principal         
component analysis.  The sensitivity of each to motion and other artifacts      
will be established by in in vivo comparisons and by computer simulations.      
From these studies, we anticipate being able to improve strategies for the      
use and interpretation of functional MRI in human studies of function and       
cognition.                                                                      
 acetazolamide; biophysics; blood flow measurement; blood vessels; blood volume; brain electrical activity; capillary; computer data analysis; computer simulation; human subject; hypercapnia; laboratory rat; magnetic resonance imaging; method development; motor neurons; nuclear magnetic resonance spectroscopy; phantom model; respiratory oxygenation; statistics /biometry; visual stimulus; water flow BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI","This proposal aims to develop an improved understanding of the mechanisms       
involved in functional MRI of the brain and to optimize imaging and data        
analysis strategies for the detection of neuronal activity.  Functional MRI     
relies on the ability to detect the changes in NMR signal that are produced     
in discrete regions of cortex in response to specific activating stimuli,       
and are believed to reflect changes in local blood flow, volume and             
oxygenation.  Functional MRI promises to be a major addition to the methods     
available for studying brain activation.  Despite the widespread claims for     
the power and successes of the method, there remain several unanswered          
questions regarding its optimal mode of use, the tissue and technical           
factors that are important in determining the signal changes detected, and      
the significance and interpretation of these signal changes.  The research      
proposed would systematically address such issues.  The underlying              
mechanism may include both susceptibility contrast effects, based on the        
BOLD effect, as well as wash-in effects, and these will be separately           
quantified.  The factors that affect each mechanism will be separately          
identified and measured.  For the BOLD effect, extensive computer modeling      
and measurements in phantoms and animals brains will be used to establish       
the relative sensitivity to vascular structures of different sizes,             
spacings and orientations, as well as other tissue properties such as the       
rate of water diffusion.  The separate sensitivities to s-called static         
field effects (T2*),  diffusive losses and other mechanisms will also be        
established.  The performance of different pulse sequences will be compared     
to devise optimal methods of scanning and detection at 1.5T.  Echo planar       
imaging, conventional gradient echo and fast spin echo imaging as well as       
more novel schemes will be compared in phantoms, animal brains and examples     
of human activation.  Human and animal activations will be produced in vivo     
using  visual and motor stimuli as well as by alteration of global blood        
flow by acetazolamide and hypercarbia.  A critical feature of current           
paradigms for detecting activation is the method of data analysis, which is     
interrelated with the nature of the task and imaging method used.  We will      
compare different methods of analyzing functional data sets, including          
statistical parameter mapping, time-correlation analyses, and principal         
component analysis.  The sensitivity of each to motion and other artifacts      
will be established by in in vivo comparisons and by computer simulations.      
From these studies, we anticipate being able to improve strategies for the      
use and interpretation of functional MRI in human studies of function and       
cognition.                                                                      
",2431248,R01NS033332,['R01NS033332'],NS,https://reporter.nih.gov/project-details/2431248,R01,1997,346221,-0.16288905405315293
"The notion of clinical significance permeates evaluations of clinical           
research results, plays an important role in changing physician behavior,       
and enters implicitly into the development of rules for knowledge-based         
computer systems. To date, however, this concept has received no formal         
examination and is left to the intuition of the reasoning agent:. The aims      
of this project are (1) to explore the role that clinical significance          
plays in physicians' evaluation of clinical-research results; (2) to            
propose formal methods that satisfy those roles; (3) to create a computer-      
based system that would implement those methods; and (4) to validate the        
proposed methods. As the test domain, we shall focus on the choice between      
two therapies as evaluated in randomized clinical trials.                       
                                                                                
(1) We shall explore, through a survey process, three central questions         
regarding clinical significance: physicians' perception of its relative         
importance in evaluating study results, physicians' need for assistance in      
its evaluation, and the variability among physicians in its assessment.         
                                                                                
(2) We shall determine measures within three formal frameworks for              
expressing the factors that the survey ascertained as important. The            
frameworks are frequentist statistics, Bayesian statistics, and decision        
theory.                                                                         
                                                                                
The central challenge in applying these formal frameworks and their             
measures into the clinical setting are the difficulty physicians have with      
numerical measures, regardless of framework. To meet this challenge, we         
propose to develop accessible methods for using the frameworks and to           
develop computer-based tools for using the methods.                             
                                                                                
The strategy we propose for developing accessible methods is to establish       
canonical models within each framework. We plan to propose a set of             
canonical decision models implicit in conclusions of clinical studies and       
to validate the set by a review of a random set of clinical trials gleaned      
from the medical research literature. To use these models, physicians need      
novel nonnumerical methods-graphical and qualitative techniques- that           
translate the statistical results into clinically meaningful terms and          
concepts.                                                                       
                                                                                
(3) Because these novel methods will be computationally more difficult          
than current practice, we propose to construct a set of computer-based          
tools that will implement them. As basis for building these tools, we           
shall use a set of novel artificial intelligence techniques.                    
                                                                                
(4) We propose to validate each of the following propositions: that             
clinicians can use these advanced methods, that they prefer one framework       
over the other, and that use of these methods reduces or explains the           
variability established by the questionnaire process.                           
 artificial intelligence; clinical trials; computer assisted medical decision making; health care model; health science research analysis /evaluation; human data; method development; physicians; questionnaires; statistics /biometry FORMALIZING THE NOTION OF CLINICAL SIGNIFICANCE","The notion of clinical significance permeates evaluations of clinical           
research results, plays an important role in changing physician behavior,       
and enters implicitly into the development of rules for knowledge-based         
computer systems. To date, however, this concept has received no formal         
examination and is left to the intuition of the reasoning agent:. The aims      
of this project are (1) to explore the role that clinical significance          
plays in physicians' evaluation of clinical-research results; (2) to            
propose formal methods that satisfy those roles; (3) to create a computer-      
based system that would implement those methods; and (4) to validate the        
proposed methods. As the test domain, we shall focus on the choice between      
two therapies as evaluated in randomized clinical trials.                       
                                                                                
(1) We shall explore, through a survey process, three central questions         
regarding clinical significance: physicians' perception of its relative         
importance in evaluating study results, physicians' need for assistance in      
its evaluation, and the variability among physicians in its assessment.         
                                                                                
(2) We shall determine measures within three formal frameworks for              
expressing the factors that the survey ascertained as important. The            
frameworks are frequentist statistics, Bayesian statistics, and decision        
theory.                                                                         
                                                                                
The central challenge in applying these formal frameworks and their             
measures into the clinical setting are the difficulty physicians have with      
numerical measures, regardless of framework. To meet this challenge, we         
propose to develop accessible methods for using the frameworks and to           
develop computer-based tools for using the methods.                             
                                                                                
The strategy we propose for developing accessible methods is to establish       
canonical models within each framework. We plan to propose a set of             
canonical decision models implicit in conclusions of clinical studies and       
to validate the set by a review of a random set of clinical trials gleaned      
from the medical research literature. To use these models, physicians need      
novel nonnumerical methods-graphical and qualitative techniques- that           
translate the statistical results into clinically meaningful terms and          
concepts.                                                                       
                                                                                
(3) Because these novel methods will be computationally more difficult          
than current practice, we propose to construct a set of computer-based          
tools that will implement them. As basis for building these tools, we           
shall use a set of novel artificial intelligence techniques.                    
                                                                                
(4) We propose to validate each of the following propositions: that             
clinicians can use these advanced methods, that they prefer one framework       
over the other, and that use of these methods reduces or explains the           
variability established by the questionnaire process.                           
",2430872,R29LM005647,['R29LM005647'],LM,https://reporter.nih.gov/project-details/2430872,R29,1997,119705,-0.08791180656101946
"DESCRIPTION: Recently, computer based videokeratography (VK) has become         
important for measuring corneal shape before and after refractive surgery       
and in diagnosing keratoconus. Regrettably, present algorithms are              
flawed because they assume light rays that form the VK image lie in the         
meridional plane before and after reflection from the cornea.  For both         
keratoconus and refractive surgery this assumption can produce                  
substantial errors. The investigators have begun development of a new           
algorithm that eliminates such errors. There are a number of topic on           
which further research is needed before the algorithm is viable.  The           
investigators will also develop software for splicing and averaging             
multiple shots to extend coverage to the whole cornea and to reduce             
noise. Also, they will develop two end-user applications: 1) Software           
for fitting hard and soft toric contact lens (for correction of                 
astigmatism) that will take into account corneal topography to  achieve         
improved lens centration and orientation. They expect to develop the            
first successful contact lens fitting software, enabling VK to become a         
standard instrument for contact lens fitting.                                   
                                                                                
Presently several thousand videokeratoscope instruments are being used          
for refractive surgery and for diagnosing keratoconus. The number of            
instruments will expand rapidly if improved algorithms for measuring            
corneal shape and improved applications software become available.  As          
one example, the market for a clinically superior contact lens fitting          
package in very large (especially true for the many countries with few          
trained contact lens specialists). The investigators expect that                
software will be the first to win clinical approval.                            
 artificial intelligence; biomedical equipment development; clinical biomedical equipment; clinical research; computer program /software; computer system design /evaluation; contact lens; cornea; diagnosis design /evaluation; eye disorder diagnosis; eye refractometry; human subject; keratoconus; mathematical model; noninvasive diagnosis; surface property; video recording system; vision tests NEW CORNEAL TOPOGRAPHY ALGORITHM AND ITS APPLICATIONS","DESCRIPTION: Recently, computer based videokeratography (VK) has become         
important for measuring corneal shape before and after refractive surgery       
and in diagnosing keratoconus. Regrettably, present algorithms are              
flawed because they assume light rays that form the VK image lie in the         
meridional plane before and after reflection from the cornea.  For both         
keratoconus and refractive surgery this assumption can produce                  
substantial errors. The investigators have begun development of a new           
algorithm that eliminates such errors. There are a number of topic on           
which further research is needed before the algorithm is viable.  The           
investigators will also develop software for splicing and averaging             
multiple shots to extend coverage to the whole cornea and to reduce             
noise. Also, they will develop two end-user applications: 1) Software           
for fitting hard and soft toric contact lens (for correction of                 
astigmatism) that will take into account corneal topography to  achieve         
improved lens centration and orientation. They expect to develop the            
first successful contact lens fitting software, enabling VK to become a         
standard instrument for contact lens fitting.                                   
                                                                                
Presently several thousand videokeratoscope instruments are being used          
for refractive surgery and for diagnosing keratoconus. The number of            
instruments will expand rapidly if improved algorithms for measuring            
corneal shape and improved applications software become available.  As          
one example, the market for a clinically superior contact lens fitting          
package in very large (especially true for the many countries with few          
trained contact lens specialists). The investigators expect that                
software will be the first to win clinical approval.                            
",2422457,R44EY011211,['R44EY011211'],EY,https://reporter.nih.gov/project-details/2422457,R44,1997,351072,-0.027132894903323997
"DESCRIPTION:  Numerous practical and theoretical problems could be addressed    
if we had a better understanding of the auditory mechanisms underlying          
phonetic recognition.  Among the practical applications of this knowledge       
are:  (1) the improvement of speech synthesis devices, (2) the development      
of robust speech recognition devices, (3) the development of acoustically       
based training devices for hearing-impaired speakers, and (4) improvement in    
Cochlear-implant signal processors.  The proposed experiments fall into         
three major categories.  One set of experiments follows in a rather direct      
way from vowel perception studies conducted during the previous grant           
period.  These experiments address issues such as the role of dynamic           
spectral cues and voice fundamental frequency in vowel perception.  A second    
series of experiments address more fundamental issues regarding the spectral    
representations that control phonetic quality.  A major goal of these           
experiments is to test the validity of a method of representing speech that     
was developed during the previous grant period.  The ""Masked Peak               
Representation"" (MPR) was developed as an alternative to both formant           
representations and whole spectrum models.  The MPR involves a series of        
spectral manipulations that are designed to remove aspects of the spectrum      
that do not appear to have a strong influence on phonetic quality, while        
retaining those features that are most relevant to phonetic quality             
judgments.  The MPR will be evaluated with:  (1) an experiment comparing        
MPR-based predictions of perceived phonetic distance with those of a more       
traditional auditory model, (2) speech recognition tests that use a Hidden      
Markov Model to map sequences of MPR spectra onto words or phonetic             
segments, and (3) listening tests with speech resynthesized from MPR            
spectra.  A third set of studies is aimed at modeling the low-level auditory    
mechanisms that are responsible for spectrum analysis.  The goal of this        
work is to evaluate a model of spectrum analysis that is carried out by the     
central auditory system rather than the auditory periphery.  A software         
simulation of the model will be developed in an effort to determine the         
extent to which the central-spectrum model can account for a broad range of     
findings from the auditory psychophysics literature.  Experiments are also      
proposed that address the implications of this model for vowel perception       
and for the representation of pitch and periodicity.                            
 behavioral /social science research tag; clinical research; hearing; human subject; mathematical model; neural information processing; perception; perceptual maskings; psychoacoustics; sound frequency; speech recognition ACOUSTIC CORRELATES OF PHONETIC PERCEPTION","DESCRIPTION:  Numerous practical and theoretical problems could be addressed    
if we had a better understanding of the auditory mechanisms underlying          
phonetic recognition.  Among the practical applications of this knowledge       
are:  (1) the improvement of speech synthesis devices, (2) the development      
of robust speech recognition devices, (3) the development of acoustically       
based training devices for hearing-impaired speakers, and (4) improvement in    
Cochlear-implant signal processors.  The proposed experiments fall into         
three major categories.  One set of experiments follows in a rather direct      
way from vowel perception studies conducted during the previous grant           
period.  These experiments address issues such as the role of dynamic           
spectral cues and voice fundamental frequency in vowel perception.  A second    
series of experiments address more fundamental issues regarding the spectral    
representations that control phonetic quality.  A major goal of these           
experiments is to test the validity of a method of representing speech that     
was developed during the previous grant period.  The ""Masked Peak               
Representation"" (MPR) was developed as an alternative to both formant           
representations and whole spectrum models.  The MPR involves a series of        
spectral manipulations that are designed to remove aspects of the spectrum      
that do not appear to have a strong influence on phonetic quality, while        
retaining those features that are most relevant to phonetic quality             
judgments.  The MPR will be evaluated with:  (1) an experiment comparing        
MPR-based predictions of perceived phonetic distance with those of a more       
traditional auditory model, (2) speech recognition tests that use a Hidden      
Markov Model to map sequences of MPR spectra onto words or phonetic             
segments, and (3) listening tests with speech resynthesized from MPR            
spectra.  A third set of studies is aimed at modeling the low-level auditory    
mechanisms that are responsible for spectrum analysis.  The goal of this        
work is to evaluate a model of spectrum analysis that is carried out by the     
central auditory system rather than the auditory periphery.  A software         
simulation of the model will be developed in an effort to determine the         
extent to which the central-spectrum model can account for a broad range of     
findings from the auditory psychophysics literature.  Experiments are also      
proposed that address the implications of this model for vowel perception       
and for the representation of pitch and periodicity.                            
",2014474,R01DC001661,['R01DC001661'],DC,https://reporter.nih.gov/project-details/2014474,R01,1997,293809,-0.040645604002232154
"The goal of this research is to develop an open and extensible software         
environment for medical image segmentation.  This environment will              
contribute to the public and scientific interest in at least three ways:        
(1) improved and efficient segmentation of medical images for various           
applications, (2) efficient creation of new image segmentation                  
algorithms, and (3) improved evaluation for medical image segmentation          
algorithms.                                                                     
Image segmentation has many applications in medical imaging; however,           
it's use in current clinical practice falls far short of its potential.         
Commercially available tools are either designed for very specific              
applications, or are general-purpose  image processing packages with            
little support for image segmentation.  The proposed environment will be        
devoted to medical image segmentation and will have an extensible and           
open architecture.  The extensible architecture will allow easy                 
customization for specific applications and will also allow users to add        
their own algorithms.  The open architecture will make the software             
platform-independent and will allow easy integration with existing              
applications, such as databases, analysis packages, or visualization            
tools.  This environment will also provide tools for evaluation of              
medical image segmentation algorithms.  To design such a software               
environment, we will use the latest innovations in design such a software       
environment, we will use latest innovations in software technology, such        
as object-oriented design and distributed objects.                              
PROPOSED COMMERCIAL APPLICATION:                                                
We envision two types of users for this software environment--(1) medical       
imaging researchers or medical imaging solution providers, and (2)              
clinicians or clinical researchers.  The first type of uses will be able        
to customize the application completely.  The second type of users will         
use the software for specific applications.  Our building-block approach        
to the design of the environment will allow rapid customization for             
different applications and for the different types of users.                    
 artificial intelligence; computer human interaction; computer program /software; computer system design /evaluation; data collection methodology /evaluation; image enhancement; image processing; interactive multimedia SOFTWARE ENVIRONMENT FOR MEDICAL IMAGE SEGMENTATION","The goal of this research is to develop an open and extensible software         
environment for medical image segmentation.  This environment will              
contribute to the public and scientific interest in at least three ways:        
(1) improved and efficient segmentation of medical images for various           
applications, (2) efficient creation of new image segmentation                  
algorithms, and (3) improved evaluation for medical image segmentation          
algorithms.                                                                     
Image segmentation has many applications in medical imaging; however,           
it's use in current clinical practice falls far short of its potential.         
Commercially available tools are either designed for very specific              
applications, or are general-purpose  image processing packages with            
little support for image segmentation.  The proposed environment will be        
devoted to medical image segmentation and will have an extensible and           
open architecture.  The extensible architecture will allow easy                 
customization for specific applications and will also allow users to add        
their own algorithms.  The open architecture will make the software             
platform-independent and will allow easy integration with existing              
applications, such as databases, analysis packages, or visualization            
tools.  This environment will also provide tools for evaluation of              
medical image segmentation algorithms.  To design such a software               
environment, we will use the latest innovations in design such a software       
environment, we will use latest innovations in software technology, such        
as object-oriented design and distributed objects.                              
PROPOSED COMMERCIAL APPLICATION:                                                
We envision two types of users for this software environment--(1) medical       
imaging researchers or medical imaging solution providers, and (2)              
clinicians or clinical researchers.  The first type of uses will be able        
to customize the application completely.  The second type of users will         
use the software for specific applications.  Our building-block approach        
to the design of the environment will allow rapid customization for             
different applications and for the different types of users.                    
",2012643,R43CA074670,['R43CA074670'],CA,https://reporter.nih.gov/project-details/2012643,R43,1997,93458,-0.0035272658928653462
"DESCRIPTION:  The proposed research is designed to develop and evaluate         
efficacious and cost-effective combinations of behavioral interventions and     
Nicotine Replacement Therapy (NRT) for smoking cessation.  NRT has been         
demonstrated to be effective but has been used by only a small percentage of    
the smokers.  In contrast, population based interventions with proactive        
recruitment and stage matched interventions have the potential to reach more    
than 80 percent of the population.  The proposed research will combine NRT      
with two recently developed, highly effective behavioral interventions that     
employ digital technology to produce individualized, low cost interventions.    
These interventions have the potential to both increase the proportion of       
smokers who receive NRT and the effectiveness or NRT.  Expert System            
interventions based on the Transtheoretical Model have demonstrated             
effectiveness in two population-based studies without NRT.                      
Telecommunications represents a low cost method of providing high frequency     
always available automated counseling services to smokers.  The proposed        
study is a population based clinical trial with 2200 smokers proactively        
recruited from a large VA system and randomly assigned to one of four           
conditions, varying from minimal intervention to high intensity.  The           
conditions include:  (1) Minimal Intervention, which involves only              
stage-matched manuals; (2) NRT Alone, which provides NRT with only manuals;     
(3) Expert System, which combines NRT, the Expert System and Manuals; and       
(4) Telecommunications, which combines the previous interventions with an       
automated telephone counseling intervention.  In the three NRT conditions,      
NRT will be provided only to those smokers for whom it is appropriate.  A       
goal of the behavioral interventions is to increase the proportion of the       
smokers who receive NRT.  Data analysis will identify the most efficacious      
interventions in preparation for dissemination to entire population.  Stage     
matched, interactive, and proactive interventions that match behavioral and     
pharmacological elements to the individual smoker have the potential to         
produce unprecedented impacts on an entire population of smokers.               
 behavioral /social science research tag; clinical research; computer assisted patient care; counseling; drug abuse chemotherapy; drug abuse therapy; health behavior; human subject; human therapy evaluation; nicotine; outcomes research; self help; smoking cessation; telecommunications AUTOMATED POPULATION BASED SMOKING CESSATION PROGRAMS","DESCRIPTION:  The proposed research is designed to develop and evaluate         
efficacious and cost-effective combinations of behavioral interventions and     
Nicotine Replacement Therapy (NRT) for smoking cessation.  NRT has been         
demonstrated to be effective but has been used by only a small percentage of    
the smokers.  In contrast, population based interventions with proactive        
recruitment and stage matched interventions have the potential to reach more    
than 80 percent of the population.  The proposed research will combine NRT      
with two recently developed, highly effective behavioral interventions that     
employ digital technology to produce individualized, low cost interventions.    
These interventions have the potential to both increase the proportion of       
smokers who receive NRT and the effectiveness or NRT.  Expert System            
interventions based on the Transtheoretical Model have demonstrated             
effectiveness in two population-based studies without NRT.                      
Telecommunications represents a low cost method of providing high frequency     
always available automated counseling services to smokers.  The proposed        
study is a population based clinical trial with 2200 smokers proactively        
recruited from a large VA system and randomly assigned to one of four           
conditions, varying from minimal intervention to high intensity.  The           
conditions include:  (1) Minimal Intervention, which involves only              
stage-matched manuals; (2) NRT Alone, which provides NRT with only manuals;     
(3) Expert System, which combines NRT, the Expert System and Manuals; and       
(4) Telecommunications, which combines the previous interventions with an       
automated telephone counseling intervention.  In the three NRT conditions,      
NRT will be provided only to those smokers for whom it is appropriate.  A       
goal of the behavioral interventions is to increase the proportion of the       
smokers who receive NRT.  Data analysis will identify the most efficacious      
interventions in preparation for dissemination to entire population.  Stage     
matched, interactive, and proactive interventions that match behavioral and     
pharmacological elements to the individual smoker have the potential to         
produce unprecedented impacts on an entire population of smokers.               
",2009991,R01CA071356,['R01CA071356'],CA,https://reporter.nih.gov/project-details/2009991,R01,1997,813620,-0.056042601713616226
"The cornea is a principal refractive element in the eye; corneal                
transparency and corneal shape determine its optical qualities.                 
Corneal epithelial edema, stromal edema and corneal shape anomalies             
can independently or collectively degrade visual performance inthe              
form of increased internal ligh scatter andoptical aberrations due to           
irregular astigmatism.  The central theme of this research proposalis           
the refinement and application of a mathematical model that                     
integrates the thermodynamic description of corneal epithelial, stromal         
and endothelial transport properties into a model of corneal hydration          
control.  This is combined with methods to classify shape anomalies             
and means to assess the optical quality of the corneal surface through          
the analysis of corneal topography.                                             
 artificial intelligence; atrial natriuretic peptide; biological transport; body water dehydration; cornea edema; corneal endothelium; corneal epithelium; corneal stroma; electrophysiology; human subject; intraocular fluid; laboratory rabbit; mathematical model; membrane permeability; model design /development; morphology; nitric oxide; refractive keratoplasty; thermodynamics; visual perception INTEGRATED ASSESSMENT OF CORNEAL FORM AND FUNCTION","The cornea is a principal refractive element in the eye; corneal                
transparency and corneal shape determine its optical qualities.                 
Corneal epithelial edema, stromal edema and corneal shape anomalies             
can independently or collectively degrade visual performance inthe              
form of increased internal ligh scatter andoptical aberrations due to           
irregular astigmatism.  The central theme of this research proposalis           
the refinement and application of a mathematical model that                     
integrates the thermodynamic description of corneal epithelial, stromal         
and endothelial transport properties into a model of corneal hydration          
control.  This is combined with methods to classify shape anomalies             
and means to assess the optical quality of the corneal surface through          
the analysis of corneal topography.                                             
",2019435,R01EY003311,['R01EY003311'],EY,https://reporter.nih.gov/project-details/2019435,R01,1997,235734,-0.033997430628076884
"Changes in soft tissue elasticity are usually related to pathological           
processes.  Because of this, palpation is still widely used for                 
diagnosis.  Its efficacy, however, is limited to abnormalities located          
relatively close to the skin surface.  The goal of quantitative                 
elasticity imaging is to develop surrogate, remote palpation, thus              
expanding its range to include deep lying lesions.  The elastic                 
properties of any continuous medium such as tissue can be assessed              
through precise measurement of mechanical deformations throughout that          
medium induced by forces applied at the surface.  Using modern medical          
imaging devices to precisely measure internal motion, it should be              
possible to estimate and even image elastic properties of internal              
organs.  In competition with other imaging modalities, ultrasound has two       
major advantages for elasticity imaging; it is inherently real-time and         
speckle artifacts limiting the quality of conventional images provide           
excellent markers for accurate tracking of tissue motion.  Elasticity can       
be imaged, therefore, by measuring motion with an ultrasound speckle            
tracking algorithm, followed by reconstruction of the elasticity                
distribution.  Although some other imaging systems, particularly real-          
time ultrasound, must be used to monitor tissue motion, elasticity              
imaging represents a fundamentally new diagnostic modality.  To                 
investigate quantitative elasticity imaging for medical diagnosis, a            
research plan addressing the important clinical problem of renal                
inflammation and scarring has been formulated.  Preliminary data support        
the hypothesis that kidney elasticity changes with renal damage and             
concomitant scarring before renal problems are detectable by traditional        
diagnostic techniques such as laboratory measurements of renal function.        
Therefore, quantitative elasticity imaging may be valuable in detecting         
and quantifying scar for conditions such as kidney transplant rejection         
where rejection is difficult to quantify from functional measurements           
alone.  Based on the results of these studies, it is the long range goal        
of this research program to develop a sensitive diagnostic technique            
based on quantitative elasticity imaging permitting surrogate palpation         
of deep lying lesions.                                                          
 animal tissue; artificial intelligence; computer program /software; elasticity; glomerular filtration rate; guinea pigs; histopathology; hydroxyproline; kidney disorder diagnosis; laboratory rabbit; mechanical stress; nephritis; phantom model ELASTICITY IMAGING FOR EARLY RENAL PATHOLOGY DETECTION","Changes in soft tissue elasticity are usually related to pathological           
processes.  Because of this, palpation is still widely used for                 
diagnosis.  Its efficacy, however, is limited to abnormalities located          
relatively close to the skin surface.  The goal of quantitative                 
elasticity imaging is to develop surrogate, remote palpation, thus              
expanding its range to include deep lying lesions.  The elastic                 
properties of any continuous medium such as tissue can be assessed              
through precise measurement of mechanical deformations throughout that          
medium induced by forces applied at the surface.  Using modern medical          
imaging devices to precisely measure internal motion, it should be              
possible to estimate and even image elastic properties of internal              
organs.  In competition with other imaging modalities, ultrasound has two       
major advantages for elasticity imaging; it is inherently real-time and         
speckle artifacts limiting the quality of conventional images provide           
excellent markers for accurate tracking of tissue motion.  Elasticity can       
be imaged, therefore, by measuring motion with an ultrasound speckle            
tracking algorithm, followed by reconstruction of the elasticity                
distribution.  Although some other imaging systems, particularly real-          
time ultrasound, must be used to monitor tissue motion, elasticity              
imaging represents a fundamentally new diagnostic modality.  To                 
investigate quantitative elasticity imaging for medical diagnosis, a            
research plan addressing the important clinical problem of renal                
inflammation and scarring has been formulated.  Preliminary data support        
the hypothesis that kidney elasticity changes with renal damage and             
concomitant scarring before renal problems are detectable by traditional        
diagnostic techniques such as laboratory measurements of renal function.        
Therefore, quantitative elasticity imaging may be valuable in detecting         
and quantifying scar for conditions such as kidney transplant rejection         
where rejection is difficult to quantify from functional measurements           
alone.  Based on the results of these studies, it is the long range goal        
of this research program to develop a sensitive diagnostic technique            
based on quantitative elasticity imaging permitting surrogate palpation         
of deep lying lesions.                                                          
",2016733,R01DK047324,['R01DK047324'],DK,https://reporter.nih.gov/project-details/2016733,R01,1997,209601,0.0039882445130775416
"This proposal describes the development of an artificial neural network         
(ANN) for breast cancer diagnosis.  The system is trained to aid in the         
decision to biopsy those patients that have suspicious mammographic             
findings.  This system will decrease the variability and increase the           
specificity of the decision to biopsy.  the decision to biopsy is a two         
stage process: 1) the mammagrapher views the mammagram and determines the       
presence of absence of image features such as caclcifications and masses,       
2) these features are merged to form a diagnosis. A recent study found          
that 52% of missed breast cancers are due to errors at the decision step.       
About 80% of the biopsies that are performed are benign.  AxIS will             
provide a service to significantly improve this performance.  The               
clinician reads a mammagram, transmits the findings to AxIS where the           
trained ANN returns a prediction of benign of malignant in less than one        
second.  This prediction will be transmitted to the clinician  who can          
then include this information in the medical decision.                          
PROPOSED COMMERCIAL APPLICATION:                                                
The service provided by AxIS will dramatically reduce the error rate for        
the decision to biopsy.   Preliminary results indicate that use of a            
computer aid could decrease by half of the number of benign cases that          
are biopsied without decreasing the number of malignancies found for a          
savings of $335million per year.  Clients include practicing clinicians         
and managed care organizations.                                                 
 artificial intelligence; biopsy; breast neoplasm /cancer diagnosis; clinical research; computer assisted medical decision making; computer system design /evaluation; diagnosis design /evaluation; fine needle aspiration; human data; mammography; outcomes research; statistics /biometry BREAST CANCER BIOPSY--A COMPUTER AID","This proposal describes the development of an artificial neural network         
(ANN) for breast cancer diagnosis.  The system is trained to aid in the         
decision to biopsy those patients that have suspicious mammographic             
findings.  This system will decrease the variability and increase the           
specificity of the decision to biopsy.  the decision to biopsy is a two         
stage process: 1) the mammagrapher views the mammagram and determines the       
presence of absence of image features such as caclcifications and masses,       
2) these features are merged to form a diagnosis. A recent study found          
that 52% of missed breast cancers are due to errors at the decision step.       
About 80% of the biopsies that are performed are benign.  AxIS will             
provide a service to significantly improve this performance.  The               
clinician reads a mammagram, transmits the findings to AxIS where the           
trained ANN returns a prediction of benign of malignant in less than one        
second.  This prediction will be transmitted to the clinician  who can          
then include this information in the medical decision.                          
PROPOSED COMMERCIAL APPLICATION:                                                
The service provided by AxIS will dramatically reduce the error rate for        
the decision to biopsy.   Preliminary results indicate that use of a            
computer aid could decrease by half of the number of benign cases that          
are biopsied without decreasing the number of malignancies found for a          
savings of $335million per year.  Clients include practicing clinicians         
and managed care organizations.                                                 
",2011264,R43CA073235,['R43CA073235'],CA,https://reporter.nih.gov/project-details/2011264,R43,1997,95631,-0.019733054290804478
"DESCRIPTION:  In this revised application of his competitive renewal, Dr.       
Moult proposes to understand the relationship between protein structure and     
sequence by extending present computational search algorithms and analyzing     
free energy functions, as well as investigating potentials based on             
alternative physical methods.  Dr. Moult will test and extend Monte Carlo,      
genetic algorithm and other co-operative algorithmic conformational search      
methods, test and improve potentials, use folding simulation methods to         
probe the extent of context independent conformational choice in fragments      
of protein structures and to investigate the folding of protein domains by      
propagation methods.                                                            
 artificial intelligence; computer simulation; computer system design /evaluation; hydropathy; intermolecular interaction; ionic bond; model design /development; protein folding; protein structure COMPUTER ALGORITHM FOR MODELING PROTEIN STRUCTURE","DESCRIPTION:  In this revised application of his competitive renewal, Dr.       
Moult proposes to understand the relationship between protein structure and     
sequence by extending present computational search algorithms and analyzing     
free energy functions, as well as investigating potentials based on             
alternative physical methods.  Dr. Moult will test and extend Monte Carlo,      
genetic algorithm and other co-operative algorithmic conformational search      
methods, test and improve potentials, use folding simulation methods to         
probe the extent of context independent conformational choice in fragments      
of protein structures and to investigate the folding of protein domains by      
propagation methods.                                                            
",2022261,R01GM041034,['R01GM041034'],GM,https://reporter.nih.gov/project-details/2022261,R01,1997,140363,-0.03676764865979244
"This proposal seeks support for our studies on the effect of spinal cord        
injury on male reproduction.  Although anejaculation is a relatively            
uncommon occurrence in the general population, over 12,000 new cases are        
reported annually.  The major cause of ejaculatory dysfunction is spinal        
cord injury, usually occurring as a result of a motor vehicle accident.         
Infertility in spinal cord injured men is reported to approach 95%.  This       
impaired fertility is thought to be due to anejaculation secondary to           
neuromuscular dysfunction, obstruction of the genital passages secondary        
to infections and/or impaired spermatogenesis.  Electroejaculation, a           
procedure for inducing the seminal fluid emission by electrically               
stimulating specific nerves of the male reproductive tract with rectal          
probe electrostimulation, has been used to obtain sperm from these              
patients.  Nevertheless, poor sperm quality has been a consistent finding       
and pregnancy rates remain low.  Although adequate sperm densities              
usually can be obtained, the low sperm motility appears to be a limiting        
factor.  The information on variables that correlate with semen quality         
and ultimate pregnancy success or failure is far from complete.  We have        
proposed to develop a standardized protocol to be used in a large multi--       
institutional study of electroejaculation (EEJ) of SCI men to correlate         
patient evaluation, treatment and data collection.  We will evaluate the        
epididymal secretion of specific proteins, as well as the hormonal and          
spermatogenic characteristics of a large population of spinal cord              
injured men.  Changes in genital tract protein secretion after neurologic       
injury will provide the potential to develop a useful prognostic assay          
for damage to the proximal ductal system.  Most significantly we have           
proposed to devise a new approach for the multivariate analysis of the          
data using an artificial neural network Traditional statistical analysis        
of fertility has proven very unsatisfactory, with life table analysis           
commonly employed as a means of approximating the evaluation of fertility       
potential.  By ""learning"" and ""generalizing"", the neural network model          
provides an ideal method of addressing such a complex issue.  Upon              
completion of these aims we will have performed the largest study to date       
on the effect of SCI on male reproductive function and will have used           
this information to develop a powerful new system for the diagnosis of          
fertility potential in this unique patient population.                          
 SDS polyacrylamide gel electrophoresis; artificial intelligence; biopsy; computational neuroscience; data collection; electronic stimulator; electrostimulus; epididymis; fertility; genital secretion; gonadotropin releasing factor; hormone regulation /control mechanism; human subject; male; male reproductive system disorder; neuromuscular disorder; neurophysiology; prognosis; semen; sperm capacitation; sperm motility; spermatogenesis; spinal cord injury; testis CORRELATES OF FERTILITY IN SPINAL CORD-INJURED MEN","This proposal seeks support for our studies on the effect of spinal cord        
injury on male reproduction.  Although anejaculation is a relatively            
uncommon occurrence in the general population, over 12,000 new cases are        
reported annually.  The major cause of ejaculatory dysfunction is spinal        
cord injury, usually occurring as a result of a motor vehicle accident.         
Infertility in spinal cord injured men is reported to approach 95%.  This       
impaired fertility is thought to be due to anejaculation secondary to           
neuromuscular dysfunction, obstruction of the genital passages secondary        
to infections and/or impaired spermatogenesis.  Electroejaculation, a           
procedure for inducing the seminal fluid emission by electrically               
stimulating specific nerves of the male reproductive tract with rectal          
probe electrostimulation, has been used to obtain sperm from these              
patients.  Nevertheless, poor sperm quality has been a consistent finding       
and pregnancy rates remain low.  Although adequate sperm densities              
usually can be obtained, the low sperm motility appears to be a limiting        
factor.  The information on variables that correlate with semen quality         
and ultimate pregnancy success or failure is far from complete.  We have        
proposed to develop a standardized protocol to be used in a large multi--       
institutional study of electroejaculation (EEJ) of SCI men to correlate         
patient evaluation, treatment and data collection.  We will evaluate the        
epididymal secretion of specific proteins, as well as the hormonal and          
spermatogenic characteristics of a large population of spinal cord              
injured men.  Changes in genital tract protein secretion after neurologic       
injury will provide the potential to develop a useful prognostic assay          
for damage to the proximal ductal system.  Most significantly we have           
proposed to devise a new approach for the multivariate analysis of the          
data using an artificial neural network Traditional statistical analysis        
of fertility has proven very unsatisfactory, with life table analysis           
commonly employed as a means of approximating the evaluation of fertility       
potential.  By ""learning"" and ""generalizing"", the neural network model          
provides an ideal method of addressing such a complex issue.  Upon              
completion of these aims we will have performed the largest study to date       
on the effect of SCI on male reproductive function and will have used           
this information to develop a powerful new system for the diagnosis of          
fertility potential in this unique patient population.                          
",2025434,R01HD030155,['R01HD030155'],HD,https://reporter.nih.gov/project-details/2025434,R01,1997,312047,0.019498607242339434
"This proposal is directed toward improving tomographic imaging in               
diagnostic radiology and nuclear medicine. It is predicated on the claim        
that significant advances will be achieved in the fidelity of the images        
that are reconstructed from the raw detector measurements of the                
tomographic scanner by changing the basic elements (called ""basis               
functions"") with which the image is built in the computer. The                  
conventional basic elements for computerized tomographic imaging are the        
voxel basis functions, and the sinusoidal basis functions of Fourier            
analysis. Two classes of promising new basis functions have been                
developed: functions that are localized in space (as are the voxel basis        
functions), and functions that are not localized (similar in many respects      
to sinusoids). The new classes of basis functions are well-suited to            
constructing faithful digital image representations of the biological           
structures that have influenced the raw tomographic scanner data. The new       
localized basis functions have a number of very desirable properties not        
shared by voxels: they are rotationally symmetric, their Fourier                
transforms are effectively localized, and they have continuous derivatives      
of any desired order.  The new non-localized basis functions are designed       
to perform a spatially-variant filtering operation that is required by a        
non-iterative method of 3D image reconstruction developed by the Principal      
Investigator.                                                                   
                                                                                
The specific aims are to develop mathematical theory, efficient computer        
algorithms, application-specific implementations and evaluation criteria        
for (1) methods of iterative reconstruction from projections, (2) methods       
of estimating the fundamental limits on the performance of the                  
reconstruction process, and (3) methods of non-iterative 3D reconstruction      
from projections. For specified imaging tasks, the level of statistical         
significance will be found for rejection of the null hypothesis that two        
methods perform a task equally well, in favor of the alternative                
hypothesis that one method performs the task better.                            
                                                                                
The basis functions of the image representation are the essential core of       
all methods for computerized image reconstruction, irrespective of the          
medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of        
new computer algorithms and their associated image representations will         
enable the full potential of scanners for functional imaging in emission        
tomography (PET and SPECT) to be realized by extracting as much                 
information as possible from fully-3D low-statistics projection data.           
 artificial intelligence; computer simulation; computer system design /evaluation; digital imaging; model design /development; phantom model; positron emission tomography DIGITAL IMAGE REPRESENTATIONS FOR TOMOGRAPHIC RADIOLOGY","This proposal is directed toward improving tomographic imaging in               
diagnostic radiology and nuclear medicine. It is predicated on the claim        
that significant advances will be achieved in the fidelity of the images        
that are reconstructed from the raw detector measurements of the                
tomographic scanner by changing the basic elements (called ""basis               
functions"") with which the image is built in the computer. The                  
conventional basic elements for computerized tomographic imaging are the        
voxel basis functions, and the sinusoidal basis functions of Fourier            
analysis. Two classes of promising new basis functions have been                
developed: functions that are localized in space (as are the voxel basis        
functions), and functions that are not localized (similar in many respects      
to sinusoids). The new classes of basis functions are well-suited to            
constructing faithful digital image representations of the biological           
structures that have influenced the raw tomographic scanner data. The new       
localized basis functions have a number of very desirable properties not        
shared by voxels: they are rotationally symmetric, their Fourier                
transforms are effectively localized, and they have continuous derivatives      
of any desired order.  The new non-localized basis functions are designed       
to perform a spatially-variant filtering operation that is required by a        
non-iterative method of 3D image reconstruction developed by the Principal      
Investigator.                                                                   
                                                                                
The specific aims are to develop mathematical theory, efficient computer        
algorithms, application-specific implementations and evaluation criteria        
for (1) methods of iterative reconstruction from projections, (2) methods       
of estimating the fundamental limits on the performance of the                  
reconstruction process, and (3) methods of non-iterative 3D reconstruction      
from projections. For specified imaging tasks, the level of statistical         
significance will be found for rejection of the null hypothesis that two        
methods perform a task equally well, in favor of the alternative                
hypothesis that one method performs the task better.                            
                                                                                
The basis functions of the image representation are the essential core of       
all methods for computerized image reconstruction, irrespective of the          
medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of        
new computer algorithms and their associated image representations will         
enable the full potential of scanners for functional imaging in emission        
tomography (PET and SPECT) to be realized by extracting as much                 
information as possible from fully-3D low-statistics projection data.           
",2414217,R01CA054356,['R01CA054356'],CA,https://reporter.nih.gov/project-details/2414217,R01,1997,229448,-0.036394980283091
"The goal of this project is to develop a computer algorithm that                
accurately predicts the tertiary fold of a small protein given only             
its amino acid sequence and secondary structure. It will use an                 
evolutionary optimization method with proprietary enhancements                  
invented by the PI. The specific aims of Phase I are to: (1)                    
develop a more natural representation of the problem; (2) implement             
an energy function appropriate for this representation; (3) improve             
the optimization algorithm; and (4) combine these into a simulator              
that successfully predicts tertiary from secondary structure. In                
Phase II this simulator will be extended to predict the tertiary                
structures of larger proteins from their sequences and from sparse              
experimental constraints or imperfect secondary structure                       
predictions.  It will be validated using many proteins of different             
sizes and fold classes and developed into software products for the             
pharmaceutical and biotechnology industries. The long-term goal is              
to help satisfy the rapidly growing demand for the accurate                     
prediction of protein structures from sequence information. This                
demand arises not only from the exponential growth of the database              
of sequences, but also from the need to understand the structure                
and function of newly discovered gene products known to be involved             
in human disease.                                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS Commercial applications include                
protein structure determination, structure-based drug design and                
protein engineering in the pharmaceutical and biotechnology                     
industries and in basic academic research.                                      
 artificial intelligence; computer program /software; computer simulation; computer system design /evaluation; intermolecular interaction; nuclear magnetic resonance spectroscopy; physical model; protein folding; protein sequence; protein structure EVOLUTIONARY ALGORITHM FOR PROTEIN STRUCTURE PREDICTION","The goal of this project is to develop a computer algorithm that                
accurately predicts the tertiary fold of a small protein given only             
its amino acid sequence and secondary structure. It will use an                 
evolutionary optimization method with proprietary enhancements                  
invented by the PI. The specific aims of Phase I are to: (1)                    
develop a more natural representation of the problem; (2) implement             
an energy function appropriate for this representation; (3) improve             
the optimization algorithm; and (4) combine these into a simulator              
that successfully predicts tertiary from secondary structure. In                
Phase II this simulator will be extended to predict the tertiary                
structures of larger proteins from their sequences and from sparse              
experimental constraints or imperfect secondary structure                       
predictions.  It will be validated using many proteins of different             
sizes and fold classes and developed into software products for the             
pharmaceutical and biotechnology industries. The long-term goal is              
to help satisfy the rapidly growing demand for the accurate                     
prediction of protein structures from sequence information. This                
demand arises not only from the exponential growth of the database              
of sequences, but also from the need to understand the structure                
and function of newly discovered gene products known to be involved             
in human disease.                                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS Commercial applications include                
protein structure determination, structure-based drug design and                
protein engineering in the pharmaceutical and biotechnology                     
industries and in basic academic research.                                      
",2423689,R43GM056593,['R43GM056593'],GM,https://reporter.nih.gov/project-details/2423689,R43,1997,99923,-0.021241869843107043
"We propose an advanced neural network technology for recognizing the            
abnormal brain electrical activity of dementia. Digital EEG                     
(electroencephalographic) studies have successfully differentiated              
Alzheimer's from multi-infarct dementia, using coherence data from a            
convential 19-channel EEG. Recent hardware advances by our company have         
made dense sensor array (64 to 256 channel) EEG inexpensive and convenient      
for routine clinical evaluation. However, effective clinical use of this        
advance in measurement requires a significant innovation in classification      
methodology. Previous classification research with EEG has applied back-        
propagation or self-organizing map networks. These architectures are            
inherently limited in their ability to characterize the dynamic properties      
of multi-channel time-series data, including EEG coherence. In order to         
surmount this limitation, we propose to apply the nested reentrant and          
recurrent Helmholtz machine recently developed in our laboratory. The           
dynamics of this network implement a complex high-dimensional Kalman            
filter, extracting a minimum-description-length (MDL) model of parameters       
defining the time-series data. In this Phase I project, the EEG                 
classification performance of the Helmholtz machine architecture will be        
compared with the performance of a standard back-propagation network. Both      
time series measures and multivariate coherence are used to reduce the          
dimensionality of the dense array EEG prior to classification.                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
As drug and other therapies for dementia provide significant improvements       
in functioning, an inexpensive, repeatable assessment of neurological           
function could provide quantitative data on therapeutic efficacy in each        
patient. A brief memory test, accompanied by a quick, comfortable EEG,          
could become a criterion measure of neurological health to guide the            
effective medical management of each aged person.                               
 arousal; artificial intelligence; brain electrical activity; computer system design /evaluation; dementia; electroencephalography; electrophysiology; human data; neurophysiology; parallel processing REENTRANT NETWORK CLASSIFICATION OF EEG IN DEMENTIA","We propose an advanced neural network technology for recognizing the            
abnormal brain electrical activity of dementia. Digital EEG                     
(electroencephalographic) studies have successfully differentiated              
Alzheimer's from multi-infarct dementia, using coherence data from a            
convential 19-channel EEG. Recent hardware advances by our company have         
made dense sensor array (64 to 256 channel) EEG inexpensive and convenient      
for routine clinical evaluation. However, effective clinical use of this        
advance in measurement requires a significant innovation in classification      
methodology. Previous classification research with EEG has applied back-        
propagation or self-organizing map networks. These architectures are            
inherently limited in their ability to characterize the dynamic properties      
of multi-channel time-series data, including EEG coherence. In order to         
surmount this limitation, we propose to apply the nested reentrant and          
recurrent Helmholtz machine recently developed in our laboratory. The           
dynamics of this network implement a complex high-dimensional Kalman            
filter, extracting a minimum-description-length (MDL) model of parameters       
defining the time-series data. In this Phase I project, the EEG                 
classification performance of the Helmholtz machine architecture will be        
compared with the performance of a standard back-propagation network. Both      
time series measures and multivariate coherence are used to reduce the          
dimensionality of the dense array EEG prior to classification.                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
As drug and other therapies for dementia provide significant improvements       
in functioning, an inexpensive, repeatable assessment of neurological           
function could provide quantitative data on therapeutic efficacy in each        
patient. A brief memory test, accompanied by a quick, comfortable EEG,          
could become a criterion measure of neurological health to guide the            
effective medical management of each aged person.                               
",2422223,R43MH057574,['R43MH057574'],MH,https://reporter.nih.gov/project-details/2422223,R43,1997,91993,-0.06042685736542608
"Here we propose methodological research on multidimsional scaling               
methods in response to research topic 108.c in the program description          
for NIMH. Multidimensional scaling (MDS) is a psychometric method with          
wide application in general behavioral science research. We propose to          
investigate and develop software for a new class of individual                  
differences MDS (INDSCAL) models. In these new models parameters                
associated with individuals are modeled as random effects rather than as        
fixed parameters. For the INDSCAL models, these parameters are the              
subject weights. The resulting random effects MDS model has many                
advantages over its classical counterpart. For example, we are able to          
compute individual estimates of all subject weights even when only one          
dissimilarity is observed on the individual, and we are able to directly        
use our results in inferences about the sampled population of subject           
weights. We propose to study computational algorithms for computing             
estimates in this new class of MDS models, and to develop model building        
and other statistical techniques appropriate for the method. Also               
important during Phase I is demonstrating the advantages of these new           
models by using the prototypical software developed in Phase I to               
analyze real data sets.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
Multidimensional scaling is a psychometric method widely used in market         
research, psychology, the social sciences, genetics, chemometrics, and          
other areas of behavioral and scientific research. The proposed models          
have significant advantages over existing techniques. A well designed           
and comprehensive module for implementing these models will find a ready        
market.                                                                         
 artificial intelligence; behavioral /social science research tag; computer program /software; computer system design /evaluation; mathematical model; model design /development; psychometrics; statistics /biometry SOFTWARE FOR A RANDOM EFFECTS INDSCAL MODEL","Here we propose methodological research on multidimsional scaling               
methods in response to research topic 108.c in the program description          
for NIMH. Multidimensional scaling (MDS) is a psychometric method with          
wide application in general behavioral science research. We propose to          
investigate and develop software for a new class of individual                  
differences MDS (INDSCAL) models. In these new models parameters                
associated with individuals are modeled as random effects rather than as        
fixed parameters. For the INDSCAL models, these parameters are the              
subject weights. The resulting random effects MDS model has many                
advantages over its classical counterpart. For example, we are able to          
compute individual estimates of all subject weights even when only one          
dissimilarity is observed on the individual, and we are able to directly        
use our results in inferences about the sampled population of subject           
weights. We propose to study computational algorithms for computing             
estimates in this new class of MDS models, and to develop model building        
and other statistical techniques appropriate for the method. Also               
important during Phase I is demonstrating the advantages of these new           
models by using the prototypical software developed in Phase I to               
analyze real data sets.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
Multidimensional scaling is a psychometric method widely used in market         
research, psychology, the social sciences, genetics, chemometrics, and          
other areas of behavioral and scientific research. The proposed models          
have significant advantages over existing techniques. A well designed           
and comprehensive module for implementing these models will find a ready        
market.                                                                         
",2421999,R43MH057559,['R43MH057559'],MH,https://reporter.nih.gov/project-details/2421999,R43,1997,93458,-0.0010472595316998407
"DESCRIPTION:  Computing with biochemical reactions is increasingly important    
in studying genomes, assessing toxicity, and developing therapeutics.  There    
are several important information sources, but their data are rudimentary       
and often inaccurate.  Incorporation of biochemical information into            
databases is extremely slow compared to that of sequence and structural         
information, and will lag further as large-scale surveys of gene expression     
and other reactions accelerate over the next few years.  Mechanisms for         
review exist, but are manual, paper-dependent, and can be delayed for a year    
or more.                                                                        
                                                                                
As curators and coordinators of biochemical information sources, the            
applicants share a number of problems in the collection and review of           
information.  Moreover, they are mutually dependent for the means to do so:     
compound information is critical in checking reaction data, reaction            
information is needed to spot errors in compound information, and the           
automatic verification algorithms for either are closely related and need       
both.  The applicants, therefore, propose to build a curatorial exchange for    
the deposit and review of biochemical information by the scientific             
community.  The applicants' goal is to demonstrate a system that will           
encourage the mandating of deposit while ensuring that the information is of    
the highest quality.                                                            
                                                                                
The role of the exchange is to receive deposits, check and classify their       
biochemical information automatically, forward them to panels of human          
reviewers for vetting, and publish the information by release to the            
participating data sources--all over the World-Wide Web. It will track the      
origin and status of deposits and reviews, serve computations for the           
relevant pattern matching and simulation, and maintain an archival copy of      
data.  The databases remain independent, and separately provide additional      
information.  Algorithm development and testing depends on an adequate          
information infrastructure, so the applicants will complete a basic data set    
of compounds and reactions.  They will use this experience to develop a more    
comprehensive domain model that better captures modern biochemistry, and        
implement it for deposit and review.  Since the basic data and algorithms       
will be valuable to the community at large, they plan to serve these to the     
World-Wide Web. the exchange and its underlying data form the infrastructure    
necessary for sustainable, cost-effective development of biochemical            
informatics resources for biomedical research.                                  
 artificial intelligence; biochemistry; chemical information system; chemical reaction; chemical structure; computer program /software; computer system design /evaluation; technology /technique development COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES","DESCRIPTION:  Computing with biochemical reactions is increasingly important    
in studying genomes, assessing toxicity, and developing therapeutics.  There    
are several important information sources, but their data are rudimentary       
and often inaccurate.  Incorporation of biochemical information into            
databases is extremely slow compared to that of sequence and structural         
information, and will lag further as large-scale surveys of gene expression     
and other reactions accelerate over the next few years.  Mechanisms for         
review exist, but are manual, paper-dependent, and can be delayed for a year    
or more.                                                                        
                                                                                
As curators and coordinators of biochemical information sources, the            
applicants share a number of problems in the collection and review of           
information.  Moreover, they are mutually dependent for the means to do so:     
compound information is critical in checking reaction data, reaction            
information is needed to spot errors in compound information, and the           
automatic verification algorithms for either are closely related and need       
both.  The applicants, therefore, propose to build a curatorial exchange for    
the deposit and review of biochemical information by the scientific             
community.  The applicants' goal is to demonstrate a system that will           
encourage the mandating of deposit while ensuring that the information is of    
the highest quality.                                                            
                                                                                
The role of the exchange is to receive deposits, check and classify their       
biochemical information automatically, forward them to panels of human          
reviewers for vetting, and publish the information by release to the            
participating data sources--all over the World-Wide Web. It will track the      
origin and status of deposits and reviews, serve computations for the           
relevant pattern matching and simulation, and maintain an archival copy of      
data.  The databases remain independent, and separately provide additional      
information.  Algorithm development and testing depends on an adequate          
information infrastructure, so the applicants will complete a basic data set    
of compounds and reactions.  They will use this experience to develop a more    
comprehensive domain model that better captures modern biochemistry, and        
implement it for deposit and review.  Since the basic data and algorithms       
will be valuable to the community at large, they plan to serve these to the     
World-Wide Web. the exchange and its underlying data form the infrastructure    
necessary for sustainable, cost-effective development of biochemical            
informatics resources for biomedical research.                                  
",2418025,R01GM056529,['R01GM056529'],GM,https://reporter.nih.gov/project-details/2418025,R01,1997,279257,-0.02626465235189005
"The use of Ventricular Assist Devices (VAD) in therapy for end stage heart      
disease patients is becoming a clinical reality. A new generation of            
compact implantable rotary VAD's are being developed to supplant the            
current generation of bulky pusher-plate VAD's.  Control schemes for the        
rotary pumps must be compatible with physiologic needs.                         
                                                                                
The objective of this proposed program is the development of a control          
scheme that (1) is sensitive to filing pressure thus mimicking the              
Starling response, (2) utilizes pump motor parameters for optimal flow          
control without causing inflow suction, and (3) combine these two control       
schemes as redundant algorithms for rotary blood pump operation in either       
univentricular, biventricular, or total heart operation. The use of the         
pump motor parameters for flow control provides a significant added             
benefit without the need for an additional sensor, providing a                  
complementary physiologic control for the VAD in addition to pressure           
control.                                                                        
                                                                                
The Phase I program will focus on the development of the motor parameter        
control scheme to demonstrate stable operation under various physiologic        
conditions in in vivo studies.                                                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There are potentially 70,000 end stage heart disease patients that die          
annually who can be helped by a compact ventricular assist device. Only         
2300 of these subjects are receiving transplantation due to the shortage        
of donor hearts. An implantable ventricular assist device that is               
physiologically responsive with simple control schemes can not only save        
lives but can provide a decent quality of life to the recipient.                
 artificial intelligence; biomedical equipment development; blood flow measurement; circulatory assist; clinical biomedical equipment; computer system hardware; heart ventricle; implant; swine CONTROL SYSTEM FOR ROTARY BIVENTRICULAR ASSIST IMPLANT","The use of Ventricular Assist Devices (VAD) in therapy for end stage heart      
disease patients is becoming a clinical reality. A new generation of            
compact implantable rotary VAD's are being developed to supplant the            
current generation of bulky pusher-plate VAD's.  Control schemes for the        
rotary pumps must be compatible with physiologic needs.                         
                                                                                
The objective of this proposed program is the development of a control          
scheme that (1) is sensitive to filing pressure thus mimicking the              
Starling response, (2) utilizes pump motor parameters for optimal flow          
control without causing inflow suction, and (3) combine these two control       
schemes as redundant algorithms for rotary blood pump operation in either       
univentricular, biventricular, or total heart operation. The use of the         
pump motor parameters for flow control provides a significant added             
benefit without the need for an additional sensor, providing a                  
complementary physiologic control for the VAD in addition to pressure           
control.                                                                        
                                                                                
The Phase I program will focus on the development of the motor parameter        
control scheme to demonstrate stable operation under various physiologic        
conditions in in vivo studies.                                                  
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There are potentially 70,000 end stage heart disease patients that die          
annually who can be helped by a compact ventricular assist device. Only         
2300 of these subjects are receiving transplantation due to the shortage        
of donor hearts. An implantable ventricular assist device that is               
physiologically responsive with simple control schemes can not only save        
lives but can provide a decent quality of life to the recipient.                
",2421232,R43HL059006,['R43HL059006'],HL,https://reporter.nih.gov/project-details/2421232,R43,1997,92340,-0.04826482890675982
"The transport system of the corneal endothelium maintains the cornea at         
the low level of hydration required for transparency and good vision. If        
this transport system does not function properly then permanent corneal         
edema, loss of transparency, and eventual blindness may occur.                  
                                                                                
Our long term objective is to understand how this transport system              
functions under normal conditions and how it changes in aged, injured, and      
diseased corneas.                                                               
                                                                                
Ion channel proteins in the cell membrane are an essential component of         
this transport system. We will study the dynamics of how these proteins         
function as ""molecular machines"".                                               
                                                                                
The specific aims for this project period are:                                  
                                                                                
1) to perform patch clamp experiments to measure and compare the                
properties of ion channels from freshly excised and cultured corneal            
endothelial cells,                                                              
                                                                                
2) to develop and apply new mathematical methods, including powerful new        
methods of fractals and nonlinear dynamics (chaos) to analyze this data,        
and                                                                             
                                                                                
3) to determine the properties of different types of dynamical models to        
understand the molecular mechanisms responsible for the channel                 
properties.                                                                     
 aging; artificial intelligence; computer data analysis; computer program /software; cornea disorder; corneal endothelium; electrophysiology; eye injury; ion transport; laboratory rabbit; mathematical model; membrane channels; membrane transport proteins; model design /development; molecular biology; molecular dynamics; tissue /cell culture; voltage /patch clamp ION CURRENT ANALYSIS IN THE CORNEA","The transport system of the corneal endothelium maintains the cornea at         
the low level of hydration required for transparency and good vision. If        
this transport system does not function properly then permanent corneal         
edema, loss of transparency, and eventual blindness may occur.                  
                                                                                
Our long term objective is to understand how this transport system              
functions under normal conditions and how it changes in aged, injured, and      
diseased corneas.                                                               
                                                                                
Ion channel proteins in the cell membrane are an essential component of         
this transport system. We will study the dynamics of how these proteins         
function as ""molecular machines"".                                               
                                                                                
The specific aims for this project period are:                                  
                                                                                
1) to perform patch clamp experiments to measure and compare the                
properties of ion channels from freshly excised and cultured corneal            
endothelial cells,                                                              
                                                                                
2) to develop and apply new mathematical methods, including powerful new        
methods of fractals and nonlinear dynamics (chaos) to analyze this data,        
and                                                                             
                                                                                
3) to determine the properties of different types of dynamical models to        
understand the molecular mechanisms responsible for the channel                 
properties.                                                                     
",2444291,R01EY006234,['R01EY006234'],EY,https://reporter.nih.gov/project-details/2444291,R01,1997,138993,-0.0993259542455452
"This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
 artificial intelligence; behavioral /social science research tag; biomechanics; computed axial tomography; computer program /software; computer simulation; electrical measurement; human subject; magnetic resonance imaging; psychoacoustics; respiratory airflow measurement; respiratory imaging /visualization; speech; speech synthesizers; vocal cords; vocalization; voice SIMULATION OF VOICE QUALITIES IN SPEECH","This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
",2443629,R01DC002532,['R01DC002532'],DC,https://reporter.nih.gov/project-details/2443629,R01,1997,220888,-0.09770154842228262
"DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
 adolescence (12-20); aging; audiometry; auditory discrimination; behavioral /social science research tag; binaural hearing; human middle age (35-64); human old age (65+); human subject; longitudinal human study; noise; perception; psychoacoustics; sensorineural hearing loss; space perception; speech; speech recognition; young adult human (21-34) SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING","DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
",2001288,R01AG007998,['R01AG007998'],AG,https://reporter.nih.gov/project-details/2001288,R01,1997,154120,-0.13923806352653845
"DESCRIPTION:  The principal investigator notes that the Aldol addition          
reaction is one of the most important carbon-carbon bond forming                
reactions in  synthetic organic chemistry, that it is widely used in the        
synthesis of drugs  and other biologically active molecules and that the        
reaction is now most  often run in non-polar solvents, such as                  
tetrahydrofuran (THF), frequently  with lithium salts of carbonyl               
compounds (lithium enolates).  He reports that  the aggregation of these        
compounds has been characterized by solution properties, NMR and by X-ray       
crystallography and that such aggregates have  been proposed as the             
reactive species in the Aldol-type additions with other  carbonyl               
derivatives but there have been very few studies on the actual role  of         
such aggregates in reactions.  He goes on to note that by a combination         
of  UV-vis spectroscopic and proton transfer equilibrium studies of some        
lithium  enolates, aggregation constants have been obtained even in             
dilute solution and  that spectra as a function of concentration are            
analyzed by ""Singular Value  Decomposition"" to determine the number of          
different species present and to permit deconvolution to give the               
spectrum of each component.  For this purpose  a glovebox-spectrometer          
apparatus has been developed in which the sample  compartment built into        
the glovebox is connected with a spectrometer with  fiber-optic cables.         
It is noted that the apparatus permits spectroscopy of  solutions               
prepared and studied under the inert atmosphere of the glovebox.  The           
results thus far are said to suggest that the monomeric ion pairs might         
play an important kinetic role in reactions.                                    
                                                                                
It is proposed to extend such studies to additional enolates of interest        
and  to measure the reaction kinetics of their Aldol additions to               
aldehydes and  ketones and of Michael addition reactions with unsaturated       
carbonyl compounds.  It is indicated that the kinetic studies will show         
the state of aggregation of  the enolate reactive species and that              
knowledge of the relative roles of ion  pair monomers and aggregates will       
lead to more complete reaction mechanisms  and to the better                    
understanding required for sophisticated synthesis design.  The principal       
investigator notes that in particular, the roles of solvent  addends,           
such as lithium salts, hexamethylphosphoric triamide, and di- and               
triamines will be studied under carefully controlled conditions to              
determine  the role of coordination of lithium in the stereochemistry of        
the addition  reactions.                                                        
                                                                                
It is also proposed to apply the same techniques of spectroscopic study,        
proton transfer equilibria and reaction kinetics of Aldol and Michael           
addition  reactions to the dilithium salts of carboxylic acids and beta-        
diketones.  It  is noted that these dilithium salts are also being used         
increasingly in  organic synthesis but that the reaction mechanisms are         
virtually unstudied.  It is reported that these salts are also aggregated       
but nothing is known about  the relative reactivities of monomers and           
aggregates.  The proposed studies  are to provide unique information            
about these reactions, which would be  difficult to obtain in any other         
way.  It is suggested that subsequent  extension to other salts of alkali       
and alkaline earth metals, early and late  transition metals and                
lanthanides is also proposed since many of these salts  have found use          
in some stereospecific syntheses.                                               
 aldehydes; biomedical equipment development; chemical addition; chemical kinetics; enol; enolate; hydrogen transport; ketones; lithium; protonation; stereochemistry; ultraviolet spectrometry ION PAIR ALDOL ADDITION REACTIONS","DESCRIPTION:  The principal investigator notes that the Aldol addition          
reaction is one of the most important carbon-carbon bond forming                
reactions in  synthetic organic chemistry, that it is widely used in the        
synthesis of drugs  and other biologically active molecules and that the        
reaction is now most  often run in non-polar solvents, such as                  
tetrahydrofuran (THF), frequently  with lithium salts of carbonyl               
compounds (lithium enolates).  He reports that  the aggregation of these        
compounds has been characterized by solution properties, NMR and by X-ray       
crystallography and that such aggregates have  been proposed as the             
reactive species in the Aldol-type additions with other  carbonyl               
derivatives but there have been very few studies on the actual role  of         
such aggregates in reactions.  He goes on to note that by a combination         
of  UV-vis spectroscopic and proton transfer equilibrium studies of some        
lithium  enolates, aggregation constants have been obtained even in             
dilute solution and  that spectra as a function of concentration are            
analyzed by ""Singular Value  Decomposition"" to determine the number of          
different species present and to permit deconvolution to give the               
spectrum of each component.  For this purpose  a glovebox-spectrometer          
apparatus has been developed in which the sample  compartment built into        
the glovebox is connected with a spectrometer with  fiber-optic cables.         
It is noted that the apparatus permits spectroscopy of  solutions               
prepared and studied under the inert atmosphere of the glovebox.  The           
results thus far are said to suggest that the monomeric ion pairs might         
play an important kinetic role in reactions.                                    
                                                                                
It is proposed to extend such studies to additional enolates of interest        
and  to measure the reaction kinetics of their Aldol additions to               
aldehydes and  ketones and of Michael addition reactions with unsaturated       
carbonyl compounds.  It is indicated that the kinetic studies will show         
the state of aggregation of  the enolate reactive species and that              
knowledge of the relative roles of ion  pair monomers and aggregates will       
lead to more complete reaction mechanisms  and to the better                    
understanding required for sophisticated synthesis design.  The principal       
investigator notes that in particular, the roles of solvent  addends,           
such as lithium salts, hexamethylphosphoric triamide, and di- and               
triamines will be studied under carefully controlled conditions to              
determine  the role of coordination of lithium in the stereochemistry of        
the addition  reactions.                                                        
                                                                                
It is also proposed to apply the same techniques of spectroscopic study,        
proton transfer equilibria and reaction kinetics of Aldol and Michael           
addition  reactions to the dilithium salts of carboxylic acids and beta-        
diketones.  It  is noted that these dilithium salts are also being used         
increasingly in  organic synthesis but that the reaction mechanisms are         
virtually unstudied.  It is reported that these salts are also aggregated       
but nothing is known about  the relative reactivities of monomers and           
aggregates.  The proposed studies  are to provide unique information            
about these reactions, which would be  difficult to obtain in any other         
way.  It is suggested that subsequent  extension to other salts of alkali       
and alkaline earth metals, early and late  transition metals and                
lanthanides is also proposed since many of these salts  have found use          
in some stereospecific syntheses.                                               
",2021901,R01GM030369,['R01GM030369'],GM,https://reporter.nih.gov/project-details/2021901,R01,1997,157897,-0.0081058861807331
"The long-term goal of this project is the development of an adaptable           
digital-signal-processing-based alerting system for person with hearing         
disabilities that will be used to make the owner aware of important sounds      
(e.g.. doorbell, telephone, smoke detector, infant crying) in his/her hone,     
office car classroom, or other environment. The product must be capable of      
detecting these sounds in ""acoustically hostile"" environments such as when      
a television is operating.  The system will be flexible and portable and        
will be trained by the owner to recognize an array of sounds of the owner's     
choosing.  The critical technological issues to be explored are the             
development of a robust signal detection system based on artificial neural      
network leaning algorithms to ""teach"" the system new sounds  (and to            
extinguish alerts to inappropriate ones), and, ultimately, the design and       
manufacture of application specific integrated circuitry to implement the       
algorithms.                                                                     
PROPOSED COMMERCIAL APPLICATIONS:                                               
This portable alerting system with the capability to ""learn"" sounds and         
adapt to various environments will not only improve the lives of deaf and       
hearing impaired people in the home environment,but will also enhance           
employment opportunities for and improve access to educational                  
opportunities for people with hearing disabilities.  These features have        
distinct  commercial advantages over all currently available and will have      
broad  appeal among the 28 million deaf and profoundly hearing disabled         
individuals U.S. alone.                                                         
 bioengineering /biomedical engineering; biomedical equipment development; computational neuroscience; computer assisted patient care; deaf aid; self care; sensory signal detection ADAPTIVE ALERTING SYSTEM FOR DEAF PERSONS","The long-term goal of this project is the development of an adaptable           
digital-signal-processing-based alerting system for person with hearing         
disabilities that will be used to make the owner aware of important sounds      
(e.g.. doorbell, telephone, smoke detector, infant crying) in his/her hone,     
office car classroom, or other environment. The product must be capable of      
detecting these sounds in ""acoustically hostile"" environments such as when      
a television is operating.  The system will be flexible and portable and        
will be trained by the owner to recognize an array of sounds of the owner's     
choosing.  The critical technological issues to be explored are the             
development of a robust signal detection system based on artificial neural      
network leaning algorithms to ""teach"" the system new sounds  (and to            
extinguish alerts to inappropriate ones), and, ultimately, the design and       
manufacture of application specific integrated circuitry to implement the       
algorithms.                                                                     
PROPOSED COMMERCIAL APPLICATIONS:                                               
This portable alerting system with the capability to ""learn"" sounds and         
adapt to various environments will not only improve the lives of deaf and       
hearing impaired people in the home environment,but will also enhance           
employment opportunities for and improve access to educational                  
opportunities for people with hearing disabilities.  These features have        
distinct  commercial advantages over all currently available and will have      
broad  appeal among the 28 million deaf and profoundly hearing disabled         
individuals U.S. alone.                                                         
",2014634,R43DC002911,['R43DC002911'],DC,https://reporter.nih.gov/project-details/2014634,R43,1997,80323,-0.03095857018070271
"The PI is a Neurophysiologist and Pediatric Neurosurgeon who will be the        
Associate Director of Center V (Behavioral and Neurobiology Research), with     
special responsibility for Neurobiology programs, at the Children""s             
Research Institute.  This RCA will permit the PI to devote nearly full time     
to research.                                                                    
                                                                                
Short term goals are to test the hypothesis that neuronal ensembles have        
nonlinear deterministic properties.  If so, they will 1) have activity that     
can be characterized and controlled through unstable periodic orbits, 2)        
hen noise driven will exhibit stochastic resonance, and 3) because of           
coupling will exhibit generalized (nonlinear) synchrony and emergence.          
Long-term goals are to achieve a better understanding of neuronal network       
and brain behavior, and to develop novel methods of treating dynamical          
diseases.                                                                       
                                                                                
The research project will involve theoretical work on the detection of          
unstable orbits in In Vitro brain slices and human epileptic foci.  Such        
orbit information forms a novel method of characterizing the deterministic      
properties of complex systems despite nonstationarity, and can be used to       
control those systems.  Nonlinear systems also optimize their response to       
weak signals in the presence of noise - stochastic resonance.  We will          
define the statistical mechanics of stochastic resonance through                
simultaneous measurements of single neuron and neuronal ensemble activity.      
Since neuronal ensembles may demonstrate nonlinear generalized synchrony,       
we will quantify spatio-temporal generalized synchrony through dual             
simultaneous single cell recordings as a function of separation in a            
neuronal network.  Both unstable orbit detection and generalized synchrony      
will be used to define the emergence of nonlinear behaviors in neuronal         
ensembles.                                                                      
                                                                                
The results of this research will fundamentally alter the way that neuronal     
dynamics can be characterized and controlled, will provide a means to deal      
with neuronal nonstationarity, will further explain the role of noise in        
the nervous system, and may provide a novel approach for the control of         
pathological neuronal ensembles in dynamical diseases such as epilepsy,         
spasticity, and tremor.                                                         
 artificial intelligence; brain electrical activity; computational neuroscience; electric field; electroencephalography; extracellular; hippocampus; human data; intracellular; laboratory rat; microelectrodes; neural information processing; neuroanatomy; neurons; neurosurgery; noise; statistics /biometry NONLINEAR DYNAMICS OF NEURONAL ENSEMBLES","The PI is a Neurophysiologist and Pediatric Neurosurgeon who will be the        
Associate Director of Center V (Behavioral and Neurobiology Research), with     
special responsibility for Neurobiology programs, at the Children""s             
Research Institute.  This RCA will permit the PI to devote nearly full time     
to research.                                                                    
                                                                                
Short term goals are to test the hypothesis that neuronal ensembles have        
nonlinear deterministic properties.  If so, they will 1) have activity that     
can be characterized and controlled through unstable periodic orbits, 2)        
hen noise driven will exhibit stochastic resonance, and 3) because of           
coupling will exhibit generalized (nonlinear) synchrony and emergence.          
Long-term goals are to achieve a better understanding of neuronal network       
and brain behavior, and to develop novel methods of treating dynamical          
diseases.                                                                       
                                                                                
The research project will involve theoretical work on the detection of          
unstable orbits in In Vitro brain slices and human epileptic foci.  Such        
orbit information forms a novel method of characterizing the deterministic      
properties of complex systems despite nonstationarity, and can be used to       
control those systems.  Nonlinear systems also optimize their response to       
weak signals in the presence of noise - stochastic resonance.  We will          
define the statistical mechanics of stochastic resonance through                
simultaneous measurements of single neuron and neuronal ensemble activity.      
Since neuronal ensembles may demonstrate nonlinear generalized synchrony,       
we will quantify spatio-temporal generalized synchrony through dual             
simultaneous single cell recordings as a function of separation in a            
neuronal network.  Both unstable orbit detection and generalized synchrony      
will be used to define the emergence of nonlinear behaviors in neuronal         
ensembles.                                                                      
                                                                                
The results of this research will fundamentally alter the way that neuronal     
dynamics can be characterized and controlled, will provide a means to deal      
with neuronal nonstationarity, will further explain the role of noise in        
the nervous system, and may provide a novel approach for the control of         
pathological neuronal ensembles in dynamical diseases such as epilepsy,         
spasticity, and tremor.                                                         
",2373024,K02MH001493,['K02MH001493'],MH,https://reporter.nih.gov/project-details/2373024,K02,1997,124041,-0.029855055518916304
"The diagnosis of hematologic malignancies requires many qualitative             
ancillary tests to substantiate the impressions made from hematoxylin and       
eosin stained slides.  We propose to use artificial intelligence                
techniques to expedite and integrate evaluation of information received         
form histologic morphology, special chemical and enzymatic stains,              
immunoperoxidase stains, flow cytometry, and molecular diagnostic studies.      
The goals for this fellowship include:  1)  design and implement a              
database of the important clinical laboratory variables relevant to the         
diagnosis of hematologic malignancies, 2) develop an expert system based        
on knowledge-based rules derived from the medical literature and from           
internal clinical material, and 3) validate the system with retrospective       
and prospective clinical data.  To accomplish these goals and to build          
this expert system, techniques from two major disciplines in computer           
science will be applied.  First, the relational model of database               
management will be used to store and retrieve clinical data.  Second,           
artificial intelligence techniques such as predicate calculus, case-based       
reasoning, neural networks, and fuzzy logic will be used to develop this        
expert system.                                                                  
 artificial intelligence; blood disorder diagnosis; computer assisted diagnosis; computer assisted medical decision making; computer data analysis; data management; diagnosis design /evaluation; human data; information systems; leukemia; lymphoma; mathematical model; neoplasm /cancer diagnosis EXPERT SYSTEM IN THE DIAGNOSIS OF LYMPHOMA AND LEUKEMIA","The diagnosis of hematologic malignancies requires many qualitative             
ancillary tests to substantiate the impressions made from hematoxylin and       
eosin stained slides.  We propose to use artificial intelligence                
techniques to expedite and integrate evaluation of information received         
form histologic morphology, special chemical and enzymatic stains,              
immunoperoxidase stains, flow cytometry, and molecular diagnostic studies.      
The goals for this fellowship include:  1)  design and implement a              
database of the important clinical laboratory variables relevant to the         
diagnosis of hematologic malignancies, 2) develop an expert system based        
on knowledge-based rules derived from the medical literature and from           
internal clinical material, and 3) validate the system with retrospective       
and prospective clinical data.  To accomplish these goals and to build          
this expert system, techniques from two major disciplines in computer           
science will be applied.  First, the relational model of database               
management will be used to store and retrieve clinical data.  Second,           
artificial intelligence techniques such as predicate calculus, case-based       
reasoning, neural networks, and fuzzy logic will be used to develop this        
expert system.                                                                  
",2555455,F38LM000061,['F38LM000061'],LM,https://reporter.nih.gov/project-details/2555455,F38,1997,52500,-0.01537060394281743
"This acquisition will provide technical assistance to continue the              
development and enhancement of GRATEFUL MED as well as provide technical        
software development support for the analysis, design, implementation,          
integration, documentation, and maintenance of related system components.       
These include: computer aided learning, intermachine implementation,            
artificial intelligence, new telecommunication interfaces, e.g., FTS            
2000, Graphical User Interfaces, network integration and                        
interoperability, and document delivery systems.                                
  GRATEFUL MED@ SOFTWARE DEVELOPMENT & MANAGEMENT SUPPORT","This acquisition will provide technical assistance to continue the              
development and enhancement of GRATEFUL MED as well as provide technical        
software development support for the analysis, design, implementation,          
integration, documentation, and maintenance of related system components.       
These include: computer aided learning, intermachine implementation,            
artificial intelligence, new telecommunication interfaces, e.g., FTS            
2000, Graphical User Interfaces, network integration and                        
interoperability, and document delivery systems.                                
",2652451,01LM063511,['N01LM063511'],LM,https://reporter.nih.gov/project-details/2652451,N01,1997,45352,-0.1162082197423741
 artificial intelligence; cell cycle; computer data analysis; computer program /software; computer system design /evaluation; flow cytometry; immunofluorescence technique; transcription factor INTEGRATED MULTIPARAMETER PROTEIN CONTENT,,2651267,R43CA078087,['R43CA078087'],CA,https://reporter.nih.gov/project-details/2651267,R43,1997,87502,-0.03452754059889469
"It has been long recognized by clinicians that porcelain restorations can       
cause a significant amount of wear and abrasion of the opposing natural         
tooth structure. The literature suggests that the leucite component of          
dental porcelain may be an important factor in the abrasiveness of dental       
porcelain.                                                                      
                                                                                
The long term goal of this work is to define the compositional and              
microstructural parameters which are critical in the formulation of dental      
ceramics that have abrasive wear properties compatible with natural teeth.      
The objectives of this research proposal are:                                   
                                                                                
1) to develop a test methodology for specifically measuring wear rates of       
natural tooth enamel when exposed to dental porcelain. and                      
2) To evaluate the effect of changes in corn position and microstructure of     
feldspathic dental porcelain (specifically, the volume fraction of leucite)     
on the abrasive wear of the dental porcelain against natural tooth enamel.      
                                                                                
The MLS servohydaulic artificial mouth is to be used to simulate the            
chewing action of tooth enamel against five experimental porcelain              
compositions, and three commercially available feldspathic porcelain            
products. Volume fractions of leucite in the experimental compositions will     
be controlled at 0, 10. 20, 30, and 40%. Samples of porcelain resembling an     
upper central incisor will be abraded against natural lower incisors. The       
tooth enamel and porcelain samples will be measured for volume loss due to      
abrasive wear at intervals corresponding to six months, one year, two years     
and three years of natural chewing activity. The results of the volume loss     
measurements for each porcelain/ tooth enamel combination will be compared      
statistically to determine if the wear of enamel is dependent on the volume     
fraction of leucite in the porcelain. If the central hypothesis is true         
(i.e., if the wear rate of the enamel is a function of the volume fraction      
of leucite in feldspathic porcelain), this new information will be useful       
in the design of dental porcelain compositions which have wear                  
characteristics more compatible with the opposing tooth structure.              
 artificial intelligence; biomaterial evaluation; biomechanics; chemical structure; computer graphics /printing; computer simulation; dental material wear; mastication; oral facial restoration material; porcelain; tensile strength; thermostability; tooth; tooth enamel; tooth surface WEAR OF ENAMEL BY DENTAL PORCELAIN--EFFECT OF LEUCITE","It has been long recognized by clinicians that porcelain restorations can       
cause a significant amount of wear and abrasion of the opposing natural         
tooth structure. The literature suggests that the leucite component of          
dental porcelain may be an important factor in the abrasiveness of dental       
porcelain.                                                                      
                                                                                
The long term goal of this work is to define the compositional and              
microstructural parameters which are critical in the formulation of dental      
ceramics that have abrasive wear properties compatible with natural teeth.      
The objectives of this research proposal are:                                   
                                                                                
1) to develop a test methodology for specifically measuring wear rates of       
natural tooth enamel when exposed to dental porcelain. and                      
2) To evaluate the effect of changes in corn position and microstructure of     
feldspathic dental porcelain (specifically, the volume fraction of leucite)     
on the abrasive wear of the dental porcelain against natural tooth enamel.      
                                                                                
The MLS servohydaulic artificial mouth is to be used to simulate the            
chewing action of tooth enamel against five experimental porcelain              
compositions, and three commercially available feldspathic porcelain            
products. Volume fractions of leucite in the experimental compositions will     
be controlled at 0, 10. 20, 30, and 40%. Samples of porcelain resembling an     
upper central incisor will be abraded against natural lower incisors. The       
tooth enamel and porcelain samples will be measured for volume loss due to      
abrasive wear at intervals corresponding to six months, one year, two years     
and three years of natural chewing activity. The results of the volume loss     
measurements for each porcelain/ tooth enamel combination will be compared      
statistically to determine if the wear of enamel is dependent on the volume     
fraction of leucite in the porcelain. If the central hypothesis is true         
(i.e., if the wear rate of the enamel is a function of the volume fraction      
of leucite in feldspathic porcelain), this new information will be useful       
in the design of dental porcelain compositions which have wear                  
characteristics more compatible with the opposing tooth structure.              
",2391216,R03DE010929,['R03DE010929'],DE,https://reporter.nih.gov/project-details/2391216,R03,1997,38613,-0.09275723400563242
"Post-traumatic stress disorder (PTSD) is one of the most disabling              
psychopathological conditions affecting the veteran population.                 
Approximately 15.2% of the men and 8.5% of the women stationed in               
Vietnam were found to be suffering from PTSD 15 or more years after             
their service. In the Atlanta metropolitan area, some 9000 Vietnam              
veterans suffer from complete or partial PTSD. The psychological,               
social, occupational and economic consequences of the disorder for              
patients and their families are devastating. No therapeutic approach            
has proven to be consistently effective in the management of combat-            
related PTSD. The present proposal intends to exploit the potential             
therapeutic effectiveness of recent advances in computer and display            
technology referred to as Virtual Reality. Virtual reality exposure             
(VRE) takes place in an immersive, computer-driven environment.                 
Patients would be exposed to virtual Huey helicopters flying them over          
the jungles of Vietnam.  They will be encouraged to relive their                
traumatic memories, much as in standard exposure therapy, but immersed          
in Vietnam stimuli. Ultimate control is possible in the virtual                 
environment, changing levels of intensity of exposure instantly. The            
proposed project aims to develop virtual reality exposure therapy for           
Vietnam veterans with PTSD, revise and perfect the treatment,                   
construct a treatment manual, and gather preliminary evidence of its            
efficacy in a small group design. A series of five case studies will            
be run to develop and revise the treatment. Following this, Vietnam             
veterans (n=40) with current DSM-IV PTSD diagnoses will be randomly             
assigned to VRE or a wait-list control. Treatment will be delivered             
in nine 60-minute individual sessions conducted over 5 weeks.                   
Assessments will be conducted at pre-treatment, post-treatment and              
follow-ups of 6 and 12 months post-treatment. Assessments will be               
conducted by an independent assessor who will be kept blind to the              
treatment condition. Objective clinician-rated and self report                  
measures of PTSD will be incorporated.                                          
 adult human (21+); artificial intelligence; behavioral /social science research tag; clinical research; computer simulation; computer system design /evaluation; desensitization psychotherapy; human subject; human therapy evaluation; interview; longitudinal human study; memory; posttraumatic stress disorder; psychological shock; veterans; war /peace VIRTUAL REALITY EXPOSURE THERAPY FOR VETERANS WITH PTSD","Post-traumatic stress disorder (PTSD) is one of the most disabling              
psychopathological conditions affecting the veteran population.                 
Approximately 15.2% of the men and 8.5% of the women stationed in               
Vietnam were found to be suffering from PTSD 15 or more years after             
their service. In the Atlanta metropolitan area, some 9000 Vietnam              
veterans suffer from complete or partial PTSD. The psychological,               
social, occupational and economic consequences of the disorder for              
patients and their families are devastating. No therapeutic approach            
has proven to be consistently effective in the management of combat-            
related PTSD. The present proposal intends to exploit the potential             
therapeutic effectiveness of recent advances in computer and display            
technology referred to as Virtual Reality. Virtual reality exposure             
(VRE) takes place in an immersive, computer-driven environment.                 
Patients would be exposed to virtual Huey helicopters flying them over          
the jungles of Vietnam.  They will be encouraged to relive their                
traumatic memories, much as in standard exposure therapy, but immersed          
in Vietnam stimuli. Ultimate control is possible in the virtual                 
environment, changing levels of intensity of exposure instantly. The            
proposed project aims to develop virtual reality exposure therapy for           
Vietnam veterans with PTSD, revise and perfect the treatment,                   
construct a treatment manual, and gather preliminary evidence of its            
efficacy in a small group design. A series of five case studies will            
be run to develop and revise the treatment. Following this, Vietnam             
veterans (n=40) with current DSM-IV PTSD diagnoses will be randomly             
assigned to VRE or a wait-list control. Treatment will be delivered             
in nine 60-minute individual sessions conducted over 5 weeks.                   
Assessments will be conducted at pre-treatment, post-treatment and              
follow-ups of 6 and 12 months post-treatment. Assessments will be               
conducted by an independent assessor who will be kept blind to the              
treatment condition. Objective clinician-rated and self report                  
measures of PTSD will be incorporated.                                          
",2034652,R21MH055555,['R21MH055555'],MH,https://reporter.nih.gov/project-details/2034652,R21,1997,142410,-0.03913193548957673
"DESCRIPTION (adapted from applicant's abstract):  Behavior patterns             
(work habits, coping strategies and physiological responses to stress)          
have been identified as major factors in premature morbidity and                
mortality.  Of particular interest here is the role of ""stress"" or              
exposure to ""stressors"" recognized as playing a key role in health              
outcomes.  But the harmful effects of these dysfunctional behavior              
patterns can be mitigated or even reversed if personal change is                
instituted.  Behavior change, however, is notoriously difficult to              
accomplish.  Even when people come to the point of deciding to change           
behavior, their good resolutions are often short-lived.  The applicant          
plans to develop a software program to aid in the assessment and change         
of behavior patterns which can prove harmful to the individual in               
either the short or long term.  The software package will be developed          
based on currently accepted theories of the causes of illness as well           
as empirically-grounded theoretical approaches to behavior change.              
                                                                                
The specific aims of Phase I will be: 1.  Develop a prototype software          
package which assists the user in making change.  The Phase I prototype         
will include: a.  an assessment module with a computerized version of           
the Essi Systems' StressMap; b.  an expert system that provides problem         
identification and targets behavior change; c.  a behavior change guide         
built around the Essi Systems StressMap 21 Day Rule Action Planning             
workbook for guiding change around on discrete behavior at a time; d.           
exercises that provide intermittent reinforcement for behavior change.          
2.  Test the useability and the efficacy of the software package to act         
as a self-managed assessment and behavior change tool through an                
experimental design which compares those who did and did not use the            
Essi Systems Inc.  software.                                                    
 behavior modification; case history; clinical research; computer assisted diagnosis; computer assisted instruction; computer human interaction; computer system design /evaluation; coping; disease /disorder proneness /risk; human subject; relaxation; stress; stress management INTERACTIVE STRESS ASSESSMENT & BEHAVIOR CHANGE SOFTWARE","DESCRIPTION (adapted from applicant's abstract):  Behavior patterns             
(work habits, coping strategies and physiological responses to stress)          
have been identified as major factors in premature morbidity and                
mortality.  Of particular interest here is the role of ""stress"" or              
exposure to ""stressors"" recognized as playing a key role in health              
outcomes.  But the harmful effects of these dysfunctional behavior              
patterns can be mitigated or even reversed if personal change is                
instituted.  Behavior change, however, is notoriously difficult to              
accomplish.  Even when people come to the point of deciding to change           
behavior, their good resolutions are often short-lived.  The applicant          
plans to develop a software program to aid in the assessment and change         
of behavior patterns which can prove harmful to the individual in               
either the short or long term.  The software package will be developed          
based on currently accepted theories of the causes of illness as well           
as empirically-grounded theoretical approaches to behavior change.              
                                                                                
The specific aims of Phase I will be: 1.  Develop a prototype software          
package which assists the user in making change.  The Phase I prototype         
will include: a.  an assessment module with a computerized version of           
the Essi Systems' StressMap; b.  an expert system that provides problem         
identification and targets behavior change; c.  a behavior change guide         
built around the Essi Systems StressMap 21 Day Rule Action Planning             
workbook for guiding change around on discrete behavior at a time; d.           
exercises that provide intermittent reinforcement for behavior change.          
2.  Test the useability and the efficacy of the software package to act         
as a self-managed assessment and behavior change tool through an                
experimental design which compares those who did and did not use the            
Essi Systems Inc.  software.                                                    
",2034357,R43MH054421,['R43MH054421'],MH,https://reporter.nih.gov/project-details/2034357,R43,1997,98680,-0.25258808710060465
"DESCRIPTION (Adapted from applicant's abstract):  The proposed EEG              
MagicMarker software offers a new methodology for the display and               
analysis of digitized EEG records.  Segments of similar EEG                     
activity are clustered together, clearly differentiating                        
background, paroxysmal activity, and patient state transitions,                 
e.g., sleep stages.  The analysis only considers the content of                 
the current EEG and requires no thresholds or classification                    
functions derived from a training set.                                          
                                                                                
A novel user interface allows interactive partitioning of the                   
hierarchical cluster dendrogram in a method already familiar to                 
many users.  Each node can be expanded or collapsed revealing more              
or less detail by clicking on the plus or minus sign to the left                
of the node.  The user manipulates the tree to display the                      
appropriate partition and prints the report for a summary of their              
findings.  Each node contains a complete visual summary of the                  
segments in that power spectrum or by contour plots of delta,                   
theta, alpha, and beta activities.  Hyper-links from the cluster                
nodes to the applicant's Insight EEG review software offers                     
immediate access to pages of interest.                                          
 artificial intelligence; brain disorder diagnosis; computer assisted diagnosis; computer human interaction; computer program /software; computer system design /evaluation; diagnosis design /evaluation; electroencephalography; epilepsy; gene complementation; human data; medical records; method development; sleep disorders CLUSTERING AND HYPER LINKING OF LONG TERM EEGS","DESCRIPTION (Adapted from applicant's abstract):  The proposed EEG              
MagicMarker software offers a new methodology for the display and               
analysis of digitized EEG records.  Segments of similar EEG                     
activity are clustered together, clearly differentiating                        
background, paroxysmal activity, and patient state transitions,                 
e.g., sleep stages.  The analysis only considers the content of                 
the current EEG and requires no thresholds or classification                    
functions derived from a training set.                                          
                                                                                
A novel user interface allows interactive partitioning of the                   
hierarchical cluster dendrogram in a method already familiar to                 
many users.  Each node can be expanded or collapsed revealing more              
or less detail by clicking on the plus or minus sign to the left                
of the node.  The user manipulates the tree to display the                      
appropriate partition and prints the report for a summary of their              
findings.  Each node contains a complete visual summary of the                  
segments in that power spectrum or by contour plots of delta,                   
theta, alpha, and beta activities.  Hyper-links from the cluster                
nodes to the applicant's Insight EEG review software offers                     
immediate access to pages of interest.                                          
",2034824,R43MH055895,['R43MH055895'],MH,https://reporter.nih.gov/project-details/2034824,R43,1997,75076,-0.042278395236715165
"DESCRIPTION (Taken from application abstract):  The long-term aim of this       
project is to use natural language methods in order to enhance the              
functionality of the electronic medical record, which is a source of            
abundant clinical data.  However, the data is mostly in textual form and        
therefore unusable for automated clinical applications, such as decision        
support, research, quality assurance, and outcomes assessment.  By using a      
natural language processor to map the clinical information in the reports       
into structured codified clinical data, the data will be made readily           
accessible so that it could be utilized by subsequent automated clinical        
applications.  We have already shown that it is possible to build an            
effective text processor that accurately codifies textual reports within the    
specialized domain of radiology.  In this project we intend to build upon       
our successful experience and will extend the processor to another limited      
domain that is different from radiology and to a broad domain in order to       
study the feasibility of transferring the processor to all of medicine.         
                                                                                
More specifically, we will broaden the processor so that it codifies            
clinical information in the physical examination section of the discharge       
summary and then to all of the discharge summary, where we will focus on        
coding diagnoses.  The emphasis of our work will not only be concerned with     
extending the language processor but will also focus on scalability,            
evaluation of the performance, the effort, and the portability aspects.  In     
addition, because discharge summaries are so complex and comprehensive, we      
will have to extend the formal representational model of the clinical           
information and also develop new natural language processing techniques and     
new vocabulary development tools.  This work will continue to be performed      
within an operational clinical setting.                                         
 abstracting; automated medical record system; computer system design /evaluation; human data; information retrieval; method development; vocabulary development for information system UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING","DESCRIPTION (Taken from application abstract):  The long-term aim of this       
project is to use natural language methods in order to enhance the              
functionality of the electronic medical record, which is a source of            
abundant clinical data.  However, the data is mostly in textual form and        
therefore unusable for automated clinical applications, such as decision        
support, research, quality assurance, and outcomes assessment.  By using a      
natural language processor to map the clinical information in the reports       
into structured codified clinical data, the data will be made readily           
accessible so that it could be utilized by subsequent automated clinical        
applications.  We have already shown that it is possible to build an            
effective text processor that accurately codifies textual reports within the    
specialized domain of radiology.  In this project we intend to build upon       
our successful experience and will extend the processor to another limited      
domain that is different from radiology and to a broad domain in order to       
study the feasibility of transferring the processor to all of medicine.         
                                                                                
More specifically, we will broaden the processor so that it codifies            
clinical information in the physical examination section of the discharge       
summary and then to all of the discharge summary, where we will focus on        
coding diagnoses.  The emphasis of our work will not only be concerned with     
extending the language processor but will also focus on scalability,            
evaluation of the performance, the effort, and the portability aspects.  In     
addition, because discharge summaries are so complex and comprehensive, we      
will have to extend the formal representational model of the clinical           
information and also develop new natural language processing techniques and     
new vocabulary development tools.  This work will continue to be performed      
within an operational clinical setting.                                         
",2032409,R01LM006274,['R01LM006274'],LM,https://reporter.nih.gov/project-details/2032409,R01,1997,218581,-0.12297610233900969
"We propose to design efficient computer algorithms providing novel and/or       
improved methods and software for a number of computational problems in         
molecular biology.  The proposal and the principal investigator's current       
research efforts are divided into three projects, as follows.                   
                                                                                
The first project centers on computational problems in the sequencing of        
DNA and physical mapping of genomes.  We propose to continue refining our       
algorithms and software library for the fragment assembly problem, i.e.,        
determining the most likely complete DNA sequence consistent with               
electrophoresis data gathered from cloned fragments.  The refinements           
consist of improved algorithms for all phases of the computation: ultra-        
rapid overlap detection, assembly in the presence of constraints modeling       
additional experimental information, a formulation of the problem that          
correctly handles repetitive sequence, and a multi-alignment component          
that accommodates base-calling quality figures.  We also propose work on        
ordered shotgun sequencing (OSS) and a cDNA database for the data being         
generated at Washington University under contract to Merck.  Lastly, there      
is much similarity between fragment assembly and physical mapping, save         
that the relative level of experimental error is higher.  We propose an         
algorithm for STS content mapping based on rules-of-inference that are          
true with very high probability.                                                
                                                                                
The second project is to design better algorithms for a number of               
computational problems arising in molecular biology.  Progress in this          
arena tends to be inspired rather than calculated.  We demonstrate our          
track record of producing interesting results and then describe the             
following problems for which we have a number of ideas and preliminary          
results: sublinear Smith-Waterman database searches, determining                
restriction maps from digest data, grammar-based pattern matching,              
selecting PCR primers, predicting RNA secondary structure, and docking          
rigid molecules (3D matching).                                                  
                                                                                
The final project involves the introduction into our funded research            
activities of a new area, molecular graphics.  Earlier, we developed            
MacMolecule 1.7 which rendered space-filling, ball-and-stock, and wire-         
frame views of molecules.  We estimate 10,000 copies are currently in use       
around the world.  Our aim is high-speed, high-quality graphics, on low-        
end machines achieved by virtue of very efficient rendering algorithms.         
We have begun to develop a new version of MacMolecule, called Linus 2.0,        
which will deliver superior performance and visualizations along with           
greater capabilities, including ribbon renderings of protein secondary          
structure, zoom, hi-lighting, picking, and additional visualization modes.      
We further propose to build a helper application supporting ""Linus""             
content files so that Linus may be used with the World-Wide Web.  In            
further years, we will support molecular animation, construction, and           
possibly dynamics.                                                              
 Internet; artificial intelligence; computer assisted sequence analysis; computer graphics /printing; computer program /software; computer system design /evaluation; nucleic acid sequence; protein sequence EFFICIENT SOFTWARE FOR THE ANALYSIS OF BIOSEQUENCES","We propose to design efficient computer algorithms providing novel and/or       
improved methods and software for a number of computational problems in         
molecular biology.  The proposal and the principal investigator's current       
research efforts are divided into three projects, as follows.                   
                                                                                
The first project centers on computational problems in the sequencing of        
DNA and physical mapping of genomes.  We propose to continue refining our       
algorithms and software library for the fragment assembly problem, i.e.,        
determining the most likely complete DNA sequence consistent with               
electrophoresis data gathered from cloned fragments.  The refinements           
consist of improved algorithms for all phases of the computation: ultra-        
rapid overlap detection, assembly in the presence of constraints modeling       
additional experimental information, a formulation of the problem that          
correctly handles repetitive sequence, and a multi-alignment component          
that accommodates base-calling quality figures.  We also propose work on        
ordered shotgun sequencing (OSS) and a cDNA database for the data being         
generated at Washington University under contract to Merck.  Lastly, there      
is much similarity between fragment assembly and physical mapping, save         
that the relative level of experimental error is higher.  We propose an         
algorithm for STS content mapping based on rules-of-inference that are          
true with very high probability.                                                
                                                                                
The second project is to design better algorithms for a number of               
computational problems arising in molecular biology.  Progress in this          
arena tends to be inspired rather than calculated.  We demonstrate our          
track record of producing interesting results and then describe the             
following problems for which we have a number of ideas and preliminary          
results: sublinear Smith-Waterman database searches, determining                
restriction maps from digest data, grammar-based pattern matching,              
selecting PCR primers, predicting RNA secondary structure, and docking          
rigid molecules (3D matching).                                                  
                                                                                
The final project involves the introduction into our funded research            
activities of a new area, molecular graphics.  Earlier, we developed            
MacMolecule 1.7 which rendered space-filling, ball-and-stock, and wire-         
frame views of molecules.  We estimate 10,000 copies are currently in use       
around the world.  Our aim is high-speed, high-quality graphics, on low-        
end machines achieved by virtue of very efficient rendering algorithms.         
We have begun to develop a new version of MacMolecule, called Linus 2.0,        
which will deliver superior performance and visualizations along with           
greater capabilities, including ribbon renderings of protein secondary          
structure, zoom, hi-lighting, picking, and additional visualization modes.      
We further propose to build a helper application supporting ""Linus""             
content files so that Linus may be used with the World-Wide Web.  In            
further years, we will support molecular animation, construction, and           
possibly dynamics.                                                              
",2032335,R01LM004960,['R01LM004960'],LM,https://reporter.nih.gov/project-details/2032335,R01,1997,157056,-0.07187245250318569
"Determination of homologies/sequence similarities between proteins              
provides insight into structure-function relationships and evolutionary         
processes.  Current computer methods employ an initial alignment of             
protein amino acid sequences in order to recognize consensus regions and        
quantify similarities.  Neural network methodologies have been used to          
study DNA and protein sequences to predict domains serving putative             
functions.  We propose to improve on the self-organizing neural network         
(after Kohonen) to demonstrate structure-function relationships in              
proteins.  Our method employs a novel means of identifying homologous           
relationships.  For input the method uses 400 component vectors, the            
elements of which are normalized bipeptide nearest neighbor statistics.         
Initial evaluation of the method will focus on the G-protein coupling           
receptor super-family.  These proteins exhibit hydropathy plots                 
predicting seven transmembrane spanning regions and many diverse                
biochemical functions are transduced by its members.  Our data set for          
this family presently contains 157 members, ranging from the chloride           
pump bacteriorhodopsin to neurotransmitter receptors in the human.  The         
anticipated methodological improvements may eliminate ambiguities in the        
clusters produced by Kohonen-like methods and provide more useful               
indicators of homology or molecular evolutionary patterns.                      
PROPOSED COMMERCIAL APPLICATION                                                 
Software for the identification/prediction of structure-function                
relationships in proteins.                                                      
 G protein; artificial intelligence; biochemical evolution; classification; computer assisted sequence analysis; computer program /software; computer system design /evaluation; data management; protein sequence PROTEIN HOMOLOGY FROM NEAREST NEIGHBOR NEURAL NETWORKS","Determination of homologies/sequence similarities between proteins              
provides insight into structure-function relationships and evolutionary         
processes.  Current computer methods employ an initial alignment of             
protein amino acid sequences in order to recognize consensus regions and        
quantify similarities.  Neural network methodologies have been used to          
study DNA and protein sequences to predict domains serving putative             
functions.  We propose to improve on the self-organizing neural network         
(after Kohonen) to demonstrate structure-function relationships in              
proteins.  Our method employs a novel means of identifying homologous           
relationships.  For input the method uses 400 component vectors, the            
elements of which are normalized bipeptide nearest neighbor statistics.         
Initial evaluation of the method will focus on the G-protein coupling           
receptor super-family.  These proteins exhibit hydropathy plots                 
predicting seven transmembrane spanning regions and many diverse                
biochemical functions are transduced by its members.  Our data set for          
this family presently contains 157 members, ranging from the chloride           
pump bacteriorhodopsin to neurotransmitter receptors in the human.  The         
anticipated methodological improvements may eliminate ambiguities in the        
clusters produced by Kohonen-like methods and provide more useful               
indicators of homology or molecular evolutionary patterns.                      
PROPOSED COMMERCIAL APPLICATION                                                 
Software for the identification/prediction of structure-function                
relationships in proteins.                                                      
",2023106,R43GM053417,['R43GM053417'],GM,https://reporter.nih.gov/project-details/2023106,R43,1997,93811,-0.045625439806500245
"Neural network optimization algorithms greatly enhance our ability to           
construct large-scale, dynamical models of highly interconnected networks.      
Until now, optimization has only been applied to networks of simplistic         
processing units, ignoring the integrative and temporal response                
properties of single neurons, thus limiting the predictive power of the         
models.  The long-term goal of this project is to develop a hybrid              
modeling strategy in which optimization methods are applied to networks of      
realistic,multicompartmental model neurons. To accomplish this goal, we         
will construct a hybrid model of an actual distributed processing network       
composed of repeatably identifiable sensory, motor, and interneurons that       
computes a well-defined behavioral input-output function. Optimization          
will be used to predict the connectivity of as-yet-unidentified                 
interneurons in the actual network and the predictions will be tested by        
identifying the interneurons by physiological and morphological means.          
Performance of the hybrid model will be assessed by comparing it to the         
performance of an a priori model in which all connection strengths are          
determined physiologically. The final model will be used to predict the         
loci of synaptic plasticity underlying nonassociative conditioning of the       
reflex by incorporating local learning rules and by optimization methods.       
The predictions will be tested by determining the actual plastic sites          
physiologically.  This project will have the combined effect of enhancing       
the predictive power of optimized network models and illuminating the           
relation between computations at the single-neuron and network levels.          
 Hirudinea; artificial intelligence; computational neuroscience; computer program /software; interneurons; model design /development; neural information processing; neural plasticity OPTIMIZED NETWORKS OF MULTICOMPARTMENTAL NEURONS","Neural network optimization algorithms greatly enhance our ability to           
construct large-scale, dynamical models of highly interconnected networks.      
Until now, optimization has only been applied to networks of simplistic         
processing units, ignoring the integrative and temporal response                
properties of single neurons, thus limiting the predictive power of the         
models.  The long-term goal of this project is to develop a hybrid              
modeling strategy in which optimization methods are applied to networks of      
realistic,multicompartmental model neurons. To accomplish this goal, we         
will construct a hybrid model of an actual distributed processing network       
composed of repeatably identifiable sensory, motor, and interneurons that       
computes a well-defined behavioral input-output function. Optimization          
will be used to predict the connectivity of as-yet-unidentified                 
interneurons in the actual network and the predictions will be tested by        
identifying the interneurons by physiological and morphological means.          
Performance of the hybrid model will be assessed by comparing it to the         
performance of an a priori model in which all connection strengths are          
determined physiologically. The final model will be used to predict the         
loci of synaptic plasticity underlying nonassociative conditioning of the       
reflex by incorporating local learning rules and by optimization methods.       
The predictions will be tested by determining the actual plastic sites          
physiologically.  This project will have the combined effect of enhancing       
the predictive power of optimized network models and illuminating the           
relation between computations at the single-neuron and network levels.          
",2416012,R29MH051383,['R29MH051383'],MH,https://reporter.nih.gov/project-details/2416012,R29,1997,85155,-0.029356469708609
"The cornea is a principal refractive element in the eye; corneal                
transparency and corneal shape determine its optical qualities.                 
Corneal epithelial edema, stromal edema and corneal shape anomalies             
can independently or collectively degrade visual performance inthe              
form of increased internal ligh scatter andoptical aberrations due to           
irregular astigmatism.  The central theme of this research proposalis           
the refinement and application of a mathematical model that                     
integrates the thermodynamic description of corneal epithelial, stromal         
and endothelial transport properties into a model of corneal hydration          
control.  This is combined with methods to classify shape anomalies             
and means to assess the optical quality of the corneal surface through          
the analysis of corneal topography.                                             
 artificial intelligence; atrial natriuretic peptide; biological transport; body water dehydration; cornea edema; corneal endothelium; corneal epithelium; corneal stroma; electrophysiology; human subject; intraocular fluid; laboratory rabbit; mathematical model; membrane permeability; model design /development; morphology; nitric oxide; refractive keratoplasty; thermodynamics; visual perception INTEGRATED ASSESSMENT OF CORNEAL FORM AND FUNCTION","The cornea is a principal refractive element in the eye; corneal                
transparency and corneal shape determine its optical qualities.                 
Corneal epithelial edema, stromal edema and corneal shape anomalies             
can independently or collectively degrade visual performance inthe              
form of increased internal ligh scatter andoptical aberrations due to           
irregular astigmatism.  The central theme of this research proposalis           
the refinement and application of a mathematical model that                     
integrates the thermodynamic description of corneal epithelial, stromal         
and endothelial transport properties into a model of corneal hydration          
control.  This is combined with methods to classify shape anomalies             
and means to assess the optical quality of the corneal surface through          
the analysis of corneal topography.                                             
",2710832,R01EY003311,['R01EY003311'],EY,https://reporter.nih.gov/project-details/2710832,R01,1998,239717,-0.01681662655529709
"The goal of this proposal is to generate method and tools to link               
knowledge-based systems (KBSs) to real clinical databases.  The linking         
is done via quires that map conceptual entities in the KBS to actual            
entries in the database. They may be used when a KBS is first created,          
when an existing KBS is linked to a database or when a KBS is transferred       
to another institution.  The primary purpose of the queries is to apply         
the KBS to individual patients for direct patient care, rather than to          
extend the knowledge base itself.  Several of the tools (Aims 3 and 4)          
are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All         
the proposed tools will be applied to CPMC's MLM knowledge base, which          
is actively used for patient care.                                              
                                                                                
AIM 1 to increase the accuracy while reducing the writing time and              
technical skills required to author clinical database queries.  Query by        
Review will provide a familiar interface to novice query authors,               
enabling them to write queries by traversing the same type of result            
review screens that they use every day in clinical care. When data are          
selected from the screen, the tools will generate an appropriate query          
(in Arden Syntax and HL7) that can be inserted into a KBS or used for           
clinical research.                                                              
                                                                                
AIM 2 to facilitate the testing of queries, and to improve the match            
between a query's result and the needs to a KBS.  The Clinical Database         
Brower will allow author to characterize the data returned by a query in        
order to determine if they query is appropriate.  It will also allow the        
author to determine whether additional logic is necessary to convert the        
raw data into form expected by the KBS.  The Brower's design is unique          
in its use of a semantic network to aggregate complex categorical data.         
                                                                                
AIM 3 to provide an environment for inserting queries and additional            
logic into the KBS.  The Advanced MLM Editor will include a mechanism for       
inserting queries generated by the Query by Review tool into MLMs.  It          
will also allow authors to reuse queries and logic employed in existing         
MLMs.                                                                           
                                                                                
AIM 4 to facilitate the testing of queries within the environment of the        
KBS.  The Event Playback tool will allow the MLM author to run an MLM           
against the clinical database to see whether the MLM performs as                
expected.  Rather than presenting a snapshot of the database, the tool          
will run the MLM as if medical events (e.g, clinical database                   
transactions) were occurring in real time, better simulating actual use.        
The Interactive MLM Interpreter will allow an author to debug an MLM by         
running it line-by line; the author will be able to respond to each query       
manually.  It will also support the batch testing of MLMs using a test          
data set.                                                                       
                                                                                
AIM 5 to evaluate and disseminate the proposed tools.  The impact of the        
Query by Review tool on query authoring time and query accuracy will be         
measured.  To assess the usefulness of the tools for non-Arden Syntax           
KBS, findings from the QMR vocabulary will be studied.  Usage of the            
tools will be measured for CPMC's production KBS.  Tools, components, and       
methods will be disseminated.                                                   
 abstracting; artificial intelligence; computer assisted patient care; computer human interaction; health care facility information system; human subject; information retrieval; physicians; vocabulary development for information system LINKING KNOWLEDGE-BASED SYSTEMS TO CLINICAL DATABASES","The goal of this proposal is to generate method and tools to link               
knowledge-based systems (KBSs) to real clinical databases.  The linking         
is done via quires that map conceptual entities in the KBS to actual            
entries in the database. They may be used when a KBS is first created,          
when an existing KBS is linked to a database or when a KBS is transferred       
to another institution.  The primary purpose of the queries is to apply         
the KBS to individual patients for direct patient care, rather than to          
extend the knowledge base itself.  Several of the tools (Aims 3 and 4)          
are specific to the Arden Syntax for Medical Logic Modules (MLMs).  All         
the proposed tools will be applied to CPMC's MLM knowledge base, which          
is actively used for patient care.                                              
                                                                                
AIM 1 to increase the accuracy while reducing the writing time and              
technical skills required to author clinical database queries.  Query by        
Review will provide a familiar interface to novice query authors,               
enabling them to write queries by traversing the same type of result            
review screens that they use every day in clinical care. When data are          
selected from the screen, the tools will generate an appropriate query          
(in Arden Syntax and HL7) that can be inserted into a KBS or used for           
clinical research.                                                              
                                                                                
AIM 2 to facilitate the testing of queries, and to improve the match            
between a query's result and the needs to a KBS.  The Clinical Database         
Brower will allow author to characterize the data returned by a query in        
order to determine if they query is appropriate.  It will also allow the        
author to determine whether additional logic is necessary to convert the        
raw data into form expected by the KBS.  The Brower's design is unique          
in its use of a semantic network to aggregate complex categorical data.         
                                                                                
AIM 3 to provide an environment for inserting queries and additional            
logic into the KBS.  The Advanced MLM Editor will include a mechanism for       
inserting queries generated by the Query by Review tool into MLMs.  It          
will also allow authors to reuse queries and logic employed in existing         
MLMs.                                                                           
                                                                                
AIM 4 to facilitate the testing of queries within the environment of the        
KBS.  The Event Playback tool will allow the MLM author to run an MLM           
against the clinical database to see whether the MLM performs as                
expected.  Rather than presenting a snapshot of the database, the tool          
will run the MLM as if medical events (e.g, clinical database                   
transactions) were occurring in real time, better simulating actual use.        
The Interactive MLM Interpreter will allow an author to debug an MLM by         
running it line-by line; the author will be able to respond to each query       
manually.  It will also support the batch testing of MLMs using a test          
data set.                                                                       
                                                                                
AIM 5 to evaluate and disseminate the proposed tools.  The impact of the        
Query by Review tool on query authoring time and query accuracy will be         
measured.  To assess the usefulness of the tools for non-Arden Syntax           
KBS, findings from the QMR vocabulary will be studied.  Usage of the            
tools will be measured for CPMC's production KBS.  Tools, components, and       
methods will be disseminated.                                                   
",2714213,R29LM005627,['R29LM005627'],LM,https://reporter.nih.gov/project-details/2714213,R29,1998,117687,-0.0013889104312835537
"This proposal aims to develop an improved understanding of the mechanisms       
involved in functional MRI of the brain and to optimize imaging and data        
analysis strategies for the detection of neuronal activity.  Functional MRI     
relies on the ability to detect the changes in NMR signal that are produced     
in discrete regions of cortex in response to specific activating stimuli,       
and are believed to reflect changes in local blood flow, volume and             
oxygenation.  Functional MRI promises to be a major addition to the methods     
available for studying brain activation.  Despite the widespread claims for     
the power and successes of the method, there remain several unanswered          
questions regarding its optimal mode of use, the tissue and technical           
factors that are important in determining the signal changes detected, and      
the significance and interpretation of these signal changes.  The research      
proposed would systematically address such issues.  The underlying              
mechanism may include both susceptibility contrast effects, based on the        
BOLD effect, as well as wash-in effects, and these will be separately           
quantified.  The factors that affect each mechanism will be separately          
identified and measured.  For the BOLD effect, extensive computer modeling      
and measurements in phantoms and animals brains will be used to establish       
the relative sensitivity to vascular structures of different sizes,             
spacings and orientations, as well as other tissue properties such as the       
rate of water diffusion.  The separate sensitivities to s-called static         
field effects (T2*),  diffusive losses and other mechanisms will also be        
established.  The performance of different pulse sequences will be compared     
to devise optimal methods of scanning and detection at 1.5T.  Echo planar       
imaging, conventional gradient echo and fast spin echo imaging as well as       
more novel schemes will be compared in phantoms, animal brains and examples     
of human activation.  Human and animal activations will be produced in vivo     
using  visual and motor stimuli as well as by alteration of global blood        
flow by acetazolamide and hypercarbia.  A critical feature of current           
paradigms for detecting activation is the method of data analysis, which is     
interrelated with the nature of the task and imaging method used.  We will      
compare different methods of analyzing functional data sets, including          
statistical parameter mapping, time-correlation analyses, and principal         
component analysis.  The sensitivity of each to motion and other artifacts      
will be established by in in vivo comparisons and by computer simulations.      
From these studies, we anticipate being able to improve strategies for the      
use and interpretation of functional MRI in human studies of function and       
cognition.                                                                      
 acetazolamide; biophysics; blood flow measurement; blood vessels; blood volume; brain electrical activity; capillary; computer data analysis; computer simulation; human subject; hypercapnia; laboratory rat; magnetic resonance imaging; method development; motor neurons; nuclear magnetic resonance spectroscopy; phantom model; respiratory oxygenation; statistics /biometry; visual stimulus; water flow BIOPHYSICAL BASIS OF FUNCTIONAL BRAIN MRI","This proposal aims to develop an improved understanding of the mechanisms       
involved in functional MRI of the brain and to optimize imaging and data        
analysis strategies for the detection of neuronal activity.  Functional MRI     
relies on the ability to detect the changes in NMR signal that are produced     
in discrete regions of cortex in response to specific activating stimuli,       
and are believed to reflect changes in local blood flow, volume and             
oxygenation.  Functional MRI promises to be a major addition to the methods     
available for studying brain activation.  Despite the widespread claims for     
the power and successes of the method, there remain several unanswered          
questions regarding its optimal mode of use, the tissue and technical           
factors that are important in determining the signal changes detected, and      
the significance and interpretation of these signal changes.  The research      
proposed would systematically address such issues.  The underlying              
mechanism may include both susceptibility contrast effects, based on the        
BOLD effect, as well as wash-in effects, and these will be separately           
quantified.  The factors that affect each mechanism will be separately          
identified and measured.  For the BOLD effect, extensive computer modeling      
and measurements in phantoms and animals brains will be used to establish       
the relative sensitivity to vascular structures of different sizes,             
spacings and orientations, as well as other tissue properties such as the       
rate of water diffusion.  The separate sensitivities to s-called static         
field effects (T2*),  diffusive losses and other mechanisms will also be        
established.  The performance of different pulse sequences will be compared     
to devise optimal methods of scanning and detection at 1.5T.  Echo planar       
imaging, conventional gradient echo and fast spin echo imaging as well as       
more novel schemes will be compared in phantoms, animal brains and examples     
of human activation.  Human and animal activations will be produced in vivo     
using  visual and motor stimuli as well as by alteration of global blood        
flow by acetazolamide and hypercarbia.  A critical feature of current           
paradigms for detecting activation is the method of data analysis, which is     
interrelated with the nature of the task and imaging method used.  We will      
compare different methods of analyzing functional data sets, including          
statistical parameter mapping, time-correlation analyses, and principal         
component analysis.  The sensitivity of each to motion and other artifacts      
will be established by in in vivo comparisons and by computer simulations.      
From these studies, we anticipate being able to improve strategies for the      
use and interpretation of functional MRI in human studies of function and       
cognition.                                                                      
",2714543,R01NS033332,['R01NS033332'],NS,https://reporter.nih.gov/project-details/2714543,R01,1998,360071,-0.07768352122422495
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
Substance abuse is the major health problem in the United States.  The cost     
to society in terms of death, disease, injury and emotional distress            
attributable to alcohol, nicotine, and other drugs is immeasurable.  The        
goal of researchers interested in the neurobiological substrates of drug        
abuse is to understand the powerful motivating and addictive properties of      
drugs, and to develop better treatments for intoxication, craving, and          
relapse.  In recent years, much progress has been made toward the first part    
of this goal.  The neural pathways governing the reinforcing effects of         
drugs are well known, and very recently remarkable advances have been           
achieved in learning about effects of drugs on molecular mechanisms and gene    
expression.  The objective of this proposal is to study in detail the basic     
behavioral functions of the nucleus accumbens, integrating behavioral,          
pharmacological and molecular approaches.  The nucleus accumbens, located       
within the ventromedial striatum, is an important neural substrate for drug     
reinforcement.  It is assumed, therefore, that since drugs appear to exert      
their rewarding properties in this region and its associated circuits, the      
accumbens may play a fundamental role in biological reinforcement and           
regulation of appetitive behaviors.  Relatively little is known, however,       
about the specific neuromolecular mechanisms that underlie the functions of     
the accumbens.  The aims of this project are to investigate the behavioral      
functions of the recently designated ""core"" and ""shell"" subregions of the       
nucleus accumbens.  Specifically, we will 1) investigate the role of            
dopaminergic and glutamatergic receptor mechanisms and their associated         
intracellular mechanisms within the accumbens core in response-reinforcement    
learning, and 2) we will study transmitter mechanisms and circuitry involved    
in controlling feeding behavior located within the accumbens shell.             
Techniques to be utilized include local microinfusions of specific compounds    
that act of receptor and intracellular transduction mechanisms, in              
combination with behavioral analysis, and measurement of transcription          
factors and phosphorylating enzymes that may play a role in learning or         
motivated behavior.  Further basic understanding of the systems and             
substrates upon which drugs act, which could potentially result from this       
research, could help to develop better addiction therapies.                     
 G protein; adenylate cyclase; behavioral /social science research tag; biological signal transduction; cAMP response element binding protein; dopamine; dopamine receptor; drug abuse; enzyme activity; gamma aminobutyrate; gel mobility shift assay; genetic regulation; glutamate receptor; immunocytochemistry; laboratory rat; learning; microinjections; neural plasticity; neuropharmacology; neuroregulation; neurotransmitter metabolism; nucleus accumbens; nutrient intake activity; operant conditionings; psychopharmacology; reinforcer NEURAL MECHANISMS IN THE NUCLEUS ACCUMBENS AND BEHAVIOR","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
Substance abuse is the major health problem in the United States.  The cost     
to society in terms of death, disease, injury and emotional distress            
attributable to alcohol, nicotine, and other drugs is immeasurable.  The        
goal of researchers interested in the neurobiological substrates of drug        
abuse is to understand the powerful motivating and addictive properties of      
drugs, and to develop better treatments for intoxication, craving, and          
relapse.  In recent years, much progress has been made toward the first part    
of this goal.  The neural pathways governing the reinforcing effects of         
drugs are well known, and very recently remarkable advances have been           
achieved in learning about effects of drugs on molecular mechanisms and gene    
expression.  The objective of this proposal is to study in detail the basic     
behavioral functions of the nucleus accumbens, integrating behavioral,          
pharmacological and molecular approaches.  The nucleus accumbens, located       
within the ventromedial striatum, is an important neural substrate for drug     
reinforcement.  It is assumed, therefore, that since drugs appear to exert      
their rewarding properties in this region and its associated circuits, the      
accumbens may play a fundamental role in biological reinforcement and           
regulation of appetitive behaviors.  Relatively little is known, however,       
about the specific neuromolecular mechanisms that underlie the functions of     
the accumbens.  The aims of this project are to investigate the behavioral      
functions of the recently designated ""core"" and ""shell"" subregions of the       
nucleus accumbens.  Specifically, we will 1) investigate the role of            
dopaminergic and glutamatergic receptor mechanisms and their associated         
intracellular mechanisms within the accumbens core in response-reinforcement    
learning, and 2) we will study transmitter mechanisms and circuitry involved    
in controlling feeding behavior located within the accumbens shell.             
Techniques to be utilized include local microinfusions of specific compounds    
that act of receptor and intracellular transduction mechanisms, in              
combination with behavioral analysis, and measurement of transcription          
factors and phosphorylating enzymes that may play a role in learning or         
motivated behavior.  Further basic understanding of the systems and             
substrates upon which drugs act, which could potentially result from this       
research, could help to develop better addiction therapies.                     
",2619777,R01DA004788,['R01DA004788'],DA,https://reporter.nih.gov/project-details/2619777,R01,1998,187610,0.0039882445130775416
"DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating            
Characteristic (ROC) analysis is recognized widely as the best way of           
measuring and specifying the accuracies of diagnostic procedures, because it    
is able to distinguish between actual differences in discrimination             
capacity, on one hand, and apparent differences that are due only to            
decision-threshold effects, on the other.  Key methodological needs remain      
to be satisfied before ROC analysis can address all of the practically          
important situations that arise in diagnostic applications, however.  This      
project employs signal detection theory and computer simulation to address      
several of those needs, by:  (1) refining and continuing distribution of        
software developed previously by the applicants for fitting ROC curves and      
for testing the statistical significance of differences between ROC curve       
estimates; (2) developing and evaluating new algorithms for ROC                 
curve-Fitting and statistical testing, based on their recently-developed        
""proper"" binormal model, that should provide more meaningful results in         
experimental situations that involve small samples of cases; (3)                
investigating the usefulness of a form of ROC methodology that is based on      
mixture distributions in order to rduce the need for diagnostic truth in ROC    
experiments; (4) investigating the effect of case-saple difficulty on the       
statistical power tests for differences between ROC curves, in order to         
determine the optimal difficulty of cases that shouldbe studied on rank         
diagnostic systems; and (5) developing methods for training artificial          
neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC          
analysis and signal detection theory.                                           
 artificial intelligence; computer assisted diagnosis; computer system design /evaluation; method development; statistics /biometry NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY","DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating            
Characteristic (ROC) analysis is recognized widely as the best way of           
measuring and specifying the accuracies of diagnostic procedures, because it    
is able to distinguish between actual differences in discrimination             
capacity, on one hand, and apparent differences that are due only to            
decision-threshold effects, on the other.  Key methodological needs remain      
to be satisfied before ROC analysis can address all of the practically          
important situations that arise in diagnostic applications, however.  This      
project employs signal detection theory and computer simulation to address      
several of those needs, by:  (1) refining and continuing distribution of        
software developed previously by the applicants for fitting ROC curves and      
for testing the statistical significance of differences between ROC curve       
estimates; (2) developing and evaluating new algorithms for ROC                 
curve-Fitting and statistical testing, based on their recently-developed        
""proper"" binormal model, that should provide more meaningful results in         
experimental situations that involve small samples of cases; (3)                
investigating the usefulness of a form of ROC methodology that is based on      
mixture distributions in order to rduce the need for diagnostic truth in ROC    
experiments; (4) investigating the effect of case-saple difficulty on the       
statistical power tests for differences between ROC curves, in order to         
determine the optimal difficulty of cases that shouldbe studied on rank         
diagnostic systems; and (5) developing methods for training artificial          
neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC          
analysis and signal detection theory.                                           
",2605375,R01GM057622,['R01GM057622'],GM,https://reporter.nih.gov/project-details/2605375,R01,1998,215961,-0.07187245250318569
"DESCRIPTION (Adapted from the Investigator's Abstract):  To date no unitary     
theory of cortical function has emerged despite a long history of cortical      
research.  Single cell approaches in primary visual cortex, as exemplified      
by Hubeland Wiesel's studies and by recent work on parallel visual pathways,    
have produced functional circuit diagrams arguing for hierarchical,             
feedforward processing.  Alternatively, artificial neural network research      
argues that the cortex might represent a distributed feedback circuit in        
which intrinsic dynamics converge in stable states that represent               
computational solutions.  these two types of models predict very different      
activation patterns of the circuit.  The goal of the research is to             
elucidate the three-dimensional spatio-temporal activity patterns intrinsic     
to the cortical microcircuit and to identify their underlying circuits.         
Studies will be carried out with brain slices from mouse visual cortex using    
calcium imaging with a cooled CCD camera, a photodiode array and two-photon     
microscope.  These techniques allow the investigators to follow the activity    
of neuronal ensembles across the entire slice with single-cell and              
submillisecond resolution.  Specifically, the investigators will (i)            
determine the three-dimensional activity patterns present in a brain slice      
(ii) establish the anatomical and functional connectivity underlying these      
dynamics and (iii) identify neurons playing key roles and study their effect    
in altering circuit dynamics.  These studies may help determine whether         
cortical neurons can activate in preferential labeled lines, as predicted by    
feedforward models or in a widely distributed pattern, as predicted by          
feedback models, shedding light on the functional units of cortical             
microcircuitry and their co-ordination in cortical function as a whole.         
Finally, they will help understand the central pathophysiological               
consequences of amblyopia and strabismus, as well as help design therapeutic    
strategies aimed at compensating for these defect.  A more complete             
understanding of the circuitry will also improve the analysis of visual         
evoked potentials (VEP) and thus the measurement of acuity, contrast            
sensitivity and chromatic sensitivity of preverbal children and in early        
diagnosis of visual pathologies.                                                
 calcium indicator; confocal scanning microscopy; electrophysiology; evoked potentials; image processing; laboratory mouse; neural information processing; neuroanatomy; neurons; single cell analysis; synapses; vision disorders; visual cortex IMAGING FUNCTIONAL CONNECTIVITY IN VISUAL CORTEX","DESCRIPTION (Adapted from the Investigator's Abstract):  To date no unitary     
theory of cortical function has emerged despite a long history of cortical      
research.  Single cell approaches in primary visual cortex, as exemplified      
by Hubeland Wiesel's studies and by recent work on parallel visual pathways,    
have produced functional circuit diagrams arguing for hierarchical,             
feedforward processing.  Alternatively, artificial neural network research      
argues that the cortex might represent a distributed feedback circuit in        
which intrinsic dynamics converge in stable states that represent               
computational solutions.  these two types of models predict very different      
activation patterns of the circuit.  The goal of the research is to             
elucidate the three-dimensional spatio-temporal activity patterns intrinsic     
to the cortical microcircuit and to identify their underlying circuits.         
Studies will be carried out with brain slices from mouse visual cortex using    
calcium imaging with a cooled CCD camera, a photodiode array and two-photon     
microscope.  These techniques allow the investigators to follow the activity    
of neuronal ensembles across the entire slice with single-cell and              
submillisecond resolution.  Specifically, the investigators will (i)            
determine the three-dimensional activity patterns present in a brain slice      
(ii) establish the anatomical and functional connectivity underlying these      
dynamics and (iii) identify neurons playing key roles and study their effect    
in altering circuit dynamics.  These studies may help determine whether         
cortical neurons can activate in preferential labeled lines, as predicted by    
feedforward models or in a widely distributed pattern, as predicted by          
feedback models, shedding light on the functional units of cortical             
microcircuitry and their co-ordination in cortical function as a whole.         
Finally, they will help understand the central pathophysiological               
consequences of amblyopia and strabismus, as well as help design therapeutic    
strategies aimed at compensating for these defect.  A more complete             
understanding of the circuitry will also improve the analysis of visual         
evoked potentials (VEP) and thus the measurement of acuity, contrast            
sensitivity and chromatic sensitivity of preverbal children and in early        
diagnosis of visual pathologies.                                                
",2485367,R01EY011787,['R01EY011787'],EY,https://reporter.nih.gov/project-details/2485367,R01,1998,255314,-0.042278395236715165
"DESCRIPTION:  (Applicant's Abstract) In the field of computational biology,     
an important emerging specialty is the use of intelligent systems in support    
of molecular biology.  The Intelligent Systems for Molecular Biology (ISMB)     
conference provides a unique venue for the dissemination of recent              
developments in this field and for the interchange of ideas between             
experimental molecular biologists and computer scientists, mathematicians,      
and statisticians.  The ISMB-97 conference will be held in Halkidiki, Greece    
and the ISMB-98 conference will be held at the University of Montreal,          
Canada.  These locales continue the tradition of alternating between North      
America and Europe.                                                             
                                                                                
This grant request is for travel funds to enable students and young             
scientists to attend ISMB-97 and ISMB-98.  We are requesting $20,900 over       
two years to fund partial reimbursement of expenses for up to 20 students or    
postdocs each year.                                                             
 artificial intelligence; international cooperation; meeting /conference /symposium; molecular biology; travel 5TH AND 6TH INTERNATIONAL CONFERENCES ON ISMB","DESCRIPTION:  (Applicant's Abstract) In the field of computational biology,     
an important emerging specialty is the use of intelligent systems in support    
of molecular biology.  The Intelligent Systems for Molecular Biology (ISMB)     
conference provides a unique venue for the dissemination of recent              
developments in this field and for the interchange of ideas between             
experimental molecular biologists and computer scientists, mathematicians,      
and statisticians.  The ISMB-97 conference will be held in Halkidiki, Greece    
and the ISMB-98 conference will be held at the University of Montreal,          
Canada.  These locales continue the tradition of alternating between North      
America and Europe.                                                             
                                                                                
This grant request is for travel funds to enable students and young             
scientists to attend ISMB-97 and ISMB-98.  We are requesting $20,900 over       
two years to fund partial reimbursement of expenses for up to 20 students or    
postdocs each year.                                                             
",2674267,R13HG001683,['R13HG001683'],HG,https://reporter.nih.gov/project-details/2674267,R13,1998,10500,-0.02195731362119251
"DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be      
taken to advance our understanding of the genetic and associated co-            
variates affecting the inheritance of complex diseases. To that end, this       
proposal will develop improved quantitative methods to detect genetic           
factors contributing to increased susceptibility to complex disorders and       
implement these methods in software for distribution to the research            
community.                                                                      
                                                                                
The methods will concentrate on the use of classification techniques            
applied to allele sharing data and other risk factors which affect the          
trait. Allele sharing methods for mapping genes will be extended to             
include the classification methods known as latent class models, cluster        
analysis, and artificial neural networks, as well as a novel use of             
logistic regression Co-variates such as gender, parental diagnosis, or          
other concomitant factors will be systematically studied through                
applications to both stimulated and existing data sets. An additional goal      
is to determine the optimal distribution of relative pairs (e.g. siblings,      
first cousins) for these methods. Of great importance to this proposal is       
the development of well-documented, user-friendly software and                  
documentation which will be distributed to the scientific community via         
the Internet. Existing software developed by the PI will be extensively         
expanded for latent class models. Existing cluster analysis software will       
be modified and combined for ease of use.                                       
                                                                                
This proposal consists of theoretical exploration, computer simulation,         
data analysis, and software development. First, solutions of theoretical        
questions relating to classification techniques will be pursued; second,        
adaptation of computer programs to implement the analytic methods, and          
investigation into alternative research strategies will be accomplished.        
The new strategies will be applied to stimulated data, and finally, to          
existing data sets of pedigrees in which a complex trait has been               
diagnosed. Findings from this research may contribute to the ability to         
locate susceptibility loci in complex traits and to the clarification of        
those etiological mechanisms responsible for susceptibility.                    
 alleles; analytical method; artificial intelligence; biomedical resource; computer program /software; computer simulation; data collection methodology /evaluation; disease /disorder classification; disease /disorder etiology; family genetics; gene environment interaction; gene expression; genetic disorder; genetic disorder diagnosis; genetic mapping; genetic markers; genetic susceptibility; human data; mathematical model; model design /development; quantitative trait loci; statistics /biometry CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI","DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be      
taken to advance our understanding of the genetic and associated co-            
variates affecting the inheritance of complex diseases. To that end, this       
proposal will develop improved quantitative methods to detect genetic           
factors contributing to increased susceptibility to complex disorders and       
implement these methods in software for distribution to the research            
community.                                                                      
                                                                                
The methods will concentrate on the use of classification techniques            
applied to allele sharing data and other risk factors which affect the          
trait. Allele sharing methods for mapping genes will be extended to             
include the classification methods known as latent class models, cluster        
analysis, and artificial neural networks, as well as a novel use of             
logistic regression Co-variates such as gender, parental diagnosis, or          
other concomitant factors will be systematically studied through                
applications to both stimulated and existing data sets. An additional goal      
is to determine the optimal distribution of relative pairs (e.g. siblings,      
first cousins) for these methods. Of great importance to this proposal is       
the development of well-documented, user-friendly software and                  
documentation which will be distributed to the scientific community via         
the Internet. Existing software developed by the PI will be extensively         
expanded for latent class models. Existing cluster analysis software will       
be modified and combined for ease of use.                                       
                                                                                
This proposal consists of theoretical exploration, computer simulation,         
data analysis, and software development. First, solutions of theoretical        
questions relating to classification techniques will be pursued; second,        
adaptation of computer programs to implement the analytic methods, and          
investigation into alternative research strategies will be accomplished.        
The new strategies will be applied to stimulated data, and finally, to          
existing data sets of pedigrees in which a complex trait has been               
diagnosed. Findings from this research may contribute to the ability to         
locate susceptibility loci in complex traits and to the clarification of        
those etiological mechanisms responsible for susceptibility.                    
",2865173,R01AA012239,['R01AA012239'],AA,https://reporter.nih.gov/project-details/2865173,R01,1998,162275,-5.0046344393969534e-05
"Persons aged 65 years or older are the fastest growing portion of the           
population in the United States.  However, there are disproportionately         
few specialists in geriatric medicine to provide the majority of these          
individuals with high quality medical care.  One solution to this               
problem is through the development and use of computer-based medical            
expert systems. Based on techniques of artificial intelligence, such            
systems have the ability to assist non-specialist clinicians or allied          
health professionals in the preliminary evaluation and diagnosis of             
common disorders in the elderly.  Depression and dementia are the two           
most prevalent of such disorders, yet these conditions frequently are           
misdiagnosed by non-specialist clinicians. Failure to detect or                 
distinguish the two disorders leads to missed treatment opportunities           
and preventable disability.  The proposed project will compare the              
feasibility, statistical accuracy, and user acceptance of two clinical          
decision support systems -- a rule-based expert system and a neural             
network computational classifier - to be used in the evaluation of              
depression and dementia in the elderly.  The user interface for the two         
systems will be designed and tested to meet usability criteria desired          
by practicing clinicians, including voice-recognition input, computer-          
speech output, interactive graphic environments, ranked differential            
diagnoses, and the ability to explain the system's reasoning,                   
conclusions, and recommendations. The system will include an automatic          
foreign language translator initially programmed for Spanish, but               
customizable to other languages.  The project will be conducted in four         
phases: 1) Knowledge Acquisition, 2) Prototype Development, 3) Usability        
Testing, and 4) System Validation. Specific goals include: 1) efficient         
implementation of diagnostic rules and criteria obtained from clinician         
interview and literature review, 2) successful development of a                 
prototype system to meet formalized usability standards, 3) preliminary         
validation of the system to classify with 90 percent accuracy a large           
set of existing clinical data obtained from referrals to a medical              
school geriatric psychiatry clinic, and 4) statistical comparison of the        
classification accuracy of the prototype expert system with the neural          
network classifier.  The completed expert system will form the                  
foundation for development of future clinical modules applicable to             
other geriatric disorders.  Ultimately, systems developed from this             
prototype will provide diagnostic and treatment recommendations in              
medical settings without ready access to clinical specialists, such as          
in rural health care or under-served urban areas.                               
 brain disorder diagnosis; clinical depression; computer assisted diagnosis; computer system design /evaluation; dementia; diagnosis design /evaluation; human data; human old age (65+); language translation; mental disorder diagnosis; psychometrics EXPERT SYSTEM DIAGNOSIS OF DEPRESSION AND DEMENTIA","Persons aged 65 years or older are the fastest growing portion of the           
population in the United States.  However, there are disproportionately         
few specialists in geriatric medicine to provide the majority of these          
individuals with high quality medical care.  One solution to this               
problem is through the development and use of computer-based medical            
expert systems. Based on techniques of artificial intelligence, such            
systems have the ability to assist non-specialist clinicians or allied          
health professionals in the preliminary evaluation and diagnosis of             
common disorders in the elderly.  Depression and dementia are the two           
most prevalent of such disorders, yet these conditions frequently are           
misdiagnosed by non-specialist clinicians. Failure to detect or                 
distinguish the two disorders leads to missed treatment opportunities           
and preventable disability.  The proposed project will compare the              
feasibility, statistical accuracy, and user acceptance of two clinical          
decision support systems -- a rule-based expert system and a neural             
network computational classifier - to be used in the evaluation of              
depression and dementia in the elderly.  The user interface for the two         
systems will be designed and tested to meet usability criteria desired          
by practicing clinicians, including voice-recognition input, computer-          
speech output, interactive graphic environments, ranked differential            
diagnoses, and the ability to explain the system's reasoning,                   
conclusions, and recommendations. The system will include an automatic          
foreign language translator initially programmed for Spanish, but               
customizable to other languages.  The project will be conducted in four         
phases: 1) Knowledge Acquisition, 2) Prototype Development, 3) Usability        
Testing, and 4) System Validation. Specific goals include: 1) efficient         
implementation of diagnostic rules and criteria obtained from clinician         
interview and literature review, 2) successful development of a                 
prototype system to meet formalized usability standards, 3) preliminary         
validation of the system to classify with 90 percent accuracy a large           
set of existing clinical data obtained from referrals to a medical              
school geriatric psychiatry clinic, and 4) statistical comparison of the        
classification accuracy of the prototype expert system with the neural          
network classifier.  The completed expert system will form the                  
foundation for development of future clinical modules applicable to             
other geriatric disorders.  Ultimately, systems developed from this             
prototype will provide diagnostic and treatment recommendations in              
medical settings without ready access to clinical specialists, such as          
in rural health care or under-served urban areas.                               
",2871178,R03HS009828,['R03HS009828'],HS,https://reporter.nih.gov/project-details/2871178,R03,1998,48977,-0.024871539071547696
"The overall goal of this research program is to investigate the role of         
parent behavior on the development of repetitive arm, hand, and finger          
movements in young infants-gestures that have recently been studied as          
an analog to infant vocal babble. Specifically, this project will               
investigate the responsiveness of parents to these repetitive infant            
gestures and will experimentally test the role of parental reinforcement        
on their production over time. Taking a Dynamic Systems approach, the           
overall hypothesis is that children are born with innate predispositions        
to produce behaviors that appear to adults as very speech- or sign-like-        
repetitive vocalizations and gestures, respectively. As children mature,        
they have an expanding ""pool' of motor behaviors that may be used in            
various contexts. In contexts where these behaviors are functional (e.g.        
parents respond to and reinforce them), they are maintained and continue        
to pull from the available pool. In contexts where they are not                 
functional, they do not change and eventually drop out. The present             
project will consist of two longitudinal small-sample studies and one           
large-sample study of adult responsiveness to repetitive infant gestures.       
In Study 1 of the present project, the responses of hearing parents to          
repetitive gestures produced by their hearing infants from 6 to 12 months       
of age will be studied and compared to those of deaf, American Sign             
Language (ASL)-signing parents of deaf infants. These data will provide         
information about the natural responses of parents, both hearing and            
deaf, to repetitive babble-like gestures produced by young infants. In          
Study 2, the ability of hearing, nonsigning adults to identify repetitive       
infant gestures from a ""background' of ongoing infant motor activity will       
be assessed. These data will provide new information about the potential        
of hearing adults to recognize babble-like gestures in young infants. In        
Study 3, the effect of parent reinforcement on repetitive infant gestures       
will be experimentally tested by training hearing parents to provide            
social reinforcement for nonreferential repetitive gestures produced by         
their hearing infants between 6 and 16 months of age. These data will           
inform our models of language learning by examining the role of parent          
responsitivity in the development of a manual form of prelinguistic             
babble and provide clinically relevant information about gestural parent-       
child interaction-important to programs encouraging gestural                    
communication between hearing parents and deaf children.                        
 attention; behavioral /social science research tag; child (0-11); child behavior; clinical research; deafness; human subject; infant human (0-1 year); learning; paralinguistic behavior; parent offspring interaction; psychological models; psychological reinforcement; videotape /videodisc PARENT RESPONSE EFFECTS ON INFANT GESTURAL BABBLE","The overall goal of this research program is to investigate the role of         
parent behavior on the development of repetitive arm, hand, and finger          
movements in young infants-gestures that have recently been studied as          
an analog to infant vocal babble. Specifically, this project will               
investigate the responsiveness of parents to these repetitive infant            
gestures and will experimentally test the role of parental reinforcement        
on their production over time. Taking a Dynamic Systems approach, the           
overall hypothesis is that children are born with innate predispositions        
to produce behaviors that appear to adults as very speech- or sign-like-        
repetitive vocalizations and gestures, respectively. As children mature,        
they have an expanding ""pool' of motor behaviors that may be used in            
various contexts. In contexts where these behaviors are functional (e.g.        
parents respond to and reinforce them), they are maintained and continue        
to pull from the available pool. In contexts where they are not                 
functional, they do not change and eventually drop out. The present             
project will consist of two longitudinal small-sample studies and one           
large-sample study of adult responsiveness to repetitive infant gestures.       
In Study 1 of the present project, the responses of hearing parents to          
repetitive gestures produced by their hearing infants from 6 to 12 months       
of age will be studied and compared to those of deaf, American Sign             
Language (ASL)-signing parents of deaf infants. These data will provide         
information about the natural responses of parents, both hearing and            
deaf, to repetitive babble-like gestures produced by young infants. In          
Study 2, the ability of hearing, nonsigning adults to identify repetitive       
infant gestures from a ""background' of ongoing infant motor activity will       
be assessed. These data will provide new information about the potential        
of hearing adults to recognize babble-like gestures in young infants. In        
Study 3, the effect of parent reinforcement on repetitive infant gestures       
will be experimentally tested by training hearing parents to provide            
social reinforcement for nonreferential repetitive gestures produced by         
their hearing infants between 6 and 16 months of age. These data will           
inform our models of language learning by examining the role of parent          
responsitivity in the development of a manual form of prelinguistic             
babble and provide clinically relevant information about gestural parent-       
child interaction-important to programs encouraging gestural                    
communication between hearing parents and deaf children.                        
",2799251,R03DC003247,['R03DC003247'],DC,https://reporter.nih.gov/project-details/2799251,R03,1998,16344,-0.0305898437699804
"DESCRIPTION:  The principal investigator notes that the Aldol addition          
reaction is one of the most important carbon-carbon bond forming                
reactions in  synthetic organic chemistry, that it is widely used in the        
synthesis of drugs  and other biologically active molecules and that the        
reaction is now most  often run in non-polar solvents, such as                  
tetrahydrofuran (THF), frequently  with lithium salts of carbonyl               
compounds (lithium enolates).  He reports that  the aggregation of these        
compounds has been characterized by solution properties, NMR and by X-ray       
crystallography and that such aggregates have  been proposed as the             
reactive species in the Aldol-type additions with other  carbonyl               
derivatives but there have been very few studies on the actual role  of         
such aggregates in reactions.  He goes on to note that by a combination         
of  UV-vis spectroscopic and proton transfer equilibrium studies of some        
lithium  enolates, aggregation constants have been obtained even in             
dilute solution and  that spectra as a function of concentration are            
analyzed by ""Singular Value  Decomposition"" to determine the number of          
different species present and to permit deconvolution to give the               
spectrum of each component.  For this purpose  a glovebox-spectrometer          
apparatus has been developed in which the sample  compartment built into        
the glovebox is connected with a spectrometer with  fiber-optic cables.         
It is noted that the apparatus permits spectroscopy of  solutions               
prepared and studied under the inert atmosphere of the glovebox.  The           
results thus far are said to suggest that the monomeric ion pairs might         
play an important kinetic role in reactions.                                    
                                                                                
It is proposed to extend such studies to additional enolates of interest        
and  to measure the reaction kinetics of their Aldol additions to               
aldehydes and  ketones and of Michael addition reactions with unsaturated       
carbonyl compounds.  It is indicated that the kinetic studies will show         
the state of aggregation of  the enolate reactive species and that              
knowledge of the relative roles of ion  pair monomers and aggregates will       
lead to more complete reaction mechanisms  and to the better                    
understanding required for sophisticated synthesis design.  The principal       
investigator notes that in particular, the roles of solvent  addends,           
such as lithium salts, hexamethylphosphoric triamide, and di- and               
triamines will be studied under carefully controlled conditions to              
determine  the role of coordination of lithium in the stereochemistry of        
the addition  reactions.                                                        
                                                                                
It is also proposed to apply the same techniques of spectroscopic study,        
proton transfer equilibria and reaction kinetics of Aldol and Michael           
addition  reactions to the dilithium salts of carboxylic acids and beta-        
diketones.  It  is noted that these dilithium salts are also being used         
increasingly in  organic synthesis but that the reaction mechanisms are         
virtually unstudied.  It is reported that these salts are also aggregated       
but nothing is known about  the relative reactivities of monomers and           
aggregates.  The proposed studies  are to provide unique information            
about these reactions, which would be  difficult to obtain in any other         
way.  It is suggested that subsequent  extension to other salts of alkali       
and alkaline earth metals, early and late  transition metals and                
lanthanides is also proposed since many of these salts  have found use          
in some stereospecific syntheses.                                               
 aldehydes; biomedical equipment development; chemical addition; chemical kinetics; enol; enolate; hydrogen transport; ketones; lithium; protonation; stereochemistry; ultraviolet spectrometry ION PAIR ALDOL ADDITION REACTIONS","DESCRIPTION:  The principal investigator notes that the Aldol addition          
reaction is one of the most important carbon-carbon bond forming                
reactions in  synthetic organic chemistry, that it is widely used in the        
synthesis of drugs  and other biologically active molecules and that the        
reaction is now most  often run in non-polar solvents, such as                  
tetrahydrofuran (THF), frequently  with lithium salts of carbonyl               
compounds (lithium enolates).  He reports that  the aggregation of these        
compounds has been characterized by solution properties, NMR and by X-ray       
crystallography and that such aggregates have  been proposed as the             
reactive species in the Aldol-type additions with other  carbonyl               
derivatives but there have been very few studies on the actual role  of         
such aggregates in reactions.  He goes on to note that by a combination         
of  UV-vis spectroscopic and proton transfer equilibrium studies of some        
lithium  enolates, aggregation constants have been obtained even in             
dilute solution and  that spectra as a function of concentration are            
analyzed by ""Singular Value  Decomposition"" to determine the number of          
different species present and to permit deconvolution to give the               
spectrum of each component.  For this purpose  a glovebox-spectrometer          
apparatus has been developed in which the sample  compartment built into        
the glovebox is connected with a spectrometer with  fiber-optic cables.         
It is noted that the apparatus permits spectroscopy of  solutions               
prepared and studied under the inert atmosphere of the glovebox.  The           
results thus far are said to suggest that the monomeric ion pairs might         
play an important kinetic role in reactions.                                    
                                                                                
It is proposed to extend such studies to additional enolates of interest        
and  to measure the reaction kinetics of their Aldol additions to               
aldehydes and  ketones and of Michael addition reactions with unsaturated       
carbonyl compounds.  It is indicated that the kinetic studies will show         
the state of aggregation of  the enolate reactive species and that              
knowledge of the relative roles of ion  pair monomers and aggregates will       
lead to more complete reaction mechanisms  and to the better                    
understanding required for sophisticated synthesis design.  The principal       
investigator notes that in particular, the roles of solvent  addends,           
such as lithium salts, hexamethylphosphoric triamide, and di- and               
triamines will be studied under carefully controlled conditions to              
determine  the role of coordination of lithium in the stereochemistry of        
the addition  reactions.                                                        
                                                                                
It is also proposed to apply the same techniques of spectroscopic study,        
proton transfer equilibria and reaction kinetics of Aldol and Michael           
addition  reactions to the dilithium salts of carboxylic acids and beta-        
diketones.  It  is noted that these dilithium salts are also being used         
increasingly in  organic synthesis but that the reaction mechanisms are         
virtually unstudied.  It is reported that these salts are also aggregated       
but nothing is known about  the relative reactivities of monomers and           
aggregates.  The proposed studies  are to provide unique information            
about these reactions, which would be  difficult to obtain in any other         
way.  It is suggested that subsequent  extension to other salts of alkali       
and alkaline earth metals, early and late  transition metals and                
lanthanides is also proposed since many of these salts  have found use          
in some stereospecific syntheses.                                               
",2608789,R01GM030369,['R01GM030369'],GM,https://reporter.nih.gov/project-details/2608789,R01,1998,164212,-0.09770154842228262
"We propose developing statistical fractal features which quantify lesion        
border roughness on mammograms and using these features to distinguish          
malignant and benign breast lesions.  Objective measures of lesion              
roughness are important in the diagnosis and staging of breast cancer.          
In this novel approach, we generate a space of fractal models, and              
evaluate the fractal dimension (fd) of the lesion from the statistics           
of the model space.                                                             
                                                                                
Using intensity as the third dimension, we generate a 3-dimensional             
image of the mammogram and select a set of intensity levels found in the        
lesion.  For each selected intensity level, we construct a binary               
thresholded image and trace the lesion border.  Analysis is restricted          
to segments of thresholded borders which are in high-gradient portions          
of the 3-dimensional image and can be determined with high precision.           
                                                                                
Each selected boundary segments is analyzed to identify self-affine             
subsegments which are modeled using multiple fractal interpolation              
functions having known fd values.  Thus we construct a large sample             
space of fd values which are computed from high-precision boundary              
traces occurring on a range of threshold levels.  The statistics of the         
fd sample space are the features used to distinguish between malignant          
and benign lesions.                                                             
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Our product will have significant value to the diagnostician who must           
distinguish malignant from benign breast lesions.  The algorithm is             
readily integrated into both computer-aided diagnosis systems and               
digital mammogram systems which display and process mammographic images         
for the expert diagnostician.                                                   
 artificial intelligence; breast neoplasm /cancer diagnosis; computer assisted diagnosis; computer program /software; computer system design /evaluation; diagnosis design /evaluation; mathematical model; neoplasm /cancer classification /staging; statistics /biometry FRACTAL FEATURES FOR DIAGNOSING MAMMOGRAPHIC MASSES","We propose developing statistical fractal features which quantify lesion        
border roughness on mammograms and using these features to distinguish          
malignant and benign breast lesions.  Objective measures of lesion              
roughness are important in the diagnosis and staging of breast cancer.          
In this novel approach, we generate a space of fractal models, and              
evaluate the fractal dimension (fd) of the lesion from the statistics           
of the model space.                                                             
                                                                                
Using intensity as the third dimension, we generate a 3-dimensional             
image of the mammogram and select a set of intensity levels found in the        
lesion.  For each selected intensity level, we construct a binary               
thresholded image and trace the lesion border.  Analysis is restricted          
to segments of thresholded borders which are in high-gradient portions          
of the 3-dimensional image and can be determined with high precision.           
                                                                                
Each selected boundary segments is analyzed to identify self-affine             
subsegments which are modeled using multiple fractal interpolation              
functions having known fd values.  Thus we construct a large sample             
space of fd values which are computed from high-precision boundary              
traces occurring on a range of threshold levels.  The statistics of the         
fd sample space are the features used to distinguish between malignant          
and benign lesions.                                                             
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Our product will have significant value to the diagnostician who must           
distinguish malignant from benign breast lesions.  The algorithm is             
readily integrated into both computer-aided diagnosis systems and               
digital mammogram systems which display and process mammographic images         
for the expert diagnostician.                                                   
",2651907,R43CA078108,['R43CA078108'],CA,https://reporter.nih.gov/project-details/2651907,R43,1998,93996,-0.12998730458869884
"DESCRIPTION (Taken from application abstract):  In the rapidly changing         
health care environment the increasing prominence of managed care has           
prompted a greater reliance on formal clinical guidelines to suppose            
clinical decisions.  Guidelines have been advocated with increasing             
frequency to reduce inappropriate care, control geographic variations in        
practice patterns and make more effective use of health care resources.         
However, guidelines often have little impact on clinical practice because       
physicians are unaware of them, lack confidence in them because the             
justification for their recommendations is not clear or because they are        
inaccessible at the time of patient care or difficult to apply.  Guidelines     
also quickly become out of date as new research data becomes available.         
                                                                                
In order to enhance the quality and usefulness of clinical guidelines, the      
general goal of this project is to develop, deploy and evaluate interactive     
computer based guidelines, supported by an integrated decision theoretic        
model and a linked knowledge base.  This arrangement will use patient           
characteristics to tailor guideline advice to the individual patient.  The      
integrated decision model will provide recommendations for situations not       
addressed by the guideline and also will help to justify guideline              
recommendations by calculating the effectiveness and costs of various           
strategies.  The proposed system also contains links to a knowledge base        
containing the sources of data used in the guideline and the model, so that     
physicians using the system can examine the studies supporting the guideline    
recommendations.  Natural language explanations, generated automatically        
based on the structure of the decision model and the guideline will justify     
the recommendations.  A query capability will enable physicians to look up      
specific data from the knowledge base.  The linked knowledge based also         
ensures that the supported decision model and guideline will be updated         
automatically as new research data is published.                                
                                                                                
The computer system will be used to implement an interactive version of the     
Guidelines for Medical Treatment for Stroke Prevention, developed by the        
American College of Physicians.  The system will be bench tested using a        
series of cases abstracted from the General Medicine practice at the Robert     
Wood Johnson Medical School.  Faculty and house staff internists will serve     
as research subjects to perform a field trial of the system.  This will         
include the extent of previous compliance with the guideline, pre and           
post-testing of medical knowledge pertinent to the guideline, and the degree    
to which the computer-based guideline changes behavior compared to the          
traditional guideline format.  A decision theoretic measure of potential        
benefit will be calculated by comparing decision model evaluations of           
physicians' unaided choices with those recommended by the guideline system.     
 Internet; artificial intelligence; cardiovascular disorder prevention; clinical research; computer assisted medical decision making; computer assisted patient care; computer human interaction; computer program /software; computer system design /evaluation; human data; human subject; information systems; medical records; physicians; questionnaires; stroke DECISION ANALYTIC SUPPORT FOR CLINICAL GUIDELINES","DESCRIPTION (Taken from application abstract):  In the rapidly changing         
health care environment the increasing prominence of managed care has           
prompted a greater reliance on formal clinical guidelines to suppose            
clinical decisions.  Guidelines have been advocated with increasing             
frequency to reduce inappropriate care, control geographic variations in        
practice patterns and make more effective use of health care resources.         
However, guidelines often have little impact on clinical practice because       
physicians are unaware of them, lack confidence in them because the             
justification for their recommendations is not clear or because they are        
inaccessible at the time of patient care or difficult to apply.  Guidelines     
also quickly become out of date as new research data becomes available.         
                                                                                
In order to enhance the quality and usefulness of clinical guidelines, the      
general goal of this project is to develop, deploy and evaluate interactive     
computer based guidelines, supported by an integrated decision theoretic        
model and a linked knowledge base.  This arrangement will use patient           
characteristics to tailor guideline advice to the individual patient.  The      
integrated decision model will provide recommendations for situations not       
addressed by the guideline and also will help to justify guideline              
recommendations by calculating the effectiveness and costs of various           
strategies.  The proposed system also contains links to a knowledge base        
containing the sources of data used in the guideline and the model, so that     
physicians using the system can examine the studies supporting the guideline    
recommendations.  Natural language explanations, generated automatically        
based on the structure of the decision model and the guideline will justify     
the recommendations.  A query capability will enable physicians to look up      
specific data from the knowledge base.  The linked knowledge based also         
ensures that the supported decision model and guideline will be updated         
automatically as new research data is published.                                
                                                                                
The computer system will be used to implement an interactive version of the     
Guidelines for Medical Treatment for Stroke Prevention, developed by the        
American College of Physicians.  The system will be bench tested using a        
series of cases abstracted from the General Medicine practice at the Robert     
Wood Johnson Medical School.  Faculty and house staff internists will serve     
as research subjects to perform a field trial of the system.  This will         
include the extent of previous compliance with the guideline, pre and           
post-testing of medical knowledge pertinent to the guideline, and the degree    
to which the computer-based guideline changes behavior compared to the          
traditional guideline format.  A decision theoretic measure of potential        
benefit will be calculated by comparing decision model evaluations of           
physicians' unaided choices with those recommended by the guideline system.     
",2460263,R01LM006321,['R01LM006321'],LM,https://reporter.nih.gov/project-details/2460263,R01,1998,237536,-0.023297369767514765
"The proposed mentored research award aims to develop the applicant's            
basic research skills and to prepare her for independent                        
investigations directed at exploring and dissecting the neural bases of         
motor and related dysfunctions in neurological disorders and normal             
aging using imaging techniques, psychophysical studies and brain                
network modeling. The primary mentor for this plan is Dr. David                 
Eidelberg, a neurologist with vast experience in the field of PET               
imaging and movement disorders. Dr. Claude Ghez, a neuroscientist               
who has worked in the field of motor control for over 25 years will             
serve as co-mentor. Dr. Glyn Johnson, experienced physicist in the              
field of magnetic resonance, will serve as consultant to instruct the           
applicant in functional MRI. Drs. Jim Moeller and Eva Petkova will              
further the applicant's education in advanced statistics. Dr. Vijay             
Dhawan will be consultant for PET biophysics and quantification. Dr.            
Ken Perrine, neuropsychologist, will also be available as consultant.           
                                                                                
Research plan: The goal of this study is to characterize changes in             
neural processing underlying implicit and explicit forms of motor               
learning in patients with Parkinson's disease and in a control normal           
aging population. A new family of motor tasks developed in Dr. Ghez's           
lab will be used during 150-H2O positron emission tomography. A new             
analytical methodology based on principal component analysis will be            
applied on measures of regional blood flow to determine how inter-              
subject differences in motor learning are reflected in modulations of           
brain network expression. These same methods will be used to                    
determine the neural bases of two therapeutic inventions in                     
Parkinson's disease, ventral pallidotomy and pallidal stimulation, and          
to assess their effectiveness in reversing alterations in motor                 
learning.                                                                       
                                                                                
Educational plan: The educational plan will focus on the following              
objectives: 1) Completion of the research plan. 2) Learning imaging             
techniques, including image acquisition and analysis using PET and              
fMRI. 3) Development of testing techniques to be used in fMRI. 4)               
Acquisition of skills in brain network analysis (including SSM) to be           
applied to both PET and NMR imaging. 5) Furthering the applicants               
statistical and computational skills through courses taken at Columbia          
University School of Public Health. 6) Developing the applicant's               
laboratory management, supervision, research communication and                  
mentoring skills. 7) Receiving instruction in the responsible conduct           
of research.                                                                    
 Parkinson's disease; blood flow measurement; brain electronic stimulator; brain imaging /visualization /scanning; clinical research; computational neuroscience; functional magnetic resonance imaging; human subject; learning; neural information processing; positron emission tomography; psychosurgery MOTOR LEARNING IN PARKINSONS DISEASE","The proposed mentored research award aims to develop the applicant's            
basic research skills and to prepare her for independent                        
investigations directed at exploring and dissecting the neural bases of         
motor and related dysfunctions in neurological disorders and normal             
aging using imaging techniques, psychophysical studies and brain                
network modeling. The primary mentor for this plan is Dr. David                 
Eidelberg, a neurologist with vast experience in the field of PET               
imaging and movement disorders. Dr. Claude Ghez, a neuroscientist               
who has worked in the field of motor control for over 25 years will             
serve as co-mentor. Dr. Glyn Johnson, experienced physicist in the              
field of magnetic resonance, will serve as consultant to instruct the           
applicant in functional MRI. Drs. Jim Moeller and Eva Petkova will              
further the applicant's education in advanced statistics. Dr. Vijay             
Dhawan will be consultant for PET biophysics and quantification. Dr.            
Ken Perrine, neuropsychologist, will also be available as consultant.           
                                                                                
Research plan: The goal of this study is to characterize changes in             
neural processing underlying implicit and explicit forms of motor               
learning in patients with Parkinson's disease and in a control normal           
aging population. A new family of motor tasks developed in Dr. Ghez's           
lab will be used during 150-H2O positron emission tomography. A new             
analytical methodology based on principal component analysis will be            
applied on measures of regional blood flow to determine how inter-              
subject differences in motor learning are reflected in modulations of           
brain network expression. These same methods will be used to                    
determine the neural bases of two therapeutic inventions in                     
Parkinson's disease, ventral pallidotomy and pallidal stimulation, and          
to assess their effectiveness in reversing alterations in motor                 
learning.                                                                       
                                                                                
Educational plan: The educational plan will focus on the following              
objectives: 1) Completion of the research plan. 2) Learning imaging             
techniques, including image acquisition and analysis using PET and              
fMRI. 3) Development of testing techniques to be used in fMRI. 4)               
Acquisition of skills in brain network analysis (including SSM) to be           
applied to both PET and NMR imaging. 5) Furthering the applicants               
statistical and computational skills through courses taken at Columbia          
University School of Public Health. 6) Developing the applicant's               
laboratory management, supervision, research communication and                  
mentoring skills. 7) Receiving instruction in the responsible conduct           
of research.                                                                    
",2771880,K08NS001961,['K08NS001961'],NS,https://reporter.nih.gov/project-details/2771880,K08,1998,100047,-0.04059576706968617
"DESCRIPTION (Taken from project abstract):  In the past two decades new         
imaging technology has given neurologists noninvasive tools which reveal the    
structure of the brain with a clarity that is little short of miraculous.       
At the same time, neuroscientists, often in animal studies, have developed      
ways to reveal hundreds of chemical and functional features of the brain        
that are relevant to human brain function.  The problem of integrating new      
knowledge to the benefit of human patients is exploding in both magnitude       
and complexity.                                                                 
                                                                                
This project addresses that problem by developing a Brain Information           
Management System for knowledge obtained from human and non-human primate       
research.  The system will allow the most precise possible indexation of        
written and pictorial information into a knowledge base that is accessible      
through the standard terminology of the National Library of Medicine's          
Unified Medical Language System.  Clinicians and neuroscientists anywhere in    
the country will be able to access the system via the World Wide Web to         
determine what is known about the involvement of any brain structure with       
any of the characteristics described in the neuroscientific knowledge base.     
The system will be evaluated by using it to identify neural pathways in the     
primate brain that are likely to mediate the rewarding effects of electrical    
stimulation of the brain.                                                       
                                                                                
The computerized Brain Information Management System is intended to             
accelerate the application of basic neuroscientific knowledge in the            
clinical discipline neurosurgery, neurology and neuropsychiatry.  The           
research project used to test the system holds promise of better                
understanding of drug abuse, depression and dementia.                           
 Macaca fascicularis; artificial intelligence; brain mapping; central neural pathway /tract; computational neuroscience; electrostimulus; information display; information retrieval; information systems; neural information processing; physical model; reinforcer; stereotaxic techniques; vocabulary development for information system SPATIAL/SYMBOLIC BRAIN INFORMATION MANAGEMENT SYSTEM","DESCRIPTION (Taken from project abstract):  In the past two decades new         
imaging technology has given neurologists noninvasive tools which reveal the    
structure of the brain with a clarity that is little short of miraculous.       
At the same time, neuroscientists, often in animal studies, have developed      
ways to reveal hundreds of chemical and functional features of the brain        
that are relevant to human brain function.  The problem of integrating new      
knowledge to the benefit of human patients is exploding in both magnitude       
and complexity.                                                                 
                                                                                
This project addresses that problem by developing a Brain Information           
Management System for knowledge obtained from human and non-human primate       
research.  The system will allow the most precise possible indexation of        
written and pictorial information into a knowledge base that is accessible      
through the standard terminology of the National Library of Medicine's          
Unified Medical Language System.  Clinicians and neuroscientists anywhere in    
the country will be able to access the system via the World Wide Web to         
determine what is known about the involvement of any brain structure with       
any of the characteristics described in the neuroscientific knowledge base.     
The system will be evaluated by using it to identify neural pathways in the     
primate brain that are likely to mediate the rewarding effects of electrical    
stimulation of the brain.                                                       
                                                                                
The computerized Brain Information Management System is intended to             
accelerate the application of basic neuroscientific knowledge in the            
clinical discipline neurosurgery, neurology and neuropsychiatry.  The           
research project used to test the system holds promise of better                
understanding of drug abuse, depression and dementia.                           
",2771701,R01LM006243,['R01LM006243'],LM,https://reporter.nih.gov/project-details/2771701,R01,1998,204510,-0.10826961778162025
"The long term goal of our research is to understand the flow of                 
information from the genome to the phenotype of organisms. In this              
proposal, we will attempt to use Bayesian networks and near-optimal             
sequence alignments to represent protein secondary structures and motifs.       
A Bayesian network describes the likelihood of amino acids at each              
position in a motif as well as the dependence of amino acids in one             
position on the amino acids at other position. Hence, Bayesian networks         
can describe both the conservation of amino acids at single positions and       
the conservation of correlations between two positions simultaneously.          
                                                                                
Conserved amino acids result from evolutionary selection for a specific         
amino acid or type of amino acid at one position in a protein structure.        
These positions often have important functional or structural                   
requirements. Correlated changes between amino acids generally result from      
side-chain side-chain interactions between pairs of amino acids in a            
protein's structure. The types of correlations we have represented with         
Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-      
bond donor and acceptor and inversely correlated packing volumes among          
others. These Bayesian networks can be used to 1) discover side-chain           
side--chain interactions within protei motifs and 2) to search sequence         
databases for motifs showing both correlations and conserved amino acids.       
                                                                                
Near-optimal alignments between two sequences can display regions that          
have been more highly conserved or less highly conserved using the              
information contained in only two sequences. The most highly conserved          
region correspond to the most highly structured regions and the most            
highly variable regions correspond to loops and coils and other                 
hypervariable regions. We propose to use near-optimal alignments to             
display conserved secondary structures of proteins and hypervariable            
regions. We will use secondary-structure specific amino acid substitution       
matrices to provide specificity.                                                
                                                                                
The goals of this proposal are to 1) build a database of Bayesian networks      
that represent protein motifs, 2) test these networks for their ability to      
detect motifs using test sets and crossvalidation methods, 3) compare           
these networks with other methods for searching protein databases , 4)          
build an integrated set of Bayesian networks to predict protein secondary       
structure, 5) compare the prediction of protein secondary structure with        
existing method 6) build a near-optimal sequence alignment workbench, and       
7) predict structured and unstructured regions in proteins from near-           
optimal alignments.                                                             
 artificial intelligence; biochemical evolution; chemical information system; computer assisted sequence analysis; hydrogen bond; hydropathy; ionic bond; model design /development; physical model; protein sequence; protein structure function; structural biology MULTIPLE REPRESENTATIONS OF BIOLOGICAL SEQUENCES","The long term goal of our research is to understand the flow of                 
information from the genome to the phenotype of organisms. In this              
proposal, we will attempt to use Bayesian networks and near-optimal             
sequence alignments to represent protein secondary structures and motifs.       
A Bayesian network describes the likelihood of amino acids at each              
position in a motif as well as the dependence of amino acids in one             
position on the amino acids at other position. Hence, Bayesian networks         
can describe both the conservation of amino acids at single positions and       
the conservation of correlations between two positions simultaneously.          
                                                                                
Conserved amino acids result from evolutionary selection for a specific         
amino acid or type of amino acid at one position in a protein structure.        
These positions often have important functional or structural                   
requirements. Correlated changes between amino acids generally result from      
side-chain side-chain interactions between pairs of amino acids in a            
protein's structure. The types of correlations we have represented with         
Bayesian networks include electrostatic charges, hydrophobicity, hydrogen-      
bond donor and acceptor and inversely correlated packing volumes among          
others. These Bayesian networks can be used to 1) discover side-chain           
side--chain interactions within protei motifs and 2) to search sequence         
databases for motifs showing both correlations and conserved amino acids.       
                                                                                
Near-optimal alignments between two sequences can display regions that          
have been more highly conserved or less highly conserved using the              
information contained in only two sequences. The most highly conserved          
region correspond to the most highly structured regions and the most            
highly variable regions correspond to loops and coils and other                 
hypervariable regions. We propose to use near-optimal alignments to             
display conserved secondary structures of proteins and hypervariable            
regions. We will use secondary-structure specific amino acid substitution       
matrices to provide specificity.                                                
                                                                                
The goals of this proposal are to 1) build a database of Bayesian networks      
that represent protein motifs, 2) test these networks for their ability to      
detect motifs using test sets and crossvalidation methods, 3) compare           
these networks with other methods for searching protein databases , 4)          
build an integrated set of Bayesian networks to predict protein secondary       
structure, 5) compare the prediction of protein secondary structure with        
existing method 6) build a near-optimal sequence alignment workbench, and       
7) predict structured and unstructured regions in proteins from near-           
optimal alignments.                                                             
",2771696,R01LM005716,['R01LM005716'],LM,https://reporter.nih.gov/project-details/2771696,R01,1998,222416,-0.17035510151251182
"THIS IS A SHANNON AWARD PROVIDING PARTIAL SUPPORT FOR THE RESEARCH              
PROJECTS THAT FALL SHORT OF THE ASSIGNED INSTITUTE'S FUNDING RANGE BUT          
ARE IN THE MARGIN OF EXCELLENCE. THE SHANNON AWARD IS INTENDED TO PROVIDE       
SUPPORT TO TEST THE FEASIBILITY OF THE APPROACH; DEVELOP FURTHER TESTS          
AND REFINE RESEARCH TECHNIQUES; PERFORM SECONDARY ANALYSIS OF AVAILABLE         
DATA SETS; OR CONDUCT DISCRETE PROJECTS THAT CAN DEMONSTRATE THE PI'S           
RESEARCH CAPABILITIES OR LEAD ADDITIONAL WEIGHT TO AN ALREADY MERITORIOUS       
APPLICATION. THE APPLICATION BELOW IS TAKEN FROM THE ORIGINAL DOCUMENT          
SUBMITTED BY THE PRINCIPAL INVESTIGATOR.                                        
                                                                                
This proposal describes research intended to support informatics                
requirements for the development, implementation, and evaluation of             
clinical practice guidelines. The work is expected to result both in the        
development of improved guidelines and, concomitantly, to produce               
structured, computer-accessible repositories of medical knowledge that          
can be used for decision support. A research program with the following         
4 specific aims is proposed:                                                    
                                                                                
(l) To devise and extend fundamental techniques for modeling and                
processing guideline knowledge using augmented decision tables. Methods         
will be defined to deal with guideline knowledge complexity and                 
uncertainty.                                                                    
                                                                                
(2) To refine and disseminate techniques by which knowledge can be              
acquired for clinical guidelines and verified to assure its consistency         
and comprehensiveness. Working with the American Academy of Pediatrics,         
newly developed guidelines will be logically analyzed and verified prior        
to publication.                                                                 
                                                                                
(3) To evaluate the effectiveness of a guideline-based, decision support        
device to improve compliance with a national guideline for outpatient           
management of asthma in children. A randomized, controlled trial will be        
conducted to measure the effect of the AsthMonitor system on guideline          
adherence, encounter documentation, patient outcomes, and costs of care.        
                                                                                
(4) To develop a suite of interactive computer applications that support        
decision making and clinical workflow for the management of a broad range       
of common pediatric problems. Knowledge gained from the clinical trial          
will be combined with workflow analysis, sound application design               
principles, and advanced digital technologies to create useful and usable       
tools for guideline implementation.                                             
                                                                                
Individual providers will benefit from the availability of decision             
support applications based on guidelines knowledge. From a public health        
perspective, effective implementation of evidence-based guidelines for          
the outpatient management of asthma and other common pediatric disorders        
should contribute to a reduction in inappropriate practice and to               
improved quality of care.                                                       
 adolescence (12-20); artificial intelligence; asthma; clinical research; computer assisted medical decision making; computer human interaction; decision making; health care professional practice; health care quality; health care service evaluation; human subject; middle childhood (6-11); outpatient care; patient care management; pediatrics; primary care physician KNOWLEDGE PROCESSING FOR CLINICAL PRACTICE GUIDELINES","THIS IS A SHANNON AWARD PROVIDING PARTIAL SUPPORT FOR THE RESEARCH              
PROJECTS THAT FALL SHORT OF THE ASSIGNED INSTITUTE'S FUNDING RANGE BUT          
ARE IN THE MARGIN OF EXCELLENCE. THE SHANNON AWARD IS INTENDED TO PROVIDE       
SUPPORT TO TEST THE FEASIBILITY OF THE APPROACH; DEVELOP FURTHER TESTS          
AND REFINE RESEARCH TECHNIQUES; PERFORM SECONDARY ANALYSIS OF AVAILABLE         
DATA SETS; OR CONDUCT DISCRETE PROJECTS THAT CAN DEMONSTRATE THE PI'S           
RESEARCH CAPABILITIES OR LEAD ADDITIONAL WEIGHT TO AN ALREADY MERITORIOUS       
APPLICATION. THE APPLICATION BELOW IS TAKEN FROM THE ORIGINAL DOCUMENT          
SUBMITTED BY THE PRINCIPAL INVESTIGATOR.                                        
                                                                                
This proposal describes research intended to support informatics                
requirements for the development, implementation, and evaluation of             
clinical practice guidelines. The work is expected to result both in the        
development of improved guidelines and, concomitantly, to produce               
structured, computer-accessible repositories of medical knowledge that          
can be used for decision support. A research program with the following         
4 specific aims is proposed:                                                    
                                                                                
(l) To devise and extend fundamental techniques for modeling and                
processing guideline knowledge using augmented decision tables. Methods         
will be defined to deal with guideline knowledge complexity and                 
uncertainty.                                                                    
                                                                                
(2) To refine and disseminate techniques by which knowledge can be              
acquired for clinical guidelines and verified to assure its consistency         
and comprehensiveness. Working with the American Academy of Pediatrics,         
newly developed guidelines will be logically analyzed and verified prior        
to publication.                                                                 
                                                                                
(3) To evaluate the effectiveness of a guideline-based, decision support        
device to improve compliance with a national guideline for outpatient           
management of asthma in children. A randomized, controlled trial will be        
conducted to measure the effect of the AsthMonitor system on guideline          
adherence, encounter documentation, patient outcomes, and costs of care.        
                                                                                
(4) To develop a suite of interactive computer applications that support        
decision making and clinical workflow for the management of a broad range       
of common pediatric problems. Knowledge gained from the clinical trial          
will be combined with workflow analysis, sound application design               
principles, and advanced digital technologies to create useful and usable       
tools for guideline implementation.                                             
                                                                                
Individual providers will benefit from the availability of decision             
support applications based on guidelines knowledge. From a public health        
perspective, effective implementation of evidence-based guidelines for          
the outpatient management of asthma and other common pediatric disorders        
should contribute to a reduction in inappropriate practice and to               
improved quality of care.                                                       
",2771694,R29LM005552,['R29LM005552'],LM,https://reporter.nih.gov/project-details/2771694,R29,1998,111212,-0.05931451748393117
"This application is a request for a NIH Research Scientist Development          
Award, Level II RSDA-II) to extend work supported under the RSDA-I              
previously awarded to the applicant (DA00139).  (The RSDA-I granted in 1989     
was titled ""Innovative Statistical Approaches to Drug Abuse Data""; this         
application for the RSDA-II is titled ""Drug Abuse: Epidemiology, Treatment      
Processes, and Outcomes."") The RSDA-Il will continue to ensure financial        
stability and release time from the pursuit of funding for actual research      
work.  The major focus for the applicant during this five-year award is         
continuation of her professional work examining drug use epidemiology and       
treatment interventions for problematic drug abuse. Examining the               
implications of research findings for treatment strategies and developing       
the necessary social policy changes to support the implementation of            
improved treatment strategies is of further interest. The applicant will        
continue her professional work applying innovative statistical                  
methodologies to drug abuse data. To this end, three convergent lines of        
current research will be continued. The first examines drug use and             
treatment utilization among subjects recruited through hospital emergency       
rooms sexually transmitted disease clinics, and jails.  The second project      
is to improve the efficacy and efficiency of matching drug users' treatment     
needs to services. The third involves examining and evaluating the process      
of treatment service delivery with a special focus on the roles and             
functions of drug treatment counselors.                                         
                                                                                
The applicant's supporting institution is a research unit, the                  
Neuropsychiatric Institute (NPI), organized within the Department of            
Psychiatry, School of Medicine, UCLA. Affiliated with the NPI is the UCLA       
Drug Abuse Research Center, which has been conducting research in drug          
abuse epidemiology natural history of narcotics addiction, treatment            
evaluation, and social policy over the past 20 years.  In this setting, the     
applicant will conduct the proposed research and will receive additional        
training in psychiatric aspects of drug abuse treatment and in the              
implementation of treatment services.  Furthermore. the applicant's             
considerable psychosocial research knowledge and skills in drug abuse           
issues will contribute to the NPI's general program in drug abuse research      
by complementing the Institute's biobehavioral perspective.                     
                                                                                
During the award period, the applicant also expects to grow professionally      
as Associate Director of the UCLA Drug Abuse Research Center. in addition       
to pursuing the aforementioned research. activities will include the career     
development of new investigators from various disciplines and the mentoring     
of graduate and undergraduate students in related fields.                       
 artificial intelligence; computer simulation; computer system design /evaluation; drug abuse therapy; drug addiction; health care service utilization; human subject; human therapy evaluation; interview; longitudinal human study; mental health counseling; outcomes research; prognosis; questionnaires; substance abuse epidemiology DRUG ABUSE--EPIDEMIOLOGY TREATMENT PROCESS AND OUTCOMES","This application is a request for a NIH Research Scientist Development          
Award, Level II RSDA-II) to extend work supported under the RSDA-I              
previously awarded to the applicant (DA00139).  (The RSDA-I granted in 1989     
was titled ""Innovative Statistical Approaches to Drug Abuse Data""; this         
application for the RSDA-II is titled ""Drug Abuse: Epidemiology, Treatment      
Processes, and Outcomes."") The RSDA-Il will continue to ensure financial        
stability and release time from the pursuit of funding for actual research      
work.  The major focus for the applicant during this five-year award is         
continuation of her professional work examining drug use epidemiology and       
treatment interventions for problematic drug abuse. Examining the               
implications of research findings for treatment strategies and developing       
the necessary social policy changes to support the implementation of            
improved treatment strategies is of further interest. The applicant will        
continue her professional work applying innovative statistical                  
methodologies to drug abuse data. To this end, three convergent lines of        
current research will be continued. The first examines drug use and             
treatment utilization among subjects recruited through hospital emergency       
rooms sexually transmitted disease clinics, and jails.  The second project      
is to improve the efficacy and efficiency of matching drug users' treatment     
needs to services. The third involves examining and evaluating the process      
of treatment service delivery with a special focus on the roles and             
functions of drug treatment counselors.                                         
                                                                                
The applicant's supporting institution is a research unit, the                  
Neuropsychiatric Institute (NPI), organized within the Department of            
Psychiatry, School of Medicine, UCLA. Affiliated with the NPI is the UCLA       
Drug Abuse Research Center, which has been conducting research in drug          
abuse epidemiology natural history of narcotics addiction, treatment            
evaluation, and social policy over the past 20 years.  In this setting, the     
applicant will conduct the proposed research and will receive additional        
training in psychiatric aspects of drug abuse treatment and in the              
implementation of treatment services.  Furthermore. the applicant's             
considerable psychosocial research knowledge and skills in drug abuse           
issues will contribute to the NPI's general program in drug abuse research      
by complementing the Institute's biobehavioral perspective.                     
                                                                                
During the award period, the applicant also expects to grow professionally      
as Associate Director of the UCLA Drug Abuse Research Center. in addition       
to pursuing the aforementioned research. activities will include the career     
development of new investigators from various disciplines and the mentoring     
of graduate and undergraduate students in related fields.                       
",2749013,K02DA000139,['K02DA000139'],DA,https://reporter.nih.gov/project-details/2749013,K02,1998,99340,-0.033997430628076884
"Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
 AIDS; Cryptococcus neoformans; antibody specificity; antifungal antibody; artificial intelligence; carbohydrate structure; cell wall; epitope mapping; fungal antigens; information systems; microorganism classification; nuclear magnetic resonance spectroscopy; opportunistic infections; polysaccharides CRYPTOCOCCUS NEOFORMANS--EPITOPE ANTIBODIES & STRUCTURE","Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
",2667723,R01AI031769,['R01AI031769'],AI,https://reporter.nih.gov/project-details/2667723,R01,1998,225752,-0.07041976447498796
"We propose to design efficient computer algorithms providing novel and/or       
improved methods and software for a number of computational problems in         
molecular biology.  The proposal and the principal investigator's current       
research efforts are divided into three projects, as follows.                   
                                                                                
The first project centers on computational problems in the sequencing of        
DNA and physical mapping of genomes.  We propose to continue refining our       
algorithms and software library for the fragment assembly problem, i.e.,        
determining the most likely complete DNA sequence consistent with               
electrophoresis data gathered from cloned fragments.  The refinements           
consist of improved algorithms for all phases of the computation: ultra-        
rapid overlap detection, assembly in the presence of constraints modeling       
additional experimental information, a formulation of the problem that          
correctly handles repetitive sequence, and a multi-alignment component          
that accommodates base-calling quality figures.  We also propose work on        
ordered shotgun sequencing (OSS) and a cDNA database for the data being         
generated at Washington University under contract to Merck.  Lastly, there      
is much similarity between fragment assembly and physical mapping, save         
that the relative level of experimental error is higher.  We propose an         
algorithm for STS content mapping based on rules-of-inference that are          
true with very high probability.                                                
                                                                                
The second project is to design better algorithms for a number of               
computational problems arising in molecular biology.  Progress in this          
arena tends to be inspired rather than calculated.  We demonstrate our          
track record of producing interesting results and then describe the             
following problems for which we have a number of ideas and preliminary          
results: sublinear Smith-Waterman database searches, determining                
restriction maps from digest data, grammar-based pattern matching,              
selecting PCR primers, predicting RNA secondary structure, and docking          
rigid molecules (3D matching).                                                  
                                                                                
The final project involves the introduction into our funded research            
activities of a new area, molecular graphics.  Earlier, we developed            
MacMolecule 1.7 which rendered space-filling, ball-and-stock, and wire-         
frame views of molecules.  We estimate 10,000 copies are currently in use       
around the world.  Our aim is high-speed, high-quality graphics, on low-        
end machines achieved by virtue of very efficient rendering algorithms.         
We have begun to develop a new version of MacMolecule, called Linus 2.0,        
which will deliver superior performance and visualizations along with           
greater capabilities, including ribbon renderings of protein secondary          
structure, zoom, hi-lighting, picking, and additional visualization modes.      
We further propose to build a helper application supporting ""Linus""             
content files so that Linus may be used with the World-Wide Web.  In            
further years, we will support molecular animation, construction, and           
possibly dynamics.                                                              
 Internet; artificial intelligence; computer assisted sequence analysis; computer graphics /printing; computer program /software; computer system design /evaluation; nucleic acid sequence; protein sequence EFFICIENT SOFTWARE FOR THE ANALYSIS OF BIOSEQUENCES","We propose to design efficient computer algorithms providing novel and/or       
improved methods and software for a number of computational problems in         
molecular biology.  The proposal and the principal investigator's current       
research efforts are divided into three projects, as follows.                   
                                                                                
The first project centers on computational problems in the sequencing of        
DNA and physical mapping of genomes.  We propose to continue refining our       
algorithms and software library for the fragment assembly problem, i.e.,        
determining the most likely complete DNA sequence consistent with               
electrophoresis data gathered from cloned fragments.  The refinements           
consist of improved algorithms for all phases of the computation: ultra-        
rapid overlap detection, assembly in the presence of constraints modeling       
additional experimental information, a formulation of the problem that          
correctly handles repetitive sequence, and a multi-alignment component          
that accommodates base-calling quality figures.  We also propose work on        
ordered shotgun sequencing (OSS) and a cDNA database for the data being         
generated at Washington University under contract to Merck.  Lastly, there      
is much similarity between fragment assembly and physical mapping, save         
that the relative level of experimental error is higher.  We propose an         
algorithm for STS content mapping based on rules-of-inference that are          
true with very high probability.                                                
                                                                                
The second project is to design better algorithms for a number of               
computational problems arising in molecular biology.  Progress in this          
arena tends to be inspired rather than calculated.  We demonstrate our          
track record of producing interesting results and then describe the             
following problems for which we have a number of ideas and preliminary          
results: sublinear Smith-Waterman database searches, determining                
restriction maps from digest data, grammar-based pattern matching,              
selecting PCR primers, predicting RNA secondary structure, and docking          
rigid molecules (3D matching).                                                  
                                                                                
The final project involves the introduction into our funded research            
activities of a new area, molecular graphics.  Earlier, we developed            
MacMolecule 1.7 which rendered space-filling, ball-and-stock, and wire-         
frame views of molecules.  We estimate 10,000 copies are currently in use       
around the world.  Our aim is high-speed, high-quality graphics, on low-        
end machines achieved by virtue of very efficient rendering algorithms.         
We have begun to develop a new version of MacMolecule, called Linus 2.0,        
which will deliver superior performance and visualizations along with           
greater capabilities, including ribbon renderings of protein secondary          
structure, zoom, hi-lighting, picking, and additional visualization modes.      
We further propose to build a helper application supporting ""Linus""             
content files so that Linus may be used with the World-Wide Web.  In            
further years, we will support molecular animation, construction, and           
possibly dynamics.                                                              
",2635404,R01LM004960,['R01LM004960'],LM,https://reporter.nih.gov/project-details/2635404,R01,1998,157399,-0.025508474731551593
"The goal of the proposed research is to develop computer-aided diagnostic       
(CAD) schemes for detection of lung nodules, interstitial infiltrates, and      
pneumothoraces in digital chest images. We plan to develop advanced             
computerized schemes and software for improvements in sensitivity,              
specificity and efficiency in order to implement and evaluate such schemes      
in a controlled clinical environment. We believe that these computer-aided      
diagnostic schemes, which provide the radiologist with the location and/or      
quantitative measures of highly suspected lesions, have the potential to        
improve diagnostic accuracy in the detection of cancer by reducing human        
errors associated with radiologic diagnoses.                                    
                                                                                
Specifically, we plan to (l) develop an improved scheme for automated           
detection of lung nodules by (a) combinations of linear and nonlinear           
morphological filtering techniques based on a difference image method for       
enhancement and suppression of lung nodules, (b) reduction of false             
positive detections by detailed analysis of image features by chest             
radiologists and also use of artificial neural networks, (c) analysis of        
posterior ribs for reduction of false positives, (d) application of             
wavelet transform for increasing the sensitivity, and (e) observer              
performance studies for optimal use of CAD methods; (2) develop an              
improved scheme for automated lung texture analysis by (a) devising an          
automated technique for sampling numerous regions of interest (ROIs) in         
the lung fields, (b) investigation of new texture measures based on             
analysis of the shape and anisotropic properties of the power spectrum of       
lung textures, and (c) application of artificial neural networks for            
detection and classification of interstitial infiltrates; (3) develop an        
automated scheme for detection of pneumothorax by (a) application of the        
Hough transform in conjunction with an edge enhancement technique for           
detection of subtle curved lines, and (b) ROC analysis of radiologists'         
performances for evaluation of the usefulness of the CAD scheme; and (4)        
implement and evaluate the CAD schemes in a high-resolution. high-speed         
image processing system by (a) development of a prototype intelligent           
workstation with efficient algorithms and efficient man-machine                 
interfaces, and (b) carrying out pilot studies on clinical evaluation of        
our chest CAD schemes in comparison with conventional readings in terms of      
the three types of abnormalities related to lung nodules, interstitial          
infiltrates, and pneumothoraces.                                                
 artificial intelligence; computer assisted diagnosis; computer human interaction; computer system design /evaluation; digital imaging; disease /disorder classification; human data; image processing; lung neoplasms; neoplasm /cancer radiodiagnosis; pneumothorax disorder; thoracic radiography COMPUTER AIDED DIAGNOSIS IN CHEST RADIOGRAPHY","The goal of the proposed research is to develop computer-aided diagnostic       
(CAD) schemes for detection of lung nodules, interstitial infiltrates, and      
pneumothoraces in digital chest images. We plan to develop advanced             
computerized schemes and software for improvements in sensitivity,              
specificity and efficiency in order to implement and evaluate such schemes      
in a controlled clinical environment. We believe that these computer-aided      
diagnostic schemes, which provide the radiologist with the location and/or      
quantitative measures of highly suspected lesions, have the potential to        
improve diagnostic accuracy in the detection of cancer by reducing human        
errors associated with radiologic diagnoses.                                    
                                                                                
Specifically, we plan to (l) develop an improved scheme for automated           
detection of lung nodules by (a) combinations of linear and nonlinear           
morphological filtering techniques based on a difference image method for       
enhancement and suppression of lung nodules, (b) reduction of false             
positive detections by detailed analysis of image features by chest             
radiologists and also use of artificial neural networks, (c) analysis of        
posterior ribs for reduction of false positives, (d) application of             
wavelet transform for increasing the sensitivity, and (e) observer              
performance studies for optimal use of CAD methods; (2) develop an              
improved scheme for automated lung texture analysis by (a) devising an          
automated technique for sampling numerous regions of interest (ROIs) in         
the lung fields, (b) investigation of new texture measures based on             
analysis of the shape and anisotropic properties of the power spectrum of       
lung textures, and (c) application of artificial neural networks for            
detection and classification of interstitial infiltrates; (3) develop an        
automated scheme for detection of pneumothorax by (a) application of the        
Hough transform in conjunction with an edge enhancement technique for           
detection of subtle curved lines, and (b) ROC analysis of radiologists'         
performances for evaluation of the usefulness of the CAD scheme; and (4)        
implement and evaluate the CAD schemes in a high-resolution. high-speed         
image processing system by (a) development of a prototype intelligent           
workstation with efficient algorithms and efficient man-machine                 
interfaces, and (b) carrying out pilot studies on clinical evaluation of        
our chest CAD schemes in comparison with conventional readings in terms of      
the three types of abnormalities related to lung nodules, interstitial          
infiltrates, and pneumothoraces.                                                
",2667970,R01CA062625,['R01CA062625'],CA,https://reporter.nih.gov/project-details/2667970,R01,1998,362127,-0.019733054290804478
"DESCRIPTION (Adapted from applicant's abstract):  In Phase I CW Optics          
demonstrated the ability to apply differential absorption spectroscopy          
and multiple scattering techniques to the measurement of hemoglobin             
moieties. The objective for Phase II is the development of a noninvasive        
real-time blood sensor for the measurement of levels of four major              
hemoglobin moieties and hematocrit.  The moieties to be measured are            
levels of oxyhemoglobin (O2Hb) reduced or deoxyhemoglobin (RHb),                
carboxyhemoglobin (COHb) and methemoglobin (MetHb).  It also became             
evident that the approach could also be used to estimate hematocrit.            
In Phase II it is planned to modify and improve the Phase I algorithms          
to incorporate hematocrit measurement, to manufacture prototype devices         
and demonstrate the efficiency of this device in a clinical study and           
then to explore a suitable design for a commercial instrument.  The             
effects of band width on the accuracy of the measurements will be               
examined.  It is also planned to produce an integrated sensor for               
noninvasive real-time monitoring of hematocrit and the individual Hb            
components in a clinical setting.  Such real time noninvasive monitoring        
of hemoglobin components and hematocrit is very attractive in that it           
avoids blood sampling, reduces the therapeutic decision time and also           
eliminates the risk of infection.  This type of instrumentation clearly         
has commercial potential and should be of interest to an assortment of          
health care providers.                                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION: Not available.                                 
 artificial intelligence; biomedical equipment development; biotechnology; blood cell count; carboxyhemoglobin; diagnosis design /evaluation; hemoglobin; human tissue; methemoglobin; noninvasive diagnosis; optics; oxyhemoglobin; spectrometry NONINVASIC OPTICAL SENSOR FOR HEMOGLOBIN MEASUREMENTS","DESCRIPTION (Adapted from applicant's abstract):  In Phase I CW Optics          
demonstrated the ability to apply differential absorption spectroscopy          
and multiple scattering techniques to the measurement of hemoglobin             
moieties. The objective for Phase II is the development of a noninvasive        
real-time blood sensor for the measurement of levels of four major              
hemoglobin moieties and hematocrit.  The moieties to be measured are            
levels of oxyhemoglobin (O2Hb) reduced or deoxyhemoglobin (RHb),                
carboxyhemoglobin (COHb) and methemoglobin (MetHb).  It also became             
evident that the approach could also be used to estimate hematocrit.            
In Phase II it is planned to modify and improve the Phase I algorithms          
to incorporate hematocrit measurement, to manufacture prototype devices         
and demonstrate the efficiency of this device in a clinical study and           
then to explore a suitable design for a commercial instrument.  The             
effects of band width on the accuracy of the measurements will be               
examined.  It is also planned to produce an integrated sensor for               
noninvasive real-time monitoring of hematocrit and the individual Hb            
components in a clinical setting.  Such real time noninvasive monitoring        
of hemoglobin components and hematocrit is very attractive in that it           
avoids blood sampling, reduces the therapeutic decision time and also           
eliminates the risk of infection.  This type of instrumentation clearly         
has commercial potential and should be of interest to an assortment of          
health care providers.                                                          
                                                                                
PROPOSED COMMERCIAL APPLICATION: Not available.                                 
",2643586,R44HL055120,['R44HL055120'],HL,https://reporter.nih.gov/project-details/2643586,R44,1998,189965,-0.0798091472167197
"DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
 artificial intelligence; behavioral /social science research tag; computer program /software; computer system design /evaluation; digital imaging; emotions; face expression; human subject; image processing; interpersonal relations; statistics /biometry; videotape /videodisc; visual tracking FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING","DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
",2675154,R01MH051435,['R01MH051435'],MH,https://reporter.nih.gov/project-details/2675154,R01,1998,220716,-0.046781019031206836
"DESCRIPTION (adapted from the Abstract):  Over the past decade,                 
heterosexual transmission of HIV has become an increasingly important           
public health problem.  Effective behavioral interventions to increase          
condom use are needed.  Such interventions should be theory-based and           
capable of being delivered at low cost to large segments of the at-risk         
population.  The proposed study provides an evaluation of the efficacy          
of an intervention based on the conceptual framework of the                     
Transtheoretic Model.  The intervention is designed to increase condom          
use and readiness to use condoms in at-risk, heterosexually-active women        
and men.  The intervention is computer-delivered using expert systems           
technology and will be provided in health care settings.  Participants          
will be recruited from four heath clinic sties that serve local ethnic          
minority communities and will be randomly assigned by a computer to             
either a stage-matched, individualized expert system intervention, or           
to an HIV-information comparison group.  At the final time point, 400           
economically disadvantaged women and men who are at risk for HIV                
infection will be in the study.  Assessment for both groups will be             
conducted at baseline, 6, 12, and 18 months post-intervention.  All             
participant will receive comparable group-specific intervention                 
materials at baseline, 2 and 4 months.  Specifically, participants              
randomized to the stage-matched treatment group will receive                    
individualized, stage-matched feedback and stage-tailored manuals, and          
participants randomized to the HIV information comparison group will            
receive HIV information feedback and the best available informational           
manual.  Incentives for participation and alternative methods for               
collecting data will be used to maximize participant retention.  A              
secondary aim of the proposed project is to examine additional                  
psychosocial mediators of condom use by testing the predictive efficacy         
of the Multifaceted Model of HIV risk.  An important objective of the           
study is to increase the understanding of sexual behavior change and to         
demonstrate the efficacy of a promising intervention technology for             
increasing condom use.  The study will also provide longitudinal outcome        
data on condom adoption in at-risk heterosexual men and women.  Research        
on innovative approaches to sexual health promotion is central to               
achieving the Year 2000 goal of reducing HIV transmission and mortality.        
 AIDS education /prevention; behavior modification; behavioral /social science research tag; clinical research; condoms; health science research analysis /evaluation; heterosexuals; human subject; longitudinal human study; psychological models; sex behavior INCREASING CONDOM USE WITH A STAGE MATCHED INTERVENTION","DESCRIPTION (adapted from the Abstract):  Over the past decade,                 
heterosexual transmission of HIV has become an increasingly important           
public health problem.  Effective behavioral interventions to increase          
condom use are needed.  Such interventions should be theory-based and           
capable of being delivered at low cost to large segments of the at-risk         
population.  The proposed study provides an evaluation of the efficacy          
of an intervention based on the conceptual framework of the                     
Transtheoretic Model.  The intervention is designed to increase condom          
use and readiness to use condoms in at-risk, heterosexually-active women        
and men.  The intervention is computer-delivered using expert systems           
technology and will be provided in health care settings.  Participants          
will be recruited from four heath clinic sties that serve local ethnic          
minority communities and will be randomly assigned by a computer to             
either a stage-matched, individualized expert system intervention, or           
to an HIV-information comparison group.  At the final time point, 400           
economically disadvantaged women and men who are at risk for HIV                
infection will be in the study.  Assessment for both groups will be             
conducted at baseline, 6, 12, and 18 months post-intervention.  All             
participant will receive comparable group-specific intervention                 
materials at baseline, 2 and 4 months.  Specifically, participants              
randomized to the stage-matched treatment group will receive                    
individualized, stage-matched feedback and stage-tailored manuals, and          
participants randomized to the HIV information comparison group will            
receive HIV information feedback and the best available informational           
manual.  Incentives for participation and alternative methods for               
collecting data will be used to maximize participant retention.  A              
secondary aim of the proposed project is to examine additional                  
psychosocial mediators of condom use by testing the predictive efficacy         
of the Multifaceted Model of HIV risk.  An important objective of the           
study is to increase the understanding of sexual behavior change and to         
demonstrate the efficacy of a promising intervention technology for             
increasing condom use.  The study will also provide longitudinal outcome        
data on condom adoption in at-risk heterosexual men and women.  Research        
on innovative approaches to sexual health promotion is central to               
achieving the Year 2000 goal of reducing HIV transmission and mortality.        
",2718250,R01AI041323,['R01AI041323'],AI,https://reporter.nih.gov/project-details/2718250,R01,1998,452629,-0.018010272736929858
"Automated screening of Pap smear slides is challenging due the high             
processing and data transfer requirements placed on the processing              
engine. These requirements can be significantly reduced by processing           
the images at the image plane of the camera, and reading out only the           
relevant data, which results in lower cost and higher performance               
systems. Image sensors with smarts or computational capability at each          
pixel can be advantageously used in the application to build extremely          
compact and low-cost automated screening systems. Morphological                 
filtering algorithms have been shown to be effective at detecting object        
size and shape, which are distinguishing features in diagnostic                 
microscopy. The goal of this research is to design, simulate and                
fabricate a CMOS chip with morphological filtering circuits at each             
pixel, which will allow detecting suspicious cells in a Pap Smear at            
more than 1000 frames/second. In Phase I, Bosonics will (1) determine           
the desired imager's technical capabilities for a compact microscope            
mountable smart camera (2) design candidate morphological filtering             
architectures and circuits; (3) simulate the morphological algorithms           
with realistic circuits models; and (4) design and fabricate a 5x5              
imager demonstration chip in CMOS.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The chips produced under this program will lower the cost of machine            
vision systems by providing an integrated detector/processing function          
as well as increasing performance by reducing the output bandwidth              
requirements. The detector arrays developed under this program will have        
wide application in automatic target recognition, machine vision for            
automated manufacturing, medical diagnostic imaging, and remote sensing         
and surveillance.                                                               
 artificial intelligence; biomedical automation; biomedical equipment development; cervical /vaginal smear; computer simulation; computer system design /evaluation; cytology; image processing; morphology; photomicrography CAMERA TO DETECT REGIONS OF INTEREST IN A PAP SMEAR","Automated screening of Pap smear slides is challenging due the high             
processing and data transfer requirements placed on the processing              
engine. These requirements can be significantly reduced by processing           
the images at the image plane of the camera, and reading out only the           
relevant data, which results in lower cost and higher performance               
systems. Image sensors with smarts or computational capability at each          
pixel can be advantageously used in the application to build extremely          
compact and low-cost automated screening systems. Morphological                 
filtering algorithms have been shown to be effective at detecting object        
size and shape, which are distinguishing features in diagnostic                 
microscopy. The goal of this research is to design, simulate and                
fabricate a CMOS chip with morphological filtering circuits at each             
pixel, which will allow detecting suspicious cells in a Pap Smear at            
more than 1000 frames/second. In Phase I, Bosonics will (1) determine           
the desired imager's technical capabilities for a compact microscope            
mountable smart camera (2) design candidate morphological filtering             
architectures and circuits; (3) simulate the morphological algorithms           
with realistic circuits models; and (4) design and fabricate a 5x5              
imager demonstration chip in CMOS.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The chips produced under this program will lower the cost of machine            
vision systems by providing an integrated detector/processing function          
as well as increasing performance by reducing the output bandwidth              
requirements. The detector arrays developed under this program will have        
wide application in automatic target recognition, machine vision for            
automated manufacturing, medical diagnostic imaging, and remote sensing         
and surveillance.                                                               
",2715812,R43CA079295,['R43CA079295'],CA,https://reporter.nih.gov/project-details/2715812,R43,1998,93605,-0.016554631297549496
"The research and development of teleradiology and telemedicine systems          
has progressed through many technical and clinical endeavors. When              
dealing with large volume image transmission and storage, lossy data            
compression is an outstanding issue in medical applications to which            
current techniques were not designed to address. The technical                  
objectives of this SBIR project are to develop a lossless as well as            
an error-controllable compression scheme for medical image compression.         
                                                                                
Based on our recent research, we found that any wavelet transform can           
be implemented in an integer form prior to the coding. We would like            
to study the technical advantages by using the integer wavelet                  
decomposition method and to develop software as compression tools for           
clinical images such as chest radiographs, CT, and MR images. We will           
compare the compression results (i.e., compression ratio and                    
computation speed) of the proposed compression method with those of             
the current wavelet compression method (e.g., embedded zero-tree for            
wavelet compression). In Phase II, the research and development will            
be extended to 3-D slab wavelet transform and adaptive wavelet                  
compression for the optimization of this SBIR study. Other related              
software functions and the user interface will be included to                   
facilitate the use of the system.                                               
                                                                                
As the field of telemedicine is rapidly growing, we believe that the            
development of dedicated compression module for economical storage and          
fast communication of patient data (particularly for patient images)            
is necessary. This SBIR is designed to address the related technical            
issues with a strong clinical consideration.                                    
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
This development will be inseparable form the field of telemedicine as          
a whole in the future. The commercial potential of the proposed                 
product is very high.                                                           
 artificial intelligence; biomedical equipment development; computed axial tomography; computer program /software; computer system design /evaluation; data management; human data; image processing; magnetic resonance imaging; telemedicine; thoracic radiography LOSSLESS AND ERROR CONTROLLABLE INTEGER WAVELET CODING","The research and development of teleradiology and telemedicine systems          
has progressed through many technical and clinical endeavors. When              
dealing with large volume image transmission and storage, lossy data            
compression is an outstanding issue in medical applications to which            
current techniques were not designed to address. The technical                  
objectives of this SBIR project are to develop a lossless as well as            
an error-controllable compression scheme for medical image compression.         
                                                                                
Based on our recent research, we found that any wavelet transform can           
be implemented in an integer form prior to the coding. We would like            
to study the technical advantages by using the integer wavelet                  
decomposition method and to develop software as compression tools for           
clinical images such as chest radiographs, CT, and MR images. We will           
compare the compression results (i.e., compression ratio and                    
computation speed) of the proposed compression method with those of             
the current wavelet compression method (e.g., embedded zero-tree for            
wavelet compression). In Phase II, the research and development will            
be extended to 3-D slab wavelet transform and adaptive wavelet                  
compression for the optimization of this SBIR study. Other related              
software functions and the user interface will be included to                   
facilitate the use of the system.                                               
                                                                                
As the field of telemedicine is rapidly growing, we believe that the            
development of dedicated compression module for economical storage and          
fast communication of patient data (particularly for patient images)            
is necessary. This SBIR is designed to address the related technical            
issues with a strong clinical consideration.                                    
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
This development will be inseparable form the field of telemedicine as          
a whole in the future. The commercial potential of the proposed                 
product is very high.                                                           
",2715093,R43CA079289,['R43CA079289'],CA,https://reporter.nih.gov/project-details/2715093,R43,1998,94176,-0.06748208361455944
"The notion of clinical significance permeates evaluations of clinical           
research results, plays an important role in changing physician behavior,       
and enters implicitly into the development of rules for knowledge-based         
computer systems. To date, however, this concept has received no formal         
examination and is left to the intuition of the reasoning agent:. The aims      
of this project are (1) to explore the role that clinical significance          
plays in physicians' evaluation of clinical-research results; (2) to            
propose formal methods that satisfy those roles; (3) to create a computer-      
based system that would implement those methods; and (4) to validate the        
proposed methods. As the test domain, we shall focus on the choice between      
two therapies as evaluated in randomized clinical trials.                       
                                                                                
(1) We shall explore, through a survey process, three central questions         
regarding clinical significance: physicians' perception of its relative         
importance in evaluating study results, physicians' need for assistance in      
its evaluation, and the variability among physicians in its assessment.         
                                                                                
(2) We shall determine measures within three formal frameworks for              
expressing the factors that the survey ascertained as important. The            
frameworks are frequentist statistics, Bayesian statistics, and decision        
theory.                                                                         
                                                                                
The central challenge in applying these formal frameworks and their             
measures into the clinical setting are the difficulty physicians have with      
numerical measures, regardless of framework. To meet this challenge, we         
propose to develop accessible methods for using the frameworks and to           
develop computer-based tools for using the methods.                             
                                                                                
The strategy we propose for developing accessible methods is to establish       
canonical models within each framework. We plan to propose a set of             
canonical decision models implicit in conclusions of clinical studies and       
to validate the set by a review of a random set of clinical trials gleaned      
from the medical research literature. To use these models, physicians need      
novel nonnumerical methods-graphical and qualitative techniques- that           
translate the statistical results into clinically meaningful terms and          
concepts.                                                                       
                                                                                
(3) Because these novel methods will be computationally more difficult          
than current practice, we propose to construct a set of computer-based          
tools that will implement them. As basis for building these tools, we           
shall use a set of novel artificial intelligence techniques.                    
                                                                                
(4) We propose to validate each of the following propositions: that             
clinicians can use these advanced methods, that they prefer one framework       
over the other, and that use of these methods reduces or explains the           
variability established by the questionnaire process.                           
 artificial intelligence; clinical trials; computer assisted medical decision making; health care model; health science research analysis /evaluation; human data; method development; physicians; questionnaires; statistics /biometry FORMALIZING THE NOTION OF CLINICAL SIGNIFICANCE","The notion of clinical significance permeates evaluations of clinical           
research results, plays an important role in changing physician behavior,       
and enters implicitly into the development of rules for knowledge-based         
computer systems. To date, however, this concept has received no formal         
examination and is left to the intuition of the reasoning agent:. The aims      
of this project are (1) to explore the role that clinical significance          
plays in physicians' evaluation of clinical-research results; (2) to            
propose formal methods that satisfy those roles; (3) to create a computer-      
based system that would implement those methods; and (4) to validate the        
proposed methods. As the test domain, we shall focus on the choice between      
two therapies as evaluated in randomized clinical trials.                       
                                                                                
(1) We shall explore, through a survey process, three central questions         
regarding clinical significance: physicians' perception of its relative         
importance in evaluating study results, physicians' need for assistance in      
its evaluation, and the variability among physicians in its assessment.         
                                                                                
(2) We shall determine measures within three formal frameworks for              
expressing the factors that the survey ascertained as important. The            
frameworks are frequentist statistics, Bayesian statistics, and decision        
theory.                                                                         
                                                                                
The central challenge in applying these formal frameworks and their             
measures into the clinical setting are the difficulty physicians have with      
numerical measures, regardless of framework. To meet this challenge, we         
propose to develop accessible methods for using the frameworks and to           
develop computer-based tools for using the methods.                             
                                                                                
The strategy we propose for developing accessible methods is to establish       
canonical models within each framework. We plan to propose a set of             
canonical decision models implicit in conclusions of clinical studies and       
to validate the set by a review of a random set of clinical trials gleaned      
from the medical research literature. To use these models, physicians need      
novel nonnumerical methods-graphical and qualitative techniques- that           
translate the statistical results into clinically meaningful terms and          
concepts.                                                                       
                                                                                
(3) Because these novel methods will be computationally more difficult          
than current practice, we propose to construct a set of computer-based          
tools that will implement them. As basis for building these tools, we           
shall use a set of novel artificial intelligence techniques.                    
                                                                                
(4) We propose to validate each of the following propositions: that             
clinicians can use these advanced methods, that they prefer one framework       
over the other, and that use of these methods reduces or explains the           
variability established by the questionnaire process.                           
",2714214,R29LM005647,['R29LM005647'],LM,https://reporter.nih.gov/project-details/2714214,R29,1998,125131,-0.17605784763552695
"DESCRIPTION (Adapted from applicant's abstract):  This research will            
determine the feasibility of automatically detecting the borders of the         
left ventricle (LV) from a three-dimensional (3D) echocardiographic             
scan.  The goal is to delineate the endocardium and epicardium as 3D            
surfaces for use in measuring cardiac parameters.  The approach is to           
fit a 3D model of the LV and associated structures to the image data.           
The hypothesis is that incorporating knowledge of the range of heart            
shapes observed among normal subjects and patients with diseased hearts         
into a 3D model will facilitate automated border detection (ABD) by             
constraining the process to only yield heart-like shapes.  The                  
investigators have already developed the computer algorithms for                
reconstructing the LV endocardial and epicardial surfaces in 3D and for         
calculating the mean and covariance matrix for the 3D shape of the LV           
based on data acquired from a sample of 3D reconstructions. Additional          
3D scans of 51 normal subjects and 35 patients with diseased hearts have        
been acquired and partially analyzed.  The specific aims of the proposed        
work are:  1) to complete the ultrasound model of the gray scale                
appearance of the heart in any plane:  the right ventricle, pericardium,        
and papillary muscles will be added to the current LV model; 2) to              
complete the algorithms for feature extraction from images, and for mesh        
optimization the procedure using the extracted features to fit the 3D           
model to the patients image data; and 3) to demonstrate the feasibility         
of the ABD process using simulated images.  In Phase II these components        
of the 3D ultrasound border detection process will be tested and                
modified as necessary to yield clinical acceptable levels of accuracy           
and performance.  In Phase III a quantitative 3D echocardiographic              
system will be developed by installing the ABD algorithm into a hardware        
and software system for acquiring and analyzing 3D echo scans of the            
heart.  This research is important because the improved accuracy                
afforded by 3D echo cannot be applied to clinical practice without              
automation.                                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The proposed commercial application for this grant is to develop a              
hardware and software system for acquiring three dimensional                    
echocardiograms and for performing quantitative analysis of the size,           
shape, and function of the left ventricle.                                      
 artificial intelligence; biomedical automation; biomedical equipment development; computer system design /evaluation; diagnosis design /evaluation; echocardiography; heart disorder diagnosis; heart ventricle; image processing; papillary muscles; pericardium AUTOMATED QUANTITATION OF 3D ECHOCARDIOGRAMS","DESCRIPTION (Adapted from applicant's abstract):  This research will            
determine the feasibility of automatically detecting the borders of the         
left ventricle (LV) from a three-dimensional (3D) echocardiographic             
scan.  The goal is to delineate the endocardium and epicardium as 3D            
surfaces for use in measuring cardiac parameters.  The approach is to           
fit a 3D model of the LV and associated structures to the image data.           
The hypothesis is that incorporating knowledge of the range of heart            
shapes observed among normal subjects and patients with diseased hearts         
into a 3D model will facilitate automated border detection (ABD) by             
constraining the process to only yield heart-like shapes.  The                  
investigators have already developed the computer algorithms for                
reconstructing the LV endocardial and epicardial surfaces in 3D and for         
calculating the mean and covariance matrix for the 3D shape of the LV           
based on data acquired from a sample of 3D reconstructions. Additional          
3D scans of 51 normal subjects and 35 patients with diseased hearts have        
been acquired and partially analyzed.  The specific aims of the proposed        
work are:  1) to complete the ultrasound model of the gray scale                
appearance of the heart in any plane:  the right ventricle, pericardium,        
and papillary muscles will be added to the current LV model; 2) to              
complete the algorithms for feature extraction from images, and for mesh        
optimization the procedure using the extracted features to fit the 3D           
model to the patients image data; and 3) to demonstrate the feasibility         
of the ABD process using simulated images.  In Phase II these components        
of the 3D ultrasound border detection process will be tested and                
modified as necessary to yield clinical acceptable levels of accuracy           
and performance.  In Phase III a quantitative 3D echocardiographic              
system will be developed by installing the ABD algorithm into a hardware        
and software system for acquiring and analyzing 3D echo scans of the            
heart.  This research is important because the improved accuracy                
afforded by 3D echo cannot be applied to clinical practice without              
automation.                                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The proposed commercial application for this grant is to develop a              
hardware and software system for acquiring three dimensional                    
echocardiograms and for performing quantitative analysis of the size,           
shape, and function of the left ventricle.                                      
",2714130,R41HL059054,['R41HL059054'],HL,https://reporter.nih.gov/project-details/2714130,R41,1998,100000,-0.012907773174617181
"LONG-TERM OBJECTIVES 1. Develop a computerized system, based on                 
hierarchical neural network pattern recognition technology, for reliable        
identification of plants.  2. Identify poisonous plants. 3. Expedite            
discovery of new medicinal plants.  4. Create an image database directly        
from plant material and link with existing medicinal plant databases.           
5. Develop commercial product for pharmaceutical companies, agriculture         
and others. SPECIFIC AIMS 1. Design hierarchical system of neural               
networks to follow natural plant taxonomy groupings and extend our              
identification technology to a large number of plant species.  2.               
Improve accuracy of identification.  3. Design a prototype workstation          
for botanical and agricultural field stations and laboratories.                 
                                                                                
RESEARCH DESIGN AND METHODS FOR ACHIEVING GOALS.  1. Digitize large             
number of plant species from special collections.  2. Measure                   
automatically venation patterns and shape.  3. Design hierarchical              
neural networks to divide plants into natural groupings.  4. Accumulate         
virtual herbarium  database as leaves are digitized (scanned or                 
photographed).                                                                  
                                                                                
POTENTIAL FOR TECHNOLOGICAL INNOVATION This system is unique in                 
capturing botanical recognition knowledge in a hierarchy of neural              
networks and is the first fully-computerized system for plant                   
identification utilizing information digitized directly from plants.            
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
1. Expedite discovery of new medicinal plants for pharmaceutical                
industry.  2. Create valuable database directly from plants.  3.                
Identification of poisonous plants.  4. Valuable for rapid                      
identification of invasive weeds.                                               
 artificial intelligence; carcinogens; classification; digital imaging; geographic site; information systems; leaf; medicinal plants; plant morphology; plant poisons; systematic biology IDENTIFICATION OF PLANTS USING NEURAL NETWORK TECHNOLOGY","LONG-TERM OBJECTIVES 1. Develop a computerized system, based on                 
hierarchical neural network pattern recognition technology, for reliable        
identification of plants.  2. Identify poisonous plants. 3. Expedite            
discovery of new medicinal plants.  4. Create an image database directly        
from plant material and link with existing medicinal plant databases.           
5. Develop commercial product for pharmaceutical companies, agriculture         
and others. SPECIFIC AIMS 1. Design hierarchical system of neural               
networks to follow natural plant taxonomy groupings and extend our              
identification technology to a large number of plant species.  2.               
Improve accuracy of identification.  3. Design a prototype workstation          
for botanical and agricultural field stations and laboratories.                 
                                                                                
RESEARCH DESIGN AND METHODS FOR ACHIEVING GOALS.  1. Digitize large             
number of plant species from special collections.  2. Measure                   
automatically venation patterns and shape.  3. Design hierarchical              
neural networks to divide plants into natural groupings.  4. Accumulate         
virtual herbarium  database as leaves are digitized (scanned or                 
photographed).                                                                  
                                                                                
POTENTIAL FOR TECHNOLOGICAL INNOVATION This system is unique in                 
capturing botanical recognition knowledge in a hierarchy of neural              
networks and is the first fully-computerized system for plant                   
identification utilizing information digitized directly from plants.            
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
1. Expedite discovery of new medicinal plants for pharmaceutical                
industry.  2. Create valuable database directly from plants.  3.                
Identification of poisonous plants.  4. Valuable for rapid                      
identification of invasive weeds.                                               
",2644669,R44CA067559,['R44CA067559'],CA,https://reporter.nih.gov/project-details/2644669,R44,1998,353293,-0.0001339163026085335
"In the US, almost 400,000 patients annually receive balloon angioplasty         
to improve arterial blood flow.  About 40 percent of these procedures           
are unsuccessful because of restenosis, which is estimated to cost the          
US health care system approximately 0.8-2 billion dollars annually.             
Endo-vascular irradiation is a new and innovative approach offering the         
potential of effective restenosis prevention.                                   
                                                                                
Almost all the used/proposed radionuclides have short radioactive half-         
lives so that, during the useful life of the source, exposure times must        
be continuously extended because of the decreasing source activity.             
Since the exposure times are comparable to the half-time of sublethal           
damage repair (T1/2), the biological effect of a given dose will vary           
significantly over the practical lifetime of the source.                        
                                                                                
The goal is to produce a reliable user-friendly software tool allowing          
the physician to find the optimal way to adjust for a given reduction           
in the activity (and therefore the dose rate) of the source, so that the        
biological effect will remain unchanged. The basic information required         
is the T1/2 of the target cells, together with the other relevant               
radiobiological response parameter (alpha/beta).  In Phase I, these will        
be determined for one coronary artery smooth muscle cell line of human          
origin, and a pilot computer program will be written and assessed, to           
adjust exposure times to obtain a constant biologically effective dose          
as the radioactive source decays and treatment times vary.                      
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Restenosis costs the US health care system approximately 0.8-2 billion          
dollars/yr.  Vascular irradiation is a new technique potentially                
offering effective restenosis prevention, and is likely to have                 
widespread application.  Because the radioactive sources used decay over        
their useful lifetime, a flexible tool giving biologically-based                
adjustments in the dose prescription for changes in dose rate will be           
essential.                                                                      
 artificial intelligence; catheterization; cell line; clearance rate; computer assisted medical decision making; computer program /software; computer system design /evaluation; intravenous administration; pharmacokinetics; radiation therapy dosage TOOL TO ADJUST FOR SOURCE DECAY IN VACULAR IRRADIATION","In the US, almost 400,000 patients annually receive balloon angioplasty         
to improve arterial blood flow.  About 40 percent of these procedures           
are unsuccessful because of restenosis, which is estimated to cost the          
US health care system approximately 0.8-2 billion dollars annually.             
Endo-vascular irradiation is a new and innovative approach offering the         
potential of effective restenosis prevention.                                   
                                                                                
Almost all the used/proposed radionuclides have short radioactive half-         
lives so that, during the useful life of the source, exposure times must        
be continuously extended because of the decreasing source activity.             
Since the exposure times are comparable to the half-time of sublethal           
damage repair (T1/2), the biological effect of a given dose will vary           
significantly over the practical lifetime of the source.                        
                                                                                
The goal is to produce a reliable user-friendly software tool allowing          
the physician to find the optimal way to adjust for a given reduction           
in the activity (and therefore the dose rate) of the source, so that the        
biological effect will remain unchanged. The basic information required         
is the T1/2 of the target cells, together with the other relevant               
radiobiological response parameter (alpha/beta).  In Phase I, these will        
be determined for one coronary artery smooth muscle cell line of human          
origin, and a pilot computer program will be written and assessed, to           
adjust exposure times to obtain a constant biologically effective dose          
as the radioactive source decays and treatment times vary.                      
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Restenosis costs the US health care system approximately 0.8-2 billion          
dollars/yr.  Vascular irradiation is a new technique potentially                
offering effective restenosis prevention, and is likely to have                 
widespread application.  Because the radioactive sources used decay over        
their useful lifetime, a flexible tool giving biologically-based                
adjustments in the dose prescription for changes in dose rate will be           
essential.                                                                      
",2645513,R43HL060365,['R43HL060365'],HL,https://reporter.nih.gov/project-details/2645513,R43,1998,74276,-0.12297610233900969
"Skin cancer is the fastest growing cancer in the United States today.           
Approximately 34,100 Americans developed cutaneous melanoma in 1995, and        
7,200 died of the disease; of the survivors, many must contend with the         
ongoing trauma of disfigurement and fear.  Skin biopsies are now the most       
frequently performed medical procedure reimbursed by Medicare.  It is           
axiomatic among dermatologists that early detection and diagnosis are           
critical in the care and treatment of skin cancer patients.  Great              
strides have been made in recent years in early detection of suspect skin       
lesions; however, the diagnosis remains based in the subjective                 
evaluation of which skin lesions to biopsy.  This decision is the basis         
of a great dilemma for physicians of at-risk patients who develop               
literally hundreds of lesions which could be pre-cancerous or cancerous.        
On one hand biopsies are expensive and traumatic; on the other, failure         
to biopsy the right lesion can lead to severe consequences.  The dilemma        
is further exacerbated by the fact that 50-80% of biopsies prove                
unnecessary after the fact, contributing to an enormous of valuable             
health care dollars, patient trauma and negative patient behavior               
feedback.  Recent developments in dermatological spectroscopy used to           
train an artificial neural net technology suggest that an automated             
clinical diagnostic aid which produces a quantitative rather than               
qualitative diagnostic assessment of skin lesions is possible.  This            
project proposes development and testing of such a product.                     
Spectroscopic samples of approximately 500 patients with abnormal skin          
lesions will be coupled with an equal number of normal skin spectra and         
used to train an artificial neural net classifier.  This automated              
diagnostic aid will be tested against a large number of test samples for        
which a histological diagnosis is available for evaluation of the system.       
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The proposed project will lead to a non-invasive, in-office, real-time          
test to provide an automated, repeatable diagnostic probability of the          
nature of skin lesions prior to biopsy. Skin biopsies are now the most          
frequently performed reimbursed Medicare procedure, and as many as 50-80%       
are found not to be necessary after the fact. The low cost of this test,        
and rapid amortization of the system, coupled with the enormous health          
care cost savings possible in conjunction with a significant and widely         
recognized health problem, suggest that this product could have great           
commercial potential.                                                           
 artificial intelligence; biomedical automation; biomedical equipment development; clinical research; diagnosis design /evaluation; fluorescence spectrometry; histology; human subject; neoplasm /cancer classification /staging; neoplasm /cancer diagnosis; noninvasive diagnosis; reflection spectrometry; skin neoplasms; spectrometry NONINVASIVE DERMATOLOGICAL LESION CLASSIFIER","Skin cancer is the fastest growing cancer in the United States today.           
Approximately 34,100 Americans developed cutaneous melanoma in 1995, and        
7,200 died of the disease; of the survivors, many must contend with the         
ongoing trauma of disfigurement and fear.  Skin biopsies are now the most       
frequently performed medical procedure reimbursed by Medicare.  It is           
axiomatic among dermatologists that early detection and diagnosis are           
critical in the care and treatment of skin cancer patients.  Great              
strides have been made in recent years in early detection of suspect skin       
lesions; however, the diagnosis remains based in the subjective                 
evaluation of which skin lesions to biopsy.  This decision is the basis         
of a great dilemma for physicians of at-risk patients who develop               
literally hundreds of lesions which could be pre-cancerous or cancerous.        
On one hand biopsies are expensive and traumatic; on the other, failure         
to biopsy the right lesion can lead to severe consequences.  The dilemma        
is further exacerbated by the fact that 50-80% of biopsies prove                
unnecessary after the fact, contributing to an enormous of valuable             
health care dollars, patient trauma and negative patient behavior               
feedback.  Recent developments in dermatological spectroscopy used to           
train an artificial neural net technology suggest that an automated             
clinical diagnostic aid which produces a quantitative rather than               
qualitative diagnostic assessment of skin lesions is possible.  This            
project proposes development and testing of such a product.                     
Spectroscopic samples of approximately 500 patients with abnormal skin          
lesions will be coupled with an equal number of normal skin spectra and         
used to train an artificial neural net classifier.  This automated              
diagnostic aid will be tested against a large number of test samples for        
which a histological diagnosis is available for evaluation of the system.       
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The proposed project will lead to a non-invasive, in-office, real-time          
test to provide an automated, repeatable diagnostic probability of the          
nature of skin lesions prior to biopsy. Skin biopsies are now the most          
frequently performed reimbursed Medicare procedure, and as many as 50-80%       
are found not to be necessary after the fact. The low cost of this test,        
and rapid amortization of the system, coupled with the enormous health          
care cost savings possible in conjunction with a significant and widely         
recognized health problem, suggest that this product could have great           
commercial potential.                                                           
",2645334,R43CA078006,['R43CA078006'],CA,https://reporter.nih.gov/project-details/2645334,R43,1998,93460,-0.0081058861807331
"Post-traumatic stress disorder (PTSD) is one of the most disabling              
psychopathological conditions affecting the veteran population.                 
Approximately 15.2% of the men and 8.5% of the women stationed in               
Vietnam were found to be suffering from PTSD 15 or more years after             
their service. In the Atlanta metropolitan area, some 9000 Vietnam              
veterans suffer from complete or partial PTSD. The psychological,               
social, occupational and economic consequences of the disorder for              
patients and their families are devastating. No therapeutic approach            
has proven to be consistently effective in the management of combat-            
related PTSD. The present proposal intends to exploit the potential             
therapeutic effectiveness of recent advances in computer and display            
technology referred to as Virtual Reality. Virtual reality exposure             
(VRE) takes place in an immersive, computer-driven environment.                 
Patients would be exposed to virtual Huey helicopters flying them over          
the jungles of Vietnam.  They will be encouraged to relive their                
traumatic memories, much as in standard exposure therapy, but immersed          
in Vietnam stimuli. Ultimate control is possible in the virtual                 
environment, changing levels of intensity of exposure instantly. The            
proposed project aims to develop virtual reality exposure therapy for           
Vietnam veterans with PTSD, revise and perfect the treatment,                   
construct a treatment manual, and gather preliminary evidence of its            
efficacy in a small group design. A series of five case studies will            
be run to develop and revise the treatment. Following this, Vietnam             
veterans (n=40) with current DSM-IV PTSD diagnoses will be randomly             
assigned to VRE or a wait-list control. Treatment will be delivered             
in nine 60-minute individual sessions conducted over 5 weeks.                   
Assessments will be conducted at pre-treatment, post-treatment and              
follow-ups of 6 and 12 months post-treatment. Assessments will be               
conducted by an independent assessor who will be kept blind to the              
treatment condition. Objective clinician-rated and self report                  
measures of PTSD will be incorporated.                                          
 adult human (21+); artificial intelligence; behavioral /social science research tag; clinical research; computer simulation; computer system design /evaluation; desensitization psychotherapy; human subject; human therapy evaluation; interview; longitudinal human study; memory; posttraumatic stress disorder; psychological shock; veterans; war /peace VIRTUAL REALITY EXPOSURE THERAPY FOR VETERANS WITH PTSD","Post-traumatic stress disorder (PTSD) is one of the most disabling              
psychopathological conditions affecting the veteran population.                 
Approximately 15.2% of the men and 8.5% of the women stationed in               
Vietnam were found to be suffering from PTSD 15 or more years after             
their service. In the Atlanta metropolitan area, some 9000 Vietnam              
veterans suffer from complete or partial PTSD. The psychological,               
social, occupational and economic consequences of the disorder for              
patients and their families are devastating. No therapeutic approach            
has proven to be consistently effective in the management of combat-            
related PTSD. The present proposal intends to exploit the potential             
therapeutic effectiveness of recent advances in computer and display            
technology referred to as Virtual Reality. Virtual reality exposure             
(VRE) takes place in an immersive, computer-driven environment.                 
Patients would be exposed to virtual Huey helicopters flying them over          
the jungles of Vietnam.  They will be encouraged to relive their                
traumatic memories, much as in standard exposure therapy, but immersed          
in Vietnam stimuli. Ultimate control is possible in the virtual                 
environment, changing levels of intensity of exposure instantly. The            
proposed project aims to develop virtual reality exposure therapy for           
Vietnam veterans with PTSD, revise and perfect the treatment,                   
construct a treatment manual, and gather preliminary evidence of its            
efficacy in a small group design. A series of five case studies will            
be run to develop and revise the treatment. Following this, Vietnam             
veterans (n=40) with current DSM-IV PTSD diagnoses will be randomly             
assigned to VRE or a wait-list control. Treatment will be delivered             
in nine 60-minute individual sessions conducted over 5 weeks.                   
Assessments will be conducted at pre-treatment, post-treatment and              
follow-ups of 6 and 12 months post-treatment. Assessments will be               
conducted by an independent assessor who will be kept blind to the              
treatment condition. Objective clinician-rated and self report                  
measures of PTSD will be incorporated.                                          
",2635521,R21MH055555,['R21MH055555'],MH,https://reporter.nih.gov/project-details/2635521,R21,1998,132141,-0.06634009470812134
"DESCRIPTION (Taken from application abstract):  The purpose of this proposal    
is to design and evaluate a novel graphical system for the presentation of      
data obtained from routine bedside monitoring of patients undergoing            
surgical intensive care (SICU).  The system, designated the V/Q-P/Q             
Assistant, is intended to enable expert and trainee physicians, nurses and      
other personnel to arrive at therapeutic decisions concerning pulmonary gas     
exchange and hemodynamics more efficiently.  This will be achieved at a         
human-computer interface by providing easier visualization of measured data,    
improved representation of functional abnormalities and the ability to          
simulate the particular patient so that trials of virtual therapy may guide     
the choice of treatment.  Analysis suggests that the task of a SICU             
physician is to translate the dense and unrelenting data stream of the SICU     
environment into a mental representation of the pathophysiological state.       
The data sources have been analyzed and software designed to collect data       
from typical classes of patients, at preselected intervals, from each           
monitoring device for storage in an Access database.  These data will form      
the basis for the experimental evaluation and design of the V/Q-P/Q             
Assistant.  Previous work on pulmonary pathophysiology has been synthesized     
into a model that unites the causes of impaired oxygenation                     
(ventilation/perfusion maldistribution) with a complete, and actively           
regulated pulmonary vasculature.  This model, combined with traditional and     
non-traditional clinical measurements, provides the computational basis for     
the system.  The human-computer interface will be developed under a             
cognitive system design framework, using an iterative process of rapid          
prototyping based on formative evaluations.  A series of computer display       
screens is proposed organized in five levels of increasing information.         
Screen designs use configural representations that are evolved to enhance       
information transfer, understanding of basic pathophysiology and evaluation     
of functional trajectories.  The ability of the V/Q-P/Q Assistant to improve    
medical care, enhance learning and contribute economically will be finally      
tested in a series of summative studies leading to the derivation of a multi    
attribute utility index.                                                        
 artificial intelligence; clinical research; computer assisted medical decision making; computer graphics /printing; computer human interaction; computer system design /evaluation; hemodynamics; human subject; information display; intensive care; patient monitoring device; respiratory airway volume; respiratory function; respiratory gas; respiratory hypoxia INFORMATION INTEGRATION AND VIRTUAL THERAPY IN THE SICU","DESCRIPTION (Taken from application abstract):  The purpose of this proposal    
is to design and evaluate a novel graphical system for the presentation of      
data obtained from routine bedside monitoring of patients undergoing            
surgical intensive care (SICU).  The system, designated the V/Q-P/Q             
Assistant, is intended to enable expert and trainee physicians, nurses and      
other personnel to arrive at therapeutic decisions concerning pulmonary gas     
exchange and hemodynamics more efficiently.  This will be achieved at a         
human-computer interface by providing easier visualization of measured data,    
improved representation of functional abnormalities and the ability to          
simulate the particular patient so that trials of virtual therapy may guide     
the choice of treatment.  Analysis suggests that the task of a SICU             
physician is to translate the dense and unrelenting data stream of the SICU     
environment into a mental representation of the pathophysiological state.       
The data sources have been analyzed and software designed to collect data       
from typical classes of patients, at preselected intervals, from each           
monitoring device for storage in an Access database.  These data will form      
the basis for the experimental evaluation and design of the V/Q-P/Q             
Assistant.  Previous work on pulmonary pathophysiology has been synthesized     
into a model that unites the causes of impaired oxygenation                     
(ventilation/perfusion maldistribution) with a complete, and actively           
regulated pulmonary vasculature.  This model, combined with traditional and     
non-traditional clinical measurements, provides the computational basis for     
the system.  The human-computer interface will be developed under a             
cognitive system design framework, using an iterative process of rapid          
prototyping based on formative evaluations.  A series of computer display       
screens is proposed organized in five levels of increasing information.         
Screen designs use configural representations that are evolved to enhance       
information transfer, understanding of basic pathophysiology and evaluation     
of functional trajectories.  The ability of the V/Q-P/Q Assistant to improve    
medical care, enhance learning and contribute economically will be finally      
tested in a series of summative studies leading to the derivation of a multi    
attribute utility index.                                                        
",2771700,R01LM005997,['R01LM005997'],LM,https://reporter.nih.gov/project-details/2771700,R01,1998,224691,-0.045625439806500245
 artificial intelligence; biomedical automation; computer simulation; digital imaging; evaluation /testing; human subject; method development; model design /development; phantom model; volunteer SYSTEM FOR TESTING RESOLUTION OF ULTRASOUND SCANNERS,,2771048,R42GM054377,['R42GM054377'],GM,https://reporter.nih.gov/project-details/2771048,R42,1998,225669,-0.0744797721085124
"DESCRIPTION (Taken from the application abstract):  Gene expression             
databases will take on growing importance as the complexities of development    
and differentiation, and normal versus diseased states are studied in           
greater detail.  Vast amounts of sequence data are now available for the        
study of gene expression, along with an anticipated surge in high-throughput    
data on differential gene expression.  Much of the key information remains      
in the primary literature inaccessible for computational analysis.  The goal    
of the proposed project is to provide in a single integrated system, the        
information management, analysis, and visualization tools containing these      
data sources.  Such a system requires the representation of gene expression     
encompassing spatial, temporal and quantitative dimensions; the collection      
and encoding of information from online resources and the primary               
literature; the integration of analysis methodologies tailored to the study     
of gene expression; and the availability of interfaces able to query and        
visualize the data in human comprehensible form.  The prototype system,         
EpoDB, focuses on erythropoiesis, but will generalize to the study of gene      
expression along any pathway of differentiation.  This research will enhance    
and extend the existing information management technology through               
integration of a declarative constraint language into the representation        
language, development of an integrity constraint system to facilitate           
synchronization with external databases, and implementation of a query          
language and optimizer.  Schemas and controlled vocabularies will be            
tailored to represent DNA and chromosomal features relating to gene             
regulation, temporal events describing expression levels during development     
and differentiation, and descriptions of gene control processes, pathways       
and networks.  The foundation for EpoDB will be extracted from online           
resources (GenBank, TRANSFAC, MedLine, etc.), restructured and analyzed to      
remove errors.  Data relevant to gene expression during erythropoiesis will     
be entered from the literature by trained annotators.  Improved versions of     
data entry editing tools will be developed to improve quality control, ease     
of annotation, and allow annotation by external users through Web               
interfaces.  Incorporated into EpoDB will also be results of data analysis      
such as transcriptional regulatory patterns discovered by statistical           
techniques, by pattern matching techniques, and by classification               
hierarchies of genes and patterns.  EpoDB will be accessible through query      
interfaces and visualization tools built for the WWW using the evolving         
bioTk system.  Data and the system tools will be distributed on a regular       
basis.                                                                          
 artificial intelligence; cell differentiation; computer assisted sequence analysis; computer system design /evaluation; data collection methodology /evaluation; erythropoiesis; gene expression; genetic regulatory element; information retrieval; information system analysis; information systems; nucleic acid sequence; vocabulary development for information system KNOWLEDGE BASED BIOLOGICAL MODELING INFORMATION SYSTEM","DESCRIPTION (Taken from the application abstract):  Gene expression             
databases will take on growing importance as the complexities of development    
and differentiation, and normal versus diseased states are studied in           
greater detail.  Vast amounts of sequence data are now available for the        
study of gene expression, along with an anticipated surge in high-throughput    
data on differential gene expression.  Much of the key information remains      
in the primary literature inaccessible for computational analysis.  The goal    
of the proposed project is to provide in a single integrated system, the        
information management, analysis, and visualization tools containing these      
data sources.  Such a system requires the representation of gene expression     
encompassing spatial, temporal and quantitative dimensions; the collection      
and encoding of information from online resources and the primary               
literature; the integration of analysis methodologies tailored to the study     
of gene expression; and the availability of interfaces able to query and        
visualize the data in human comprehensible form.  The prototype system,         
EpoDB, focuses on erythropoiesis, but will generalize to the study of gene      
expression along any pathway of differentiation.  This research will enhance    
and extend the existing information management technology through               
integration of a declarative constraint language into the representation        
language, development of an integrity constraint system to facilitate           
synchronization with external databases, and implementation of a query          
language and optimizer.  Schemas and controlled vocabularies will be            
tailored to represent DNA and chromosomal features relating to gene             
regulation, temporal events describing expression levels during development     
and differentiation, and descriptions of gene control processes, pathways       
and networks.  The foundation for EpoDB will be extracted from online           
resources (GenBank, TRANSFAC, MedLine, etc.), restructured and analyzed to      
remove errors.  Data relevant to gene expression during erythropoiesis will     
be entered from the literature by trained annotators.  Improved versions of     
data entry editing tools will be developed to improve quality control, ease     
of annotation, and allow annotation by external users through Web               
interfaces.  Incorporated into EpoDB will also be results of data analysis      
such as transcriptional regulatory patterns discovered by statistical           
techniques, by pattern matching techniques, and by classification               
hierarchies of genes and patterns.  EpoDB will be accessible through query      
interfaces and visualization tools built for the WWW using the evolving         
bioTk system.  Data and the system tools will be distributed on a regular       
basis.                                                                          
",2797084,R01RR004026,['R01RR004026'],RR,https://reporter.nih.gov/project-details/2797084,R01,1998,375398,-0.04355516736949109
"DESCRIPTION:  (Applicant's abstract) The development of true microscale         
interfaces for protein mass spectrometry based on silicon chip                  
micromachining is proposed.  The devices would integrate electrospray           
ionization with a variety of protein isolation, digestion, and/or separation    
steps into a single monolithic structure that can be mass produced in much      
the same way as computer chips are made.  The silicon chip devices would be     
coupled to state-of-the-art tandem mass spectrometer systems capable of         
providing detailed structural information on protein samples.  Functional       
utility of the systems will be increased by the development of automated        
data collection procedures capable of analyzing incoming spectra and            
adjusting analysis parameters in real time.  In addition to optimizing          
general factors such as relative collison energy and resolution, spectra        
would be evaluated in the context of the problem to be solved.  A fast,         
multiprocessor computer will provide realtime access to a knowledge base and    
spectra will be analyzed according to a user defined set of rules.  This        
expert system approach will be applied to a number of different kinds of        
applications including the rapid determination of protein variants and          
posttranslational modifications, the definition of specific protein protein     
interactions, and the identification of MHC complex peptides.  A fundamental    
goal of the proposed research is to create a system in which both hardware      
and analytical expertise can be readily transferred to other labs and other     
researchers in the form of inexpensive, disposable interfaces and problem       
specific data collection and analysis programs.  The development of such        
tools will greatly facilitate work to define the human proteome.                
 artificial intelligence; biomedical equipment development; computer system design /evaluation; computer system hardware; histocompatibility antigens; mass spectrometry; posttranslational modifications INTEGRATED STRUCTURES FOR PROTEIN MASS SPECTRAL ANALYSIS","DESCRIPTION:  (Applicant's abstract) The development of true microscale         
interfaces for protein mass spectrometry based on silicon chip                  
micromachining is proposed.  The devices would integrate electrospray           
ionization with a variety of protein isolation, digestion, and/or separation    
steps into a single monolithic structure that can be mass produced in much      
the same way as computer chips are made.  The silicon chip devices would be     
coupled to state-of-the-art tandem mass spectrometer systems capable of         
providing detailed structural information on protein samples.  Functional       
utility of the systems will be increased by the development of automated        
data collection procedures capable of analyzing incoming spectra and            
adjusting analysis parameters in real time.  In addition to optimizing          
general factors such as relative collison energy and resolution, spectra        
would be evaluated in the context of the problem to be solved.  A fast,         
multiprocessor computer will provide realtime access to a knowledge base and    
spectra will be analyzed according to a user defined set of rules.  This        
expert system approach will be applied to a number of different kinds of        
applications including the rapid determination of protein variants and          
posttranslational modifications, the definition of specific protein protein     
interactions, and the identification of MHC complex peptides.  A fundamental    
goal of the proposed research is to create a system in which both hardware      
and analytical expertise can be readily transferred to other labs and other     
researchers in the form of inexpensive, disposable interfaces and problem       
specific data collection and analysis programs.  The development of such        
tools will greatly facilitate work to define the human proteome.                
",2797088,R01RR006217,['R01RR006217'],RR,https://reporter.nih.gov/project-details/2797088,R01,1998,472789,-0.1329495785825062
"Multidimensional scaling (MDS) is a psychometric method with wide               
application in behavioral science research. We propose to develop               
software for a new class of MDS models. In these new models parameters          
associated with individuals are modeled as random effects rather than as        
fixed parameters. For the diagonal metric (or INDSCAL) models, these            
parameters are the subject weights. The resulting random effects MDS            
model has many advantages over its classical counterpart. For example,          
we are better able to estimate subject weights even when only one               
dissimilarity is observed on an individual, and we can make model-based         
inferences about the sampled population of subject weights.                     
                                                                                
We propose to develop a comprehensive module of computational algorithms        
for computing estimates in this new class of MDS models. Included in this       
module will be software for model fitting, inference, diagnostics, and          
other appropriate statistical techniques, a graphical user interface, a         
users manual, and online documentation. The software will also contain          
procedures for robust estimation.                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
Multidimensional scaling is a psychometric method widely used in market         
research, psychology, sociology, political science, genetics,                   
chemometrics, and other areas of behavioral and scientific research.  The       
proposed models have significant advantages over existing techniques. A         
well designed and comprehensive module for implementing these models will       
find a ready market.                                                            
 artificial intelligence; behavioral /social science research tag; computer graphics /printing; computer program /software; computer system design /evaluation; mathematical model; model design /development; psychometrics; statistics /biometry MIXED EFFECTS MULTIDIMENSIONAL SCALING","Multidimensional scaling (MDS) is a psychometric method with wide               
application in behavioral science research. We propose to develop               
software for a new class of MDS models. In these new models parameters          
associated with individuals are modeled as random effects rather than as        
fixed parameters. For the diagonal metric (or INDSCAL) models, these            
parameters are the subject weights. The resulting random effects MDS            
model has many advantages over its classical counterpart. For example,          
we are better able to estimate subject weights even when only one               
dissimilarity is observed on an individual, and we can make model-based         
inferences about the sampled population of subject weights.                     
                                                                                
We propose to develop a comprehensive module of computational algorithms        
for computing estimates in this new class of MDS models. Included in this       
module will be software for model fitting, inference, diagnostics, and          
other appropriate statistical techniques, a graphical user interface, a         
users manual, and online documentation. The software will also contain          
procedures for robust estimation.                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
Multidimensional scaling is a psychometric method widely used in market         
research, psychology, sociology, political science, genetics,                   
chemometrics, and other areas of behavioral and scientific research.  The       
proposed models have significant advantages over existing techniques. A         
well designed and comprehensive module for implementing these models will       
find a ready market.                                                            
",2776571,R44MH057559,['R44MH057559'],MH,https://reporter.nih.gov/project-details/2776571,R44,1998,237444,-0.14041347924818587
"The Minorities in Research and Science Achievement (MIRASA) Program at The      
University of Texas at San Antonio is intended to (1) strengthen the            
biomedical research capabilities of the University and faculty, (2)             
increase the number and quality of minority scientists by providing             
biomedical research training opportunities for students at the                  
undergraduate the graduate levels, and (3) expose minority science students     
to a variety of scientists and their research.  To accomplish these             
endeavors, nineteen research projects actively involving 23 Graduate, 23        
Undergraduate Students, 16 Principal Investigators, 3 Associate                 
Investigators, 3 co-principal investigators, 17 collaborators and 5             
consultants will direct biomedical research projects which will provide         
training opportunities for minority students.  These nineteen research          
projects include;  (1)  Isolation of regulatory kinase from the visual          
system of Drosophila (2)  Effects of neuronal activity on maturation of         
hippocampal neurons (3)  The role of the endoplasmic reticulum and other        
organelles in GAP-43 sorting (4)  3-D image processing for automated            
digital mammography (5) Biological and biochemical properties of                
glycosylated human growth hormone (6)  The role of versican in myelin and       
multiple sclerosis (7)  Molecular genetics of a nematode acetylcholine          
receptor (8)  Expression of HP chimeric genes in response to inflammatory       
factors and hormones (9)  New approaches tot he preparation of carcinogen-      
deoxynucleoside adducts (10)  Purple membrane proton pump (11)  Artificial      
neural network classification of fetal heart rate signals (12)  Optical and     
thermal characterization of ocular tissue (13)  Statistical analysis of         
long DNA sequences (14)  Muscle control using below-lesion                      
electromyographic signals: an innovative methodology (15)  Metal-mediated       
molecular and supramolecular aggregates of guanine and adenine nucleotides      
(16)  Regulation of myogenesis and motoneuron innervation by the                
neurotrophins (17) Ca+ homeostasis hormone sensitive Ca2+ channels (18)         
Augmented sympathoinhibition by area postrema noradrenergic neurons (19)        
11-cis retinyl ester hydrolase in the eye.  Moreover, student and faculty       
research potential will be enhanced by exposing them to sophisticated           
research instruments and modern research techniques employed in                 
biochemistry, bioengineering, chemistry, genetics, molecular biology,           
neuroanatomy, neurobiochemistry, neurophysiology, biophysics and                
biostatistics.  In addition, students and faculty will interact with            
regional and national scientists actively involved in biomedical research.      
Students and faculty will publish their results in prestigious refereed         
journals and  present their research at professional, national and              
international symposiums.  Institutional and class seminars will be             
continued featuring outstanding scientists and in cooperation with other        
institutions.                                                                   
 minority institution research support MINORITIES RESEARCH AND SCIENCE ACHIEVEMENT PROGRAM","The Minorities in Research and Science Achievement (MIRASA) Program at The      
University of Texas at San Antonio is intended to (1) strengthen the            
biomedical research capabilities of the University and faculty, (2)             
increase the number and quality of minority scientists by providing             
biomedical research training opportunities for students at the                  
undergraduate the graduate levels, and (3) expose minority science students     
to a variety of scientists and their research.  To accomplish these             
endeavors, nineteen research projects actively involving 23 Graduate, 23        
Undergraduate Students, 16 Principal Investigators, 3 Associate                 
Investigators, 3 co-principal investigators, 17 collaborators and 5             
consultants will direct biomedical research projects which will provide         
training opportunities for minority students.  These nineteen research          
projects include;  (1)  Isolation of regulatory kinase from the visual          
system of Drosophila (2)  Effects of neuronal activity on maturation of         
hippocampal neurons (3)  The role of the endoplasmic reticulum and other        
organelles in GAP-43 sorting (4)  3-D image processing for automated            
digital mammography (5) Biological and biochemical properties of                
glycosylated human growth hormone (6)  The role of versican in myelin and       
multiple sclerosis (7)  Molecular genetics of a nematode acetylcholine          
receptor (8)  Expression of HP chimeric genes in response to inflammatory       
factors and hormones (9)  New approaches tot he preparation of carcinogen-      
deoxynucleoside adducts (10)  Purple membrane proton pump (11)  Artificial      
neural network classification of fetal heart rate signals (12)  Optical and     
thermal characterization of ocular tissue (13)  Statistical analysis of         
long DNA sequences (14)  Muscle control using below-lesion                      
electromyographic signals: an innovative methodology (15)  Metal-mediated       
molecular and supramolecular aggregates of guanine and adenine nucleotides      
(16)  Regulation of myogenesis and motoneuron innervation by the                
neurotrophins (17) Ca+ homeostasis hormone sensitive Ca2+ channels (18)         
Augmented sympathoinhibition by area postrema noradrenergic neurons (19)        
11-cis retinyl ester hydrolase in the eye.  Moreover, student and faculty       
research potential will be enhanced by exposing them to sophisticated           
research instruments and modern research techniques employed in                 
biochemistry, bioengineering, chemistry, genetics, molecular biology,           
neuroanatomy, neurobiochemistry, neurophysiology, biophysics and                
biostatistics.  In addition, students and faculty will interact with            
regional and national scientists actively involved in biomedical research.      
Students and faculty will publish their results in prestigious refereed         
journals and  present their research at professional, national and              
international symposiums.  Institutional and class seminars will be             
continued featuring outstanding scientists and in cooperation with other        
institutions.                                                                   
",2749732,S06GM008194,['S06GM008194'],GM,https://reporter.nih.gov/project-details/2749732,S06,1998,1637495,-0.13923806352653845
"DESCRIPTION:  The proposed research is designed to develop and evaluate         
efficacious and cost-effective combinations of behavioral interventions and     
Nicotine Replacement Therapy (NRT) for smoking cessation.  NRT has been         
demonstrated to be effective but has been used by only a small percentage of    
the smokers.  In contrast, population based interventions with proactive        
recruitment and stage matched interventions have the potential to reach more    
than 80 percent of the population.  The proposed research will combine NRT      
with two recently developed, highly effective behavioral interventions that     
employ digital technology to produce individualized, low cost interventions.    
These interventions have the potential to both increase the proportion of       
smokers who receive NRT and the effectiveness or NRT.  Expert System            
interventions based on the Transtheoretical Model have demonstrated             
effectiveness in two population-based studies without NRT.                      
Telecommunications represents a low cost method of providing high frequency     
always available automated counseling services to smokers.  The proposed        
study is a population based clinical trial with 2200 smokers proactively        
recruited from a large VA system and randomly assigned to one of four           
conditions, varying from minimal intervention to high intensity.  The           
conditions include:  (1) Minimal Intervention, which involves only              
stage-matched manuals; (2) NRT Alone, which provides NRT with only manuals;     
(3) Expert System, which combines NRT, the Expert System and Manuals; and       
(4) Telecommunications, which combines the previous interventions with an       
automated telephone counseling intervention.  In the three NRT conditions,      
NRT will be provided only to those smokers for whom it is appropriate.  A       
goal of the behavioral interventions is to increase the proportion of the       
smokers who receive NRT.  Data analysis will identify the most efficacious      
interventions in preparation for dissemination to entire population.  Stage     
matched, interactive, and proactive interventions that match behavioral and     
pharmacological elements to the individual smoker have the potential to         
produce unprecedented impacts on an entire population of smokers.               
 behavioral /social science research tag; clinical research; computer assisted patient care; counseling; drug abuse chemotherapy; drug abuse therapy; health behavior; human subject; human therapy evaluation; nicotine; outcomes research; self help; smoking cessation; telecommunications AUTOMATED POPULATION BASED SMOKING CESSATION PROGRAMS","DESCRIPTION:  The proposed research is designed to develop and evaluate         
efficacious and cost-effective combinations of behavioral interventions and     
Nicotine Replacement Therapy (NRT) for smoking cessation.  NRT has been         
demonstrated to be effective but has been used by only a small percentage of    
the smokers.  In contrast, population based interventions with proactive        
recruitment and stage matched interventions have the potential to reach more    
than 80 percent of the population.  The proposed research will combine NRT      
with two recently developed, highly effective behavioral interventions that     
employ digital technology to produce individualized, low cost interventions.    
These interventions have the potential to both increase the proportion of       
smokers who receive NRT and the effectiveness or NRT.  Expert System            
interventions based on the Transtheoretical Model have demonstrated             
effectiveness in two population-based studies without NRT.                      
Telecommunications represents a low cost method of providing high frequency     
always available automated counseling services to smokers.  The proposed        
study is a population based clinical trial with 2200 smokers proactively        
recruited from a large VA system and randomly assigned to one of four           
conditions, varying from minimal intervention to high intensity.  The           
conditions include:  (1) Minimal Intervention, which involves only              
stage-matched manuals; (2) NRT Alone, which provides NRT with only manuals;     
(3) Expert System, which combines NRT, the Expert System and Manuals; and       
(4) Telecommunications, which combines the previous interventions with an       
automated telephone counseling intervention.  In the three NRT conditions,      
NRT will be provided only to those smokers for whom it is appropriate.  A       
goal of the behavioral interventions is to increase the proportion of the       
smokers who receive NRT.  Data analysis will identify the most efficacious      
interventions in preparation for dissemination to entire population.  Stage     
matched, interactive, and proactive interventions that match behavioral and     
pharmacological elements to the individual smoker have the potential to         
produce unprecedented impacts on an entire population of smokers.               
",2748865,R01CA071356,['R01CA071356'],CA,https://reporter.nih.gov/project-details/2748865,R01,1998,1007246,-0.19601508726274847
"DESCRIPTION (Taken from application abstract):  The long-term aim of this       
project is to use natural language methods in order to enhance the              
functionality of the electronic medical record, which is a source of            
abundant clinical data.  However, the data is mostly in textual form and        
therefore unusable for automated clinical applications, such as decision        
support, research, quality assurance, and outcomes assessment.  By using a      
natural language processor to map the clinical information in the reports       
into structured codified clinical data, the data will be made readily           
accessible so that it could be utilized by subsequent automated clinical        
applications.  We have already shown that it is possible to build an            
effective text processor that accurately codifies textual reports within the    
specialized domain of radiology.  In this project we intend to build upon       
our successful experience and will extend the processor to another limited      
domain that is different from radiology and to a broad domain in order to       
study the feasibility of transferring the processor to all of medicine.         
                                                                                
More specifically, we will broaden the processor so that it codifies            
clinical information in the physical examination section of the discharge       
summary and then to all of the discharge summary, where we will focus on        
coding diagnoses.  The emphasis of our work will not only be concerned with     
extending the language processor but will also focus on scalability,            
evaluation of the performance, the effort, and the portability aspects.  In     
addition, because discharge summaries are so complex and comprehensive, we      
will have to extend the formal representational model of the clinical           
information and also develop new natural language processing techniques and     
new vocabulary development tools.  This work will continue to be performed      
within an operational clinical setting.                                         
 abstracting; automated medical record system; computer system design /evaluation; human data; information retrieval; method development; vocabulary development for information system UNLOCKING DATA FROM MEDICAL RECORDS WITH TEXT PROCESSING","DESCRIPTION (Taken from application abstract):  The long-term aim of this       
project is to use natural language methods in order to enhance the              
functionality of the electronic medical record, which is a source of            
abundant clinical data.  However, the data is mostly in textual form and        
therefore unusable for automated clinical applications, such as decision        
support, research, quality assurance, and outcomes assessment.  By using a      
natural language processor to map the clinical information in the reports       
into structured codified clinical data, the data will be made readily           
accessible so that it could be utilized by subsequent automated clinical        
applications.  We have already shown that it is possible to build an            
effective text processor that accurately codifies textual reports within the    
specialized domain of radiology.  In this project we intend to build upon       
our successful experience and will extend the processor to another limited      
domain that is different from radiology and to a broad domain in order to       
study the feasibility of transferring the processor to all of medicine.         
                                                                                
More specifically, we will broaden the processor so that it codifies            
clinical information in the physical examination section of the discharge       
summary and then to all of the discharge summary, where we will focus on        
coding diagnoses.  The emphasis of our work will not only be concerned with     
extending the language processor but will also focus on scalability,            
evaluation of the performance, the effort, and the portability aspects.  In     
addition, because discharge summaries are so complex and comprehensive, we      
will have to extend the formal representational model of the clinical           
information and also develop new natural language processing techniques and     
new vocabulary development tools.  This work will continue to be performed      
within an operational clinical setting.                                         
",2735428,R01LM006274,['R01LM006274'],LM,https://reporter.nih.gov/project-details/2735428,R01,1998,204002,-0.13532027225902754
"Ordered categorical variables arise frequently in cancer clinical trials        
and other biomedical studies. The statistical procedures for analyzing          
such data are well known and software for performing the analysis is            
readily available. The basic idea is to condition on the margins of the         
contingency table created by the categorical data and thereby obtain a          
distribution free test that automatically corrects for ties. Despite the        
popularity of this conditional approach for analyzing ordered categorical       
data there has been very little work done on power and sample-size              
considerations at the design phase. A biomedical investigator about to          
launch a clinical trial for comparing two treatments with ordered               
categorical outcomes will find it extremely difficult to determine what         
sample size is needed. Either the investigator must assume that the data        
are continuous, or else that the data are binary, since these are the only      
cases for which reliable methods and software are available. Both               
approaches are inappropriate for ordered categorical data. We propose to        
fill the void by providing new exact and Monte Carlo methods that provide       
accurate power and sample-size estimates for conditional tests on ordered       
categorical data.                                                               
 artificial intelligence; computer data analysis; computer program /software; computer system design /evaluation; data collection methodology /evaluation; data management; mathematical model; statistics /biometry SAMPLE SIZE SOFTWARE FOR ORDERED CATEGORICAL DATA","Ordered categorical variables arise frequently in cancer clinical trials        
and other biomedical studies. The statistical procedures for analyzing          
such data are well known and software for performing the analysis is            
readily available. The basic idea is to condition on the margins of the         
contingency table created by the categorical data and thereby obtain a          
distribution free test that automatically corrects for ties. Despite the        
popularity of this conditional approach for analyzing ordered categorical       
data there has been very little work done on power and sample-size              
considerations at the design phase. A biomedical investigator about to          
launch a clinical trial for comparing two treatments with ordered               
categorical outcomes will find it extremely difficult to determine what         
sample size is needed. Either the investigator must assume that the data        
are continuous, or else that the data are binary, since these are the only      
cases for which reliable methods and software are available. Both               
approaches are inappropriate for ordered categorical data. We propose to        
fill the void by providing new exact and Monte Carlo methods that provide       
accurate power and sample-size estimates for conditional tests on ordered       
categorical data.                                                               
",2733109,R44CA065358,['R44CA065358'],CA,https://reporter.nih.gov/project-details/2733109,R44,1998,353808,-0.08056485212065131
"A prototype robotic instrument, ""Stericulture,"" has been developed to feed      
and harvest cell cultures in an aseptic environment.  The instrument is         
microprocessor controlled.  It was designed to protect the cultures from        
contamination and the technologist from hazardous exposure, by                  
dramatically reducing the handling of the petri dishes.  During operation       
the repetitive exacting tasks of media dispensing and removal, and petri        
dish manipulation are mechanically accomplished within a hood or P3             
containment facility.  Culture dish lids are removed and replaced, media        
is added and removed in timed sequences.  The concept can be expanded to        
virtually any cell culture lab.  The specific aims of this application are      
(1) Test the mechanical reliability, accuracy and durability of                 
Stericulture and its peripheral pumping systems.  (2) Conduct beta site         
tests at two cytogenetic laboratories and one molecular biology cell            
culture laboratory to:  a) Evaluate existing computer programming and           
written documentation.  b) Test the device for reliability in the feeding       
and harvesting of cell cultures from amniocytes and biopsies                    
(fibroblasts).  The long-term objective (Phase II) is to develop an             
integrated liquid handling system (LHS) with Stericulture as its core.          
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Development of robotic instrument that        
functions in an aseptic environment in a cell culture lab.                      
 antiseptic sterilization; artificial intelligence; bioengineering /biomedical engineering; biomedical automation; biomedical equipment development; imaging /visualization /scanning; karyotype; laboratory accident /infection; robotics; tissue /cell culture ROBOTIC DEVICE FOR ASEPTIC PROCESSING OF CELL CULTURES","A prototype robotic instrument, ""Stericulture,"" has been developed to feed      
and harvest cell cultures in an aseptic environment.  The instrument is         
microprocessor controlled.  It was designed to protect the cultures from        
contamination and the technologist from hazardous exposure, by                  
dramatically reducing the handling of the petri dishes.  During operation       
the repetitive exacting tasks of media dispensing and removal, and petri        
dish manipulation are mechanically accomplished within a hood or P3             
containment facility.  Culture dish lids are removed and replaced, media        
is added and removed in timed sequences.  The concept can be expanded to        
virtually any cell culture lab.  The specific aims of this application are      
(1) Test the mechanical reliability, accuracy and durability of                 
Stericulture and its peripheral pumping systems.  (2) Conduct beta site         
tests at two cytogenetic laboratories and one molecular biology cell            
culture laboratory to:  a) Evaluate existing computer programming and           
written documentation.  b) Test the device for reliability in the feeding       
and harvesting of cell cultures from amniocytes and biopsies                    
(fibroblasts).  The long-term objective (Phase II) is to develop an             
integrated liquid handling system (LHS) with Stericulture as its core.          
                                                                                
PROPOSED COMMERCIAL APPLICATION:  Development of robotic instrument that        
functions in an aseptic environment in a cell culture lab.                      
",2672772,R44AI039895,['R44AI039895'],AI,https://reporter.nih.gov/project-details/2672772,R44,1998,125226,-0.031196961029689968
"Heuristics have been developed that allow the prediction of the secondary       
structure of proteins starting from a set of aligned homologous protein         
sequences. These heuristics extract conformational information from             
patterns of conservation and variation within the family of proteins. The       
tools as presently implemented involve both automated and manually applied      
tools. They have been tested by making bona fide prediction of protein          
secondary structure, those announced before an experimental structure           
becomes available, for approximately a dozen proteins. These predictions        
have proven to be remarkably accurate. Further, they have defined a few         
secondary structural element types that are difficult to predict, thus          
focusing future work. Thus, they are a significant step towards meeting         
one goal of the National Library of Medicine to develop ""algorithms             
capable of predicting structure [of proteins] based on primary sequences        
of amino acids"" (Item 103.D). The work to be funded under this proposal         
will transfer from Switzerland to the United States these prediction            
technologies when the Principal Investigator moves to the University of         
Florida in 1995. The proposed work will test the feasibility of preparing       
computer software that can be used on workstations to automate those            
aspects of the structure prediction tools that are presently applied by         
hand.                                                                           
                                                                                
PROPOSED COMMERCIAL APPLICATION: Tools that enable the prediction of the        
folded structure of proteins from sequence data are commercially                
marketable as computer software, as well as core units in drug discovery        
and drug development programs.                                                  
 artificial intelligence; biochemical evolution; chemical models; computer assisted sequence analysis; computer data analysis; computer human interaction; computer program /software; computer simulation; computer system design /evaluation; genome; information systems; mathematics; model design /development; protein sequence; protein structure; structural biology EXPERT SYSTEM FOR PREDICTING PROTEIN SECONDARY STRUCTURE","Heuristics have been developed that allow the prediction of the secondary       
structure of proteins starting from a set of aligned homologous protein         
sequences. These heuristics extract conformational information from             
patterns of conservation and variation within the family of proteins. The       
tools as presently implemented involve both automated and manually applied      
tools. They have been tested by making bona fide prediction of protein          
secondary structure, those announced before an experimental structure           
becomes available, for approximately a dozen proteins. These predictions        
have proven to be remarkably accurate. Further, they have defined a few         
secondary structural element types that are difficult to predict, thus          
focusing future work. Thus, they are a significant step towards meeting         
one goal of the National Library of Medicine to develop ""algorithms             
capable of predicting structure [of proteins] based on primary sequences        
of amino acids"" (Item 103.D). The work to be funded under this proposal         
will transfer from Switzerland to the United States these prediction            
technologies when the Principal Investigator moves to the University of         
Florida in 1995. The proposed work will test the feasibility of preparing       
computer software that can be used on workstations to automate those            
aspects of the structure prediction tools that are presently applied by         
hand.                                                                           
                                                                                
PROPOSED COMMERCIAL APPLICATION: Tools that enable the prediction of the        
folded structure of proteins from sequence data are commercially                
marketable as computer software, as well as core units in drug discovery        
and drug development programs.                                                  
",2675458,R42MH055479,['R42MH055479'],MH,https://reporter.nih.gov/project-details/2675458,R42,1998,203764,-0.02678088706870986
"DESCRIPTION:  (Adapted from the applicant's abstract):  Medical and             
biological data often come in the form of digitized signals and images, for     
example magnetic resonance images (MRI), ion channel electrical series, and     
human gait paths.  As data acquisition becomes easier, sequences of such        
images or signals are collected, often along with other covariate               
measurements, resulting in data sets where the basic unit of measurement or     
response is a high dimensional object.  This project proposes a battery of      
statistical techniques for modeling and understanding such data, that           
explicitly takes into account and indeed exploits the inherent, spatial, or     
temporal correlation, and when appropriate, relates it to covariate             
information.  By imposing spatial smoothness in the image or signal domain,     
pixel-wise regression, and canonical correlation models can borrow strength     
from neighboring pixels.  This not only improves the overall efficiency of      
these techniques, but also allows identification of important regions rather    
than individual pixels.  The project develops appropriate versions of           
nonparametric regressions for such series of images, as well as data            
descriptions such as clustering, principal component, and singular value        
decomposition models.  In many cases, wavelets will be used to achieve          
spatial smoothness.  In the case of ion channel data, the models are used to    
isolate particular weak high frequency components from correlated noise.        
Much of this work will be carried out in collaboration with radiologists,       
physiologists, and other biomedical researchers working on cancer, heart        
disease and stroke, brain mapping, and gait analysis.                           
 computer data analysis; diagnosis design /evaluation; digital imaging; mathematical model; statistics /biometry NEW STATISTICAL METHODS FOR MEDICAL SIGNALS AND IMAGES","DESCRIPTION:  (Adapted from the applicant's abstract):  Medical and             
biological data often come in the form of digitized signals and images, for     
example magnetic resonance images (MRI), ion channel electrical series, and     
human gait paths.  As data acquisition becomes easier, sequences of such        
images or signals are collected, often along with other covariate               
measurements, resulting in data sets where the basic unit of measurement or     
response is a high dimensional object.  This project proposes a battery of      
statistical techniques for modeling and understanding such data, that           
explicitly takes into account and indeed exploits the inherent, spatial, or     
temporal correlation, and when appropriate, relates it to covariate             
information.  By imposing spatial smoothness in the image or signal domain,     
pixel-wise regression, and canonical correlation models can borrow strength     
from neighboring pixels.  This not only improves the overall efficiency of      
these techniques, but also allows identification of important regions rather    
than individual pixels.  The project develops appropriate versions of           
nonparametric regressions for such series of images, as well as data            
descriptions such as clustering, principal component, and singular value        
decomposition models.  In many cases, wavelets will be used to achieve          
spatial smoothness.  In the case of ion channel data, the models are used to    
isolate particular weak high frequency components from correlated noise.        
Much of this work will be carried out in collaboration with radiologists,       
physiologists, and other biomedical researchers working on cancer, heart        
disease and stroke, brain mapping, and gait analysis.                           
",2769887,R01CA072028,['R01CA072028'],CA,https://reporter.nih.gov/project-details/2769887,R01,1998,146191,-0.16288905405315293
"DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed        
to  develop adaptive imaging algorithms and instrumentation to compensate       
for  tissue-induced ultrasonic image degradation.  Theoretical and              
simulation  studies are proposed to optimize the accuracy, stability, and       
speed of  adaptive algorithms and to explore the impact of transducer           
design on adaptive  imaging.  In addition, the applicants proposed to           
collect high-quality tissue  echo data and through-transmission data to         
investigate the nature of tissue- induced image degradation.  The               
adaptive imaging techniques would be implemented in real-time on an             
advanced engineering prototype scanner.  Synthetic receive aperture (SRA)       
techniques, combined with adaptive imaging,  would be used to address           
1000 and 2000 element two dimensional (2-D) arrays  and to form very high       
resolution images.  Specialized analog multiplexers and  other hardware         
would be constructed for this system.  Clinical trials would  evaluate          
the performance of the adaptive/SRA system in imaging breast lesions  and       
breast microcalcifications, and in renal and adrenal gland imaging              
studies.  The applicants hypothesized that the proposed techniques and          
system  would markedly improve ultrasonic image quality in a wide variety       
of clinical  applications.                                                      
 adrenal disorder; artificial intelligence; calcification; computer simulation; computer system design /evaluation; endocrine disorder diagnosis; human subject; image processing; kidney disorder diagnosis; mammary disorder; nephrolithiasis; reproductive system disorder diagnosis; ultrasonography ADAPTIVE ULTRASONIC IMAGING","DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed        
to  develop adaptive imaging algorithms and instrumentation to compensate       
for  tissue-induced ultrasonic image degradation.  Theoretical and              
simulation  studies are proposed to optimize the accuracy, stability, and       
speed of  adaptive algorithms and to explore the impact of transducer           
design on adaptive  imaging.  In addition, the applicants proposed to           
collect high-quality tissue  echo data and through-transmission data to         
investigate the nature of tissue- induced image degradation.  The               
adaptive imaging techniques would be implemented in real-time on an             
advanced engineering prototype scanner.  Synthetic receive aperture (SRA)       
techniques, combined with adaptive imaging,  would be used to address           
1000 and 2000 element two dimensional (2-D) arrays  and to form very high       
resolution images.  Specialized analog multiplexers and  other hardware         
would be constructed for this system.  Clinical trials would  evaluate          
the performance of the adaptive/SRA system in imaging breast lesions  and       
breast microcalcifications, and in renal and adrenal gland imaging              
studies.  The applicants hypothesized that the proposed techniques and          
system  would markedly improve ultrasonic image quality in a wide variety       
of clinical  applications.                                                      
",2608036,R01CA043334,['R01CA043334'],CA,https://reporter.nih.gov/project-details/2608036,R01,1998,239717,-0.08524413715695396
"The development of a prototype software package, the Gen-Pep Algorithm,         
an entirely new, receptor sequence, hydrophobic free energy                     
eigenfunction-based proprietary system for short peptide design, will           
be completed.  Its feasibility with respect to a yield of new peptides          
which elicit target receptor-mediated biological activation in receptor         
cDNA transfected cell lines will be tested using Cytosensor                     
Microphysiometry.   Gen-Pep exploits a sequence of linear                       
transformations of the hydrophobic free energies of the amino acid              
sequences of peptide and their receptors and includes eigenvalue                
decomposition and complex pole spectral and wavelet transformation and          
a constructive step using the leading eigenvectors.  Given a peptide            
receptor sequence, this system generates a family of candidate peptides         
to interact with it.  As exemplars, short peptide analogues of                  
neurotensin and cholecystokinin will be designed and tested for their           
recently demonstrated direct action on the D2 dopamine receptor and the         
dopamine transporter proteins respectively.  New short peptide analogues        
of (longer peptide) epidemoid growth factor, transforming growth factor         
and fibrobast growth factor will be designed and tested similarly.              
Licensing and support of Gen-Pep including a superfamily hydrophobic            
free energy eigenfunction library, contract new peptide development for         
biotechnology, companies and new peptide-drug development constitute the        
three commercial goals of the enterprise.                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
1) Licensing Gen-Pep software, support and eigenfunction library for an         
annual fee to biotechnology and pharmaceutical research concerns.  2)           
Contracting with similar entities for Cielo Institute, Inc. to generate         
and preliminarily test promising candidate, short peptides designed for         
specific purposes.  3) Developing and testing new peptide ligands for           
patenting by Cielo Institute, Inc.                                              
 artificial intelligence; cell line; cholecystokinin; computer assisted sequence analysis; computer program /software; computer system design /evaluation; dopamine receptor; hydropathy; neuropeptide receptor; neuropeptides; neurotensin; protein engineering; protein sequence; receptor expression COMPUTATIONAL NEUROPEPTIDE DESIGN AND RECEPTOR MATCHING","The development of a prototype software package, the Gen-Pep Algorithm,         
an entirely new, receptor sequence, hydrophobic free energy                     
eigenfunction-based proprietary system for short peptide design, will           
be completed.  Its feasibility with respect to a yield of new peptides          
which elicit target receptor-mediated biological activation in receptor         
cDNA transfected cell lines will be tested using Cytosensor                     
Microphysiometry.   Gen-Pep exploits a sequence of linear                       
transformations of the hydrophobic free energies of the amino acid              
sequences of peptide and their receptors and includes eigenvalue                
decomposition and complex pole spectral and wavelet transformation and          
a constructive step using the leading eigenvectors.  Given a peptide            
receptor sequence, this system generates a family of candidate peptides         
to interact with it.  As exemplars, short peptide analogues of                  
neurotensin and cholecystokinin will be designed and tested for their           
recently demonstrated direct action on the D2 dopamine receptor and the         
dopamine transporter proteins respectively.  New short peptide analogues        
of (longer peptide) epidemoid growth factor, transforming growth factor         
and fibrobast growth factor will be designed and tested similarly.              
Licensing and support of Gen-Pep including a superfamily hydrophobic            
free energy eigenfunction library, contract new peptide development for         
biotechnology, companies and new peptide-drug development constitute the        
three commercial goals of the enterprise.                                       
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
1) Licensing Gen-Pep software, support and eigenfunction library for an         
annual fee to biotechnology and pharmaceutical research concerns.  2)           
Contracting with similar entities for Cielo Institute, Inc. to generate         
and preliminarily test promising candidate, short peptides designed for         
specific purposes.  3) Developing and testing new peptide ligands for           
patenting by Cielo Institute, Inc.                                              
",2536176,R43MH058026,['R43MH058026'],MH,https://reporter.nih.gov/project-details/2536176,R43,1998,95050,-0.10339831716942133
"DESCRIPTION:  Limitations of many disabled individuals' production efficiency, communication rate and vocabulary organization impact negatively on their ability to communicate competently using present-day Augmentative and Alternative Communication (AAC) devices.  This project proposes a re-conceptualization in the way language units are organized and retrieved in present-day AAC technologies, by operationalizing recent linguistic research in frame theory.  A frame-based organization for utterances is proposed by exploiting natural category systems found in the context, culture and conceptual structures of the individual.  A method and system for implementing these knowledge structures on a computer system will be developed. Specifically, the purpose of Phase l of this project is to provide a proof of concept for the Frametalker technology design.  During the course of the year we will a) develop a software prototype of the Frametalker, b) integrate it with a database of communication frame-structured utterances and c) submit Frametalker to laboratory and field-testing.  artificial intelligence; biomedical equipment development; clinical biomedical equipment; communication disorder aid; computer program /software; computer system design /evaluation; language; speech synthesizers; vocabulary FRAMETALKER--AN UTTERANCE BASED AUGMENTATIVE DEVICE","DESCRIPTION:  Limitations of many disabled individuals' production efficiency, communication rate and vocabulary organization impact negatively on their ability to communicate competently using present-day Augmentative and Alternative Communication (AAC) devices.  This project proposes a re-conceptualization in the way language units are organized and retrieved in present-day AAC technologies, by operationalizing recent linguistic research in frame theory.  A frame-based organization for utterances is proposed by exploiting natural category systems found in the context, culture and conceptual structures of the individual.  A method and system for implementing these knowledge structures on a computer system will be developed. Specifically, the purpose of Phase l of this project is to provide a proof of concept for the Frametalker technology design.  During the course of the year we will a) develop a software prototype of the Frametalker, b) integrate it with a database of communication frame-structured utterances and c) submit Frametalker to laboratory and field-testing. ",2850474,R43CA080715,['R43CA080715'],CA,https://reporter.nih.gov/project-details/2850474,R43,1998,93455,-0.029356469708609
"Breast cancer is clearly a major public health concern. Currently, the          
most effective method of detecting breast cancer is mammography. However,       
while most mammographic abnormalities are benign, they must all be              
confirmed by additional studies which may include additional mammograms,        
ultrasound, and biopsy. A computer aided diagnostic (CAD) aide, which can       
decrease the uncertainty inherent in the evaluation of mammograms, offers       
the potential of sparing many women the trauma involved in undergoing           
biopsies to confirm a diagnosis.                                                
                                                                                
This SBIR proposal, submitted by Technology/Engineering Management, Inc.,       
describes a research program to establish the feasibility of applying a         
linear programming (LP) based pattern classifier to the computer aided          
diagnosis of breast cancer. The ultimate goal of this research is to lay        
the foundation for a software-based advisor that would serve as a               
decision aide to the human diagnostician in assessing electronic images         
of breast tumors, thereby enabling a totally noninvasive diagnosis.             
Working in conjunction with a research team from the University of              
Chicago Department of Radiology (UCDR), we plan to evaluate the efficacy        
of the LP approach to discriminating malignant from benign lesions and          
compare it to a mammography diagnostic aide that relies on an artificial        
neural network (ANN) based classifier.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
This research will form the foundation for a computer aided diagnostic          
(CAD) system that can be used by radiologists to assist in the diagnosis        
of mammographic abnormalities. The technique can also be applied to other       
imaging procedures including magnetic resonance imaging and positron            
emission tomography.                                                            
 artificial intelligence; bioengineering /biomedical engineering; breast neoplasm /cancer diagnosis; computer assisted diagnosis; computer assisted medical decision making; computer program /software; computer system design /evaluation; diagnosis design /evaluation; human data; mammography; neoplasm /cancer classification /staging; noninvasive diagnosis LP AND NN BASED CLASSIFIERS FOR BREAST CANCER DIAGNOSIS","Breast cancer is clearly a major public health concern. Currently, the          
most effective method of detecting breast cancer is mammography. However,       
while most mammographic abnormalities are benign, they must all be              
confirmed by additional studies which may include additional mammograms,        
ultrasound, and biopsy. A computer aided diagnostic (CAD) aide, which can       
decrease the uncertainty inherent in the evaluation of mammograms, offers       
the potential of sparing many women the trauma involved in undergoing           
biopsies to confirm a diagnosis.                                                
                                                                                
This SBIR proposal, submitted by Technology/Engineering Management, Inc.,       
describes a research program to establish the feasibility of applying a         
linear programming (LP) based pattern classifier to the computer aided          
diagnosis of breast cancer. The ultimate goal of this research is to lay        
the foundation for a software-based advisor that would serve as a               
decision aide to the human diagnostician in assessing electronic images         
of breast tumors, thereby enabling a totally noninvasive diagnosis.             
Working in conjunction with a research team from the University of              
Chicago Department of Radiology (UCDR), we plan to evaluate the efficacy        
of the LP approach to discriminating malignant from benign lesions and          
compare it to a mammography diagnostic aide that relies on an artificial        
neural network (ANN) based classifier.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
This research will form the foundation for a computer aided diagnostic          
(CAD) system that can be used by radiologists to assist in the diagnosis        
of mammographic abnormalities. The technique can also be applied to other       
imaging procedures including magnetic resonance imaging and positron            
emission tomography.                                                            
",2715063,R43CA079259,['R43CA079259'],CA,https://reporter.nih.gov/project-details/2715063,R43,1998,82799,-0.08791180656101946
"The main goal of this research is to develop an open and extensible             
software environment for medical image segmentation.  Image segmentation        
has many applications in medical imaging, including tumor localization,         
and radiation therapy planning; however its current use falls far short         
of its potential.  One reason for this is the lack of support for image         
segmentation in commercially available general-purpose software tools.          
Our proposed research will overcome this limitation by providing a              
software environment will include a wide variety of state-of-the-art            
segmentation algorithms, and it will be extensible so that users can add        
new algorithms easily. Our software environment will further segmentation       
algorithm research by providing the basic infrastructure for image              
segmentation algorithm development, such as tools for image display,            
region-of-interest selection, manual segmentation, and algorithm                
evaluation, as well as the data structures for segmentation.  In Phase          
I of this research, we have developed a prototype of such a software            
environment using recent advances in software engineering.  In Phase II         
we will further develop the prototype to produce a full-featured software       
system.  We will design and add new segmentation algorithms and expand          
the algorithm evaluation capabilities. We will also apply the software          
to three important medical problems where reliable image segmentation is        
the key requirement.                                                            
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
We envision several families of products from this research: (l) Software       
applications for solving specific medical problems, targeted to clinical        
specialists. (2) General-purpose segmentation program for clinical              
generalists, to be combined with a develop infrastructure kit for use by        
software vendors and algorithm researchers. (3) Software components for         
use by software vendors to add segmentation capabilities to their image         
processing software. (4) Add in Software Modules                                
 artificial intelligence; computer human interaction; computer program /software; computer system design /evaluation; data collection methodology /evaluation; image enhancement; image processing; interactive multimedia SOFTWARE ENVIRONMENT FOR MEDICAL IMAGE SEGMENTION","The main goal of this research is to develop an open and extensible             
software environment for medical image segmentation.  Image segmentation        
has many applications in medical imaging, including tumor localization,         
and radiation therapy planning; however its current use falls far short         
of its potential.  One reason for this is the lack of support for image         
segmentation in commercially available general-purpose software tools.          
Our proposed research will overcome this limitation by providing a              
software environment will include a wide variety of state-of-the-art            
segmentation algorithms, and it will be extensible so that users can add        
new algorithms easily. Our software environment will further segmentation       
algorithm research by providing the basic infrastructure for image              
segmentation algorithm development, such as tools for image display,            
region-of-interest selection, manual segmentation, and algorithm                
evaluation, as well as the data structures for segmentation.  In Phase          
I of this research, we have developed a prototype of such a software            
environment using recent advances in software engineering.  In Phase II         
we will further develop the prototype to produce a full-featured software       
system.  We will design and add new segmentation algorithms and expand          
the algorithm evaluation capabilities. We will also apply the software          
to three important medical problems where reliable image segmentation is        
the key requirement.                                                            
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
We envision several families of products from this research: (l) Software       
applications for solving specific medical problems, targeted to clinical        
specialists. (2) General-purpose segmentation program for clinical              
generalists, to be combined with a develop infrastructure kit for use by        
software vendors and algorithm researchers. (3) Software components for         
use by software vendors to add segmentation capabilities to their image         
processing software. (4) Add in Software Modules                                
",2714991,R44CA074670,['R44CA074670'],CA,https://reporter.nih.gov/project-details/2714991,R44,1998,350421,-0.056042601713616226
"This proposal is directed toward improving tomographic imaging in               
diagnostic radiology and nuclear medicine. It is predicated on the claim        
that significant advances will be achieved in the fidelity of the images        
that are reconstructed from the raw detector measurements of the                
tomographic scanner by changing the basic elements (called ""basis               
functions"") with which the image is built in the computer. The                  
conventional basic elements for computerized tomographic imaging are the        
voxel basis functions, and the sinusoidal basis functions of Fourier            
analysis. Two classes of promising new basis functions have been                
developed: functions that are localized in space (as are the voxel basis        
functions), and functions that are not localized (similar in many respects      
to sinusoids). The new classes of basis functions are well-suited to            
constructing faithful digital image representations of the biological           
structures that have influenced the raw tomographic scanner data. The new       
localized basis functions have a number of very desirable properties not        
shared by voxels: they are rotationally symmetric, their Fourier                
transforms are effectively localized, and they have continuous derivatives      
of any desired order.  The new non-localized basis functions are designed       
to perform a spatially-variant filtering operation that is required by a        
non-iterative method of 3D image reconstruction developed by the Principal      
Investigator.                                                                   
                                                                                
The specific aims are to develop mathematical theory, efficient computer        
algorithms, application-specific implementations and evaluation criteria        
for (1) methods of iterative reconstruction from projections, (2) methods       
of estimating the fundamental limits on the performance of the                  
reconstruction process, and (3) methods of non-iterative 3D reconstruction      
from projections. For specified imaging tasks, the level of statistical         
significance will be found for rejection of the null hypothesis that two        
methods perform a task equally well, in favor of the alternative                
hypothesis that one method performs the task better.                            
                                                                                
The basis functions of the image representation are the essential core of       
all methods for computerized image reconstruction, irrespective of the          
medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of        
new computer algorithms and their associated image representations will         
enable the full potential of scanners for functional imaging in emission        
tomography (PET and SPECT) to be realized by extracting as much                 
information as possible from fully-3D low-statistics projection data.           
 artificial intelligence; computer simulation; computer system design /evaluation; digital imaging; model design /development; phantom model; positron emission tomography DIGITAL IMAGE REPRESENTATIONS FOR TOMOGRAPHIC RADIOLOGY","This proposal is directed toward improving tomographic imaging in               
diagnostic radiology and nuclear medicine. It is predicated on the claim        
that significant advances will be achieved in the fidelity of the images        
that are reconstructed from the raw detector measurements of the                
tomographic scanner by changing the basic elements (called ""basis               
functions"") with which the image is built in the computer. The                  
conventional basic elements for computerized tomographic imaging are the        
voxel basis functions, and the sinusoidal basis functions of Fourier            
analysis. Two classes of promising new basis functions have been                
developed: functions that are localized in space (as are the voxel basis        
functions), and functions that are not localized (similar in many respects      
to sinusoids). The new classes of basis functions are well-suited to            
constructing faithful digital image representations of the biological           
structures that have influenced the raw tomographic scanner data. The new       
localized basis functions have a number of very desirable properties not        
shared by voxels: they are rotationally symmetric, their Fourier                
transforms are effectively localized, and they have continuous derivatives      
of any desired order.  The new non-localized basis functions are designed       
to perform a spatially-variant filtering operation that is required by a        
non-iterative method of 3D image reconstruction developed by the Principal      
Investigator.                                                                   
                                                                                
The specific aims are to develop mathematical theory, efficient computer        
algorithms, application-specific implementations and evaluation criteria        
for (1) methods of iterative reconstruction from projections, (2) methods       
of estimating the fundamental limits on the performance of the                  
reconstruction process, and (3) methods of non-iterative 3D reconstruction      
from projections. For specified imaging tasks, the level of statistical         
significance will be found for rejection of the null hypothesis that two        
methods perform a task equally well, in favor of the alternative                
hypothesis that one method performs the task better.                            
                                                                                
The basis functions of the image representation are the essential core of       
all methods for computerized image reconstruction, irrespective of the          
medical imaging modality (e.g., CT, PET, SPECT, MRI). The development of        
new computer algorithms and their associated image representations will         
enable the full potential of scanners for functional imaging in emission        
tomography (PET and SPECT) to be realized by extracting as much                 
information as possible from fully-3D low-statistics projection data.           
",2700454,R01CA054356,['R01CA054356'],CA,https://reporter.nih.gov/project-details/2700454,R01,1998,237756,-0.07768352122422495
"The genes of the human leukocyte antigen (HLA) region control a variety         
Of functions involved in the immune response, and influence                     
susceptibility to over 40 diseases.  Our understanding of the structure         
and function of the HLA genes, their disease associations, and the              
evolutionary features of this multigene family has benefitted from recent       
advances in molecular biology, immunology, disease modelling and                
population genetics.  Theoretical studies in the development of models to       
determine the modes of inheritance of the HLA associated diseases have          
led to a better understanding of the inheritance patterns in insulin            
dependent diabetes mellitus, rheumatoid arthritis, multiple sclerosis,          
ankylosing spondylitis, hemochromatosis, celiac disease, and others.  It        
is now clear that many of the HLA associated diseases involve                   
heterogeneity in their HLA components, as well as non-HLA genetic               
components.                                                                     
                                                                                
The specific aims of our research are to study the genetic components in        
the etiology of the HLA associated diseases, and population genetic             
features of the HLA system.  A variety of methods to test modes of              
inheritance of diseases using marker allele information, will be                
developed.  Methods appropriate for the analysis of marker systems which        
are not highly polymorphic, to both detect linkage and determine modes of       
inheritance, will be investigated.  The information content of particular       
pedigree types for LOD score analysis will be investigated.  Two methods        
using patterns of linkage disequilibrium will be investigated to                
determine their usefulness in mapping disease predisposing genes.  A            
number of large collaborative data sets of HLA associated diseases will         
be analyzed.  A framework for genetic counselling of HLA associated, and        
other complex diseases, will be developed.  The results of our studies          
are generally applicable to the mapping and characterization of complex         
human genetic traits.                                                           
 European; Hodgkin's disease; MHC class I antigen; MHC class II antigen; T cell receptor; alleles; antiserum; artificial intelligence; biochemical evolution; celiac disease; computer assisted sequence analysis; computer data analysis; disease /disorder proneness /risk; gene frequency; genetic counseling; genetic disorder diagnosis; genetic markers; genetic models; genetic polymorphism; genotype; hereditary hemochromatosis; heterozygote; histocompatibility antigens; histocompatibility gene; homozygote; human genetic material tag; human population genetics; immunogenetics; insulin dependent diabetes mellitus; linkage disequilibriums; mathematical model; molecular genetics; multiple sclerosis; nucleic acid sequence; oligonucleotides; restriction fragment length polymorphism; rheumatoid arthritis; serotyping; statistics /biometry MODELS IN POPULATION GENETICS","The genes of the human leukocyte antigen (HLA) region control a variety         
Of functions involved in the immune response, and influence                     
susceptibility to over 40 diseases.  Our understanding of the structure         
and function of the HLA genes, their disease associations, and the              
evolutionary features of this multigene family has benefitted from recent       
advances in molecular biology, immunology, disease modelling and                
population genetics.  Theoretical studies in the development of models to       
determine the modes of inheritance of the HLA associated diseases have          
led to a better understanding of the inheritance patterns in insulin            
dependent diabetes mellitus, rheumatoid arthritis, multiple sclerosis,          
ankylosing spondylitis, hemochromatosis, celiac disease, and others.  It        
is now clear that many of the HLA associated diseases involve                   
heterogeneity in their HLA components, as well as non-HLA genetic               
components.                                                                     
                                                                                
The specific aims of our research are to study the genetic components in        
the etiology of the HLA associated diseases, and population genetic             
features of the HLA system.  A variety of methods to test modes of              
inheritance of diseases using marker allele information, will be                
developed.  Methods appropriate for the analysis of marker systems which        
are not highly polymorphic, to both detect linkage and determine modes of       
inheritance, will be investigated.  The information content of particular       
pedigree types for LOD score analysis will be investigated.  Two methods        
using patterns of linkage disequilibrium will be investigated to                
determine their usefulness in mapping disease predisposing genes.  A            
number of large collaborative data sets of HLA associated diseases will         
be analyzed.  A framework for genetic counselling of HLA associated, and        
other complex diseases, will be developed.  The results of our studies          
are generally applicable to the mapping and characterization of complex         
human genetic traits.                                                           
",2685132,R01GM056688,['R01GM056688'],GM,https://reporter.nih.gov/project-details/2685132,R01,1998,172242,-0.0035272658928653462
"A central problem for theories of memory development is how the superior        
memory of children and adults evolves from the memory abilities of              
infants and whether the mechanisms underlying this evolution are the            
same or different.  The paucity of data on infant long-term memory              
precludes a current solution.  In this application, research is proposed        
that will narrow the large gap between what is known about long-term            
memory in children and adults and what is known about short-term                
recognition memory in infants.  Three questions fundamental to an               
understanding of infant memory development will be addressed:  (1) How          
do selective attention, perception, and differences in information              
processing affect what is encoded and retrieved at different ages?  (2)         
How do new experiences interact with and modify established memories,           
and what are the temporal constraints on such interactions at different         
ages?  and (3) What are the consequences of retrieving active and               
inactive memories for the accessibility and organization of their               
different components?  Answers will be achieved via an instrumental             
learning procedure previously used to study memory from 2-6 months and          
analogous procedures that will be developed for use at 9-12 months.  In         
these procedures, infants learn a response that activates a distinctive         
toy in a distinctive context; 1 day later, their memory contents are            
probed with a retrieval cue.  Infants indicate whether or not the               
information in the cue or context was encoded by whether or not they            
perform the learned response.  The stable relation between effective            
retrieval cues after 1 day and effective memory primes after 2-3 weeks          
allows a convergent test of the results via a reminding procedure.  This        
research will provide new information about normal memory development in        
infancy and its relation both to memory processes of children and adults        
and to neuropsychological research on brain mechanisms implicated in            
memory formation.  From a health perspective, the research will provide         
a standard for detecting early cognitive deficits (or giftedness),              
particularly deficits that subsequently surface in tasks that require           
the utilization (retrieval) of accumulated, experienced-based                   
information.                                                                    
 association learning; behavioral /social science research tag; child psychology; cognition; cues; discrimination learning; human subject; infant human (0-1 year); learning; long term memory; neural information processing; neuropsychological tests; neuropsychology; perception; psychological reinforcement; short term memory; stimulus /response; visual stimulus CONDITIONING ANALYSIS OF INFANT MEMORY","A central problem for theories of memory development is how the superior        
memory of children and adults evolves from the memory abilities of              
infants and whether the mechanisms underlying this evolution are the            
same or different.  The paucity of data on infant long-term memory              
precludes a current solution.  In this application, research is proposed        
that will narrow the large gap between what is known about long-term            
memory in children and adults and what is known about short-term                
recognition memory in infants.  Three questions fundamental to an               
understanding of infant memory development will be addressed:  (1) How          
do selective attention, perception, and differences in information              
processing affect what is encoded and retrieved at different ages?  (2)         
How do new experiences interact with and modify established memories,           
and what are the temporal constraints on such interactions at different         
ages?  and (3) What are the consequences of retrieving active and               
inactive memories for the accessibility and organization of their               
different components?  Answers will be achieved via an instrumental             
learning procedure previously used to study memory from 2-6 months and          
analogous procedures that will be developed for use at 9-12 months.  In         
these procedures, infants learn a response that activates a distinctive         
toy in a distinctive context; 1 day later, their memory contents are            
probed with a retrieval cue.  Infants indicate whether or not the               
information in the cue or context was encoded by whether or not they            
perform the learned response.  The stable relation between effective            
retrieval cues after 1 day and effective memory primes after 2-3 weeks          
allows a convergent test of the results via a reminding procedure.  This        
research will provide new information about normal memory development in        
infancy and its relation both to memory processes of children and adults        
and to neuropsychological research on brain mechanisms implicated in            
memory formation.  From a health perspective, the research will provide         
a standard for detecting early cognitive deficits (or giftedness),              
particularly deficits that subsequently surface in tasks that require           
the utilization (retrieval) of accumulated, experienced-based                   
information.                                                                    
",2674789,R37MH032307,['R37MH032307'],MH,https://reporter.nih.gov/project-details/2674789,R37,1998,345581,-0.10645959862826546
"Efforts to apply computer methods to assess and improve the quality of          
care in the hospital have been stymied by limited access to clinical            
data.  Free-text data have detailed clinical descriptions of patients           
that would be useful in computer altering systems and computer reminder         
systems.  However, free-text data cannot be interpreted by most clinical        
computer systems.  In this proposal, we describe research specifically          
aimed at making free-text data accessible to computer-based applications        
for assessing and improving the quality of care.  In particular the             
research plan focuses on the development of technologies that would allow       
free-text data to be used in clinical alert systems for critical test           
results; in reminder systems to encourage adherence to practice                 
guidelines; and in data collection systems for severity of illness models       
applied in the assessment of risk adjusted outcomes.  The approach              
described in the research plan emphasizes the development of statistical        
and probabilistic methods for interpretation of data derived from medical       
language processing systems.  We will test the methods developed for            
language processing and interpretation developed under this proposal in         
3 area: 1) the identification of concepts related to severity from the          
MedisGroups and the Computerized Severity Index models of patient               
severity of illness; 2) the identification of chest x ray reports and           
mammography reports with potentially malignant findings that require            
radiological follow-up; 3) and the automatic assessment of                      
appropriateness of coronary artery bypass grafting (CABG) surgery from          
free-text descriptions of patients based on the application of a clinical       
practice guideline for CABG surgery.                                            
 abstracting; artificial intelligence; automated medical record system; computer assisted diagnosis; computer assisted medical decision making; computer human interaction; coronary bypass; health care model; health care quality; human data; information retrieval; mammography; method development; statistics /biometry; thoracic radiography; vocabulary development for information system COMPUTER INTERPRETATION OF FREE-TEXT DATA","Efforts to apply computer methods to assess and improve the quality of          
care in the hospital have been stymied by limited access to clinical            
data.  Free-text data have detailed clinical descriptions of patients           
that would be useful in computer altering systems and computer reminder         
systems.  However, free-text data cannot be interpreted by most clinical        
computer systems.  In this proposal, we describe research specifically          
aimed at making free-text data accessible to computer-based applications        
for assessing and improving the quality of care.  In particular the             
research plan focuses on the development of technologies that would allow       
free-text data to be used in clinical alert systems for critical test           
results; in reminder systems to encourage adherence to practice                 
guidelines; and in data collection systems for severity of illness models       
applied in the assessment of risk adjusted outcomes.  The approach              
described in the research plan emphasizes the development of statistical        
and probabilistic methods for interpretation of data derived from medical       
language processing systems.  We will test the methods developed for            
language processing and interpretation developed under this proposal in         
3 area: 1) the identification of concepts related to severity from the          
MedisGroups and the Computerized Severity Index models of patient               
severity of illness; 2) the identification of chest x ray reports and           
mammography reports with potentially malignant findings that require            
radiological follow-up; 3) and the automatic assessment of                      
appropriateness of coronary artery bypass grafting (CABG) surgery from          
free-text descriptions of patients based on the application of a clinical       
practice guideline for CABG surgery.                                            
",2909061,R29LM005626,['R29LM005626'],LM,https://reporter.nih.gov/project-details/2909061,R29,1998,53241,-0.011566087842029909
"Mandala Sciences (MSI) CODA project has 2 main objectives: (1) develop          
analysis tools to test hypotheses regarding effectiveness of surgical           
procedures and patient outcomes and (2) generate proprietary decision           
support prediction models for hip and knee replacements.  MSI hybrid            
Neural Network/Expert System methodology uses an Entropy NN TM structure        
which has the innovative ability to generate a rule base.  The discovered       
rules will be used to create ""portable"" Expert System predictive modules.       
Phase II progress is built upon successful MSI collaboration with Henry         
Ford Health System to show NN techniques can generate and evaluate              
prognostic models using outcomes data.  Consultation with orthopedic            
surgeons identified 13 patient-provided variables as potential predictors       
of hip replacement surgery failure. An NN trained on these data predicted       
the 1-year post-surgical change in the patient's self-assessed pain and         
physical function scores.  Comparison with standard statistical analysis        
techniques showed superior accuracy of NN-based predictions.  Phase II          
research will generalize the product by adopting the ASTM-E-1238 interface      
standard for data collection from multiple sources.  NN/Expert prediction       
models will be improved by pooling data from geographically diverse sites       
and field trial performance to evaluate physician-rated adoption,               
usefulness, and influence on their actual decision making.                      
                                                                                
Proposed commercial applications:                                               
MSI will build a user-friendly, stand-alone outcomes database analysis          
tool. By using new proprietary neural network techniques, the system will       
have the predictive power of NN combined with the explanatory capabilities      
of an expert system. This computer system will be adopted by the widest         
possible audience because it will be far easier to use than conventional        
statistical packages, and by virtue of being designed for compatibility         
with the ASTM-E-1238 standard for outcomes data transmission.                   
 artificial intelligence; computer assisted medical decision making; computer human interaction; computer program /software; computer system design /evaluation; data collection methodology /evaluation; health care model; hip prosthesis; human data; human therapy evaluation; mathematical model; model design /development; outcomes research; postoperative state; prognosis COMPUTER TOOLS FOR OUTCOMES ANALYSIS OF HIP REPLACEMENT","Mandala Sciences (MSI) CODA project has 2 main objectives: (1) develop          
analysis tools to test hypotheses regarding effectiveness of surgical           
procedures and patient outcomes and (2) generate proprietary decision           
support prediction models for hip and knee replacements.  MSI hybrid            
Neural Network/Expert System methodology uses an Entropy NN TM structure        
which has the innovative ability to generate a rule base.  The discovered       
rules will be used to create ""portable"" Expert System predictive modules.       
Phase II progress is built upon successful MSI collaboration with Henry         
Ford Health System to show NN techniques can generate and evaluate              
prognostic models using outcomes data.  Consultation with orthopedic            
surgeons identified 13 patient-provided variables as potential predictors       
of hip replacement surgery failure. An NN trained on these data predicted       
the 1-year post-surgical change in the patient's self-assessed pain and         
physical function scores.  Comparison with standard statistical analysis        
techniques showed superior accuracy of NN-based predictions.  Phase II          
research will generalize the product by adopting the ASTM-E-1238 interface      
standard for data collection from multiple sources.  NN/Expert prediction       
models will be improved by pooling data from geographically diverse sites       
and field trial performance to evaluate physician-rated adoption,               
usefulness, and influence on their actual decision making.                      
                                                                                
Proposed commercial applications:                                               
MSI will build a user-friendly, stand-alone outcomes database analysis          
tool. By using new proprietary neural network techniques, the system will       
have the predictive power of NN combined with the explanatory capabilities      
of an expert system. This computer system will be adopted by the widest         
possible audience because it will be far easier to use than conventional        
statistical packages, and by virtue of being designed for compatibility         
with the ASTM-E-1238 standard for outcomes data transmission.                   
",2769332,R44AG012308,['R44AG012308'],AG,https://reporter.nih.gov/project-details/2769332,R44,1998,351541,-0.007178963642176563
"Much progress has been made in the fight against breast cancer through          
early detection with screening mammography. Unfortunately, breast cancer        
can be missed on mammograms due to the difficulty of interpretation and         
inter-reader variability. For this reason, MedDetect, has worked to build       
a hybrid optical and digital processor for the identification of                
potentially cancerous lesions on mammograms. The processor is comprised         
of an optical correlator (OC) combined with a neural network (NN). It is        
designed to mimic the radiologist - the OC being the eyes rapidly               
scanning for areas of interest, and the NN being the brain making a             
recommendation that the case is normal or has suspicious areas.                 
MedDetect's proposed hybrid processor takes advantage of the best of both       
worlds -- the best elements of optical processing and digital computing         
to create a complete Computer-aided Diagnosis (CAD) system. Supporting          
MedDetect in this effort are radiologists Drs. Brenner, Sadowsky, and           
Levy, University of South Florida (Dr. L. Clarke) algorithm experts, and        
optics experts at Lockheed Martin. If the proposed hypotheses are proven,       
this innovative technology will be ready for rapid transition into the          
clinical setting where it can assist radiologists in the early detection        
of breast cancer.                                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The proposed technology is responsive to a significant market. 25 million       
mammograms are performed each year and the number is growing each year.         
A similar volume of studies exists internationally. The ability to find         
breast cancer more consistently and perhaps earlier is of great interest        
to patients, radiologists, and payors. Thus, success with this research         
should lead to significant business opportunities.                              
 artificial intelligence; computer assisted diagnosis; computer data analysis; computer system design /evaluation; human data; image processing; mammography; optics OPTICAL PROCESSOR BASED CAD FOR MAMMOGRAPHY","Much progress has been made in the fight against breast cancer through          
early detection with screening mammography. Unfortunately, breast cancer        
can be missed on mammograms due to the difficulty of interpretation and         
inter-reader variability. For this reason, MedDetect, has worked to build       
a hybrid optical and digital processor for the identification of                
potentially cancerous lesions on mammograms. The processor is comprised         
of an optical correlator (OC) combined with a neural network (NN). It is        
designed to mimic the radiologist - the OC being the eyes rapidly               
scanning for areas of interest, and the NN being the brain making a             
recommendation that the case is normal or has suspicious areas.                 
MedDetect's proposed hybrid processor takes advantage of the best of both       
worlds -- the best elements of optical processing and digital computing         
to create a complete Computer-aided Diagnosis (CAD) system. Supporting          
MedDetect in this effort are radiologists Drs. Brenner, Sadowsky, and           
Levy, University of South Florida (Dr. L. Clarke) algorithm experts, and        
optics experts at Lockheed Martin. If the proposed hypotheses are proven,       
this innovative technology will be ready for rapid transition into the          
clinical setting where it can assist radiologists in the early detection        
of breast cancer.                                                               
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The proposed technology is responsive to a significant market. 25 million       
mammograms are performed each year and the number is growing each year.         
A similar volume of studies exists internationally. The ability to find         
breast cancer more consistently and perhaps earlier is of great interest        
to patients, radiologists, and payors. Thus, success with this research         
should lead to significant business opportunities.                              
",2756670,R43CA079410,['R43CA079410'],CA,https://reporter.nih.gov/project-details/2756670,R43,1998,98741,-0.03676764865979244
"The goal of this proposal is the development of Computer-Aided                  
Instrument Design (CAID) tools and resources for the creation of                
embedded biomedical instruments. The Phase I work has demonstrated              
the conceptual feasibility of our approach through the development              
of a programmable, portable, hardware platform and a reusable set               
of software libraries. These tools and libraries were used to create            
a prototype of a Cardiac Monitor instrument. The Phase II work is               
aimed at expanding and improving the existing rudimentary CAID tools            
and resources to create a complete, turnkey environment for the                 
design of many different physiological instruments. The Phase II                
work will additionally involve creating and testing two                         
representative instruments: one will be an advanced ECG research                
device suggested and defined by our collaborators at JHU (Johns                 
Hopkins University); the other will be an Airway Monitor, suggested             
and defined by our collaborators at EVMS (Eastern Virginia Medical              
School).                                                                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The potential commercial                       
opportunities of our proposed technology are compelling. In addition            
to our selected Phase II experimental projects, various potential               
collaborators suggested applications including: an advanced Holter              
monitor with real-time detection of specific arrhythmias, fetal                 
heart monitors (using acoustic-based sensors of different                       
technologies), vital signs acquisition and logging,                             
ambulatory/portable EEG devices for sleep studies, seizure, and                 
brain injury detection, and many others.                                        
 artificial intelligence; biomedical equipment development; clinical biomedical equipment; computer program /software; computer simulation; computer system design /evaluation; electrical measurement; electrocardiography PHYSIOLOGICAL SIGNAL PROCESSOR","The goal of this proposal is the development of Computer-Aided                  
Instrument Design (CAID) tools and resources for the creation of                
embedded biomedical instruments. The Phase I work has demonstrated              
the conceptual feasibility of our approach through the development              
of a programmable, portable, hardware platform and a reusable set               
of software libraries. These tools and libraries were used to create            
a prototype of a Cardiac Monitor instrument. The Phase II work is               
aimed at expanding and improving the existing rudimentary CAID tools            
and resources to create a complete, turnkey environment for the                 
design of many different physiological instruments. The Phase II                
work will additionally involve creating and testing two                         
representative instruments: one will be an advanced ECG research                
device suggested and defined by our collaborators at JHU (Johns                 
Hopkins University); the other will be an Airway Monitor, suggested             
and defined by our collaborators at EVMS (Eastern Virginia Medical              
School).                                                                        
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The potential commercial                       
opportunities of our proposed technology are compelling. In addition            
to our selected Phase II experimental projects, various potential               
collaborators suggested applications including: an advanced Holter              
monitor with real-time detection of specific arrhythmias, fetal                 
heart monitors (using acoustic-based sensors of different                       
technologies), vital signs acquisition and logging,                             
ambulatory/portable EEG devices for sleep studies, seizure, and                 
brain injury detection, and many others.                                        
",2771527,R44HL056545,['R44HL056545'],HL,https://reporter.nih.gov/project-details/2771527,R44,1998,370851,0.019498607242339434
"Clinically significant delays in the acquisition of language are the most       
prevalent developmental problem in preschool children.  Early                   
intervention is crucial since children with language delays are at greatly      
increased risk for learning disabilities and behavioral disorders, but          
intervention resources are scarce and not sufficient to provide the             
intensive individualized instruction that effective intervention requires.      
The objective of this project is to develop a computer-based Intelligent        
Early Language Intervention System that will supplement the efforts of          
those who work with language impaired children.  The system will                
combine traditional intervention methods with strategies derived from           
contemporary linguistic theory, and will incorporate an intelligent             
computer-aided training (ICAT) system that uses artificial intelligence         
to generate customized language intervention strategies based on an             
individual~s performance.  In Phase I we developed and field tested a           
prototype system.  Our Phase II objectives are to (a) further develop           
the prototype activities, (b) develop additional activities for children at     
later stages of language development, ' field test and validate the             
effectiveness of program components, and (d) integrate all components           
into the ICAT framework.  We anticipate this system will be an asset            
to clinical service providers, teachers, and parents who work with              
children who have language impairments.                                         
 artificial intelligence; behavioral /social science research tag; clinical research; computer assisted instruction; computer program /software; human subject; language development; language disorders; method development; preschool child (1-5) INTELLIGENT EARLY LANGUAGE INTERVENTION SYSTEM","Clinically significant delays in the acquisition of language are the most       
prevalent developmental problem in preschool children.  Early                   
intervention is crucial since children with language delays are at greatly      
increased risk for learning disabilities and behavioral disorders, but          
intervention resources are scarce and not sufficient to provide the             
intensive individualized instruction that effective intervention requires.      
The objective of this project is to develop a computer-based Intelligent        
Early Language Intervention System that will supplement the efforts of          
those who work with language impaired children.  The system will                
combine traditional intervention methods with strategies derived from           
contemporary linguistic theory, and will incorporate an intelligent             
computer-aided training (ICAT) system that uses artificial intelligence         
to generate customized language intervention strategies based on an             
individual~s performance.  In Phase I we developed and field tested a           
prototype system.  Our Phase II objectives are to (a) further develop           
the prototype activities, (b) develop additional activities for children at     
later stages of language development, ' field test and validate the             
effectiveness of program components, and (d) integrate all components           
into the ICAT framework.  We anticipate this system will be an asset            
to clinical service providers, teachers, and parents who work with              
children who have language impairments.                                         
",2608281,R44DC002601,['R44DC002601'],DC,https://reporter.nih.gov/project-details/2608281,R44,1998,341009,-0.1462032657682059
"DESCRIPTION (Adapted from the applicant's abstract):  Neuroblastoma is the      
most common extra cranial tumor of childhood, and cases are highly              
heterogenous with regard to clinical behavior.  Although patient groups with    
different expected survivals can be identified by clinical staging at           
diagnosis, individual clinically defined risk groups include patients with      
quite different outcomes.  Inter- and intra-stage diversity provides            
opportunities for identifying molecular genetic and biologic properties of      
tumors that are associated with treatment outcome.  Such correlations can       
identify risk groups that otherwise are not recognizable, thus aiding in the    
interpretation of clinical studies, and they can provide new criteria for       
more appropriate therapy assignment.  They may also suggest new approaches      
to therapy.  The long-term goal of the proposed studies is to develop tests     
that improve definition of risk groups so that the most appropriate and         
effective therapy can be given to each patient.                                 
                                                                                
The hypothesis of this application is that subsets of neuroblastomas can be     
identified by evaluating (1) genes that are critically involved in              
neuroblastoma growth, differentiation, and survival and (2) the ability of      
tumor cells to grow continuously in vitro.  Plans are to build upon previous    
studies of N-myc proto-oncogene expression in neuroblastoma and of the          
neurotrophins (nerve growth factor, NGF, and brain derived neurotrophic         
factor, BDNF) and their receptors (e.g., TrkAl), and of tumor cell growth in    
vitro, which demonstrated the potential importance of these markers in          
prognostication.                                                                
                                                                                
The specific aims are as follows:  (1) determine if tumor phenotype defined     
by MYCN gene amplification and expression, TrkA expression, telomerase RNA      
expression, and growth in vitro correlates with disease progression during      
or after therapy; (2) develop multivariate statistical models for predicting    
outcome based upon clinical presentation (stage and age) and laboratory data    
(tumor MYCN gene amplification, TrkA expression, telomerase RNA expression,     
growth in vitro, and histopathology); (3) determine in a pilot study if         
expression of other neurotrophins and their receptors defines risk groups.      
If so, include these in the above analyses.                                     
                                                                                
The CCG performs phase III studies in which newly diagnosed patients receive    
therapy according to risk classification, which is currently based upon         
clinical stage, age, histopathology, and N-myc gene amplification status.       
Approximately 200 patients are registered annually in studies for low,          
intermediate, and high-risk neuroblastoma.  Tumor tissues including (in some    
patients) bone marrow with tumor are obtained at diagnosis prospectively.       
Tumors are tested to determine their phenotype with regard to N-myc             
amplification, trk RNA expression level and pattern of expression, BDNF RNA     
expression, telomerase RNA expression and activity, chromosome 1p deletions,    
and tumor cell growth in vitro.  The goal is to determine if these              
laboratory test results can identify clinically important but small subsets     
of patients who may benefit from different therapy.  If preliminary studies     
indicate that specific tests are particularly important in predicting           
prognosis, large-scale studies will be conducted to better define the value     
of these new tests (e.g., their ability to identify patients who will likely    
fail treatment).                                                                
                                                                                
It is anticipated that these studies will improve prognostication and that      
they may contribute to development of novel and possibly more effective         
therapies for high-risk patients, while diminishing the risks of treatment      
for low risk patients.                                                          
 adolescence (12-20); artificial intelligence; cancer risk; child (0-11); clinical research; cooperative study; gene expression; growth factor receptors; human subject; human therapy evaluation; loss of heterozygosity; mathematical model; molecular oncology; neoplasm /cancer chemotherapy; neoplasm /cancer genetics; neuroblastoma; neurotrophic factors; pediatric neoplasm /cancer; polymerase chain reaction; protooncogene; receptor expression; telomerase; telomere; tumor suppressor genes CLINICAL CORRELATIVE STUDIES OF NEUROBLASTOMA","DESCRIPTION (Adapted from the applicant's abstract):  Neuroblastoma is the      
most common extra cranial tumor of childhood, and cases are highly              
heterogenous with regard to clinical behavior.  Although patient groups with    
different expected survivals can be identified by clinical staging at           
diagnosis, individual clinically defined risk groups include patients with      
quite different outcomes.  Inter- and intra-stage diversity provides            
opportunities for identifying molecular genetic and biologic properties of      
tumors that are associated with treatment outcome.  Such correlations can       
identify risk groups that otherwise are not recognizable, thus aiding in the    
interpretation of clinical studies, and they can provide new criteria for       
more appropriate therapy assignment.  They may also suggest new approaches      
to therapy.  The long-term goal of the proposed studies is to develop tests     
that improve definition of risk groups so that the most appropriate and         
effective therapy can be given to each patient.                                 
                                                                                
The hypothesis of this application is that subsets of neuroblastomas can be     
identified by evaluating (1) genes that are critically involved in              
neuroblastoma growth, differentiation, and survival and (2) the ability of      
tumor cells to grow continuously in vitro.  Plans are to build upon previous    
studies of N-myc proto-oncogene expression in neuroblastoma and of the          
neurotrophins (nerve growth factor, NGF, and brain derived neurotrophic         
factor, BDNF) and their receptors (e.g., TrkAl), and of tumor cell growth in    
vitro, which demonstrated the potential importance of these markers in          
prognostication.                                                                
                                                                                
The specific aims are as follows:  (1) determine if tumor phenotype defined     
by MYCN gene amplification and expression, TrkA expression, telomerase RNA      
expression, and growth in vitro correlates with disease progression during      
or after therapy; (2) develop multivariate statistical models for predicting    
outcome based upon clinical presentation (stage and age) and laboratory data    
(tumor MYCN gene amplification, TrkA expression, telomerase RNA expression,     
growth in vitro, and histopathology); (3) determine in a pilot study if         
expression of other neurotrophins and their receptors defines risk groups.      
If so, include these in the above analyses.                                     
                                                                                
The CCG performs phase III studies in which newly diagnosed patients receive    
therapy according to risk classification, which is currently based upon         
clinical stage, age, histopathology, and N-myc gene amplification status.       
Approximately 200 patients are registered annually in studies for low,          
intermediate, and high-risk neuroblastoma.  Tumor tissues including (in some    
patients) bone marrow with tumor are obtained at diagnosis prospectively.       
Tumors are tested to determine their phenotype with regard to N-myc             
amplification, trk RNA expression level and pattern of expression, BDNF RNA     
expression, telomerase RNA expression and activity, chromosome 1p deletions,    
and tumor cell growth in vitro.  The goal is to determine if these              
laboratory test results can identify clinically important but small subsets     
of patients who may benefit from different therapy.  If preliminary studies     
indicate that specific tests are particularly important in predicting           
prognosis, large-scale studies will be conducted to better define the value     
of these new tests (e.g., their ability to identify patients who will likely    
fail treatment).                                                                
                                                                                
It is anticipated that these studies will improve prognostication and that      
they may contribute to development of novel and possibly more effective         
therapies for high-risk patients, while diminishing the risks of treatment      
for low risk patients.                                                          
",2542476,R01CA060104,['R01CA060104'],CA,https://reporter.nih.gov/project-details/2542476,R01,1998,425718,-0.016498734140993312
"The proposed research is intended to address the problem of                     
extravasation events in the neonatal and pediatric patient population.          
Extravasations of intravenous fluids in children can have serious               
consequences when gross extravasations occur. Skin necrosis can occur           
which may require treatment with skin grafting. These adverse sequelae          
are less frequent in the adult patient, but in newborns and young               
children they are much more prevalent and can be catastrophic.                  
                                                                                
The proposed research is intended to evaluate newly available Passive           
Microwave Radiometry (PMR) technology as a modality for early detection         
of extravasation events in children. The PMR technique is passive, non-         
invasive and measures subcutaneous tissue temperatures.                         
                                                                                
Emphasis will be placed on developing a small, lightweight PMR Sensor           
consisting of a transducer (antenna) element and a Microwave Monolithic         
Integrated Circuit (MMIC) radiometric receiver. This Sensor will be             
integrated into a pre-production pediatric and neonatal extravasation           
detection system. In-vivo experiments, using an appropriate animal              
model, will be used to evaluate the ability of the system to distinguish        
between normal IV infusions and extravasated infusions.                         
                                                                                
The in-vivo performance data collected in the proposed program will be          
used to develop an extravasation detection algorithm and implemented in         
an embedded microcontroller based monitor.                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is a high incidence of extravasations occurring during the                
administration of IV fluids in children. One study  indicates an 11             
percent extravasation rate. The proposed PMR technique is passive,              
noninvasive and has the potential of being adapted in a small, low cost         
package for the neonatal and pediatric market. These characteristics            
will ultimately make conventional IV infusion procedures for the                
children safer.                                                                 
 artificial intelligence; biomedical equipment development; blood vessel disorder; body temperature; cardiovascular disorder diagnosis; diagnosis design /evaluation; early diagnosis; injection /infusion; microwave radiation; newborn human (0-6 weeks); noninvasive diagnosis; pediatrics; radiodiagnosis; swine EARLY DETECTION OF NEONATAL & PEDIATRIC EXTRAVASATIONS","The proposed research is intended to address the problem of                     
extravasation events in the neonatal and pediatric patient population.          
Extravasations of intravenous fluids in children can have serious               
consequences when gross extravasations occur. Skin necrosis can occur           
which may require treatment with skin grafting. These adverse sequelae          
are less frequent in the adult patient, but in newborns and young               
children they are much more prevalent and can be catastrophic.                  
                                                                                
The proposed research is intended to evaluate newly available Passive           
Microwave Radiometry (PMR) technology as a modality for early detection         
of extravasation events in children. The PMR technique is passive, non-         
invasive and measures subcutaneous tissue temperatures.                         
                                                                                
Emphasis will be placed on developing a small, lightweight PMR Sensor           
consisting of a transducer (antenna) element and a Microwave Monolithic         
Integrated Circuit (MMIC) radiometric receiver. This Sensor will be             
integrated into a pre-production pediatric and neonatal extravasation           
detection system. In-vivo experiments, using an appropriate animal              
model, will be used to evaluate the ability of the system to distinguish        
between normal IV infusions and extravasated infusions.                         
                                                                                
The in-vivo performance data collected in the proposed program will be          
used to develop an extravasation detection algorithm and implemented in         
an embedded microcontroller based monitor.                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is a high incidence of extravasations occurring during the                
administration of IV fluids in children. One study  indicates an 11             
percent extravasation rate. The proposed PMR technique is passive,              
noninvasive and has the potential of being adapted in a small, low cost         
package for the neonatal and pediatric market. These characteristics            
will ultimately make conventional IV infusion procedures for the                
children safer.                                                                 
",2540382,R44HD029994,['R44HD029994'],HD,https://reporter.nih.gov/project-details/2540382,R44,1998,342168,-0.0577333040500775
"DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling      
of cognition, it is important to have well-justified criteria for choosing      
among differing explanations (i.e., models) of observed data.  This project     
investigates those criteria as well as their instantiation in five model        
selection methods.                                                              
                                                                                
Two lines of research will be undertaken.  In the first, a thorough             
investigation of model complexity will be conducted.  Comprehensive             
simulations re intended to determine complexity's contribution to model fit     
and to model selection.  An analytical solution will also be sought with the    
hope of quantifying model complexity.                                           
                                                                                
The second line of work examines the utility of each of the five selection      
methods in choosing among models in three topic areas in cognitive              
psychology (information integration, categorization, connectionist              
modeling), the end goal being to identify their merits and shortcomings.        
                                                                                
Findings should provide a better understanding of model selection than          
currently available and serve as a useful guide for researchers comparing       
the suitability of quantitative models of cognition.                            
                                                                                
 artificial intelligence; choice; cognition; computer simulation; information dissemination; mathematical model; psychometrics SELECTING AMONG MATHEMATICAL MODELS OF COGNITION","DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling      
of cognition, it is important to have well-justified criteria for choosing      
among differing explanations (i.e., models) of observed data.  This project     
investigates those criteria as well as their instantiation in five model        
selection methods.                                                              
                                                                                
Two lines of research will be undertaken.  In the first, a thorough             
investigation of model complexity will be conducted.  Comprehensive             
simulations re intended to determine complexity's contribution to model fit     
and to model selection.  An analytical solution will also be sought with the    
hope of quantifying model complexity.                                           
                                                                                
The second line of work examines the utility of each of the five selection      
methods in choosing among models in three topic areas in cognitive              
psychology (information integration, categorization, connectionist              
modeling), the end goal being to identify their merits and shortcomings.        
                                                                                
Findings should provide a better understanding of model selection than          
currently available and serve as a useful guide for researchers comparing       
the suitability of quantitative models of cognition.                            
                                                                                
",2694082,R01MH057472,['R01MH057472'],MH,https://reporter.nih.gov/project-details/2694082,R01,1998,77392,-0.03913193548957673
"The goal of the proposed research is to develop computerized radiographic       
methods for measuring bone structure for use in quantitatively assessing        
osteoporosis and risk of fracture. We will investigate the characteristics      
of trabecular bone structure in digital radiographs in the spine, hip and       
extremities using computerized texture analyses. We believe that our            
methods have the potential to aid in the assessment of osteoporosis and         
that the use of both BMD and bone structure information should improve the      
predictive value for assessing fracture risk over that obtainable with BMD      
alone.                                                                          
                                                                                
We will create a database in order to quantify the characteristic features      
of the trabecular pattern in high-resolution radiographic bone images of        
patients with varying degrees of osteoporosis, as well as in normal             
subjects. Specifically, we plan to (l) develop computerized texture             
analysis schemes for the automatic assessment of bone structure in              
digitized bone radiographs, (2) investigate the effects of various              
parameters of the image acquisition system, as well as of the analysis          
schemes themselves, on performance and (3) evaluate the efficacies of the       
computerized schemes in predicting risk of fracture as compared to a            
current method of measurement [dual-energy x-ray absorptiometry (DXA)]          
using a large clinical database.                                                
                                                                                
Methods that are capable of analyzing bone structure of trabeculae, along       
with bone mass measures, are expected to give additional insight to the         
evaluation of osteoporosis and risk of fracture. Our scheme is unique in        
that it attempts to quantify automatically the risk of fracture from            
texture analyses (Fourier analysis, multi-fractal analysis, gradient            
analysis and artificial neural networks) of the bone trabecular pattern as      
present in high-resolution radiographs of the spine, hip and extremities.       
The potential significance of this research project lies in the fact that       
if the detection of high-risk patients could be accomplished with a             
reliable, low-dose, economical system, then screening for osteoporosis          
could be implemented more broadly, thereby allowing earlier treatment and       
a reduction in the risk of fracture.                                            
 artificial intelligence; bone density; bone fracture; clinical research; computer assisted diagnosis; densitometry; diagnosis design /evaluation; disease /disorder proneness /risk; hip; human subject; information systems; limbs; mathematical model; noninvasive diagnosis; osteoporosis; photon absorptiometry; radiography; spine; women's health COMPUTERIZED RADIOGRAPHIC ANALYSIS OF BONE STRUCTURE","The goal of the proposed research is to develop computerized radiographic       
methods for measuring bone structure for use in quantitatively assessing        
osteoporosis and risk of fracture. We will investigate the characteristics      
of trabecular bone structure in digital radiographs in the spine, hip and       
extremities using computerized texture analyses. We believe that our            
methods have the potential to aid in the assessment of osteoporosis and         
that the use of both BMD and bone structure information should improve the      
predictive value for assessing fracture risk over that obtainable with BMD      
alone.                                                                          
                                                                                
We will create a database in order to quantify the characteristic features      
of the trabecular pattern in high-resolution radiographic bone images of        
patients with varying degrees of osteoporosis, as well as in normal             
subjects. Specifically, we plan to (l) develop computerized texture             
analysis schemes for the automatic assessment of bone structure in              
digitized bone radiographs, (2) investigate the effects of various              
parameters of the image acquisition system, as well as of the analysis          
schemes themselves, on performance and (3) evaluate the efficacies of the       
computerized schemes in predicting risk of fracture as compared to a            
current method of measurement [dual-energy x-ray absorptiometry (DXA)]          
using a large clinical database.                                                
                                                                                
Methods that are capable of analyzing bone structure of trabeculae, along       
with bone mass measures, are expected to give additional insight to the         
evaluation of osteoporosis and risk of fracture. Our scheme is unique in        
that it attempts to quantify automatically the risk of fracture from            
texture analyses (Fourier analysis, multi-fractal analysis, gradient            
analysis and artificial neural networks) of the bone trabecular pattern as      
present in high-resolution radiographs of the spine, hip and extremities.       
The potential significance of this research project lies in the fact that       
if the detection of high-risk patients could be accomplished with a             
reliable, low-dose, economical system, then screening for osteoporosis          
could be implemented more broadly, thereby allowing earlier treatment and       
a reduction in the risk of fracture.                                            
",2683317,R01AR042739,['R01AR042739'],AR,https://reporter.nih.gov/project-details/2683317,R01,1998,271506,-0.25258808710060465
"Neural network optimization algorithms greatly enhance our ability to           
construct large-scale, dynamical models of highly interconnected networks.      
Until now, optimization has only been applied to networks of simplistic         
processing units, ignoring the integrative and temporal response                
properties of single neurons, thus limiting the predictive power of the         
models.  The long-term goal of this project is to develop a hybrid              
modeling strategy in which optimization methods are applied to networks of      
realistic,multicompartmental model neurons. To accomplish this goal, we         
will construct a hybrid model of an actual distributed processing network       
composed of repeatably identifiable sensory, motor, and interneurons that       
computes a well-defined behavioral input-output function. Optimization          
will be used to predict the connectivity of as-yet-unidentified                 
interneurons in the actual network and the predictions will be tested by        
identifying the interneurons by physiological and morphological means.          
Performance of the hybrid model will be assessed by comparing it to the         
performance of an a priori model in which all connection strengths are          
determined physiologically. The final model will be used to predict the         
loci of synaptic plasticity underlying nonassociative conditioning of the       
reflex by incorporating local learning rules and by optimization methods.       
The predictions will be tested by determining the actual plastic sites          
physiologically.  This project will have the combined effect of enhancing       
the predictive power of optimized network models and illuminating the           
relation between computations at the single-neuron and network levels.          
 Hirudinea; artificial intelligence; computational neuroscience; computer program /software; interneurons; model design /development; neural information processing; neural plasticity OPTIMIZED NETWORKS OF MULTICOMPARTMENTAL NEURONS","Neural network optimization algorithms greatly enhance our ability to           
construct large-scale, dynamical models of highly interconnected networks.      
Until now, optimization has only been applied to networks of simplistic         
processing units, ignoring the integrative and temporal response                
properties of single neurons, thus limiting the predictive power of the         
models.  The long-term goal of this project is to develop a hybrid              
modeling strategy in which optimization methods are applied to networks of      
realistic,multicompartmental model neurons. To accomplish this goal, we         
will construct a hybrid model of an actual distributed processing network       
composed of repeatably identifiable sensory, motor, and interneurons that       
computes a well-defined behavioral input-output function. Optimization          
will be used to predict the connectivity of as-yet-unidentified                 
interneurons in the actual network and the predictions will be tested by        
identifying the interneurons by physiological and morphological means.          
Performance of the hybrid model will be assessed by comparing it to the         
performance of an a priori model in which all connection strengths are          
determined physiologically. The final model will be used to predict the         
loci of synaptic plasticity underlying nonassociative conditioning of the       
reflex by incorporating local learning rules and by optimization methods.       
The predictions will be tested by determining the actual plastic sites          
physiologically.  This project will have the combined effect of enhancing       
the predictive power of optimized network models and illuminating the           
relation between computations at the single-neuron and network levels.          
",2675147,R29MH051383,['R29MH051383'],MH,https://reporter.nih.gov/project-details/2675147,R29,1998,88560,-0.036363405166281
"The objective of this research is the wide accessibility of 3D                  
deconvolution (computational deblurring and visual clarification of 3D          
image data) for the neuroscientific community and other life-science            
communities. Long-term aims are to provide the most reliable, robust and        
quantitatively accurate system for 3D visualization and morphometry, and        
to provide neuroscientists and other life-scientists with easy-to-use           
tools for studying the structure and function of normal and pathologic          
tissue. The focus of this project is on neuroscientific applications. The       
commercial objective is a profitable software product, which, owing to          
innovations, well outperforms competitive products in reliability,              
robustness, quantitative accuracy, speed and ease of usage. The specific        
aims of this project are: (1) To develop and test variations of the             
algorithm for a broader range of widely used confocal microscope types,         
including the slit-scan and array-detector geomeries; (2) To improve (and       
test) the robustness of the algorithm under a number of common adverse          
conditions; (3) To further validate the correctness of the image                
reconstructions by using fabricated test objects of known geometry; (4) To      
demonstrate the system on two of the most suitable computer platforms for       
wide usage. These are the Silicon Graphics unix-based platform and the          
IBM-PC compatible platform; (5) To beta test prototypes of the product at       
several potential customer sites. Innovations introduced by us in this          
research include: (1), a blind deconvolution approach, which obviates the       
need to measure the point spread function, (2), a maximum likelihood            
optimization approach, which makes the methodology robust against               
photodetector noise and other adverse conditions, and (3), a unified            
underlying mathematical model, which makes the algorithm easily adaptable       
among confocal geometries.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We estimate on the order of 800 biological pinhole confocal microscopes         
and 150 biological slit scan confocal microscopes to exist worldwide. We        
expect these numbers to increase to 8,000 confocal microscopes over 10          
years. We expect that 1/2 of these potentIal sites will purchase                
deconvolution software. We should secure at least 1/3rd of this potentIal       
market and thereby (conservatively) Bell at,least 100 units per year over       
10 years. Numerous nonblologlcal spin-off markets (e.g., pharmeceuticals,       
chemicals) exist as well.                                                       
 artificial intelligence; computer program /software; computer system design /evaluation; confocal scanning microscopy; dendrites; digital imaging; fluorescence microscopy; glia; image enhancement; laboratory rat; sectioning; tissue /cell preparation ROBUST 3D FLUORESCENCE CONFOCAL MICROSCOPY DEBLURRING","The objective of this research is the wide accessibility of 3D                  
deconvolution (computational deblurring and visual clarification of 3D          
image data) for the neuroscientific community and other life-science            
communities. Long-term aims are to provide the most reliable, robust and        
quantitatively accurate system for 3D visualization and morphometry, and        
to provide neuroscientists and other life-scientists with easy-to-use           
tools for studying the structure and function of normal and pathologic          
tissue. The focus of this project is on neuroscientific applications. The       
commercial objective is a profitable software product, which, owing to          
innovations, well outperforms competitive products in reliability,              
robustness, quantitative accuracy, speed and ease of usage. The specific        
aims of this project are: (1) To develop and test variations of the             
algorithm for a broader range of widely used confocal microscope types,         
including the slit-scan and array-detector geomeries; (2) To improve (and       
test) the robustness of the algorithm under a number of common adverse          
conditions; (3) To further validate the correctness of the image                
reconstructions by using fabricated test objects of known geometry; (4) To      
demonstrate the system on two of the most suitable computer platforms for       
wide usage. These are the Silicon Graphics unix-based platform and the          
IBM-PC compatible platform; (5) To beta test prototypes of the product at       
several potential customer sites. Innovations introduced by us in this          
research include: (1), a blind deconvolution approach, which obviates the       
need to measure the point spread function, (2), a maximum likelihood            
optimization approach, which makes the methodology robust against               
photodetector noise and other adverse conditions, and (3), a unified            
underlying mathematical model, which makes the algorithm easily adaptable       
among confocal geometries.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
We estimate on the order of 800 biological pinhole confocal microscopes         
and 150 biological slit scan confocal microscopes to exist worldwide. We        
expect these numbers to increase to 8,000 confocal microscopes over 10          
years. We expect that 1/2 of these potentIal sites will purchase                
deconvolution software. We should secure at least 1/3rd of this potentIal       
market and thereby (conservatively) Bell at,least 100 units per year over       
10 years. Numerous nonblologlcal spin-off markets (e.g., pharmeceuticals,       
chemicals) exist as well.                                                       
",2675305,R44MH053692,['R44MH053692'],MH,https://reporter.nih.gov/project-details/2675305,R44,1998,323158,-0.21733966860243722
"DESCRIPTION (Adapted from Applicant's Abstract):  Knowledge-guided, fully       
automated image analytic procedures will be applied, and further developed      
for the extraction of diagnostic and prognostic information from                
histopathologic sections.  It is proposed to develop knowledge files for the    
grading of solar lesions, for the analysis of prostatic intraepithelial         
neoplastic lesions (PIN), for benign proliferative epithelial lesions of the    
breast, and for kidney tumors.  Quantitative progression indices will be        
derived from histometric measurements.  These may serve to identify patients    
at high risk to develop infiltrating disease, to measure rate of lesion         
progression, and to allow a numeric assessment of the efficacy of               
chemopreventive intervention.                                                   
                                                                                
Knowledge files are under development for a quantitative measurement of the     
vascularization around PIN lesions.                                             
                                                                                
For nuclei, lesions and patients, novel methodology is proposed to              
characterize these entities by identification, rather than by mere              
classification.  This will allow a significantly more precise                   
characterization of the nuceli in a lesion and of the state of lesion           
progression.  The identification methods will be integrated into the current    
diagnostic decision support system, and be given capabilities to handle         
missing data, contradictory evidence, atypical diagnostic clue expression.      
This capability relies on automated reasoning will be developed, and the        
methodology will be adapted for used in histopathologic diagnosis.              
 artificial intelligence; breast neoplasms; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; diagnosis quality /standard; digital imaging; histopathology; image processing; information system analysis; kidney neoplasms; neoplasm /cancer diagnosis; prognosis; prostate neoplasms KNOWLEDGE BASED SYSTEMS FOR DIAGNOSTIC HISTOPATHOLOGY","DESCRIPTION (Adapted from Applicant's Abstract):  Knowledge-guided, fully       
automated image analytic procedures will be applied, and further developed      
for the extraction of diagnostic and prognostic information from                
histopathologic sections.  It is proposed to develop knowledge files for the    
grading of solar lesions, for the analysis of prostatic intraepithelial         
neoplastic lesions (PIN), for benign proliferative epithelial lesions of the    
breast, and for kidney tumors.  Quantitative progression indices will be        
derived from histometric measurements.  These may serve to identify patients    
at high risk to develop infiltrating disease, to measure rate of lesion         
progression, and to allow a numeric assessment of the efficacy of               
chemopreventive intervention.                                                   
                                                                                
Knowledge files are under development for a quantitative measurement of the     
vascularization around PIN lesions.                                             
                                                                                
For nuclei, lesions and patients, novel methodology is proposed to              
characterize these entities by identification, rather than by mere              
classification.  This will allow a significantly more precise                   
characterization of the nuceli in a lesion and of the state of lesion           
progression.  The identification methods will be integrated into the current    
diagnostic decision support system, and be given capabilities to handle         
missing data, contradictory evidence, atypical diagnostic clue expression.      
This capability relies on automated reasoning will be developed, and the        
methodology will be adapted for used in histopathologic diagnosis.              
",2389351,R01CA053877,['R01CA053877'],CA,https://reporter.nih.gov/project-details/2389351,R01,1998,426688,-0.051227764049953925
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The broad objective of this proposal is to establish a quantitative             
algorithmic link between cellular and molecular events involved in reward       
processing and the behaviors that these events influence.  In particular, we    
will focus on the nature and use of information that midbrain dopamine          
systems construct and distribute to neural structures throughout the brain.     
The activity of dopamine neurons of the ventral segmental area and              
substantia nigra has long been identified with the processing of rewarding      
stimuli.  These neurons send axons to brain structures involved in              
motivation and goal directed behavior.  These same dopamine systems are         
targets for disruption by disease and by drugs of abuse like heroine and        
cocaine.  In alert primates, experiments show that the outputs of these         
neurons encode errors between the predictions of future rewards and actual      
times and magnitudes of future rewards.  The prediction errors are              
apparently encoded as changes in the instantaneous spiking rates:  values       
above baseline mean that the current state is 'better than predicted',          
values below mean that the current state is 'worse than predicted', and no      
difference means that 'things are as predicted'.  We have developed a           
computational model of the behavior of midbrain dopamine neurons using a        
method of adaptive optimizing control called the method of temporal             
differences.  The model is consistent with electrophysiological and             
behavioral data in monkeys, and also provably executes the appropriate          
computational function of determining which actions maximize long-term          
rewards.  The long term goal of this project is to provide a computational      
connection between the action of dopaminergic mechanisms at the molecular       
and cellular level and the consequences of these mechanisms on behaviors        
observed in drug addiction.  Three approaches will be used to accomplish        
this long term goal:  (1) human behavioral experiments, (2) mathematical        
analysis, and (3) computer simulations of virtual creatures in complex          
environments.  These three methods will focus on the function of                
dopaminergic systems in the primate and will be used to probe how normal        
mechanisms for making decisions in the face of rewards can be disrupted in      
addiction.                                                                      
 artificial intelligence; clinical research; computer simulation; computer system design /evaluation; decision making; dopamine; drug addiction; electrophysiology; human subject; mathematical model; model design /development; neural information processing; reinforcer COMPUTATIONAL SUBSTRATES OF ADDICTION AND REWARD","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The broad objective of this proposal is to establish a quantitative             
algorithmic link between cellular and molecular events involved in reward       
processing and the behaviors that these events influence.  In particular, we    
will focus on the nature and use of information that midbrain dopamine          
systems construct and distribute to neural structures throughout the brain.     
The activity of dopamine neurons of the ventral segmental area and              
substantia nigra has long been identified with the processing of rewarding      
stimuli.  These neurons send axons to brain structures involved in              
motivation and goal directed behavior.  These same dopamine systems are         
targets for disruption by disease and by drugs of abuse like heroine and        
cocaine.  In alert primates, experiments show that the outputs of these         
neurons encode errors between the predictions of future rewards and actual      
times and magnitudes of future rewards.  The prediction errors are              
apparently encoded as changes in the instantaneous spiking rates:  values       
above baseline mean that the current state is 'better than predicted',          
values below mean that the current state is 'worse than predicted', and no      
difference means that 'things are as predicted'.  We have developed a           
computational model of the behavior of midbrain dopamine neurons using a        
method of adaptive optimizing control called the method of temporal             
differences.  The model is consistent with electrophysiological and             
behavioral data in monkeys, and also provably executes the appropriate          
computational function of determining which actions maximize long-term          
rewards.  The long term goal of this project is to provide a computational      
connection between the action of dopaminergic mechanisms at the molecular       
and cellular level and the consequences of these mechanisms on behaviors        
observed in drug addiction.  Three approaches will be used to accomplish        
this long term goal:  (1) human behavioral experiments, (2) mathematical        
analysis, and (3) computer simulations of virtual creatures in complex          
environments.  These three methods will focus on the function of                
dopaminergic systems in the primate and will be used to probe how normal        
mechanisms for making decisions in the face of rewards can be disrupted in      
addiction.                                                                      
",2594602,R01DA011723,['R01DA011723'],DA,https://reporter.nih.gov/project-details/2594602,R01,1998,158410,-0.08332251982492558
"DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
 adolescence (12-20); aging; audiometry; auditory discrimination; behavioral /social science research tag; binaural hearing; human middle age (35-64); human old age (65+); human subject; longitudinal human study; noise; perception; psychoacoustics; sensorineural hearing loss; space perception; speech; speech recognition; young adult human (21-34) SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING","DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
",2633315,R01AG007998,['R01AG007998'],AG,https://reporter.nih.gov/project-details/2633315,R01,1998,160074,-0.2000902687151525
"DESCRIPTION:  The development of an emergency vehicle alert system for          
hearing impaired drivers is proposed.  This system will display a visual        
warning signal for hearing impaired or deaf drivers if an emergency             
vehicle is approaching the driver.  Through the display the driver can          
quickly determine the distance and direction of the approaching                 
emergency vehicle. The Phase I objective was achieved by verifying the          
feasibility of the proposed acoustic alert system concept.  In Phase II,        
prototypes will be built and field tests will be conducted to determine         
the practical variability of the alert system.  This system will consist        
of microphones, signal processor, and display.  Several products for the        
same purpose are currently or were previously marketed.  These devices          
use banks of filters and level detection and they provide only a simple         
warning sign.  False detection rates of these devices is very high and          
the performance of these devices is far from satisfactory.  The key goal        
addressed by this project is to drastically increase the accuracy and           
reliability of such systems while maintaining low cost.  Recent strides         
in signal processing in the performance of low cost microprocessors             
makes this goal practical.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                  
 artificial intelligence; biomedical equipment development; computer system design /evaluation; deaf aid; emergency health services; human subject; vehicular accident EMERGENCY VEHICLE ALERT SYSTEM FOR HEARING IMPAIRED","DESCRIPTION:  The development of an emergency vehicle alert system for          
hearing impaired drivers is proposed.  This system will display a visual        
warning signal for hearing impaired or deaf drivers if an emergency             
vehicle is approaching the driver.  Through the display the driver can          
quickly determine the distance and direction of the approaching                 
emergency vehicle. The Phase I objective was achieved by verifying the          
feasibility of the proposed acoustic alert system concept.  In Phase II,        
prototypes will be built and field tests will be conducted to determine         
the practical variability of the alert system.  This system will consist        
of microphones, signal processor, and display.  Several products for the        
same purpose are currently or were previously marketed.  These devices          
use banks of filters and level detection and they provide only a simple         
warning sign.  False detection rates of these devices is very high and          
the performance of these devices is far from satisfactory.  The key goal        
addressed by this project is to drastically increase the accuracy and           
reliability of such systems while maintaining low cost.  Recent strides         
in signal processing in the performance of low cost microprocessors             
makes this goal practical.                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                  
",2649323,R44DC003017,['R44DC003017'],DC,https://reporter.nih.gov/project-details/2649323,R44,1998,326482,-0.012317182149802666
"Virus structure determination using electron microscopy has become a            
useful research tool aimed at understanding viral assembly and                  
infectivity. Our long term goal is to broaden the group of people able          
to determine virus structures by providing an integrated software suite         
for three-dimensional virus structure determination using electron              
microscopy. The software suite will allow easy, efficient, and routine          
determination of icosahedral virus structures from electron micrographs.        
Novel aspects of this software will include a comprehensive suite of            
tools for icosahedral structure determination, incorporation of an expert       
system to guide users through the reconstruction procedure, and data            
analysis tools to ensure that structures are determined accurately.  In         
addition, the software will provide a consistent easy to use graphical          
user interface to all reconstruction tools including data analysis, data        
management, and data logging. The result of this phase one SBIR will be         
a design specification for an integrated software package allowing easy,        
efficient, and routine determination of virus structures by both                
virologists and structural biologists.  Furthermore, one aspect of the          
reconstruction procedure will be targeted for development as a prototype        
to demonstrate project feasibility.  The complete software product will         
be implemented and tested in a phase two SBIR.                                  
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The availability of a comprehensive virus structure determination               
software package will have commercial implications to virologists,              
microbiologists, and other biologists interested in performing such             
structural studies but who are not trained in the intricacies of                
icosahedral virus structure determination. In addition, this software           
package will be of interest to structural biologists and electron               
microscopists as it will provide all of the software necessary for              
structural studies in one comprehensive extendable package.                     
 artificial intelligence; computer data analysis; computer human interaction; computer program /software; computer system design /evaluation; data management; electron microscopy; structural biology; virus assembly; virus morphology VIRUS STRUCTURE DETERMINATION SOFTWARE","Virus structure determination using electron microscopy has become a            
useful research tool aimed at understanding viral assembly and                  
infectivity. Our long term goal is to broaden the group of people able          
to determine virus structures by providing an integrated software suite         
for three-dimensional virus structure determination using electron              
microscopy. The software suite will allow easy, efficient, and routine          
determination of icosahedral virus structures from electron micrographs.        
Novel aspects of this software will include a comprehensive suite of            
tools for icosahedral structure determination, incorporation of an expert       
system to guide users through the reconstruction procedure, and data            
analysis tools to ensure that structures are determined accurately.  In         
addition, the software will provide a consistent easy to use graphical          
user interface to all reconstruction tools including data analysis, data        
management, and data logging. The result of this phase one SBIR will be         
a design specification for an integrated software package allowing easy,        
efficient, and routine determination of virus structures by both                
virologists and structural biologists.  Furthermore, one aspect of the          
reconstruction procedure will be targeted for development as a prototype        
to demonstrate project feasibility.  The complete software product will         
be implemented and tested in a phase two SBIR.                                  
                                                                                
PROPOSED COMMERCIAL APPLICATIONS:                                               
The availability of a comprehensive virus structure determination               
software package will have commercial implications to virologists,              
microbiologists, and other biologists interested in performing such             
structural studies but who are not trained in the intricacies of                
icosahedral virus structure determination. In addition, this software           
package will be of interest to structural biologists and electron               
microscopists as it will provide all of the software necessary for              
structural studies in one comprehensive extendable package.                     
",2713852,R43GM058327,['R43GM058327'],GM,https://reporter.nih.gov/project-details/2713852,R43,1998,95100,-0.040645604002232154
 artificial intelligence; biomedical equipment development; computer program /software; computer system design /evaluation; confocal scanning microscopy; image enhancement; microscopy; neurosciences BRIGHTFIELD MICROSCOPY 3D IMAGE RECONSTRUCTION,,2675304,R44MH053691,['R44MH053691'],MH,https://reporter.nih.gov/project-details/2675304,R44,1998,310509,-0.0993259542455452
"In Phase I of the ""Decision Support System to Identify the At-Risk              
Fetus"" a prototype Intelligent Decision Support System (IDSS) was               
developed to interpret electronic fetal monitoring (EFM) data. In               
Phase II, the researchers propose to complete development and test              
the IDSS in preparation for full clinical trials and                            
commercialization. The research is based on the hypothesis that                 
intrapartum EFM and the resulting FHR and UC data provide                       
information that can assist physicians in more accurately                       
differentiating between healthy and at-risk fetuses. The system                 
uses morphological filters to process signals at different scales,              
a neural network to better recognize FHR and UC patterns despite                
EFM noise, and a fuzzy relational structure outcome inferencing. In             
Phase I evidence was generated supporting the hypothesis when the               
IDSS with little training on only 50 cases differentiated between               
healthy and at-risk fetuses as well, and perhaps slightly better                
than three board certified obstetricians with over 40 years of                  
combined clinical experience. The proposed Phase II improvements to             
IDSS would increase the system's sensitivity and specificity so as              
to assist clinicians in more accurately identifying at-risk                     
fetuses, as well as more confidently identifying healthy fetuses so             
as to avoid unnecessary interventions.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The successful completion of Phase             
II should lead to a system which can be used in hospitals and                   
clinicians' offices to better identify the at-risk fetus, thereby               
allowing for the timely implementation of efficacious interventions             
and a reduction in the use of invasive obstetrical interventions,               
including cesarean deliveries, without adversely impacting                      
outcomes. The use of this system in hospitals and private                       
clinicians' offices will also help reduce the costs of national                 
health care.                                                                    
 artificial intelligence; bioimaging /biomedical imaging; biomedical equipment development; clinical research; computer assisted medical decision making; computer data analysis; diagnosis design /evaluation; embryo /fetus disorder; embryo /fetus monitoring; human subject; method development; obstetrics; prenatal stress DECISION SUPPORT SYSTEM TO IDENTIFY THE AT RISK FETUS","In Phase I of the ""Decision Support System to Identify the At-Risk              
Fetus"" a prototype Intelligent Decision Support System (IDSS) was               
developed to interpret electronic fetal monitoring (EFM) data. In               
Phase II, the researchers propose to complete development and test              
the IDSS in preparation for full clinical trials and                            
commercialization. The research is based on the hypothesis that                 
intrapartum EFM and the resulting FHR and UC data provide                       
information that can assist physicians in more accurately                       
differentiating between healthy and at-risk fetuses. The system                 
uses morphological filters to process signals at different scales,              
a neural network to better recognize FHR and UC patterns despite                
EFM noise, and a fuzzy relational structure outcome inferencing. In             
Phase I evidence was generated supporting the hypothesis when the               
IDSS with little training on only 50 cases differentiated between               
healthy and at-risk fetuses as well, and perhaps slightly better                
than three board certified obstetricians with over 40 years of                  
combined clinical experience. The proposed Phase II improvements to             
IDSS would increase the system's sensitivity and specificity so as              
to assist clinicians in more accurately identifying at-risk                     
fetuses, as well as more confidently identifying healthy fetuses so             
as to avoid unnecessary interventions.                                          
                                                                                
PROPOSED COMMERCIAL APPLICATIONS The successful completion of Phase             
II should lead to a system which can be used in hospitals and                   
clinicians' offices to better identify the at-risk fetus, thereby               
allowing for the timely implementation of efficacious interventions             
and a reduction in the use of invasive obstetrical interventions,               
including cesarean deliveries, without adversely impacting                      
outcomes. The use of this system in hospitals and private                       
clinicians' offices will also help reduce the costs of national                 
health care.                                                                    
",2673758,R44HD031837,['R44HD031837'],HD,https://reporter.nih.gov/project-details/2673758,R44,1998,318636,-0.13907262722871497
"DESCRIPTION:  (Adapted from investigator's abstract) This project will          
examine new methodology for making inference about the regression parameters    
in the presence of missing covariate data for two commonly used classes of      
regression models.  In particular, we examine the class of generalized          
linear models for general types of response data and the Cox model for          
survival data.  The methodology addresses problems occurring frequently in      
clinical investigations for chronic disease, including cancer and AIDS.  The    
specific objectives of the project are to:  1) Develop and study classical      
and Bayesian methods of inference for the class of generalized linear models    
(GLM's) in the presence of missing covariate data.  In particular, we will      
i) examine methods for estimating the regression parameters when the missing    
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Also, parametric models for the covariate              
distribution will be examined.  The methods of estimation will focus on the     
Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other        
related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990)      
along with the adaptive rejection algorithm of Gilks and Wild (1992) will be    
used to sample from the conditional distribution of the missing covariates      
given the observed data.  ii) examine estimating the regression parameters      
when the missing covariates are either categorical or continuous and the        
missing data mechanism is nonignorable.  Models for the missing data            
mechanism will be studied.  iii) develop and study Bayesian methods of          
inference in the presence of missing covariate data when the missing            
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Parametric prior distributions for the regression      
coefficients are proposed.  Properties of the posterior distributions of the    
regression coefficients will be studied.  The methodology will be               
implemented using Markov Chain Monte Carlo methods similar to those of          
Tanner and Wong (1987).  iv) investigate Bayesian methods when the              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Multinomial models for the missing data             
mechanism will be studied.  Dirichlet prior distributions for the               
multinomial parameters will be investigated.                                    
                                                                                
2) Develop and study classical and Bayesian methods of inference for the Cox    
model for survival outcomes in the presence of missing covariates.              
Specifically, we will i) develop and study estimation methods for the Cox       
model for survival outcomes in the presence of missing covariates.  Methods     
for estimating the regression parameters when the missing covariates are        
either categorical or continuous will be studied.  The methods of estimation    
will focus on an EM type algorithm similar to that of Wei and Tanner (1990).    
ii) study estimation of the regression parameters when the missing              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Models for the missing data mechanism will be       
studied.  Bayesian methods similar to those of 1-iii) and iv) will be           
investigated.  Computational techniques using the Monte Carlo methods           
described in 1-iii) will be implemented.                                        
 artificial intelligence; computer data analysis; data collection methodology /evaluation; human data; mathematical model; method development; model design /development; statistics /biometry INFERENCE IN REGRESSION MODELS WITH MISSING COVARIATES","DESCRIPTION:  (Adapted from investigator's abstract) This project will          
examine new methodology for making inference about the regression parameters    
in the presence of missing covariate data for two commonly used classes of      
regression models.  In particular, we examine the class of generalized          
linear models for general types of response data and the Cox model for          
survival data.  The methodology addresses problems occurring frequently in      
clinical investigations for chronic disease, including cancer and AIDS.  The    
specific objectives of the project are to:  1) Develop and study classical      
and Bayesian methods of inference for the class of generalized linear models    
(GLM's) in the presence of missing covariate data.  In particular, we will      
i) examine methods for estimating the regression parameters when the missing    
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Also, parametric models for the covariate              
distribution will be examined.  The methods of estimation will focus on the     
Monte Carlo version of the EM algorithm (Wei and Tanner, 1990) and other        
related iterative algorithms.  The Gibbs sampler (Gelfand and Smith, 1990)      
along with the adaptive rejection algorithm of Gilks and Wild (1992) will be    
used to sample from the conditional distribution of the missing covariates      
given the observed data.  ii) examine estimating the regression parameters      
when the missing covariates are either categorical or continuous and the        
missing data mechanism is nonignorable.  Models for the missing data            
mechanism will be studied.  iii) develop and study Bayesian methods of          
inference in the presence of missing covariate data when the missing            
covariates are either categorical or continuous and the missing data            
mechanism is ignorable.  Parametric prior distributions for the regression      
coefficients are proposed.  Properties of the posterior distributions of the    
regression coefficients will be studied.  The methodology will be               
implemented using Markov Chain Monte Carlo methods similar to those of          
Tanner and Wong (1987).  iv) investigate Bayesian methods when the              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Multinomial models for the missing data             
mechanism will be studied.  Dirichlet prior distributions for the               
multinomial parameters will be investigated.                                    
                                                                                
2) Develop and study classical and Bayesian methods of inference for the Cox    
model for survival outcomes in the presence of missing covariates.              
Specifically, we will i) develop and study estimation methods for the Cox       
model for survival outcomes in the presence of missing covariates.  Methods     
for estimating the regression parameters when the missing covariates are        
either categorical or continuous will be studied.  The methods of estimation    
will focus on an EM type algorithm similar to that of Wei and Tanner (1990).    
ii) study estimation of the regression parameters when the missing              
covariates are either categorical or continuous and the missing data            
mechanism is nonignorable.  Models for the missing data mechanism will be       
studied.  Bayesian methods similar to those of 1-iii) and iv) will be           
investigated.  Computational techniques using the Monte Carlo methods           
described in 1-iii) will be implemented.                                        
",2769933,R01CA074015,['R01CA074015'],CA,https://reporter.nih.gov/project-details/2769933,R01,1998,174223,0.019498607242339434
"DESCRIPTION:(Adapted from the Investigator's Abstract) The goal of this         
grant is to determine if and when cancer risks can be estimated by              
establishing record-linkages between statewide cancer surveillance systems      
and occupational cohorts.  More specifically, the aims of this study are to:    
(1) determine the feasibility of utilizing statewide cancer surveillance        
systems in the evaluation of cancer incidence within occupational cohorts;      
(2) compare and contrast the relative merits of standardized incidence          
ratios (SIR) with standardized mortality ratios (SMR) as determined from        
cancer surveillance systems incidence data and death certificate mortality      
data, respectively; and (3) provide recommendations concerning how and when     
statewide cancer surveillance systems should be utilized in the evaluation      
of occupational cohorts.                                                        
                                                                                
SMR and SIR estimates will be calculated and compared for three occupational    
cohorts (e.g., Highway Maintenance, 3M, Conwed).  SMR analyses have already     
been completed for the Highway Maintenance and 3M cohorts; a mortality          
update and SMR analysis will be conducted for the Conwed cohort (1988-1995).    
Cancer morbidity information, for the SIR analysis, will be determined by       
linking the three cohorts with the Minnesota Cancer Surveillance System         
(MCSS).  Residency status will be required before person-years can be           
calculated, however, because inclusion in the MCSS is restricted to             
Minnesota residents.  Linkages to other data sets will be used to determine     
the Minnesota residency status.  Sensitivity analyses will be used to           
evaluate confounding and follow-up bias.  Standardized mortality ratios will    
be compared to standardized incidence ratios for the Highway Maintenance,       
3M, and Conwed cohorts.  Finally, this study will evaluate the utility and      
limitations of cancer surveillance systems as a tool for occupational cancer    
research; recommendations for its use will be developed.                        
 artificial intelligence; cancer risk; clinical research; computer data analysis; data collection methodology /evaluation; environment related neoplasm /cancer; human data; human mortality; human subject; neoplasm /cancer epidemiology; occupational disease /disorder; occupational hazard; occupational health /safety; vital statistics OCCUPATIONAL CANCER SURVEILLANCE THROUGH RECORD LINKAGE","DESCRIPTION:(Adapted from the Investigator's Abstract) The goal of this         
grant is to determine if and when cancer risks can be estimated by              
establishing record-linkages between statewide cancer surveillance systems      
and occupational cohorts.  More specifically, the aims of this study are to:    
(1) determine the feasibility of utilizing statewide cancer surveillance        
systems in the evaluation of cancer incidence within occupational cohorts;      
(2) compare and contrast the relative merits of standardized incidence          
ratios (SIR) with standardized mortality ratios (SMR) as determined from        
cancer surveillance systems incidence data and death certificate mortality      
data, respectively; and (3) provide recommendations concerning how and when     
statewide cancer surveillance systems should be utilized in the evaluation      
of occupational cohorts.                                                        
                                                                                
SMR and SIR estimates will be calculated and compared for three occupational    
cohorts (e.g., Highway Maintenance, 3M, Conwed).  SMR analyses have already     
been completed for the Highway Maintenance and 3M cohorts; a mortality          
update and SMR analysis will be conducted for the Conwed cohort (1988-1995).    
Cancer morbidity information, for the SIR analysis, will be determined by       
linking the three cohorts with the Minnesota Cancer Surveillance System         
(MCSS).  Residency status will be required before person-years can be           
calculated, however, because inclusion in the MCSS is restricted to             
Minnesota residents.  Linkages to other data sets will be used to determine     
the Minnesota residency status.  Sensitivity analyses will be used to           
evaluate confounding and follow-up bias.  Standardized mortality ratios will    
be compared to standardized incidence ratios for the Highway Maintenance,       
3M, and Conwed cohorts.  Finally, this study will evaluate the utility and      
limitations of cancer surveillance systems as a tool for occupational cancer    
research; recommendations for its use will be developed.                        
",2794336,K01OH000161,['K01OH000161'],OH,https://reporter.nih.gov/project-details/2794336,K01,1998,54000,-0.03336821307725751
"We will reimplement two successful pharmacy-related expert systems,             
DoseChecker and Adverse Drug Event (ADE) Monitor, to exploit the existence      
of an extensive telemedicine network. DoseChecker provides drug dosing          
warnings based on pharmacy orders and patient specific features such as         
age, sex, weight, and estimated creatinine clearance. ADE Monitor examines      
patient drug orders and lab and lab test results, alerting hospital             
pharmacists when it detects signs of adverse drug events such as drug           
interactions. Once redeployed, these expert systems will be accessible to       
all clinical settings from which the clinical data repository collects the      
required data. During 1996, six hospital in Missouri and Illinois--Barnes,      
Jewish, Christian NE, Christian NW, Alton Memorial, and Barnes West             
County--will have the demographic, laboratory and pharmacy data required        
for DoseChecker and ADE Monitor entering the repository. This set of            
hospitals contains two academic facilities (Barnes and Jewish), two large       
community facilities (Christian NE and Alton Memorial) a small community        
facility (Barnes West County), and a predominantly ambulatory care              
facility (Christian NW). The reimplemented systems will serve as a              
telemedicine test bed to examine the impact of large-scale, wide-are            
deployment of clinical decision support in diverse clinical environments.       
Specifically, we will evaluate the expert system performance, physician         
acceptance, and clinical settings. By exploiting the existence of a             
standard clinical vocabulary and extensive networking interconnectivity,        
we can provide identical CDS functionality at clinical practice settings        
which preciously would pose significant technological barriers and require      
substantial rework. By keeping the decision support system constant, our        
evaluation studies will allow us to study the impact of clinical-setting        
variables on the effectiveness of these tow systems. In addition, we will       
implement and evaluate multiple decision support alert notification             
methods including GUI-based applications, E-mail, fax/printer reports,          
automated paging, and interactive voice massaging using commercial              
telephony products to examine the impact of alternative notification            
methods on the timeliness and clinical impact of decision support alerts.       
Using this telecommunications-based notification infrastructure, we also        
will investigate the patient confidentiality and privacy compromises which      
may occur due to alternative notification methods. By permitting multiple       
clinicians, possibly in different locations, to be notified of alerting         
conditions, we can also assess the impact or value of collaborative use of      
this technology to treat BJC patients. With our study decision, we can          
examine explicit trade-offs between alerting methods, decision support          
effectiveness, and patient confidentiality.                                     
  HEALTH APPLICATIONS FOR THE NATIONAL INFORMATION INFRA","We will reimplement two successful pharmacy-related expert systems,             
DoseChecker and Adverse Drug Event (ADE) Monitor, to exploit the existence      
of an extensive telemedicine network. DoseChecker provides drug dosing          
warnings based on pharmacy orders and patient specific features such as         
age, sex, weight, and estimated creatinine clearance. ADE Monitor examines      
patient drug orders and lab and lab test results, alerting hospital             
pharmacists when it detects signs of adverse drug events such as drug           
interactions. Once redeployed, these expert systems will be accessible to       
all clinical settings from which the clinical data repository collects the      
required data. During 1996, six hospital in Missouri and Illinois--Barnes,      
Jewish, Christian NE, Christian NW, Alton Memorial, and Barnes West             
County--will have the demographic, laboratory and pharmacy data required        
for DoseChecker and ADE Monitor entering the repository. This set of            
hospitals contains two academic facilities (Barnes and Jewish), two large       
community facilities (Christian NE and Alton Memorial) a small community        
facility (Barnes West County), and a predominantly ambulatory care              
facility (Christian NW). The reimplemented systems will serve as a              
telemedicine test bed to examine the impact of large-scale, wide-are            
deployment of clinical decision support in diverse clinical environments.       
Specifically, we will evaluate the expert system performance, physician         
acceptance, and clinical settings. By exploiting the existence of a             
standard clinical vocabulary and extensive networking interconnectivity,        
we can provide identical CDS functionality at clinical practice settings        
which preciously would pose significant technological barriers and require      
substantial rework. By keeping the decision support system constant, our        
evaluation studies will allow us to study the impact of clinical-setting        
variables on the effectiveness of these tow systems. In addition, we will       
implement and evaluate multiple decision support alert notification             
methods including GUI-based applications, E-mail, fax/printer reports,          
automated paging, and interactive voice massaging using commercial              
telephony products to examine the impact of alternative notification            
methods on the timeliness and clinical impact of decision support alerts.       
Using this telecommunications-based notification infrastructure, we also        
will investigate the patient confidentiality and privacy compromises which      
may occur due to alternative notification methods. By permitting multiple       
clinicians, possibly in different locations, to be notified of alerting         
conditions, we can also assess the impact or value of collaborative use of      
this technology to treat BJC patients. With our study decision, we can          
examine explicit trade-offs between alerting methods, decision support          
effectiveness, and patient confidentiality.                                     
",2725393,01LM063550,['N01LM063550'],LM,https://reporter.nih.gov/project-details/2725393,N01,1998,127178,-0.03896388799133855
"DESCRIPTION:  The candidate, Dr. Adrian Casillas, is a general internist        
with board certification in clinical immunology and is an Assistant             
Professor of Clinical Immunology and Allergy at UCLA.  He has previous          
laboratory-based research experience in immunology and biochemistry.  He has    
also performed some preliminary studies the project sponsor which have not      
yet been published.                                                             
                                                                                
The sponsor, Dr. Ron Stevens, is Professor of Microbiology and Immunology       
and of Education.  He has been using computer-based tools for medical           
education since the late 1980's, and lists several publications that            
describe neural network techniques he has developed.                            
                                                                                
The candidate's career development plan includes a combination of didactic      
training and research that apply computer-based patient simulations and         
neural network strategies to the domain of childhood asthma.  Didactic work     
will include basic programming, data basis, networks, and statistics.           
                                                                                
The proposed research plan has three specific aims.  In the first aim, the      
candidate proposes to build on an existing set of computer-based clinical       
problem solving cases to construct a comprehensive set of cases that can be     
used to determine the comprehension of childhood asthma across a broad set      
of audiences.  The second aim is to employ artificial neural networks to        
evaluate performance on the computerized cases.  The third aim involves         
implementing the computer-based cases and the neural network performance        
model in three settings:  physicians in training, nursing and nurse             
practitioner students, and asthma patients in the community.  To a lesser       
extent physician experts will be evaluated with the system.  To achieve         
these aims the candidate has garnered the help of faculty in the School of      
Nursing, the Department of Medicine, and prominent members of the               
educational community in Los Angeles.  The candidate will have access to        
equipment, supplies, and technical support from the sponsor's laboratory.       
 artificial intelligence; asthma; case history; clinical research; health care model; health services research tag; human subject; information systems; model design /development; statistics /biometry MODELING THE UNDERSTANDING OF CHILDHOOD ASTHMA","DESCRIPTION:  The candidate, Dr. Adrian Casillas, is a general internist        
with board certification in clinical immunology and is an Assistant             
Professor of Clinical Immunology and Allergy at UCLA.  He has previous          
laboratory-based research experience in immunology and biochemistry.  He has    
also performed some preliminary studies the project sponsor which have not      
yet been published.                                                             
                                                                                
The sponsor, Dr. Ron Stevens, is Professor of Microbiology and Immunology       
and of Education.  He has been using computer-based tools for medical           
education since the late 1980's, and lists several publications that            
describe neural network techniques he has developed.                            
                                                                                
The candidate's career development plan includes a combination of didactic      
training and research that apply computer-based patient simulations and         
neural network strategies to the domain of childhood asthma.  Didactic work     
will include basic programming, data basis, networks, and statistics.           
                                                                                
The proposed research plan has three specific aims.  In the first aim, the      
candidate proposes to build on an existing set of computer-based clinical       
problem solving cases to construct a comprehensive set of cases that can be     
used to determine the comprehension of childhood asthma across a broad set      
of audiences.  The second aim is to employ artificial neural networks to        
evaluate performance on the computerized cases.  The third aim involves         
implementing the computer-based cases and the neural network performance        
model in three settings:  physicians in training, nursing and nurse             
practitioner students, and asthma patients in the community.  To a lesser       
extent physician experts will be evaluated with the system.  To achieve         
these aims the candidate has garnered the help of faculty in the School of      
Nursing, the Department of Medicine, and prominent members of the               
educational community in Los Angeles.  The candidate will have access to        
equipment, supplies, and technical support from the sponsor's laboratory.       
",2718770,K08AI001560,['K08AI001560'],AI,https://reporter.nih.gov/project-details/2718770,K08,1998,73980,-0.020115242840058323
"Optimizing the fit between an assistive technology and its user is              
critical for maximizing the performance with that technology. Both the          
clinician who recommends an assistive technology and that assistive             
technology itself share the responsibility for making certain that the          
client's needs are matched as closely as possible.                              
                                                                                
We propose to develop software-based ""adaptation experts  that can be           
implemented within existing (and future) computer-access software and           
augmentative communication systems.  Each adaptation expert will be             
responsible for determining the value of a specific parameter within its        
particular computer access or augmentative communication system.  In            
Phase I, adaptation experts will be implemented for a word prediction           
system.  These adaptation experts will be evaluated during clinical             
trials by both users and clinicians active in computer access and               
augmentative communication.  During Phase II adaptation experts will be         
implemented for additional input (e.g., row/column scanning, Morse code)        
and text entry rate enhancement (e.g., abbreviation expansion)                  
techniques.                                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The adaptation experts developed during Phase I will be useful for a            
variety of computer access software and augmentative communication              
systems that currently make use of word prediction. An additional               
application of the adaptation experts developed during this research is         
within a clinical assessment tool.  In this role, the adaptation experts        
will identify an optimal configuration for a word prediction interface          
based on a user's abilities.  Additional adaptation experts, developed          
during Phase II, will increase the applications in which the adaptation         
expert approach will be a viable alternative.                                   
 artificial intelligence; assistive device /technology; computer human interaction; computer program /software; human subject; method development AUTOMATIC ADAPTATION EXPERTS FOR COMPUTER ACCESS AND AAC","Optimizing the fit between an assistive technology and its user is              
critical for maximizing the performance with that technology. Both the          
clinician who recommends an assistive technology and that assistive             
technology itself share the responsibility for making certain that the          
client's needs are matched as closely as possible.                              
                                                                                
We propose to develop software-based ""adaptation experts  that can be           
implemented within existing (and future) computer-access software and           
augmentative communication systems.  Each adaptation expert will be             
responsible for determining the value of a specific parameter within its        
particular computer access or augmentative communication system.  In            
Phase I, adaptation experts will be implemented for a word prediction           
system.  These adaptation experts will be evaluated during clinical             
trials by both users and clinicians active in computer access and               
augmentative communication.  During Phase II adaptation experts will be         
implemented for additional input (e.g., row/column scanning, Morse code)        
and text entry rate enhancement (e.g., abbreviation expansion)                  
techniques.                                                                     
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The adaptation experts developed during Phase I will be useful for a            
variety of computer access software and augmentative communication              
systems that currently make use of word prediction. An additional               
application of the adaptation experts developed during this research is         
within a clinical assessment tool.  In this role, the adaptation experts        
will identify an optimal configuration for a word prediction interface          
based on a user's abilities.  Additional adaptation experts, developed          
during Phase II, will increase the applications in which the adaptation         
expert approach will be a viable alternative.                                   
",2715913,R43HD035773,['R43HD035773'],HD,https://reporter.nih.gov/project-details/2715913,R43,1998,100614,-0.08815402713569
"DESCRIPTION:  The purpose of this study is to develop a computer-aided          
diagnosi (CADx) system to predict breast lesion malignancy and invasion         
based on medica findings.  Artificial neural network (ANN) techniques will      
be used to predict whether mammographically suspect lesions are benign, in      
situ cancer, or invasive cancer.                                                
                                                                                
The ANN inputs will be derived from existing, available information such as     
patient history and radiologists descriptions of lesion morphology             
following the ACR Breast Imaging Reporting and Data System (BI-RADS).  ANNs     
are well suited for this diagnostic task because, like humans, ANNs can be      
taught to perform diagnostic tasks accurately and robustly when given           
appropriate training examples.                                                  
                                                                                
The specific aims of the proposed study are to:  (1) Develop ANNs that use      
mammography and history findings to predict malignancy and invasion of          
breast lesions among a prospectively collected patient database; (2) Refine     
the accuracy of the CADx system by optimizing the number of input findings      
and investigating more complex network architectures, and study is              
cost-effectiveness.  (3) Evaluate the CADx system clinically, by developing     
a graphical user interface and using it to retrospectively evaluate the         
systems performance.                                                           
                                                                                
In preliminary studies, an ANN accurately predicted invasion among 96           
biopsy-proven breast cancers, using BI-RADS findings and patient age as         
input findings.                                                                 
                                                                                
The immediate benefit of this proposal is a noninvasive computer-aided          
diagnosis system which provides information previously available only           
through biopsy.  This system can assist mammographers and surgeons in           
surgical planning for patients with breast lesions, and may reduce the cost     
and morbidity of unnecessary surgical biopsies.                                 
 artificial intelligence; biomedical equipment development; breast neoplasm /cancer diagnosis; case history; clinical research; computer human interaction; diagnosis design /evaluation; diagnosis quality /standard; health care cost /financing; human data; mammography; neoplasm /cancer invasiveness; noninvasive diagnosis; prognosis COMPUTER AIDED DIAGNOSIS OF BREAST CANCER INVASION","DESCRIPTION:  The purpose of this study is to develop a computer-aided          
diagnosi (CADx) system to predict breast lesion malignancy and invasion         
based on medica findings.  Artificial neural network (ANN) techniques will      
be used to predict whether mammographically suspect lesions are benign, in      
situ cancer, or invasive cancer.                                                
                                                                                
The ANN inputs will be derived from existing, available information such as     
patient history and radiologists descriptions of lesion morphology             
following the ACR Breast Imaging Reporting and Data System (BI-RADS).  ANNs     
are well suited for this diagnostic task because, like humans, ANNs can be      
taught to perform diagnostic tasks accurately and robustly when given           
appropriate training examples.                                                  
                                                                                
The specific aims of the proposed study are to:  (1) Develop ANNs that use      
mammography and history findings to predict malignancy and invasion of          
breast lesions among a prospectively collected patient database; (2) Refine     
the accuracy of the CADx system by optimizing the number of input findings      
and investigating more complex network architectures, and study is              
cost-effectiveness.  (3) Evaluate the CADx system clinically, by developing     
a graphical user interface and using it to retrospectively evaluate the         
systems performance.                                                           
                                                                                
In preliminary studies, an ANN accurately predicted invasion among 96           
biopsy-proven breast cancers, using BI-RADS findings and patient age as         
input findings.                                                                 
                                                                                
The immediate benefit of this proposal is a noninvasive computer-aided          
diagnosis system which provides information previously available only           
through biopsy.  This system can assist mammographers and surgeons in           
surgical planning for patients with breast lesions, and may reduce the cost     
and morbidity of unnecessary surgical biopsies.                                 
",2633993,R29CA075547,['R29CA075547'],CA,https://reporter.nih.gov/project-details/2633993,R29,1998,114802,-0.12030002364461878
"DESCRIPTION: Over  the past decade  a variety of  alternative computer          
based  modeling  techniques  have  been   introduced  which  show               
promise  for   the  construction of clinical decision aids. These               
techniques include statistical  regression  approaches such as                  
generalized additive  modeling, classification  tree induction such as          
ID3 or CART, and multi-layer neural  networks. Logistic  regression             
models (LR) are currently central to most probabilistic predictive              
clinical  decision aids and are fundamental to comparative analyses of          
medical  care based  risk adjusted events. These newer techniques               
have been applied on  a larger scale in the last few years. They                
appear to have unique advantages in  selected circumstances. The                
successful use  of these methods, however, depends  on understanding            
their accuracy, performance, and model transportability.                        
                                                                                
A  formal assessment of  these new techniques with four specific  aims          
is  proposed:  (1) to assess  and compare the  performance of                   
different  models to  determine  the  factors which affect                      
performance; (2)  to  develop automated  computer  based procedures             
for exploratory model  development for each method;  (3)  to develop            
hybrid models incorporating the strengths of each of the existing               
techniques, and  (4) to  determine the situations  that restrict  the           
transportability of these models.                                               
                                                                                
These specific aims will be  achieved in a three stage project. In              
the first  stage four approaches will be pursued:  (1) the                      
mathematical properties of the  different  computational  algorithms            
for the  modeling  techniques will be studied;  (2) automated                   
modeling procedures will be developed and utilized; (3)  the  factors           
that  affect  performance for  each  modeling technique  will  be               
explored and(4) new hybrid techniques will be developed and assessed.           
In the  second stage the methods  developed in the first stage will             
be used to create  and  test  models  that predict  cardiovascular              
events  on  data from  15,000  patients in  a prospective clinical              
trial. In the third stage the factors that  affect  the                         
generalizability and transportability of models to  new datasets  will          
be explored  by repeated sampling and  model construction  on                   
different  subsets of the cardiovascular database  including                    
separating the database into  subsets from each of ten different                
hospitals.                                                                      
                                                                                
This   work  will  broaden  the  understanding of  these  important             
modeling  techniques  and their  potential contributions for                    
clinical decision making,  health policy research,  and medical                 
informatics.   New modeling  techniques  might be developed which               
incorporate elements from different techniques.                                 
 artificial intelligence; cardiovascular disorder epidemiology; cardiovascular function; computer assisted medical decision making; computer simulation; disease /disorder proneness /risk; health care facility information system; human data; mathematical model; model design /development NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS","DESCRIPTION: Over  the past decade  a variety of  alternative computer          
based  modeling  techniques  have  been   introduced  which  show               
promise  for   the  construction of clinical decision aids. These               
techniques include statistical  regression  approaches such as                  
generalized additive  modeling, classification  tree induction such as          
ID3 or CART, and multi-layer neural  networks. Logistic  regression             
models (LR) are currently central to most probabilistic predictive              
clinical  decision aids and are fundamental to comparative analyses of          
medical  care based  risk adjusted events. These newer techniques               
have been applied on  a larger scale in the last few years. They                
appear to have unique advantages in  selected circumstances. The                
successful use  of these methods, however, depends  on understanding            
their accuracy, performance, and model transportability.                        
                                                                                
A  formal assessment of  these new techniques with four specific  aims          
is  proposed:  (1) to assess  and compare the  performance of                   
different  models to  determine  the  factors which affect                      
performance; (2)  to  develop automated  computer  based procedures             
for exploratory model  development for each method;  (3)  to develop            
hybrid models incorporating the strengths of each of the existing               
techniques, and  (4) to  determine the situations  that restrict  the           
transportability of these models.                                               
                                                                                
These specific aims will be  achieved in a three stage project. In              
the first  stage four approaches will be pursued:  (1) the                      
mathematical properties of the  different  computational  algorithms            
for the  modeling  techniques will be studied;  (2) automated                   
modeling procedures will be developed and utilized; (3)  the  factors           
that  affect  performance for  each  modeling technique  will  be               
explored and(4) new hybrid techniques will be developed and assessed.           
In the  second stage the methods  developed in the first stage will             
be used to create  and  test  models  that predict  cardiovascular              
events  on  data from  15,000  patients in  a prospective clinical              
trial. In the third stage the factors that  affect  the                         
generalizability and transportability of models to  new datasets  will          
be explored  by repeated sampling and  model construction  on                   
different  subsets of the cardiovascular database  including                    
separating the database into  subsets from each of ten different                
hospitals.                                                                      
                                                                                
This   work  will  broaden  the  understanding of  these  important             
modeling  techniques  and their  potential contributions for                    
clinical decision making,  health policy research,  and medical                 
informatics.   New modeling  techniques  might be developed which               
incorporate elements from different techniques.                                 
",2873360,R01LM005607,['R01LM005607'],LM,https://reporter.nih.gov/project-details/2873360,R01,1998,125000,-0.05192527914054365
"This acquisition will provide technical assistance to continue the              
development and enhancement of GRATEFUL MED as well as provide technical        
software development support for the analysis, design, implementation,          
integration, documentation, and maintenance of related system components.       
These include: computer aided learning, intermachine implementation,            
artificial intelligence, new telecommunication interfaces, e.g., FTS            
2000, Graphical User Interfaces, network integration and                        
interoperability, and document delivery systems.                                
  GRATEFUL MED@ SOFTWARE DEVELOPMENT & MANAGEMENT SUPPORT","This acquisition will provide technical assistance to continue the              
development and enhancement of GRATEFUL MED as well as provide technical        
software development support for the analysis, design, implementation,          
integration, documentation, and maintenance of related system components.       
These include: computer aided learning, intermachine implementation,            
artificial intelligence, new telecommunication interfaces, e.g., FTS            
2000, Graphical User Interfaces, network integration and                        
interoperability, and document delivery systems.                                
",2881061,01LM063511,['N01LM063511'],LM,https://reporter.nih.gov/project-details/2881061,N01,1998,105458,-0.0717158680951384
"The growing use of DNA sequence data in research, databases, diagnostic         
and therapeutic biotechnology, and even litigation dramatically                 
increases the need to improve the quality of data being used.  This             
proposal addresses the problem of assembling a large set of sequenced           
DNA fragments into a finished consensus. In order for a sequencing              
project to produce high quality finished sequence data, the assembly of         
sequence fragments must be correct and accurate both in its large scale         
structure and in the fine scale detail of the alignment of individual           
base calls. We propose to investigate new algorithms for consensus              
estimation and assembly of DNA sequence fragments.  Recent novel word-          
based approaches to consensus estimation offer promise as a method for          
de novo assembly and for exploring alternative assemblies on the large          
scale.  This will be especially important when sequences contain large          
exact or approximate repeats.  We propose to develop several main               
enhancements to these algorithms.  In particular, we will develop a             
global optimization algorithm for determining consensus sequences,              
replacing current locally optimizing methods.  Also, we propose to              
develop algorithms allowing alternative alignments in regions of                
ambiguity.  This approach will allow us to assess alignment accuracy at         
both the large and fine scale level.                                            
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Accurate assemblies are at the heart of many sequencing projects central        
to biopharmaceutical, agricultural, and basic research as well as to the        
Human Genome Project.  The proposed advances will provide the potential         
for simultaneously increasing reliability and automation in a                   
bioinformatics software market totaling about 100 million dollars per           
year.                                                                           
 artificial intelligence; computer assisted sequence analysis; computer program /software; computer system design /evaluation; method development; nucleic acid sequence NEW WORD BASED METHODS FOR DNA SEQUENCE ASSEMBLY","The growing use of DNA sequence data in research, databases, diagnostic         
and therapeutic biotechnology, and even litigation dramatically                 
increases the need to improve the quality of data being used.  This             
proposal addresses the problem of assembling a large set of sequenced           
DNA fragments into a finished consensus. In order for a sequencing              
project to produce high quality finished sequence data, the assembly of         
sequence fragments must be correct and accurate both in its large scale         
structure and in the fine scale detail of the alignment of individual           
base calls. We propose to investigate new algorithms for consensus              
estimation and assembly of DNA sequence fragments.  Recent novel word-          
based approaches to consensus estimation offer promise as a method for          
de novo assembly and for exploring alternative assemblies on the large          
scale.  This will be especially important when sequences contain large          
exact or approximate repeats.  We propose to develop several main               
enhancements to these algorithms.  In particular, we will develop a             
global optimization algorithm for determining consensus sequences,              
replacing current locally optimizing methods.  Also, we propose to              
develop algorithms allowing alternative alignments in regions of                
ambiguity.  This approach will allow us to assess alignment accuracy at         
both the large and fine scale level.                                            
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
Accurate assemblies are at the heart of many sequencing projects central        
to biopharmaceutical, agricultural, and basic research as well as to the        
Human Genome Project.  The proposed advances will provide the potential         
for simultaneously increasing reliability and automation in a                   
bioinformatics software market totaling about 100 million dollars per           
year.                                                                           
",2536784,R43HG001747,['R43HG001747'],HG,https://reporter.nih.gov/project-details/2536784,R43,1998,93458,-0.06948859116191433
"Multimodal functional brain imaging software will be developed to               
estimate and visualize the estimated spatial extent and time course of          
brain activity by combining information from magnetic resonance imaging         
(MRI) with electroencephalography (EEG) and/or magnetoencephalography           
(MEG).  Structural information from MRI will be combined with                   
extracranial EEG and/or MEG measurements through algorithms developed           
to segment the MR images and to represent scalp, skull, and brain               
boundaries as computational objects.  This structural information may           
then be used to improve the spatial accuracy and resolution of existing         
EEG and MEG source estimation algorithms, while supporting millisecond          
temporal resolution.  The software will comprise a PC/Windows-based             
program suite for analysis and display.  The methods will be verified           
both with simulated data and with physiological data.                           
                                                                                
The algorithms and software may be used to study both normal brain              
function, such as measurements in cognitive neuroscience which may be           
studied with evoked response/event related potentials or spontaneous            
EEG, and in diseases of the brain, such as epilepsy, where precise              
spatial and temporal resolution may be of value for diagnosis and               
presurgical evaluation.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The techniques which we propose are a non-invasive, non-radiological and        
relatively low cost addition to existing EEG, MEG and MRI systems, and          
provides information which is not currently available from these systems        
independently.  The resulting software will have direct application in          
clinical and cognitive neuroscience research.  If clinical value is             
demonstrated, systems based on this methodology may find applications           
in the areas of psychiatry, neurology and psychology.                           
 artificial intelligence; brain electrical activity; computer program /software; computer system design /evaluation; electroencephalography; functional magnetic resonance imaging; human data; image enhancement; image processing; magnetic resonance imaging; magnetoencephalography; positron emission tomography MULTIMODAL (MRI/EEG/MEG) IMAGING SOFTWARE","Multimodal functional brain imaging software will be developed to               
estimate and visualize the estimated spatial extent and time course of          
brain activity by combining information from magnetic resonance imaging         
(MRI) with electroencephalography (EEG) and/or magnetoencephalography           
(MEG).  Structural information from MRI will be combined with                   
extracranial EEG and/or MEG measurements through algorithms developed           
to segment the MR images and to represent scalp, skull, and brain               
boundaries as computational objects.  This structural information may           
then be used to improve the spatial accuracy and resolution of existing         
EEG and MEG source estimation algorithms, while supporting millisecond          
temporal resolution.  The software will comprise a PC/Windows-based             
program suite for analysis and display.  The methods will be verified           
both with simulated data and with physiological data.                           
                                                                                
The algorithms and software may be used to study both normal brain              
function, such as measurements in cognitive neuroscience which may be           
studied with evoked response/event related potentials or spontaneous            
EEG, and in diseases of the brain, such as epilepsy, where precise              
spatial and temporal resolution may be of value for diagnosis and               
presurgical evaluation.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The techniques which we propose are a non-invasive, non-radiological and        
relatively low cost addition to existing EEG, MEG and MRI systems, and          
provides information which is not currently available from these systems        
independently.  The resulting software will have direct application in          
clinical and cognitive neuroscience research.  If clinical value is             
demonstrated, systems based on this methodology may find applications           
in the areas of psychiatry, neurology and psychology.                           
",2536158,R44MH055915,['R44MH055915'],MH,https://reporter.nih.gov/project-details/2536158,R44,1998,355745,-0.027132894903323997
"Quantitative energy dispersive spectroscopy (EDS) x-ray microanalysis           
provides important information regarding the distribution of elements           
within a biological sample.  Low elemental concentration in biological          
specimens require dwell times of several seconds livetime to obtain             
spectra with significant statistical information, especially for                
quantitative EDS imaging.  Deadtime caused by the rejection of pulse            
pileup counts is a major limiting factor of x-ray collection during the         
acquisition of x-ray spectra.  Phase I a) demonstrated the feasibility,         
advantages, and minimal performance requirements for pulse pileup               
recovery, b) described a hardware approach using state-of-the-art               
digital signal processing, and c) showed that the resulting decrease in         
rejected pulse events can save significant time and cost by increasing          
throughput by as much as factor of four.  Phase II will advance the             
design, construct prototypes, and address requirements for commercial           
implementation.  The specific goal is to make the system low-cost and           
retrofitable into existing EDS systems.  The techniques and hardware            
will be broadly applicable to any scientific instrumentation which              
utilized pulse height analysis, such as x-ray fluorescence and gamma-           
ray spectroscopy, and therefore has great potential for                         
commercialization.                                                              
 X ray spectrometry; analytical method; artificial intelligence; biomedical equipment development; electron probe spectrometry; image processing PULSE PILEUP RECOVERY FOR ENERGY DISPERSIVE SPECTROSCOPY","Quantitative energy dispersive spectroscopy (EDS) x-ray microanalysis           
provides important information regarding the distribution of elements           
within a biological sample.  Low elemental concentration in biological          
specimens require dwell times of several seconds livetime to obtain             
spectra with significant statistical information, especially for                
quantitative EDS imaging.  Deadtime caused by the rejection of pulse            
pileup counts is a major limiting factor of x-ray collection during the         
acquisition of x-ray spectra.  Phase I a) demonstrated the feasibility,         
advantages, and minimal performance requirements for pulse pileup               
recovery, b) described a hardware approach using state-of-the-art               
digital signal processing, and c) showed that the resulting decrease in         
rejected pulse events can save significant time and cost by increasing          
throughput by as much as factor of four.  Phase II will advance the             
design, construct prototypes, and address requirements for commercial           
implementation.  The specific goal is to make the system low-cost and           
retrofitable into existing EDS systems.  The techniques and hardware            
will be broadly applicable to any scientific instrumentation which              
utilized pulse height analysis, such as x-ray fluorescence and gamma-           
ray spectroscopy, and therefore has great potential for                         
commercialization.                                                              
",2703183,R44RR008828,['R44RR008828'],RR,https://reporter.nih.gov/project-details/2703183,R44,1998,201207,-0.04007349086720499
"Abstractions of time-stamped clinical data are useful for planning              
therapy, for monitoring therapy, and for creating high-level summaries of       
time-oriented clinical databases.  Temporal abstractions also support           
explanations by an intelligent patient-record system and can be used for        
representation of the goals and intentions of clinical guidelines and           
protocols.                                                                      
                                                                                
We propose to reengineer and expand the scope of the RESUME system, a           
prototype computer program that implements the knowledge-based temporal-        
abstraction method, a conceptual and computational framework that we have       
developed for abstraction of time-stamped clinical data into clinically         
meaningful interval-based concepts. RESUME has been evaluated with highly       
encouraging results in several clinical areas. We will address the              
practical and theoretical issues of representation, acquisition,                
maintenance, and reuse of temporal-abstraction knowledge. Our specific          
aims are defined by a four-step research plan:                                  
                                                                                
1. We will define formally the knowledge requirements for five                  
computational modules (mechanisms) we employ, thus facilitating the             
acquisition, maintenance, reuse, and sharing of the required knowledge.         
                                                                                
2. We will enhance, expand, and redesign five computational temporal-           
abstraction mechanisms:                                                         
(a) Automatic formation of meaningful contexts for interpretation of            
clinical data.                                                                  
(b) Classification of clinical data that have equivalent time stamps into       
higher-level concepts.                                                          
(c) Temporal inference (e.g., the join of certain interval-based clinical       
abstractions into longer ones).                                                 
(d) Interpolation between temporally disjoint clinical abstractions,            
including a development of a probabilistic representation and semantics.        
(e) Matching of predefined and runtime temporal patterns, given time-           
stamped data and conclusions.                                                   
                                                                                
3. We will develop a tool for automated acquisition, from expert                
physicians, of temporal-abstraction knowledge, using techniques from the        
PROTEGE-II project for designing knowledge-based systems.                       
                                                                                
4. We will validate and evaluate our methodology and its implementation.        
(a) We will assess the value of the knowledge-acquisition tool in several       
experiments.                                                                    
(b) We will validate the performance of the computational mechanisms in         
the domain of therapy of patients who have insulin-dependent diabetes by        
collaboration with expert endocrinologists.                                     
(c) We will evaluate the overall framework within EON, a project in which       
researchers are implementing an integrated architecture for protocol-based      
care.                                                                           
 abstracting; artificial intelligence; computer assisted medical decision making; computer program /software; data collection methodology /evaluation; health care facility information system; human data; time resolved data KNOWLEDGE BASED TEMPORAL ABSTRACTION OF CLINICAL DATA","Abstractions of time-stamped clinical data are useful for planning              
therapy, for monitoring therapy, and for creating high-level summaries of       
time-oriented clinical databases.  Temporal abstractions also support           
explanations by an intelligent patient-record system and can be used for        
representation of the goals and intentions of clinical guidelines and           
protocols.                                                                      
                                                                                
We propose to reengineer and expand the scope of the RESUME system, a           
prototype computer program that implements the knowledge-based temporal-        
abstraction method, a conceptual and computational framework that we have       
developed for abstraction of time-stamped clinical data into clinically         
meaningful interval-based concepts. RESUME has been evaluated with highly       
encouraging results in several clinical areas. We will address the              
practical and theoretical issues of representation, acquisition,                
maintenance, and reuse of temporal-abstraction knowledge. Our specific          
aims are defined by a four-step research plan:                                  
                                                                                
1. We will define formally the knowledge requirements for five                  
computational modules (mechanisms) we employ, thus facilitating the             
acquisition, maintenance, reuse, and sharing of the required knowledge.         
                                                                                
2. We will enhance, expand, and redesign five computational temporal-           
abstraction mechanisms:                                                         
(a) Automatic formation of meaningful contexts for interpretation of            
clinical data.                                                                  
(b) Classification of clinical data that have equivalent time stamps into       
higher-level concepts.                                                          
(c) Temporal inference (e.g., the join of certain interval-based clinical       
abstractions into longer ones).                                                 
(d) Interpolation between temporally disjoint clinical abstractions,            
including a development of a probabilistic representation and semantics.        
(e) Matching of predefined and runtime temporal patterns, given time-           
stamped data and conclusions.                                                   
                                                                                
3. We will develop a tool for automated acquisition, from expert                
physicians, of temporal-abstraction knowledge, using techniques from the        
PROTEGE-II project for designing knowledge-based systems.                       
                                                                                
4. We will validate and evaluate our methodology and its implementation.        
(a) We will assess the value of the knowledge-acquisition tool in several       
experiments.                                                                    
(b) We will validate the performance of the computational mechanisms in         
the domain of therapy of patients who have insulin-dependent diabetes by        
collaboration with expert endocrinologists.                                     
(c) We will evaluate the overall framework within EON, a project in which       
researchers are implementing an integrated architecture for protocol-based      
care.                                                                           
",2702871,R29LM006245,['R29LM006245'],LM,https://reporter.nih.gov/project-details/2702871,R29,1998,111950,-0.019238224814804653
"DESCRIPTION (Adapted from Applicant's Abstract):  This Phase I project          
will produce PC-base software capable of detecting first level feature          
(FLF) of clinical utility, such as spikes, K-complexes, and artifacts           
, from routine EEG's,long-term monitoring , and sleep studies.  The goal        
following Phase II sill be software capable of identifying third level          
features (TLF), corresponding to clinical diagnostic features.                  
                                                                                
In Phase I, a panel of clinical experts will define FLF and score               
recordings from our available database.  The database will be split into        
training and tests sets.  EEG time domain and frequency domain                  
parameters from the training set will be calculated and entered into the        
first layer of an artificial neural network (ANN).  Automatic                   
classification of FLF in the test set will be compared to expert                
scoring.  A successful completion of Phase I will be 90 percent correct         
automatic.                                                                      
                                                                                
During Phase II, FLF will be entered into a second level ANN.  The              
resulting second level features (SLF) will be compared with expert              
classification of patient states, such as sleep, arousal, and seizures.         
FLF and SLF will then be combined into TLF using syntactical analysis           
and adaptive segmentation to match expert clinical classifications.  TLF        
represent clinical diagnostic features, such as a focal lesion,                 
epileptiform, or fragmented sleep.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION: Not available.                                 
 biomedical equipment development; computer program /software; electroencephalography DEFINITION AND DETECTION OF CLINICAL EEG FEATURES","DESCRIPTION (Adapted from Applicant's Abstract):  This Phase I project          
will produce PC-base software capable of detecting first level feature          
(FLF) of clinical utility, such as spikes, K-complexes, and artifacts           
, from routine EEG's,long-term monitoring , and sleep studies.  The goal        
following Phase II sill be software capable of identifying third level          
features (TLF), corresponding to clinical diagnostic features.                  
                                                                                
In Phase I, a panel of clinical experts will define FLF and score               
recordings from our available database.  The database will be split into        
training and tests sets.  EEG time domain and frequency domain                  
parameters from the training set will be calculated and entered into the        
first layer of an artificial neural network (ANN).  Automatic                   
classification of FLF in the test set will be compared to expert                
scoring.  A successful completion of Phase I will be 90 percent correct         
automatic.                                                                      
                                                                                
During Phase II, FLF will be entered into a second level ANN.  The              
resulting second level features (SLF) will be compared with expert              
classification of patient states, such as sleep, arousal, and seizures.         
FLF and SLF will then be combined into TLF using syntactical analysis           
and adaptive segmentation to match expert clinical classifications.  TLF        
represent clinical diagnostic features, such as a focal lesion,                 
epileptiform, or fragmented sleep.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION: Not available.                                 
",2649862,R43NS037636,['R43NS037636'],NS,https://reporter.nih.gov/project-details/2649862,R43,1998,99688,-0.009161271597755198
"DESCRIPTION:  Access to comprehensive medical information is literally          
a matter of life and death for health professionals.  The applicants            
propose to develop a state-of-the-art medical information system based          
on natural language processing (NLP) which provides innovative access           
first to the literature of complementary medicine and then to the               
traditional medical literature.  The goal of Phase I is dual:  to               
develop a finely tuned, immediately usable information system for               
alternative medicine, and to determine on an experimental level what            
linguistic elements within the medical subject domain are critical to           
optimizing retrieval.  For this purpose, an extensive analysis of               
medical sublanguage and the text structure of medical documents will be         
undertaken, and the results will be applied to each module of the system        
in order to optimize it for medical domain.  The Phase I system will be         
used by the medical community both for access to hard-to-find                   
information, and to begin to assess the usefulness of complementary             
medicine techniques in treating chronic problems.  Phase II will add            
traditional medical literature to the system to provide a fully                 
integrated solution for rich, precise access to medical information.            
                                                                                
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                  
 alternative medicine; artificial intelligence; computer system design /evaluation; information retrieval; information systems; language MEDLINK","DESCRIPTION:  Access to comprehensive medical information is literally          
a matter of life and death for health professionals.  The applicants            
propose to develop a state-of-the-art medical information system based          
on natural language processing (NLP) which provides innovative access           
first to the literature of complementary medicine and then to the               
traditional medical literature.  The goal of Phase I is dual:  to               
develop a finely tuned, immediately usable information system for               
alternative medicine, and to determine on an experimental level what            
linguistic elements within the medical subject domain are critical to           
optimizing retrieval.  For this purpose, an extensive analysis of               
medical sublanguage and the text structure of medical documents will be         
undertaken, and the results will be applied to each module of the system        
in order to optimize it for medical domain.  The Phase I system will be         
used by the medical community both for access to hard-to-find                   
information, and to begin to assess the usefulness of complementary             
medicine techniques in treating chronic problems.  Phase II will add            
traditional medical literature to the system to provide a fully                 
integrated solution for rich, precise access to medical information.            
                                                                                
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                  
",2647486,R43LM006671,['R43LM006671'],LM,https://reporter.nih.gov/project-details/2647486,R43,1998,92217,-0.10515502974186655
"The brain is at risk of serious cerebrovascular insult during the               
400,000 cardiopulmonary bypass (CPB) surgeries performed annually in the        
United States.  There is strong clinical evidence that over two-thirds          
of patients exhibit neurologic or neuropsychologic (NP) Postoperative           
deficits caused by emboli passing to the brain during surgery.  There           
is a critical need for a device that can not only detect emboli, but            
also classify them as to type.  Classification is critical for                  
correlating neurological deficits with emboli type, determining the             
source of the emboli, and subsequently changing surgical procedures             
and/or administering neuroprotective agents to minimize brain injury.           
Continuous-wave Doppler ultrasound equipment can detect emboli, but             
cannot provide the information needed to classify emboli composition.           
In Phase I, ORINCON and the Bowman Gray School of Medicine utilized             
broadband pulse-echo ultrasound and an artificial neural network to             
demonstrate significant classification capability on in-vitro data.  In         
Phase II, we will refine and integrate components from Phase I into a           
PC-based system capable of accurate, real-time emboli detection and             
classification in both extracorporeal pump circuits and the carotid             
artery.  Extensive in-vitro and in-vivo data will be collected to permit        
refinement and thorough evaluation of classification capabilities.              
Clinical studies will be performed to correlate neuropsychologic                
deficits with embolus composition.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is immediate commercial potential for a low-cost (less than 25K),         
real-time automated emboli detection and classification system.  The            
market includes manufacturers of cardiopulmonary pump circuit devices           
(Medtronics, Pall, Cobe, Sarns), hundreds of medical centers performing         
cardiopulmonary bypass, and producers of neuroprotective drugs (Astra,          
Bayer, Sterling).  A letter from Medtronics expressing this commercial          
potential is enclosed.                                                          
 artificial intelligence; brain injury; cardiovascular disorder diagnosis; computational neuroscience; computer system design /evaluation; diagnosis design /evaluation; dogs; embolism; extracorporeal circulation EMBOLI CLASSIFICATION SYSTEM TO REDUCE BRAIN INJURY","The brain is at risk of serious cerebrovascular insult during the               
400,000 cardiopulmonary bypass (CPB) surgeries performed annually in the        
United States.  There is strong clinical evidence that over two-thirds          
of patients exhibit neurologic or neuropsychologic (NP) Postoperative           
deficits caused by emboli passing to the brain during surgery.  There           
is a critical need for a device that can not only detect emboli, but            
also classify them as to type.  Classification is critical for                  
correlating neurological deficits with emboli type, determining the             
source of the emboli, and subsequently changing surgical procedures             
and/or administering neuroprotective agents to minimize brain injury.           
Continuous-wave Doppler ultrasound equipment can detect emboli, but             
cannot provide the information needed to classify emboli composition.           
In Phase I, ORINCON and the Bowman Gray School of Medicine utilized             
broadband pulse-echo ultrasound and an artificial neural network to             
demonstrate significant classification capability on in-vitro data.  In         
Phase II, we will refine and integrate components from Phase I into a           
PC-based system capable of accurate, real-time emboli detection and             
classification in both extracorporeal pump circuits and the carotid             
artery.  Extensive in-vitro and in-vivo data will be collected to permit        
refinement and thorough evaluation of classification capabilities.              
Clinical studies will be performed to correlate neuropsychologic                
deficits with embolus composition.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is immediate commercial potential for a low-cost (less than 25K),         
real-time automated emboli detection and classification system.  The            
market includes manufacturers of cardiopulmonary pump circuit devices           
(Medtronics, Pall, Cobe, Sarns), hundreds of medical centers performing         
cardiopulmonary bypass, and producers of neuroprotective drugs (Astra,          
Bayer, Sterling).  A letter from Medtronics expressing this commercial          
potential is enclosed.                                                          
",2644307,R44NS034208,['R44NS034208'],NS,https://reporter.nih.gov/project-details/2644307,R44,1998,383693,-0.005580234997179418
"We propose to work in the development and application of mathematical,          
statistical, and computational methods for the analysis of nucleic acid         
and amino acid sequence data. The long range goals can be placed into           
three categories. (1) Computational analysis is essential to our                
approaches to sequence data. Algorithms are being developed for shotgun         
sequence assembly, to search for tandem repeats of length up to 32              
basepairs, to find the consensus local alignment of an unknown region           
common to an unknown subset of sequences, to study the                          
thermodynamic/statistical behavior of experiments that repeatedly select        
and amplify DNA molecules, and to weight multiple and suboptimal sequence       
alignment paths. (2) Physical mapping of DNA is important in genome             
analysis. Studies include the PEP procedure to amplify single chromosomes,      
PCR is a branching process including both amplification errors and              
efficiency less than 1, the mathematical analysis of physical mapping           
using end characterized clones, and classification of multiple solutions        
of the double digest problem. (3) As sequence data increase, estimating         
statistical significance becomes more central. We will develop methods for      
estimating statistical significance of scores of tandem repeats, Poisson        
distributional results for sequence alignment in certain cases where the        
Chen-Stein method fails, the statistical distribution of correctly              
inferred sequence in shotgun sequencing projects as a function of depth         
and accuracy, and the growth of minimum free energy of secondary                
structures of a random RNA.                                                     
 RNA; RNA splicing; artificial intelligence; computer assisted sequence analysis; computer program /software; computer simulation; computer system design /evaluation; gene mutation; genetic mapping; genetic models; mathematical model; model design /development; molecular genetics; nucleic acid sequence; polymerase chain reaction; protein sequence; ribosomal RNA; statistics /biometry PATTERN RECOGNITION FOR ANALYSIS OF MOLECULAR SEQUENCES","We propose to work in the development and application of mathematical,          
statistical, and computational methods for the analysis of nucleic acid         
and amino acid sequence data. The long range goals can be placed into           
three categories. (1) Computational analysis is essential to our                
approaches to sequence data. Algorithms are being developed for shotgun         
sequence assembly, to search for tandem repeats of length up to 32              
basepairs, to find the consensus local alignment of an unknown region           
common to an unknown subset of sequences, to study the                          
thermodynamic/statistical behavior of experiments that repeatedly select        
and amplify DNA molecules, and to weight multiple and suboptimal sequence       
alignment paths. (2) Physical mapping of DNA is important in genome             
analysis. Studies include the PEP procedure to amplify single chromosomes,      
PCR is a branching process including both amplification errors and              
efficiency less than 1, the mathematical analysis of physical mapping           
using end characterized clones, and classification of multiple solutions        
of the double digest problem. (3) As sequence data increase, estimating         
statistical significance becomes more central. We will develop methods for      
estimating statistical significance of scores of tandem repeats, Poisson        
distributional results for sequence alignment in certain cases where the        
Chen-Stein method fails, the statistical distribution of correctly              
inferred sequence in shotgun sequencing projects as a function of depth         
and accuracy, and the growth of minimum free energy of secondary                
structures of a random RNA.                                                     
",2634657,R01GM036230,['R01GM036230'],GM,https://reporter.nih.gov/project-details/2634657,R01,1998,427713,-0.17035510151251182
"DESCRIPTION:  This application proposes to develop, implement, and evaluate     
a world wide web-based computerized decision support system (CDSS) to           
facilitate information exchange and guide interactions between                  
geographically distributed physicians and centrally-located experts in bone     
marrow transplant (BMT) follow-up care.  The CDSS will include standard         
practice guidelines and research findings specific for the long-term            
follow-up (LTFU) of patients post-BMT but will be designed to be adaptable      
to other disease and treatment situations.  Key elements required for the       
conduct of the project are already in place, including an ontology of           
long-term follow-up, diagnostic pathways, and practice guidelines; a            
multidisciplinary team with broad experience; a high volume of follow-up and    
consultation demand; and a network of over 2,000 primary specialists caring     
for the patients in a wide variety of practice settings.  Each year the LTFU    
unit receives over 5,000 pieces of patient-care mail, sends 4,000 letters,      
returns 8,000 phone calls, and mails over 1,200 protocols, consent forms,       
and medical recommendations.                                                    
                                                                                
The proposed project will complete and refine a networked CDSS, conduct a       
phase II pilot study of clinical use of the CDSS within the bone marrow         
transplant center, conduct a phase III randomized clinical trial of the         
benefit of the CDSS with over 250 primary care physicians randomized to         
either CDSS or the existing method of follow-up, and evaluate the impact of     
CDSS on physician behavior and practice efficiency.  Endpoints for the phase    
III portion of the project include patient outcomes and complications,          
quality of life, cost of patient care, physician satisfaction, and frequency    
of accessing the protocols/guidelines.  An attempt will also be made to         
identify factors predicting the success of the CDSS.                            
 Internet; bone marrow transplantation; clinical research; computer assisted medical decision making; computer assisted patient care; health care cost /financing; health care service evaluation; health care service utilization; human subject; longitudinal human study; quality of life; telemedicine COMPUTERIZED DECISION SUPPORT FOR POSTTRANSPLANT CARE","DESCRIPTION:  This application proposes to develop, implement, and evaluate     
a world wide web-based computerized decision support system (CDSS) to           
facilitate information exchange and guide interactions between                  
geographically distributed physicians and centrally-located experts in bone     
marrow transplant (BMT) follow-up care.  The CDSS will include standard         
practice guidelines and research findings specific for the long-term            
follow-up (LTFU) of patients post-BMT but will be designed to be adaptable      
to other disease and treatment situations.  Key elements required for the       
conduct of the project are already in place, including an ontology of           
long-term follow-up, diagnostic pathways, and practice guidelines; a            
multidisciplinary team with broad experience; a high volume of follow-up and    
consultation demand; and a network of over 2,000 primary specialists caring     
for the patients in a wide variety of practice settings.  Each year the LTFU    
unit receives over 5,000 pieces of patient-care mail, sends 4,000 letters,      
returns 8,000 phone calls, and mails over 1,200 protocols, consent forms,       
and medical recommendations.                                                    
                                                                                
The proposed project will complete and refine a networked CDSS, conduct a       
phase II pilot study of clinical use of the CDSS within the bone marrow         
transplant center, conduct a phase III randomized clinical trial of the         
benefit of the CDSS with over 250 primary care physicians randomized to         
either CDSS or the existing method of follow-up, and evaluate the impact of     
CDSS on physician behavior and practice efficiency.  Endpoints for the phase    
III portion of the project include patient outcomes and complications,          
quality of life, cost of patient care, physician satisfaction, and frequency    
of accessing the protocols/guidelines.  An attempt will also be made to         
identify factors predicting the success of the CDSS.                            
",2796842,R01HS009407,['R01HS009407'],HS,https://reporter.nih.gov/project-details/2796842,R01,1998,72062,-0.018010272736929858
"During Phase I, Flint Hills Scientific developed an algorithm for real          
time quantitative seizure detection which performs with sensitivity and         
specificity equal to expert visual analysis. Of even greater value is the       
capability of this algorithm to predict seizure onset by 13.6 seconds           
(mean), in its generic mode. Preliminary studies indicate that with             
automated individualized adaptation, prediction time can be increased to        
180 seconds or longer. To the best of our knowledge no other system in          
existence has achieved this level of success. These results lay the ground      
for the fulfillment of ""seizure prediction, early recognition and blockage      
of seizures,"" the number one AES research priority.                             
                                                                                
The main goal of Phase II will be to advance, further refine, and validate      
this technology for implementation into a portable or implantable device        
with diagnostic, warning, and therapeutic capabilities. We are confident        
that this technology, by decreasing or eliminating unpredictability, will       
minimize the potentially devastating effect of seizures on quality of life      
while decreasing morbidity, the cost of health care, and the reliance on        
the welfare system. These unique advantages will ensure widespread              
acceptance of this technology by those directly and indirectly affected by      
epilepsy and by the health care system.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
1. Software package for real time seizure prediction, detection,                
localization, imaging, and quantitative analysis. 2. Software package for       
automated, selective noise reduction 3. Portable device for the automated       
early warning of impending seizures. 4. Portable or implantable devices         
for automated early therapeutic intervention.                                   
 artificial intelligence; bioengineering /biomedical engineering; brain electrical activity; clinical biomedical equipment; clinical research; computer program /software; digital imaging; electroencephalography; epilepsy; health care cost /financing; human subject; monitoring device SOFTWARE AND DEVICES FOR REAL TIME SEIZURE DETECTION","During Phase I, Flint Hills Scientific developed an algorithm for real          
time quantitative seizure detection which performs with sensitivity and         
specificity equal to expert visual analysis. Of even greater value is the       
capability of this algorithm to predict seizure onset by 13.6 seconds           
(mean), in its generic mode. Preliminary studies indicate that with             
automated individualized adaptation, prediction time can be increased to        
180 seconds or longer. To the best of our knowledge no other system in          
existence has achieved this level of success. These results lay the ground      
for the fulfillment of ""seizure prediction, early recognition and blockage      
of seizures,"" the number one AES research priority.                             
                                                                                
The main goal of Phase II will be to advance, further refine, and validate      
this technology for implementation into a portable or implantable device        
with diagnostic, warning, and therapeutic capabilities. We are confident        
that this technology, by decreasing or eliminating unpredictability, will       
minimize the potentially devastating effect of seizures on quality of life      
while decreasing morbidity, the cost of health care, and the reliance on        
the welfare system. These unique advantages will ensure widespread              
acceptance of this technology by those directly and indirectly affected by      
epilepsy and by the health care system.                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
1. Software package for real time seizure prediction, detection,                
localization, imaging, and quantitative analysis. 2. Software package for       
automated, selective noise reduction 3. Portable device for the automated       
early warning of impending seizures. 4. Portable or implantable devices         
for automated early therapeutic intervention.                                   
",2771953,R44NS034630,['R44NS034630'],NS,https://reporter.nih.gov/project-details/2771953,R44,1998,307888,0.019498607242339434
"DESCRIPTION: In the summary statement the principal investigator was            
criticized: (1) for the lack of a central hypothesis linking together           
the three specific aims; (2) the absence of any obvious decision tree           
guiding the proposed studies; and (3) the lack of preliminary data              
documenting the feasibility of conducting a number of the proposed              
studies. In response to these specific criticisms the application has           
been considerably revised and as a result has been improved. Among              
these revisions is the elimination of all studies with antibodies               
against the muscarinic receptors m1, m3, m4 and m5 and all EM work on           
post-mortem human specimens. In addition, the specific hypotheses and           
decision trees guiding the proposed experiments are more elaborately            
presented. To this end the principal investigator prepared an overview          
figure summarizing both the known components of the cholinergic pathways        
in the primate cerebral cortex and those unknown components that will           
be addressed by the present proposal.                                           
 Alzheimer's disease; Macaca; acetylcholine; aging; cerebral cortex; electron microscopy; human tissue; neuroanatomy; neurochemistry; synapses CENTRAL CHOLINERGIC PATHWAYS","DESCRIPTION: In the summary statement the principal investigator was            
criticized: (1) for the lack of a central hypothesis linking together           
the three specific aims; (2) the absence of any obvious decision tree           
guiding the proposed studies; and (3) the lack of preliminary data              
documenting the feasibility of conducting a number of the proposed              
studies. In response to these specific criticisms the application has           
been considerably revised and as a result has been improved. Among              
these revisions is the elimination of all studies with antibodies               
against the muscarinic receptors m1, m3, m4 and m5 and all EM work on           
post-mortem human specimens. In addition, the specific hypotheses and           
decision trees guiding the proposed experiments are more elaborately            
presented. To this end the principal investigator prepared an overview          
figure summarizing both the known components of the cholinergic pathways        
in the primate cerebral cortex and those unknown components that will           
be addressed by the present proposal.                                           
",2771913,R01NS020285,['R01NS020285'],NS,https://reporter.nih.gov/project-details/2771913,R01,1998,179638,-0.09651064635603235
"Our long-term objective is the development of expert systems using              
fuzzy logic for sophisticated, rapid analysis of physiologic data in            
intensive care units.  Our immediate objectives are to tune and extend          
our Phase I pulmonary artery catheter data system to alarm nursing              
staff on invalid data, and to conduct clinical trials to test whether           
the system will improve patient care.  The existing single-bed system           
will be adapted to multi-bed and Hewlett-Packard, Marquette and Space           
Labs physiolgoic monitors.  Eighty patients at ICUs in two hospitals            
will participate in final system development, and an additional 200             
patients will participate in a clinical trial of system effectiveness           
in reducing the number and duration of pulmonary artery catheter                
abnormalities.  (Our tests indicate that the incidence of catheter              
malfunction may be badly underestimated.)  We will also study the               
expansion of the expert system to include additional information                
including SvO2 and cardiac output, and to deliver more informative              
messages to nurses and physicians.  Finally, the system will be made            
suitable for commercial use.                                                    
 cardiac output; clinical research; clinical trials; computer system design /evaluation; heart catheterization; human subject; intensive care; patient care management; patient monitoring device; pulmonary artery PULMONARY ARTERY CATHETER ALARM SYSTEM","Our long-term objective is the development of expert systems using              
fuzzy logic for sophisticated, rapid analysis of physiologic data in            
intensive care units.  Our immediate objectives are to tune and extend          
our Phase I pulmonary artery catheter data system to alarm nursing              
staff on invalid data, and to conduct clinical trials to test whether           
the system will improve patient care.  The existing single-bed system           
will be adapted to multi-bed and Hewlett-Packard, Marquette and Space           
Labs physiolgoic monitors.  Eighty patients at ICUs in two hospitals            
will participate in final system development, and an additional 200             
patients will participate in a clinical trial of system effectiveness           
in reducing the number and duration of pulmonary artery catheter                
abnormalities.  (Our tests indicate that the incidence of catheter              
malfunction may be badly underestimated.)  We will also study the               
expansion of the expert system to include additional information                
including SvO2 and cardiac output, and to deliver more informative              
messages to nurses and physicians.  Finally, the system will be made            
suitable for commercial use.                                                    
",2797132,R44RR013160,['R44RR013160'],RR,https://reporter.nih.gov/project-details/2797132,R44,1998,241201,-0.12030002364461878
"This project will use the techniques of epidemiology and clinical               
decision analysis to study the management of infants with hypoplastic           
left heart syndrome (HLHS), a congenital heart disease which if                 
untreated is uniformly fatal. The broad objective is to determine the           
optimal surgical approach to treatment, based on mortality and                  
morbidity comparisons of two strategies: staged reconstructive                  
surgeries or heart transplantation. Specific aims are to demonstrate            
how improved surgical technique and postoperative care have lowered             
mortality over time, compare current mortality rates between                    
strategies, determine predictors of mortality, the limitations                  
inherent in each strategy, the problems surviving children and their            
caregivers face, and predictors of morbidity. The project design is a           
review of the medical records of all patients born with HLHS between            
1989 and 1993, admitted to one of the four participating pediatric              
cardiac surgery centers with an intention to treat surgically.                  
Specific preoperative and postoperative factors will be compared in             
multivariate analysis as predictors of mortality and morbidity for              
each strategy. The research will produce a scientific report with a             
complete literature review and support the creation of a decision tree          
to determine the better surgical strategy based on individual                   
preoperative characteristics.                                                   
 behavioral /social science research tag; cardiovascular disorder epidemiology; clinical research; congenital heart disorder; decision making; health care policy; heart revascularization; heart transplantation; human morbidity; human subject; infant human (0-1 year); infant mortality; medical records; patient care management; preoperative state; statistics /biometry HYPOPLASTIC LEFT HEART SYNDROME--SURGICAL ATERNATIVES","This project will use the techniques of epidemiology and clinical               
decision analysis to study the management of infants with hypoplastic           
left heart syndrome (HLHS), a congenital heart disease which if                 
untreated is uniformly fatal. The broad objective is to determine the           
optimal surgical approach to treatment, based on mortality and                  
morbidity comparisons of two strategies: staged reconstructive                  
surgeries or heart transplantation. Specific aims are to demonstrate            
how improved surgical technique and postoperative care have lowered             
mortality over time, compare current mortality rates between                    
strategies, determine predictors of mortality, the limitations                  
inherent in each strategy, the problems surviving children and their            
caregivers face, and predictors of morbidity. The project design is a           
review of the medical records of all patients born with HLHS between            
1989 and 1993, admitted to one of the four participating pediatric              
cardiac surgery centers with an intention to treat surgically.                  
Specific preoperative and postoperative factors will be compared in             
multivariate analysis as predictors of mortality and morbidity for              
each strategy. The research will produce a scientific report with a             
complete literature review and support the creation of a decision tree          
to determine the better surgical strategy based on individual                   
preoperative characteristics.                                                   
",2771195,F32HL009488,['F32HL009488'],HL,https://reporter.nih.gov/project-details/2771195,F32,1998,35476,-0.0013889104312835537
"DESCRIPTION:  Computing with biochemical reactions is increasingly important    
in studying genomes, assessing toxicity, and developing therapeutics.  There    
are several important information sources, but their data are rudimentary       
and often inaccurate.  Incorporation of biochemical information into            
databases is extremely slow compared to that of sequence and structural         
information, and will lag further as large-scale surveys of gene expression     
and other reactions accelerate over the next few years.  Mechanisms for         
review exist, but are manual, paper-dependent, and can be delayed for a year    
or more.                                                                        
                                                                                
As curators and coordinators of biochemical information sources, the            
applicants share a number of problems in the collection and review of           
information.  Moreover, they are mutually dependent for the means to do so:     
compound information is critical in checking reaction data, reaction            
information is needed to spot errors in compound information, and the           
automatic verification algorithms for either are closely related and need       
both.  The applicants, therefore, propose to build a curatorial exchange for    
the deposit and review of biochemical information by the scientific             
community.  The applicants' goal is to demonstrate a system that will           
encourage the mandating of deposit while ensuring that the information is of    
the highest quality.                                                            
                                                                                
The role of the exchange is to receive deposits, check and classify their       
biochemical information automatically, forward them to panels of human          
reviewers for vetting, and publish the information by release to the            
participating data sources--all over the World-Wide Web. It will track the      
origin and status of deposits and reviews, serve computations for the           
relevant pattern matching and simulation, and maintain an archival copy of      
data.  The databases remain independent, and separately provide additional      
information.  Algorithm development and testing depends on an adequate          
information infrastructure, so the applicants will complete a basic data set    
of compounds and reactions.  They will use this experience to develop a more    
comprehensive domain model that better captures modern biochemistry, and        
implement it for deposit and review.  Since the basic data and algorithms       
will be valuable to the community at large, they plan to serve these to the     
World-Wide Web. the exchange and its underlying data form the infrastructure    
necessary for sustainable, cost-effective development of biochemical            
informatics resources for biomedical research.                                  
 artificial intelligence; biochemistry; chemical information system; chemical reaction; chemical structure; computer program /software; computer system design /evaluation; technology /technique development COMMUNITY DEPOSIT AND REVIEW OF BIOCHEMICAL DATABASES","DESCRIPTION:  Computing with biochemical reactions is increasingly important    
in studying genomes, assessing toxicity, and developing therapeutics.  There    
are several important information sources, but their data are rudimentary       
and often inaccurate.  Incorporation of biochemical information into            
databases is extremely slow compared to that of sequence and structural         
information, and will lag further as large-scale surveys of gene expression     
and other reactions accelerate over the next few years.  Mechanisms for         
review exist, but are manual, paper-dependent, and can be delayed for a year    
or more.                                                                        
                                                                                
As curators and coordinators of biochemical information sources, the            
applicants share a number of problems in the collection and review of           
information.  Moreover, they are mutually dependent for the means to do so:     
compound information is critical in checking reaction data, reaction            
information is needed to spot errors in compound information, and the           
automatic verification algorithms for either are closely related and need       
both.  The applicants, therefore, propose to build a curatorial exchange for    
the deposit and review of biochemical information by the scientific             
community.  The applicants' goal is to demonstrate a system that will           
encourage the mandating of deposit while ensuring that the information is of    
the highest quality.                                                            
                                                                                
The role of the exchange is to receive deposits, check and classify their       
biochemical information automatically, forward them to panels of human          
reviewers for vetting, and publish the information by release to the            
participating data sources--all over the World-Wide Web. It will track the      
origin and status of deposits and reviews, serve computations for the           
relevant pattern matching and simulation, and maintain an archival copy of      
data.  The databases remain independent, and separately provide additional      
information.  Algorithm development and testing depends on an adequate          
information infrastructure, so the applicants will complete a basic data set    
of compounds and reactions.  They will use this experience to develop a more    
comprehensive domain model that better captures modern biochemistry, and        
implement it for deposit and review.  Since the basic data and algorithms       
will be valuable to the community at large, they plan to serve these to the     
World-Wide Web. the exchange and its underlying data form the infrastructure    
necessary for sustainable, cost-effective development of biochemical            
informatics resources for biomedical research.                                  
",2771106,R01GM056529,['R01GM056529'],GM,https://reporter.nih.gov/project-details/2771106,R01,1998,318557,-0.011566087842029909
"This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
 adenosine; anesthetics; artificial intelligence; blood pressure; cardiovascular agents; computer simulation; drug tolerance; heart rate; model design /development; neurotransmitter metabolism; nicotine; pharmacokinetics; pulmonary respiration MODELS FOR BIOLOGICAL DATA RELEVANT TO HEALTH CARE","This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
",2771003,R29GM051197,['R29GM051197'],GM,https://reporter.nih.gov/project-details/2771003,R29,1998,104964,-0.007178963642176563
"Non-small cell lung cancer (NSCLC) is the leading cause of cancer morality      
in men and women in the United States, and the overall long-term survial is     
less than 15%.  Pathologic stage I makes up  25-35% of NSCLC cases and has      
a good prognosis.  However, cancer relapse and death rate in this subset is     
35 to 50% by 5 years.  Chemotherapy is beneficial for the treatment of          
several localized solid tumors after resection and may prove to be useful       
in the treatment of patients with stage I NSCLC.  Thr purpose of this           
project is to define tissue and serum tumor markers in patients with stage      
I NSCLC which predict for early cancer recurrence.  Pathologic stage I          
NSCLC was chosen for study to eliminate the significant influence of            
positive lymph nodes and distant metastases on survival.                        
Immunohistochemical staining will identify potential tissue tumor markers       
and radioimmunoassay (RIA) or enzyme-link immunosorbent assay (ELISA) will      
identify potential serum tumor markers.                                         
                                                                                
Specific aim #1 will examine a set of twelve tissue tumor markers in a          
retrospective cohort of 275 stage I NSCLC patients.  Markers are                
categorized by hypothetical method of action: molecular genetic markers         
(Kras, erbB-1, erbB-2, rb, p53, bcl-2), markers of metastatic propensity        
(angiogenesis factor viii), proliferation markers (K1-67) and markers of        
cellular differentiation (Blood group A, H/LeV/LeB, NCAM, CD44).  Results       
will be used to develop a prediction rule for recurrence in stage I NSCLC       
using Cox proportional hazards regression analysis and an artificial neural     
network.                                                                        
                                                                                
Specific aim #2 will examine a set of eight serum tumor markers in a            
retrospective cohort of 250 patients with stage I to IV NSCLC.  These           
markers are categorized as molecular genetic markers (anti-p53), markers of     
metastatic propensity (angiogenesis bFGF), somatamedins (growth factor IGF-     
1) and markers of cellular differentiation (CEA, CA-125, CA 15-3, CYFRA21-      
1, CD44).  The purpose of this aim is to identify any correlations between      
titers of serum markers and tumor histology, stage or mass.  One hundred        
patients in this cohort had a second serum collection after tumor               
resection.  This subgroup of serum will allow analyses of titters before        
and after cyto-reduction.  Significant correlates with tumor stage and mass     
will be evaluated in a prospective cohort of patients with stage I NSCLC.       
                                                                                
In specific aim #3, paraffin-embedded and fresh-frozen tumor tissue will be     
collected from a prospective cohort of 330 patients with stage I NSCLC to       
validate the prediction rule developed in specific aim #1.  In these same       
patients, serial serum specimens will be collected for a minimum of 2.0         
years after resection (specific aim #4).  The significant markers               
identified in specific aim #2 will be analyzed in this cohort to describe       
correlations with tumor recurrence.  Tissue and serum markers identified by     
the model can be used to select high risk patients for a prospective,           
multi-institutional chemotherapy trial for stage I NSCLC.                       
 angiogenesis factor; artificial intelligence; biomarker; cell differentiation; clinical research; enzyme linked immunosorbent assay; genetic markers; histopathology; human subject; immunocytochemistry; insulinlike growth factor; longitudinal human study; mathematical model; metastasis; neoplasm /cancer classification /staging; neoplasm /cancer diagnosis; neoplasm /cancer epidemiology; neoplasm /cancer relapse /recurrence; nonsmall cell lung cancer; prognosis; respiratory surgery; serology /serodiagnosis TISSUE AND SERUM INDICATORS OF LUNG CANCER RECURRENCE","Non-small cell lung cancer (NSCLC) is the leading cause of cancer morality      
in men and women in the United States, and the overall long-term survial is     
less than 15%.  Pathologic stage I makes up  25-35% of NSCLC cases and has      
a good prognosis.  However, cancer relapse and death rate in this subset is     
35 to 50% by 5 years.  Chemotherapy is beneficial for the treatment of          
several localized solid tumors after resection and may prove to be useful       
in the treatment of patients with stage I NSCLC.  Thr purpose of this           
project is to define tissue and serum tumor markers in patients with stage      
I NSCLC which predict for early cancer recurrence.  Pathologic stage I          
NSCLC was chosen for study to eliminate the significant influence of            
positive lymph nodes and distant metastases on survival.                        
Immunohistochemical staining will identify potential tissue tumor markers       
and radioimmunoassay (RIA) or enzyme-link immunosorbent assay (ELISA) will      
identify potential serum tumor markers.                                         
                                                                                
Specific aim #1 will examine a set of twelve tissue tumor markers in a          
retrospective cohort of 275 stage I NSCLC patients.  Markers are                
categorized by hypothetical method of action: molecular genetic markers         
(Kras, erbB-1, erbB-2, rb, p53, bcl-2), markers of metastatic propensity        
(angiogenesis factor viii), proliferation markers (K1-67) and markers of        
cellular differentiation (Blood group A, H/LeV/LeB, NCAM, CD44).  Results       
will be used to develop a prediction rule for recurrence in stage I NSCLC       
using Cox proportional hazards regression analysis and an artificial neural     
network.                                                                        
                                                                                
Specific aim #2 will examine a set of eight serum tumor markers in a            
retrospective cohort of 250 patients with stage I to IV NSCLC.  These           
markers are categorized as molecular genetic markers (anti-p53), markers of     
metastatic propensity (angiogenesis bFGF), somatamedins (growth factor IGF-     
1) and markers of cellular differentiation (CEA, CA-125, CA 15-3, CYFRA21-      
1, CD44).  The purpose of this aim is to identify any correlations between      
titers of serum markers and tumor histology, stage or mass.  One hundred        
patients in this cohort had a second serum collection after tumor               
resection.  This subgroup of serum will allow analyses of titters before        
and after cyto-reduction.  Significant correlates with tumor stage and mass     
will be evaluated in a prospective cohort of patients with stage I NSCLC.       
                                                                                
In specific aim #3, paraffin-embedded and fresh-frozen tumor tissue will be     
collected from a prospective cohort of 330 patients with stage I NSCLC to       
validate the prediction rule developed in specific aim #1.  In these same       
patients, serial serum specimens will be collected for a minimum of 2.0         
years after resection (specific aim #4).  The significant markers               
identified in specific aim #2 will be analyzed in this cohort to describe       
correlations with tumor recurrence.  Tissue and serum markers identified by     
the model can be used to select high risk patients for a prospective,           
multi-institutional chemotherapy trial for stage I NSCLC.                       
",2769931,R29CA073980,['R29CA073980'],CA,https://reporter.nih.gov/project-details/2769931,R29,1998,125595,-0.021241869843107043
"Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
 HIV infections; Internet; artificial intelligence; computer graphics /printing; computer human interaction; computer program /software; computer system design /evaluation; flow cytometry; information retrieval; information systems FACS-PENGUIN--KNOWLEDGE BASE SUPPORT FOR FLOW CYTOMETRY","Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
",2735425,R01LM004836,['R01LM004836'],LM,https://reporter.nih.gov/project-details/2735425,R01,1998,318146,-0.10645959862826546
"Efforts to apply computer methods to assess and improve the quality of          
care in the hospital have been stymied by limited access to clinical            
data.  Free-text data have detailed clinical descriptions of patients           
that would be useful in computer altering systems and computer reminder         
systems.  However, free-text data cannot be interpreted by most clinical        
computer systems.  In this proposal, we describe research specifically          
aimed at making free-text data accessible to computer-based applications        
for assessing and improving the quality of care.  In particular the             
research plan focuses on the development of technologies that would allow       
free-text data to be used in clinical alert systems for critical test           
results; in reminder systems to encourage adherence to practice                 
guidelines; and in data collection systems for severity of illness models       
applied in the assessment of risk adjusted outcomes.  The approach              
described in the research plan emphasizes the development of statistical        
and probabilistic methods for interpretation of data derived from medical       
language processing systems.  We will test the methods developed for            
language processing and interpretation developed under this proposal in         
3 area: 1) the identification of concepts related to severity from the          
MedisGroups and the Computerized Severity Index models of patient               
severity of illness; 2) the identification of chest x ray reports and           
mammography reports with potentially malignant findings that require            
radiological follow-up; 3) and the automatic assessment of                      
appropriateness of coronary artery bypass grafting (CABG) surgery from          
free-text descriptions of patients based on the application of a clinical       
practice guideline for CABG surgery.                                            
 abstracting; artificial intelligence; automated medical record system; computer assisted diagnosis; computer assisted medical decision making; computer human interaction; coronary bypass; health care model; health care quality; human data; information retrieval; mammography; method development; statistics /biometry; thoracic radiography; vocabulary development for information system COMPUTER INTERPRETATION OF FREE-TEXT DATA","Efforts to apply computer methods to assess and improve the quality of          
care in the hospital have been stymied by limited access to clinical            
data.  Free-text data have detailed clinical descriptions of patients           
that would be useful in computer altering systems and computer reminder         
systems.  However, free-text data cannot be interpreted by most clinical        
computer systems.  In this proposal, we describe research specifically          
aimed at making free-text data accessible to computer-based applications        
for assessing and improving the quality of care.  In particular the             
research plan focuses on the development of technologies that would allow       
free-text data to be used in clinical alert systems for critical test           
results; in reminder systems to encourage adherence to practice                 
guidelines; and in data collection systems for severity of illness models       
applied in the assessment of risk adjusted outcomes.  The approach              
described in the research plan emphasizes the development of statistical        
and probabilistic methods for interpretation of data derived from medical       
language processing systems.  We will test the methods developed for            
language processing and interpretation developed under this proposal in         
3 area: 1) the identification of concepts related to severity from the          
MedisGroups and the Computerized Severity Index models of patient               
severity of illness; 2) the identification of chest x ray reports and           
mammography reports with potentially malignant findings that require            
radiological follow-up; 3) and the automatic assessment of                      
appropriateness of coronary artery bypass grafting (CABG) surgery from          
free-text descriptions of patients based on the application of a clinical       
practice guideline for CABG surgery.                                            
",2714212,R29LM005626,['R29LM005626'],LM,https://reporter.nih.gov/project-details/2714212,R29,1998,40396,-0.06948859116191433
 acute disease /disorder; artificial intelligence; cardiovascular disorder diagnosis; computer assisted diagnosis; computer system design /evaluation; diagnosis design /evaluation; human data; image processing; pulmonary circulation obstruction; respiratory disorder diagnosis; thoracic radiography ACUTE PULMONARY EMBOLISM--COMPUTER AIDED DIAGNOSIS,,2714069,R29HL052826,['R29HL052826'],HL,https://reporter.nih.gov/project-details/2714069,R29,1998,97727,-0.02607267603664586
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The purpose of this 3 year randomized clinical trial project is to implement    
and test innovative social security representative payee strategies as they     
are integrated into long-term dual disorder treatment for substance abusing     
severely mentally ill outpatients in order to decrease substance abuse.  The    
project targets subjects who 1) receive SSI or SSDI disability benefits for     
severe mental illness, 2) have an alcohol/other duty disorder, and 3) have      
assigned representative payee status to their Community Mental Health Center    
(CMHC).  Two different CMHC's (urban/rural) will be used as study sites.        
After six months of baseline preintervention study, subjects will be            
randomized (stratified by baseline substance use) into either contingent or     
non-contingent payee management and compared over six months [randomized        
cohorts of subjects will be ""signed to one of two treatment orders, both Or     
which include 12 -month contingent and 6-month non contingent phases Of         
payee management].  In the contingent phase substance avoidance, regular        
treatment attendance, [clinical stability] and responsible money management     
will earn subjects increasing levels of autonomy in terms of the form (cash     
verses vouchers) and frequency (monthly/weekly/daily) of benefit                
disbursement by their representative payee case managers.  The case managers    
will use a carefully described payee log with built-in disbursement plan        
[and decision tree] to tighten or loosen form and frequency (not overall        
amount) of benefit disbursement according to the above [defined] subject        
behaviors.  In the non-contingent phase, disbursements are made once weekly     
without any behavioral contingencies.  While the primary outcome will be        
comparative substance abstinence, other key multi-dimensional outcomes will     
be assessed.  [By using a staggered phase, cross-over design,                   
multi-dimensional outcomes will be compared across and within subjects.] It     
is hypothesized that contingent management will be associated with              
significantly more abstinence from substance, better treatment adherence and    
money management; improved psychiatric symptomatology and quality of life,      
and decreased homelessness, hospitalization and incarcerations than             
noncontingent management in this high risk-high cost population.  [fewer        
adverse outcomes (hospitalization, incarceration, homelessness) and             
increased positive outcomes ([decreased substance use] greater treatment        
involvement, stability, quality Of life) than baseline or non-contingent        
management practices, in this high-risk/high-cost population.  The findings     
of this study should be immediately relevant for providers and applicable       
for the approximately half-million similarly affected individuals               
nationwide.                                                                     
 alcoholism /alcohol abuse; behavioral /social science research tag; bipolar depression; clinical research; clinical trials; comorbidity; disability insurance; functional ability; health services research tag; homeless; hospital utilization; human subject; human therapy evaluation; interview; longitudinal human study; major depression; mental disorders; outcomes research; patient care management; psychological tests; quality of life; reinforcer; schizophrenia; social service evaluation; socioeconomics; substance abuse related disorder CONTINGENT BENEFITS IN SUBSTANCE ABUSING MENTALLY ILL","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The purpose of this 3 year randomized clinical trial project is to implement    
and test innovative social security representative payee strategies as they     
are integrated into long-term dual disorder treatment for substance abusing     
severely mentally ill outpatients in order to decrease substance abuse.  The    
project targets subjects who 1) receive SSI or SSDI disability benefits for     
severe mental illness, 2) have an alcohol/other duty disorder, and 3) have      
assigned representative payee status to their Community Mental Health Center    
(CMHC).  Two different CMHC's (urban/rural) will be used as study sites.        
After six months of baseline preintervention study, subjects will be            
randomized (stratified by baseline substance use) into either contingent or     
non-contingent payee management and compared over six months [randomized        
cohorts of subjects will be ""signed to one of two treatment orders, both Or     
which include 12 -month contingent and 6-month non contingent phases Of         
payee management].  In the contingent phase substance avoidance, regular        
treatment attendance, [clinical stability] and responsible money management     
will earn subjects increasing levels of autonomy in terms of the form (cash     
verses vouchers) and frequency (monthly/weekly/daily) of benefit                
disbursement by their representative payee case managers.  The case managers    
will use a carefully described payee log with built-in disbursement plan        
[and decision tree] to tighten or loosen form and frequency (not overall        
amount) of benefit disbursement according to the above [defined] subject        
behaviors.  In the non-contingent phase, disbursements are made once weekly     
without any behavioral contingencies.  While the primary outcome will be        
comparative substance abstinence, other key multi-dimensional outcomes will     
be assessed.  [By using a staggered phase, cross-over design,                   
multi-dimensional outcomes will be compared across and within subjects.] It     
is hypothesized that contingent management will be associated with              
significantly more abstinence from substance, better treatment adherence and    
money management; improved psychiatric symptomatology and quality of life,      
and decreased homelessness, hospitalization and incarcerations than             
noncontingent management in this high risk-high cost population.  [fewer        
adverse outcomes (hospitalization, incarceration, homelessness) and             
increased positive outcomes ([decreased substance use] greater treatment        
involvement, stability, quality Of life) than baseline or non-contingent        
management practices, in this high-risk/high-cost population.  The findings     
of this study should be immediately relevant for providers and applicable       
for the approximately half-million similarly affected individuals               
nationwide.                                                                     
",2733590,R01DA010838,['R01DA010838'],DA,https://reporter.nih.gov/project-details/2733590,R01,1998,201498,-0.14041347924818587
"DESCRIPTION: Recently, computer based videokeratography (VK) has become         
important for measuring corneal shape before and after refractive surgery       
and in diagnosing keratoconus. Regrettably, present algorithms are              
flawed because they assume light rays that form the VK image lie in the         
meridional plane before and after reflection from the cornea.  For both         
keratoconus and refractive surgery this assumption can produce                  
substantial errors. The investigators have begun development of a new           
algorithm that eliminates such errors. There are a number of topic on           
which further research is needed before the algorithm is viable.  The           
investigators will also develop software for splicing and averaging             
multiple shots to extend coverage to the whole cornea and to reduce             
noise. Also, they will develop two end-user applications: 1) Software           
for fitting hard and soft toric contact lens (for correction of                 
astigmatism) that will take into account corneal topography to  achieve         
improved lens centration and orientation. They expect to develop the            
first successful contact lens fitting software, enabling VK to become a         
standard instrument for contact lens fitting.                                   
                                                                                
Presently several thousand videokeratoscope instruments are being used          
for refractive surgery and for diagnosing keratoconus. The number of            
instruments will expand rapidly if improved algorithms for measuring            
corneal shape and improved applications software become available.  As          
one example, the market for a clinically superior contact lens fitting          
package in very large (especially true for the many countries with few          
trained contact lens specialists). The investigators expect that                
software will be the first to win clinical approval.                            
 artificial intelligence; biomedical equipment development; clinical biomedical equipment; clinical research; computer program /software; computer system design /evaluation; contact lens; cornea; diagnosis design /evaluation; eye disorder diagnosis; eye refractometry; human subject; keratoconus; mathematical model; noninvasive diagnosis; surface property; video recording system; vision tests NEW CORNEAL TOPOGRAPHY ALGORITHM AND ITS APPLICATIONS","DESCRIPTION: Recently, computer based videokeratography (VK) has become         
important for measuring corneal shape before and after refractive surgery       
and in diagnosing keratoconus. Regrettably, present algorithms are              
flawed because they assume light rays that form the VK image lie in the         
meridional plane before and after reflection from the cornea.  For both         
keratoconus and refractive surgery this assumption can produce                  
substantial errors. The investigators have begun development of a new           
algorithm that eliminates such errors. There are a number of topic on           
which further research is needed before the algorithm is viable.  The           
investigators will also develop software for splicing and averaging             
multiple shots to extend coverage to the whole cornea and to reduce             
noise. Also, they will develop two end-user applications: 1) Software           
for fitting hard and soft toric contact lens (for correction of                 
astigmatism) that will take into account corneal topography to  achieve         
improved lens centration and orientation. They expect to develop the            
first successful contact lens fitting software, enabling VK to become a         
standard instrument for contact lens fitting.                                   
                                                                                
Presently several thousand videokeratoscope instruments are being used          
for refractive surgery and for diagnosing keratoconus. The number of            
instruments will expand rapidly if improved algorithms for measuring            
corneal shape and improved applications software become available.  As          
one example, the market for a clinically superior contact lens fitting          
package in very large (especially true for the many countries with few          
trained contact lens specialists). The investigators expect that                
software will be the first to win clinical approval.                            
",2711153,R44EY011211,['R44EY011211'],EY,https://reporter.nih.gov/project-details/2711153,R44,1998,319111,-0.03913193548957673
"DESCRIPTION:  The research and development of teleradiology and telemedicine    
systems has progressed through many technical and clinical endeavors.  When     
dealing with large volume image transmission and storage, image data            
compression is an outstanding issue in medical applications to which current    
techniques were not designed to address.  The technical objectives of this      
project are to develop optimized error-free as well as error-controllable       
methods for medical image compression based on wavelet transform and            
associated methods.  In this project, we employ both advanced artificial        
intelligent and compression techniques to achieve these goals.                  
                                                                                
Our recent research outcomes include:  (a) development of a mathematics         
approach to unify prediction, subband, and wavelet transforms, (b)              
development of convolution neural network training methods to obtain            
optimized wavelet kernel, (c) development of a data splitting technique to      
improve edge accuracy and to provide error-control methods, and (d)             
development of an integer implementation method for all wavelet transforms,     
etc.  Based on the above technical advances, we propose to use integer form     
of an adaptive (optimized) wavelets in conjunction with newly developed         
coding methods such as ""partitioning in hierarchical trees"" (PHT) for           
lossless compression.  For error-controllable approaches, we propose to use     
adaptive wavelets coupled with optimized neural network prediction methods      
in this study.  Since lossless compression is a part of the error -             
controllable method, both systems can be implemented in the same scheme         
which is a breakthrough approach in the field.  We will compare the             
compression results (i.e., compression ratio and speed) of the proposed         
compression methods with those of the current wavelet techniques using the      
embedded zero-tree coding method.  At the end of the project, we will           
deliver a software package for the radiological society.  Hence, the            
evaluation for various clinical applications using the proposed methods can     
be performed by the investigators.                                              
                                                                                
As the field of telemedicine is rapidly growing, we believe that development    
of a dedicated compression module for economical storage and fast               
communication of patient data (particularly for patient images) is              
necessary.  This project is designed to address the related technical issues    
with a strong clinical consideration.                                           
 artificial intelligence; charge coupled device camera; computational neuroscience; computer program /software; computer system design /evaluation; digital imaging; human data; image processing; radiology; telemedicine IMAGE PATTERN BASED WAVELET COMPRESSION FOR RADIOLOGY","DESCRIPTION:  The research and development of teleradiology and telemedicine    
systems has progressed through many technical and clinical endeavors.  When     
dealing with large volume image transmission and storage, image data            
compression is an outstanding issue in medical applications to which current    
techniques were not designed to address.  The technical objectives of this      
project are to develop optimized error-free as well as error-controllable       
methods for medical image compression based on wavelet transform and            
associated methods.  In this project, we employ both advanced artificial        
intelligent and compression techniques to achieve these goals.                  
                                                                                
Our recent research outcomes include:  (a) development of a mathematics         
approach to unify prediction, subband, and wavelet transforms, (b)              
development of convolution neural network training methods to obtain            
optimized wavelet kernel, (c) development of a data splitting technique to      
improve edge accuracy and to provide error-control methods, and (d)             
development of an integer implementation method for all wavelet transforms,     
etc.  Based on the above technical advances, we propose to use integer form     
of an adaptive (optimized) wavelets in conjunction with newly developed         
coding methods such as ""partitioning in hierarchical trees"" (PHT) for           
lossless compression.  For error-controllable approaches, we propose to use     
adaptive wavelets coupled with optimized neural network prediction methods      
in this study.  Since lossless compression is a part of the error -             
controllable method, both systems can be implemented in the same scheme         
which is a breakthrough approach in the field.  We will compare the             
compression results (i.e., compression ratio and speed) of the proposed         
compression methods with those of the current wavelet techniques using the      
embedded zero-tree coding method.  At the end of the project, we will           
deliver a software package for the radiological society.  Hence, the            
evaluation for various clinical applications using the proposed methods can     
be performed by the investigators.                                              
                                                                                
As the field of telemedicine is rapidly growing, we believe that development    
of a dedicated compression module for economical storage and fast               
communication of patient data (particularly for patient images) is              
necessary.  This project is designed to address the related technical issues    
with a strong clinical consideration.                                           
",2710010,R01CA079139,['R01CA079139'],CA,https://reporter.nih.gov/project-details/2710010,R01,1998,170688,-0.0577333040500775
"DESCRIPTION (taken from application abstract):  Heart disease continues         
to be the primary cause of death in the U.S., with 25% of all deaths            
related to coronary artery disease (CAD). In addition to the loss of            
irreplaceable human life, there are also staggering health care costs           
and losses in productivity associated with the 1.5 million myocardial           
infarctions suffered in the U.S. every year.  The present competing             
renewal application seeks to make a contribution toward this vital              
health care problem by exploring frontier computing methods to support          
and facilitate CAD assessment.  The objective of the proposed research          
is to develop and evaluate a methodology to accomplish the following            
specific aims:                                                                  
                                                                                
(1)Knowledge Discovery: To design, implement and test novel database            
(DB) ""mining"" algorithms to uncover associations and inferences imbedded        
in clinical DBs and which can improve diagnostic performance.                   
(2)Knowledge Base Enrichment: To use the knowledge resulting from DB            
mining as well as conventional knowledge-acquisition methods to create          
and evaluate a robust knowledge base (KB) with which to interpret               
cardiovascular SPECT imagery and other types of relevant, patient-              
specific information.                                                           
(3)Distributed Knowledge Discovery and Processing:  To extend both the          
Knowledge-discovery and knowledge-based processing methods to                   
distributed, Internet-based setting for a twofold purpose: (I) to               
provide users with widespread access to the resulting KB, and (ii) to           
access and mine remote multi center DBs to further improve our knowledge        
regarding the assessment of CAD.                                                
                                                                                
The proposed work represents pioneering research in several ways,               
especially: (I) the creation of innovative algorithms to mine image DBs,        
(II) the application of these algorithms to the clinical assessment of          
CAD, and (III) the creation of distributed DB mining and knowledge-based        
processing methods to link geographically dispersed users and clinical          
DBs.  The proposed research builds on our previous work on knowledge-           
guided image interpretation, and represents an interinstitutional and           
interdisciplinary effort between Georgia Tech and Emory University, a           
longstanding collaboration that has previously resulted in numerous             
joint publications and valuable insights centering on diagnostic                
imaging, and which has also supported several academic degrees.                 
 Internet; angiography; artificial intelligence; cardiovascular imaging /visualization; computer assisted diagnosis; computer assisted medical decision making; coronary disorder; diagnosis design /evaluation; heart disorder diagnosis; human data; information system analysis; mathematical model; myocardium; perfusion; single photon emission computed tomography KNOWLEDGE DISCOVERY IN DISTRIBUTED CARDIAC IMAGE BASES","DESCRIPTION (taken from application abstract):  Heart disease continues         
to be the primary cause of death in the U.S., with 25% of all deaths            
related to coronary artery disease (CAD). In addition to the loss of            
irreplaceable human life, there are also staggering health care costs           
and losses in productivity associated with the 1.5 million myocardial           
infarctions suffered in the U.S. every year.  The present competing             
renewal application seeks to make a contribution toward this vital              
health care problem by exploring frontier computing methods to support          
and facilitate CAD assessment.  The objective of the proposed research          
is to develop and evaluate a methodology to accomplish the following            
specific aims:                                                                  
                                                                                
(1)Knowledge Discovery: To design, implement and test novel database            
(DB) ""mining"" algorithms to uncover associations and inferences imbedded        
in clinical DBs and which can improve diagnostic performance.                   
(2)Knowledge Base Enrichment: To use the knowledge resulting from DB            
mining as well as conventional knowledge-acquisition methods to create          
and evaluate a robust knowledge base (KB) with which to interpret               
cardiovascular SPECT imagery and other types of relevant, patient-              
specific information.                                                           
(3)Distributed Knowledge Discovery and Processing:  To extend both the          
Knowledge-discovery and knowledge-based processing methods to                   
distributed, Internet-based setting for a twofold purpose: (I) to               
provide users with widespread access to the resulting KB, and (ii) to           
access and mine remote multi center DBs to further improve our knowledge        
regarding the assessment of CAD.                                                
                                                                                
The proposed work represents pioneering research in several ways,               
especially: (I) the creation of innovative algorithms to mine image DBs,        
(II) the application of these algorithms to the clinical assessment of          
CAD, and (III) the creation of distributed DB mining and knowledge-based        
processing methods to link geographically dispersed users and clinical          
DBs.  The proposed research builds on our previous work on knowledge-           
guided image interpretation, and represents an interinstitutional and           
interdisciplinary effort between Georgia Tech and Emory University, a           
longstanding collaboration that has previously resulted in numerous             
joint publications and valuable insights centering on diagnostic                
imaging, and which has also supported several academic degrees.                 
",2709691,R01LM006726,['R01LM006726'],LM,https://reporter.nih.gov/project-details/2709691,R01,1998,343723,-0.18718046795348325
"Our long-range objective is to understand the functional organization           
and dynamical activity of the cortex.  The discovery of the columnar            
organization of the cortex has led to the notion that the  columns  are         
fundamental building blocks, from which larger functional units are             
constructed.  The cortex is thus viewed as a crystal (a more or less            
regular array of repeating, similar modules.  Our proposal will test and        
refine this modular hypothesis.                                                 
                                                                                
We shall use optical imaging of the primary visual cortex of monkeys and        
cats, and simultaneously record electrical responses from small neuronal        
clusters and local field potentials.  We shall thus obtain a spatio-            
temporal picture of the activity in the neural ensembles which encode           
various stimulus parameters. The data will be analyzed with extensions          
of Principal Component Analysis that we have developed.                         
                                                                                
We address three major aims: 1) To test the modularity hypothesis we            
shall measure, in a large piece of cortical tissue, the full range of           
functional maps ( for orientation, color, spatial frequency etc.)               
together with the retinotopic map.  We shall measure the periodicity of,        
and correlations among, the functional maps, to determine if they are           
commensurate.  This will lead to a refined framework that could include         
possibly incommensurate cortical scales and interactions among cortical         
elements.  2) We shall investigate how the Principal Components                 
(eigenfunctions) obtained from the optical images depend on the extent          
of the visual stimulus, to determine how the dynamical dimension of the         
primary visual cortex (viewed as a dynamical system) scales with size.          
3) We shall study the concerted electrical responses of neuronal                
clusters, to clarify the link between optical signals and neuronal              
activity, and to deepen our understanding of the neuronal dynamics.             
                                                                                
Our study is aimed at an intermediate architectural level, and deals            
with the way in which the fundamental modalities of the visual world            
(orientation, size, color and so on) are analyzed in the primary visual         
cortex.  Such knowledge is crucial for the construction of cortical             
models, which are essential for any quantitative understanding of               
critical function and dysfunction.                                              
 Macaca fascicularis; brain electrical activity; brain imaging /visualization /scanning; brain mapping; cats; charge coupled device camera; computational neuroscience; image processing; mathematical model; neural information processing; optics; space perception; stereotaxic techniques; visual cortex; visual stimulus MATHEMATICAL ANALYSIS OF BRAIN FUNCTION","Our long-range objective is to understand the functional organization           
and dynamical activity of the cortex.  The discovery of the columnar            
organization of the cortex has led to the notion that the  columns  are         
fundamental building blocks, from which larger functional units are             
constructed.  The cortex is thus viewed as a crystal (a more or less            
regular array of repeating, similar modules.  Our proposal will test and        
refine this modular hypothesis.                                                 
                                                                                
We shall use optical imaging of the primary visual cortex of monkeys and        
cats, and simultaneously record electrical responses from small neuronal        
clusters and local field potentials.  We shall thus obtain a spatio-            
temporal picture of the activity in the neural ensembles which encode           
various stimulus parameters. The data will be analyzed with extensions          
of Principal Component Analysis that we have developed.                         
                                                                                
We address three major aims: 1) To test the modularity hypothesis we            
shall measure, in a large piece of cortical tissue, the full range of           
functional maps ( for orientation, color, spatial frequency etc.)               
together with the retinotopic map.  We shall measure the periodicity of,        
and correlations among, the functional maps, to determine if they are           
commensurate.  This will lead to a refined framework that could include         
possibly incommensurate cortical scales and interactions among cortical         
elements.  2) We shall investigate how the Principal Components                 
(eigenfunctions) obtained from the optical images depend on the extent          
of the visual stimulus, to determine how the dynamical dimension of the         
primary visual cortex (viewed as a dynamical system) scales with size.          
3) We shall study the concerted electrical responses of neuronal                
clusters, to clarify the link between optical signals and neuronal              
activity, and to deepen our understanding of the neuronal dynamics.             
                                                                                
Our study is aimed at an intermediate architectural level, and deals            
with the way in which the fundamental modalities of the visual world            
(orientation, size, color and so on) are analyzed in the primary visual         
cortex.  Such knowledge is crucial for the construction of cortical             
models, which are essential for any quantitative understanding of               
critical function and dysfunction.                                              
",2688886,R01MH050166,['R01MH050166'],MH,https://reporter.nih.gov/project-details/2688886,R01,1998,330381,-0.10788143891949527
"The long-term objectives of the proposed research are to elucidate the          
stages of speaking skill development in infants and the motor control           
mechanisms for speech production in adults.  These research areas are           
important for early diagnosis and proper treatment of speech disorders.         
The proposed research consists primarily of the development, refinement,        
and experimental testing of a comprehensive neural network modeling             
framework for speech production based on preliminary work described in          
Guenther (Appendices A).  Model additions will include an articulatory          
mechanism that allows synthesis of speech wave-forms and an acoustic-like       
coordinate frame for speech movement planning.  Proposed research also          
includes the development of software for producing speaker-specific vocal       
tract models based on Magnetic Resonance Imaging (MRI) scans.  These models     
will allow synthesis of speech signals that account for the differences in      
vocal tract sizes and shapes of different individuals, thus providing a         
more accurate means for investigating the acoustic/articulatory                 
relationships of individuals acting as subjects in speech production            
experiments.  An experimental investigation utilizing these speaker-            
specific vocal tract models is also proposed to test an hypothesis              
generated by the modeling framework.  Some speakers use two entirely            
different articulator configurations, called ""bunched"" and retroflex"", to       
produce /r/ in different contexts.  The proposed model predicts that the        
same target is specified tot he production mechanism in the two cases, but      
that different articulator configurations arise in different contexts due       
to two properties of the speech production mechanism: (1) movement planning     
in an acoustic-like coordinate frame, and (2) transformation of the planned     
acoustic trajectories into articulator movements via a direction-to-            
direction mapping.  Speaker-specific vocal tract models corresponding to        
two subjects will be incorporated into the proposed modeling framework,         
which will then be used to predict which configuration each subject will        
use to produce /r/ in each of four contexts.  The hypothesis will be tested     
by comparing model performance with the performance of the subjects while       
producing /r/ in the same four contests, as measured in an Electro-Magnetic     
Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling         
framework will be used to investigate several other issues in speech            
production, including two modeling studies of speech motor development in       
infants (made possible by the self-organizing nature of the proposed            
model), and an investigation of intrinsic timing issues.                        
    GRANT=R01DE09161                                                            
The ability to utilize hemin and hemin containing compounds as an iron          
source has been documented for several pathogenic bacteria, including the       
periodontopathogen, Porphyromonas gingivalis.  We have previously               
determined that P. gingivalis transports the entire hemin moiety into the       
cell by an energy-dependent mechanism and that the binding and accumulation     
of hemin are induced by growth of cultures in the presence of hemin.            
However, the specific P. gingivalis components involved in hemin binding        
and transport have not been identified.  Growth of P. gingivalis under          
hemin-replete conditions has also been shown to influence the expression of     
several virulence factors; however, the role of hemin in the regulation of      
specific virulence genes has not been precisely defined.                        
                                                                                
The primary objectives of the present application are to define the             
molecular mechanisms involved in hemin binding and transport in P.              
gingivalis and to examine the regulation of hemin responsive genes.  Four       
specific aims are proposed:                                                     
                                                                                
1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.      
gingivalis outer membrane proteins involved in hemin binding will be            
identified by hemin affinity chromatography.  The specificity of the            
putative receptor(s) will be defined by examining the binding of 14[C]hemin     
to P. gingivalis in the presence of hemin and nonahemin iron sources.           
                                                                                
2.  To clone P. gingivalis genes encoding proteins involved in hemin            
binding and transport.  P. gingivalis genes encoding hemin binding proteins     
will be cloned by screening E. coli recombinants for the ability to bind        
hemin.  P. gingivalis genes encoding proteins involved in hemin transport       
will be cloned by screening E. coli hemA mutants for the ability to grow        
with hemin.  Corresponding P. gingivalis mutants will be obtained by            
insertional inactivation of the cloned genes and characterized both in          
vitro and in vivo.                                                              
                                                                                
3.  To elucidate the regulation of hemin binding and transport genes.  The      
regulation of cloned P. gingivalis genes involved in hemin binding and          
transport will be examined by analysis of mRNA and transcriptional fusions      
under hemin-deplete and -replete conditions.                                    
                                                                                
4.  To identify P. gingivalis genes that are regulated by the ferric uptake     
regulator (Fur).  Our preliminary results indicate that the expression of       
hemin/iron-responsive genes in P. gingivalis may be controlled by the           
negative ferric uptake regulator protein, Fur.  We will begin to identify       
P. gingivalis genes that are regulated by Fur by screening a P. gingivalis      
genomic library using the Fur titration asssay.                                 
                                                                                
The results obtained in these studies will allow us to identify specific        
components of the hemin transport system in P. gingivalis and will provide      
important information on the regulation of hemin responsive genes.              
 artificial intelligence; auditory feedback; auditory stimulus; behavioral /social science research tag; clinical research; computer program /software; computer simulation; computer system design /evaluation; digital imaging; human subject; language development; model design /development; psychoacoustics; sound; speech; speech recognition; vocal cords; vocalization NEURAL NETWORK MODELING OF SPEECH PRODUCTION","The long-term objectives of the proposed research are to elucidate the          
stages of speaking skill development in infants and the motor control           
mechanisms for speech production in adults.  These research areas are           
important for early diagnosis and proper treatment of speech disorders.         
The proposed research consists primarily of the development, refinement,        
and experimental testing of a comprehensive neural network modeling             
framework for speech production based on preliminary work described in          
Guenther (Appendices A).  Model additions will include an articulatory          
mechanism that allows synthesis of speech wave-forms and an acoustic-like       
coordinate frame for speech movement planning.  Proposed research also          
includes the development of software for producing speaker-specific vocal       
tract models based on Magnetic Resonance Imaging (MRI) scans.  These models     
will allow synthesis of speech signals that account for the differences in      
vocal tract sizes and shapes of different individuals, thus providing a         
more accurate means for investigating the acoustic/articulatory                 
relationships of individuals acting as subjects in speech production            
experiments.  An experimental investigation utilizing these speaker-            
specific vocal tract models is also proposed to test an hypothesis              
generated by the modeling framework.  Some speakers use two entirely            
different articulator configurations, called ""bunched"" and retroflex"", to       
produce /r/ in different contexts.  The proposed model predicts that the        
same target is specified tot he production mechanism in the two cases, but      
that different articulator configurations arise in different contexts due       
to two properties of the speech production mechanism: (1) movement planning     
in an acoustic-like coordinate frame, and (2) transformation of the planned     
acoustic trajectories into articulator movements via a direction-to-            
direction mapping.  Speaker-specific vocal tract models corresponding to        
two subjects will be incorporated into the proposed modeling framework,         
which will then be used to predict which configuration each subject will        
use to produce /r/ in each of four contexts.  The hypothesis will be tested     
by comparing model performance with the performance of the subjects while       
producing /r/ in the same four contests, as measured in an Electro-Magnetic     
Midsagittal Articulometer (EMMA) study.  Finally, the proposed modeling         
framework will be used to investigate several other issues in speech            
production, including two modeling studies of speech motor development in       
infants (made possible by the self-organizing nature of the proposed            
model), and an investigation of intrinsic timing issues.                        
    GRANT=R01DE09161                                                            
The ability to utilize hemin and hemin containing compounds as an iron          
source has been documented for several pathogenic bacteria, including the       
periodontopathogen, Porphyromonas gingivalis.  We have previously               
determined that P. gingivalis transports the entire hemin moiety into the       
cell by an energy-dependent mechanism and that the binding and accumulation     
of hemin are induced by growth of cultures in the presence of hemin.            
However, the specific P. gingivalis components involved in hemin binding        
and transport have not been identified.  Growth of P. gingivalis under          
hemin-replete conditions has also been shown to influence the expression of     
several virulence factors; however, the role of hemin in the regulation of      
specific virulence genes has not been precisely defined.                        
                                                                                
The primary objectives of the present application are to define the             
molecular mechanisms involved in hemin binding and transport in P.              
gingivalis and to examine the regulation of hemin responsive genes.  Four       
specific aims are proposed:                                                     
                                                                                
1.  To identify and characterize athe P. gingivalis hemin receptor(s).  P.      
gingivalis outer membrane proteins involved in hemin binding will be            
identified by hemin affinity chromatography.  The specificity of the            
putative receptor(s) will be defined by examining the binding of 14[C]hemin     
to P. gingivalis in the presence of hemin and nonahemin iron sources.           
                                                                                
2.  To clone P. gingivalis genes encoding proteins involved in hemin            
binding and transport.  P. gingivalis genes encoding hemin binding proteins     
will be cloned by screening E. coli recombinants for the ability to bind        
hemin.  P. gingivalis genes encoding proteins involved in hemin transport       
will be cloned by screening E. coli hemA mutants for the ability to grow        
with hemin.  Corresponding P. gingivalis mutants will be obtained by            
insertional inactivation of the cloned genes and characterized both in          
vitro and in vivo.                                                              
                                                                                
3.  To elucidate the regulation of hemin binding and transport genes.  The      
regulation of cloned P. gingivalis genes involved in hemin binding and          
transport will be examined by analysis of mRNA and transcriptional fusions      
under hemin-deplete and -replete conditions.                                    
                                                                                
4.  To identify P. gingivalis genes that are regulated by the ferric uptake     
regulator (Fur).  Our preliminary results indicate that the expression of       
hemin/iron-responsive genes in P. gingivalis may be controlled by the           
negative ferric uptake regulator protein, Fur.  We will begin to identify       
P. gingivalis genes that are regulated by Fur by screening a P. gingivalis      
genomic library using the Fur titration asssay.                                 
                                                                                
The results obtained in these studies will allow us to identify specific        
components of the hemin transport system in P. gingivalis and will provide      
important information on the regulation of hemin responsive genes.              
",2654421,R29DC002852,['R29DC002852'],DC,https://reporter.nih.gov/project-details/2654421,R29,1998,122419,0.0039882445130775416
"The broad, long-term objective of this research project is to help              
improve the quality and cost-effectiveness of health care delivery              
through the use of innovative mathematical modeling, design of efficient        
and novel solution approaches, and effective use of the powerful new            
computing technologies that can facilitate knowledge discovery in large         
databases.  While methods drawn from computer science and statistics            
disciplines have traditionally been used for this problem, there are            
significant opportunities for exploring the capabilities of combining           
methods drawn from the operations research discipline with the                  
traditional techniques, such as using linear and nonlinear                      
programming algorithms for improving neutral network design and                 
performance, and metaheuristic search for improving genetic algorithm           
design and performance.  Drawing on these recent                                
developments, this project aims to develop a hybrid of computer science         
and operations research based methods of mining large databases.  The           
potential of this new class of methods will be demonstrated using a high        
quality clinical database developed at the University of Pittsburgh             
Medical Center through funding from the Agency for Health Care Policy           
and Research.  This database contains extensive information on patients         
with community-acquired pneumonia (CAP).                                        
                                                                                
The specific focus of this project is to address the problem of                 
predicting patient mortality in the area of CAP.  Pneumonia is an               
important problem to investigate because it affects a significant group         
of people, leads to complications requiring expensive hospitalizations,         
and is the sixth leading cause of death in the US.  This study proposes         
to predict mortality of hospitalized patients based on findings recorded        
during the initial patient-physician encounter using a prediction               
technique known as probabilistic belief networks.  This method will be          
extended by combining the special features of a metaheuristic strategy          
called tabu search to help reduce the complexity of the search process          
during the design and construction of probabilistic belief networks.            
Additional features of tabu search, such as scatter search, path                
relinking, and probabilistic tabu search, facilitate the efficient              
exploration of the search space for learning belief networks from data.         
This new class of methods will be tested on real data from a large              
clinical database on pneumonia, and compared with the capabilities of           
a number of machine learning methods that have previously been applied          
to the same problem.                                                            
 computer assisted medical decision making; health care facility information system; health services research tag; human data; human mortality; information system analysis; outcomes research; pneumonia; statistics /biometry DATA MINING FOR HEALTHCARE DECISION SUPPORT","The broad, long-term objective of this research project is to help              
improve the quality and cost-effectiveness of health care delivery              
through the use of innovative mathematical modeling, design of efficient        
and novel solution approaches, and effective use of the powerful new            
computing technologies that can facilitate knowledge discovery in large         
databases.  While methods drawn from computer science and statistics            
disciplines have traditionally been used for this problem, there are            
significant opportunities for exploring the capabilities of combining           
methods drawn from the operations research discipline with the                  
traditional techniques, such as using linear and nonlinear                      
programming algorithms for improving neutral network design and                 
performance, and metaheuristic search for improving genetic algorithm           
design and performance.  Drawing on these recent                                
developments, this project aims to develop a hybrid of computer science         
and operations research based methods of mining large databases.  The           
potential of this new class of methods will be demonstrated using a high        
quality clinical database developed at the University of Pittsburgh             
Medical Center through funding from the Agency for Health Care Policy           
and Research.  This database contains extensive information on patients         
with community-acquired pneumonia (CAP).                                        
                                                                                
The specific focus of this project is to address the problem of                 
predicting patient mortality in the area of CAP.  Pneumonia is an               
important problem to investigate because it affects a significant group         
of people, leads to complications requiring expensive hospitalizations,         
and is the sixth leading cause of death in the US.  This study proposes         
to predict mortality of hospitalized patients based on findings recorded        
during the initial patient-physician encounter using a prediction               
technique known as probabilistic belief networks.  This method will be          
extended by combining the special features of a metaheuristic strategy          
called tabu search to help reduce the complexity of the search process          
during the design and construction of probabilistic belief networks.            
Additional features of tabu search, such as scatter search, path                
relinking, and probabilistic tabu search, facilitate the efficient              
exploration of the search space for learning belief networks from data.         
This new class of methods will be tested on real data from a large              
clinical database on pneumonia, and compared with the capabilities of           
a number of machine learning methods that have previously been applied          
to the same problem.                                                            
",2638570,F38LM000065,['F38LM000065'],LM,https://reporter.nih.gov/project-details/2638570,F38,1998,69100,-0.07800478057598143
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
Dopamine (DA) activity is strongly implicated in the reinforcing properties     
of drugs of abuse.  The reinforcing properties of such drugs appear to          
result from pharmacological activation of DA substrates which mediate           
natural reinforcement processes.  Critical gaps exist in the experimental       
literature, however, regarding the precise function(s) of DA in                 
reinforcement, particularly within specific brain target sites.  Humans         
acquire associations between A) their behaviors and reward events               
(response-reward) and B) between environmental stimuli and rewards              
(CS-reward); rats acquire similar associations.  Is DA involved in the          
acquisition of response-reward, CS-reward, and/or stimulus-response             
associations during reinforcement?  What is the precise role of DA within       
these associative mechanisms at different brain target sites?                   
                                                                                
In light of recent information from single-unit and dialysis studies of DA      
activity in behaving animals, the studies in this application examine the       
precise nature of DA's role in the acquisition of CS-reward associations,       
within three brain target structures, the nucleus accumbens, caudate, and       
prefrontal cortex.  The studies will examine, for each brain target site,       
whether DA disruption of CS-reward learning is dependent upon the intensity     
of the CS, the time interval between CS and reward, and whether DA              
disruptions impair aversive as well as appetitive CS-outcome associations       
under varying intensities of the aversive outcome.                              
 behavioral /social science research tag; caudate nucleus; conditioning; dopamine; dopamine antagonists; drug abuse; electrophysiology; frontal lobe /cortex; laboratory rat; learning; neurochemistry; neurosurgery; nucleus accumbens; psychological reinforcement; psychopharmacology; putamen MECHANISMS OF DRUG ABUSE--DA AND CS REWARD LEARNING","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
Dopamine (DA) activity is strongly implicated in the reinforcing properties     
of drugs of abuse.  The reinforcing properties of such drugs appear to          
result from pharmacological activation of DA substrates which mediate           
natural reinforcement processes.  Critical gaps exist in the experimental       
literature, however, regarding the precise function(s) of DA in                 
reinforcement, particularly within specific brain target sites.  Humans         
acquire associations between A) their behaviors and reward events               
(response-reward) and B) between environmental stimuli and rewards              
(CS-reward); rats acquire similar associations.  Is DA involved in the          
acquisition of response-reward, CS-reward, and/or stimulus-response             
associations during reinforcement?  What is the precise role of DA within       
these associative mechanisms at different brain target sites?                   
                                                                                
In light of recent information from single-unit and dialysis studies of DA      
activity in behaving animals, the studies in this application examine the       
precise nature of DA's role in the acquisition of CS-reward associations,       
within three brain target structures, the nucleus accumbens, caudate, and       
prefrontal cortex.  The studies will examine, for each brain target site,       
whether DA disruption of CS-reward learning is dependent upon the intensity     
of the CS, the time interval between CS and reward, and whether DA              
disruptions impair aversive as well as appetitive CS-outcome associations       
under varying intensities of the aversive outcome.                              
",2561679,R29DA011653,['R29DA011653'],DA,https://reporter.nih.gov/project-details/2561679,R29,1998,133365,-0.03559008300268078
"DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The goal of this five year research is to provide new knowledge on:  1) the     
causes of drug use and abuse in inner city African-American and White           
adolescents, and 2) the most cost-effective and cost-beneficial                 
family-focused prevention interventions.  The research aims of this             
randomized clinical trial involving over 800 at-risk Washington, D.C.-area      
families of 7 to 11 year olds in components of the 16 week Strengthening        
Families Program (SFP) are:  1) to test the differential efficacy of the        
three major components of SFP-parent training (PT), children's skills           
training (CT), and family skills training (FT) compared to a minimal contact    
control (MT) on precursors and substance use/abuse employing a 2 x 4            
experimental design; 2) to estimate the cost-benefit and cost-effectiveness     
of the SFP components with 400 African American families and 400 White          
families; and 3) to empirically test several competing etiological and          
intervention theories of drug use/abuse underlying the intervention design      
by analyzing the survey research data from 1,600 12 to 19 year olds and the     
800 intervention families using structural equation modeling, mediational       
analysis, hierarchical linear modeling, latent growth modeling, and             
transactional modeling (cross-lagged SEM, artificial intelligence, and          
neural network simulations) for gender, race, and other sub-groups.  The        
rationale for the component design is that a major question exists              
concerning the efficacy of child-only interventions that group high-risk        
children together as well as the added benefit of working with the family       
together compared to parent-training only.  Demonstrating the importance of     
involving parents and working with families would be very valuable              
information for prevention providers.  Additional co-variate outcome            
analyses will be conducted to answer practical questions conceming the          
efficacy of the four interventions for different subgroups of clients.  A       
strong process evaluation including a management information system (MIS)       
conducted by University of Maryland researchers wili document implementation    
process and provide feedback to improve implementation fidelity.  Additional    
health services sub-aim questions will be addressed through analyses of         
recruitment, attrition, consumer satisfaction, participation variables, and     
client and trainer characteristics linked to outcomes.  Seven nationally        
recognized consultants have agreed to support the culturally-specific           
program development, implementation, and data analyses.                         
 clinical research; clinical trials; computational neuroscience; drug abuse education; drug abuse information system; drug abuse prevention; education evaluation /planning; family structure /dynamics; health care cost /financing; human subject; longitudinal human study; mathematical model; middle childhood (6-11); parent offspring interaction; racial /ethnic difference; social model; statistics /biometry; training NIDA STRENGTHENING WASHINGTON, DC FAMILIES GRANT","DESCRIPTION:  (Applicant's Abstract)                                            
                                                                                
The goal of this five year research is to provide new knowledge on:  1) the     
causes of drug use and abuse in inner city African-American and White           
adolescents, and 2) the most cost-effective and cost-beneficial                 
family-focused prevention interventions.  The research aims of this             
randomized clinical trial involving over 800 at-risk Washington, D.C.-area      
families of 7 to 11 year olds in components of the 16 week Strengthening        
Families Program (SFP) are:  1) to test the differential efficacy of the        
three major components of SFP-parent training (PT), children's skills           
training (CT), and family skills training (FT) compared to a minimal contact    
control (MT) on precursors and substance use/abuse employing a 2 x 4            
experimental design; 2) to estimate the cost-benefit and cost-effectiveness     
of the SFP components with 400 African American families and 400 White          
families; and 3) to empirically test several competing etiological and          
intervention theories of drug use/abuse underlying the intervention design      
by analyzing the survey research data from 1,600 12 to 19 year olds and the     
800 intervention families using structural equation modeling, mediational       
analysis, hierarchical linear modeling, latent growth modeling, and             
transactional modeling (cross-lagged SEM, artificial intelligence, and          
neural network simulations) for gender, race, and other sub-groups.  The        
rationale for the component design is that a major question exists              
concerning the efficacy of child-only interventions that group high-risk        
children together as well as the added benefit of working with the family       
together compared to parent-training only.  Demonstrating the importance of     
involving parents and working with families would be very valuable              
information for prevention providers.  Additional co-variate outcome            
analyses will be conducted to answer practical questions conceming the          
efficacy of the four interventions for different subgroups of clients.  A       
strong process evaluation including a management information system (MIS)       
conducted by University of Maryland researchers wili document implementation    
process and provide feedback to improve implementation fidelity.  Additional    
health services sub-aim questions will be addressed through analyses of         
recruitment, attrition, consumer satisfaction, participation variables, and     
client and trainer characteristics linked to outcomes.  Seven nationally        
recognized consultants have agreed to support the culturally-specific           
program development, implementation, and data analyses.                         
",2487768,R01DA010825,['R01DA010825'],DA,https://reporter.nih.gov/project-details/2487768,R01,1998,709663,-0.009341682080994618
"The goal of the proposed research is to develop computer-aided diagnosis        
(CAD) schemes in order to improve the diagnostic accuracy of breast cancer      
in mammography. Four specific aims are included: (l) development of             
computer programs for the detection and characterization of                     
microcalcifications, (2) development of computer programs for detection         
and characterization of masses, (3) implementation of the CAD algorithms        
in a dedicated workstation to perform a pilot preclinical testing of the        
accuracy of the CAD programs, and (4) evaluation of the effects of the CAD      
schemes on radiologists' performance. The proposed CAD schemes will aid         
radiologists in screening mammograms for suspicious lesions and provide         
estimate of the likelihood of malignancy for the detected lesions. The          
information is expected to reduce the miss rate and to improve the              
positive predictive value of the mammographic findings.                         
                                                                                
A data base of clinical mammograms which include malignant and benign           
microcalcifications and masses will be established. Physical measures           
which characterize the significant image features of the lesions will be        
developed. Based on these measures, linear discriminant classifiers or          
neural network classifiers will be optimized using a genetic algorithm          
approach to classify true and false signals and to estimate the likelihood      
of malignancy for each type of lesions.                                         
                                                                                
For automated detection and classification of microcalcifications, we will      
investigate the usefulness of multiresolution analysis for enhancement of       
the signal-to-noise ratio of the microcalcifications and for improvement        
of feature extraction techniques. Physical characteristics such as size,        
shape, frequency spectrum, spatial distribution, clustering properties,         
and texture features will be extracted and analyzed with the classifiers.       
                                                                                
For automated detection and classification of masses, we will improve the       
background correction and signal segmentation techniques, and develop           
effective false-positive reduction methods. Adaptive filtering, edge            
enhancement, and clustering segmentation methods will be developed for          
extraction of the mass margins. Physical characteristics such as size,          
density, edge sharpness, calcifications, shape, lobulation, spiculation,        
and multiresolution wavelet texture features will be extracted from the         
masses and analyzed with the classifiers.                                       
                                                                                
The algorithms will be implemented in a dedicated CAD workstation and           
preclinical testing will be conducted. The performance of the programs in       
a clinical setting will be assessed. The algorithms will be revised and         
improved based on the information obtained with the preclinical testing.        
The study is a vital step for the development of a clinically reliable CAD      
scheme.                                                                         
                                                                                
Observer performance studies using receiver operating characteristic (ROC)      
methodology will be conducted to evaluate the effects of the CAD schemes        
on radiologists'performance.                                                    
 artificial intelligence; breast neoplasm /cancer diagnosis; breast neoplasms; computational neuroscience; computer assisted diagnosis; computer assisted medical decision making; computer program /software; diagnosis design /evaluation; diagnosis quality /standard; digital imaging; image processing; mammography; mathematical model DEVELOPMENT OF COMPUTER BASED TECHNIQUES IN MAMMOGRAPHY","The goal of the proposed research is to develop computer-aided diagnosis        
(CAD) schemes in order to improve the diagnostic accuracy of breast cancer      
in mammography. Four specific aims are included: (l) development of             
computer programs for the detection and characterization of                     
microcalcifications, (2) development of computer programs for detection         
and characterization of masses, (3) implementation of the CAD algorithms        
in a dedicated workstation to perform a pilot preclinical testing of the        
accuracy of the CAD programs, and (4) evaluation of the effects of the CAD      
schemes on radiologists' performance. The proposed CAD schemes will aid         
radiologists in screening mammograms for suspicious lesions and provide         
estimate of the likelihood of malignancy for the detected lesions. The          
information is expected to reduce the miss rate and to improve the              
positive predictive value of the mammographic findings.                         
                                                                                
A data base of clinical mammograms which include malignant and benign           
microcalcifications and masses will be established. Physical measures           
which characterize the significant image features of the lesions will be        
developed. Based on these measures, linear discriminant classifiers or          
neural network classifiers will be optimized using a genetic algorithm          
approach to classify true and false signals and to estimate the likelihood      
of malignancy for each type of lesions.                                         
                                                                                
For automated detection and classification of microcalcifications, we will      
investigate the usefulness of multiresolution analysis for enhancement of       
the signal-to-noise ratio of the microcalcifications and for improvement        
of feature extraction techniques. Physical characteristics such as size,        
shape, frequency spectrum, spatial distribution, clustering properties,         
and texture features will be extracted and analyzed with the classifiers.       
                                                                                
For automated detection and classification of masses, we will improve the       
background correction and signal segmentation techniques, and develop           
effective false-positive reduction methods. Adaptive filtering, edge            
enhancement, and clustering segmentation methods will be developed for          
extraction of the mass margins. Physical characteristics such as size,          
density, edge sharpness, calcifications, shape, lobulation, spiculation,        
and multiresolution wavelet texture features will be extracted from the         
masses and analyzed with the classifiers.                                       
                                                                                
The algorithms will be implemented in a dedicated CAD workstation and           
preclinical testing will be conducted. The performance of the programs in       
a clinical setting will be assessed. The algorithms will be revised and         
improved based on the information obtained with the preclinical testing.        
The study is a vital step for the development of a clinically reliable CAD      
scheme.                                                                         
                                                                                
Observer performance studies using receiver operating characteristic (ROC)      
methodology will be conducted to evaluate the effects of the CAD schemes        
on radiologists'performance.                                                    
",2667915,R01CA048129,['R01CA048129'],CA,https://reporter.nih.gov/project-details/2667915,R01,1998,412299,-0.08683754462821579
"DESCRIPTION (Adapted from the applicant's abstract):                            
                                                                                
The proposed research will build, refine, and test in operational use, a set    
of software tools designed to help support, maintain, and iteratively           
revalidate computer-based clinical guidelines as they evolve over time.  The    
project will focus on the domain of childhood immunization, and will build      
upon IMM/Serve, a childhood immunization forecasting program that takes as      
input a child's immunization history, and produces recommendations as to        
which vaccinations are due and which vaccinations should be scheduled next.     
The effort required to modify and validate such a program as the clinical       
field evolves over time is a challenging task.  It will be extremely            
important to have a robust set of tools to assist in this process.  Partial     
prototype versions of certain of these tools already exist.                     
                                                                                
1.  The project will refine and extend computer-based tools for immunization    
knowledge maintenance.  These tools will include:  a) IMM/Def, a program        
which automatically generates the rule-based logic for the most complex         
portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program         
which automatically generates a set of test cases to help test the kernel       
logic.  The project will also develop an organized set of strategies for        
immunization test case generation, and implement those strategies in the        
refined version of IMM/Test.                                                    
                                                                                
2.  The project will build a Web site to support immunization knowledge         
maintenance.                                                                    
                                                                                
3.  The project will keep a detailed record of all modifications and            
customization of the knowledge, and will represent all the variations of the    
knowledge using a standardized format such as GLIF, the Guideline               
Interchange Format being developed as a standard for exchanging guidelines      
between sites.                                                                  
                                                                                
4.  The project will link IMM/Serve to a database designed to hold              
IMM/Serve's analysis of a set of cases, so that the resulting package can be    
used as a tool to perform compliance assessment.                                
                                                                                
5.  A set of evaluation studies will be carried out to help assess the          
efficacy of the tools and to help improve their functionality.                  
 Internet; artificial intelligence; computer assisted medical decision making; computer assisted patient care; computer human interaction; computer program /software; computer system design /evaluation; human data; immunization; information systems; medical records; pediatrics TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES","DESCRIPTION (Adapted from the applicant's abstract):                            
                                                                                
The proposed research will build, refine, and test in operational use, a set    
of software tools designed to help support, maintain, and iteratively           
revalidate computer-based clinical guidelines as they evolve over time.  The    
project will focus on the domain of childhood immunization, and will build      
upon IMM/Serve, a childhood immunization forecasting program that takes as      
input a child's immunization history, and produces recommendations as to        
which vaccinations are due and which vaccinations should be scheduled next.     
The effort required to modify and validate such a program as the clinical       
field evolves over time is a challenging task.  It will be extremely            
important to have a robust set of tools to assist in this process.  Partial     
prototype versions of certain of these tools already exist.                     
                                                                                
1.  The project will refine and extend computer-based tools for immunization    
knowledge maintenance.  These tools will include:  a) IMM/Def, a program        
which automatically generates the rule-based logic for the most complex         
portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program         
which automatically generates a set of test cases to help test the kernel       
logic.  The project will also develop an organized set of strategies for        
immunization test case generation, and implement those strategies in the        
refined version of IMM/Test.                                                    
                                                                                
2.  The project will build a Web site to support immunization knowledge         
maintenance.                                                                    
                                                                                
3.  The project will keep a detailed record of all modifications and            
customization of the knowledge, and will represent all the variations of the    
knowledge using a standardized format such as GLIF, the Guideline               
Interchange Format being developed as a standard for exchanging guidelines      
between sites.                                                                  
                                                                                
4.  The project will link IMM/Serve to a database designed to hold              
IMM/Serve's analysis of a set of cases, so that the resulting package can be    
used as a tool to perform compliance assessment.                                
                                                                                
5.  A set of evaluation studies will be carried out to help assess the          
efficacy of the tools and to help improve their functionality.                  
",2660069,R01LM006682,['R01LM006682'],LM,https://reporter.nih.gov/project-details/2660069,R01,1998,272102,-0.07041976447498796
"DESCRIPTION (Adapted from applicant's abstract):  This proposal describes       
research on inflectional and derivational morphology and their role in          
reading and related tasks.  Morphology is the aspect of language concerning     
the forms of words.  The issues to be addressed concern how this information    
is represented in lexical memory; how it relates to other types of              
information including orthography, phonology, and semantics; and how it is      
used in processing.  The research utilizes both behavioral studies of           
skilled language users and computer simulation modeling.  The issues are        
framed in terms of connectionist principles concerning knowledge                
representation, acquisition, and processing that were developed in previous     
research on reading simple, monomorphemic words.  The proposed research         
attempts to extend this theoretical framework to the processing of more         
complex words.  The main emphasis of the research is on the quasiregular        
character of inflectional and derivational morphology:  the fact that both      
systems are productive and can be described as ""rule-governed"" but also         
admit many exceptions that deviate from what is predicted from the rules.       
These phenomena have standardly been treated within dual-route models of        
lexical access in which there are separate mechanisms for the rule-governed     
cases and exceptions.  Connectionist models provide an alternative approach     
in which all forms are governed by a single network.  The experiments focus     
on phenomena that differentiate the dual- and single mechanism approaches in    
both inflectional and derivational domains.  The theoretical account is also    
tested by implementing computational models with the goal of simulating         
behavioral data in close detail.                                                
                                                                                
 behavioral /social science research tag; clinical research; communication behavior; computational neuroscience; computer simulation; human subject; language; memory; neural information processing; phonology; psychological models; reading; semantics; syntax BEHAVIORAL AND COMPUTATIONAL STUDIES OF MORPHOLOGY","DESCRIPTION (Adapted from applicant's abstract):  This proposal describes       
research on inflectional and derivational morphology and their role in          
reading and related tasks.  Morphology is the aspect of language concerning     
the forms of words.  The issues to be addressed concern how this information    
is represented in lexical memory; how it relates to other types of              
information including orthography, phonology, and semantics; and how it is      
used in processing.  The research utilizes both behavioral studies of           
skilled language users and computer simulation modeling.  The issues are        
framed in terms of connectionist principles concerning knowledge                
representation, acquisition, and processing that were developed in previous     
research on reading simple, monomorphemic words.  The proposed research         
attempts to extend this theoretical framework to the processing of more         
complex words.  The main emphasis of the research is on the quasiregular        
character of inflectional and derivational morphology:  the fact that both      
systems are productive and can be described as ""rule-governed"" but also         
admit many exceptions that deviate from what is predicted from the rules.       
These phenomena have standardly been treated within dual-route models of        
lexical access in which there are separate mechanisms for the rule-governed     
cases and exceptions.  Connectionist models provide an alternative approach     
in which all forms are governed by a single network.  The experiments focus     
on phenomena that differentiate the dual- and single mechanism approaches in    
both inflectional and derivational domains.  The theoretical account is also    
tested by implementing computational models with the goal of simulating         
behavioral data in close detail.                                                
                                                                                
",2678138,R01MH058723,['R01MH058723'],MH,https://reporter.nih.gov/project-details/2678138,R01,1998,110779,-0.12281227148146374
"DESCRIPTION (Adapted from applicant's abstract):  The physical abuse of         
children by their parents continues to be a major social problem in our         
country.  However, there has been relatively little attention given to          
empirical approaches to the treatment of physically abusive families.  A        
careful examination of the current literature suggests that abusive parents     
are characterized by high rates of negative interaction, low rates of           
positive interaction, and limited/ineffective parental disciplining             
strategies.  Conversely, physically abused children have been reported to be    
aggressive, defiant, non-compliant, and resistant to parental direction.        
Utilizing a social learning framework, this proposal argues that abusive        
parents and their children engage in a negative coercive cycle which            
characterizes many of their interactions.  At times, this cycle may escalate    
to the point of physical abuse.  This proposal describes the Parent Child       
Interaction Training (PCIT) as an intervention which targets specific           
deficits often found within physically abusive parent-child dyads.  PCIT is     
uniquely appropriate with physically abusive parent-child dyads because it      
has been shown to be highly effective with a similar population (i.e.,          
oppositional, defiant children), it incorporates both the parent and the        
child, it provides a means to alter the pattern of interactions within          
abusive relationships, and it provides a means to directly decrease negative    
affect and control-while promoting (i.e., teaching, coaching) greater           
positive affect and discipline strategies.                                      
                                                                                
Therefore, this project proposes to use a sample of 48 parent-child dyads       
(mothers and their children) who have been referred to Family Preservation      
Services and therefore have been identified as having physically abusive        
relationships.  Parent-child dyads will be administered a brief screen,         
blocked by ethnicity, then randomly assigned to either a PCIT treatment         
group (N=24) or to a comparison group (N =24) who will receive 'traditional'    
family preservation services).  It is hypothesized that dyads in the PCIT       
treatment group will have higher rates of parental verbal praise, parental      
positive physical contact, greater child compliance, and decreasing rates of    
child deviant behaviors; than dyads in the comparison treatment group.          
Additionally, it is predicted that parents in the PCIT treatment group will     
report less perceived parental stress than parents in the comparison group.     
 behavioral /social science research tag; child abuse; child behavior; child rearing; clinical research; dyadic interaction; family; family therapy; human subject; human therapy evaluation; learning; middle childhood (6-11); mother child interaction; preschool child (1-5); psychological reinforcement; psychometrics; social behavior; videotape /videodisc; women's health PARENT/CHILD PHYSICAL ABUSE TREATMENT PROGRAM","DESCRIPTION (Adapted from applicant's abstract):  The physical abuse of         
children by their parents continues to be a major social problem in our         
country.  However, there has been relatively little attention given to          
empirical approaches to the treatment of physically abusive families.  A        
careful examination of the current literature suggests that abusive parents     
are characterized by high rates of negative interaction, low rates of           
positive interaction, and limited/ineffective parental disciplining             
strategies.  Conversely, physically abused children have been reported to be    
aggressive, defiant, non-compliant, and resistant to parental direction.        
Utilizing a social learning framework, this proposal argues that abusive        
parents and their children engage in a negative coercive cycle which            
characterizes many of their interactions.  At times, this cycle may escalate    
to the point of physical abuse.  This proposal describes the Parent Child       
Interaction Training (PCIT) as an intervention which targets specific           
deficits often found within physically abusive parent-child dyads.  PCIT is     
uniquely appropriate with physically abusive parent-child dyads because it      
has been shown to be highly effective with a similar population (i.e.,          
oppositional, defiant children), it incorporates both the parent and the        
child, it provides a means to alter the pattern of interactions within          
abusive relationships, and it provides a means to directly decrease negative    
affect and control-while promoting (i.e., teaching, coaching) greater           
positive affect and discipline strategies.                                      
                                                                                
Therefore, this project proposes to use a sample of 48 parent-child dyads       
(mothers and their children) who have been referred to Family Preservation      
Services and therefore have been identified as having physically abusive        
relationships.  Parent-child dyads will be administered a brief screen,         
blocked by ethnicity, then randomly assigned to either a PCIT treatment         
group (N=24) or to a comparison group (N =24) who will receive 'traditional'    
family preservation services).  It is hypothesized that dyads in the PCIT       
treatment group will have higher rates of parental verbal praise, parental      
positive physical contact, greater child compliance, and decreasing rates of    
child deviant behaviors; than dyads in the comparison treatment group.          
Additionally, it is predicted that parents in the PCIT treatment group will     
report less perceived parental stress than parents in the comparison group.     
",2675342,R21MH054221,['R21MH054221'],MH,https://reporter.nih.gov/project-details/2675342,R21,1998,116717,-0.027132894903323997
"Colorectal carcinoma is the second leading cause of cancer deaths in the        
United States today.  In an effort to reduce mortality, Congress                
recently included a provision in the Balanced Budget Act of 1997 to             
support screening colonoscopy as a means for early detection and removal        
of colorectal polyps, the precursors to cancer.  In this country alone,         
more than 68 million people are eligible for colorectal screening, but          
the majority are unlikely to comply with screening recommendations              
because of the costs, risks, discomfort, and inconvenience associated           
with traditional endoscopy.  Furthermore, even if a small fraction of           
eligible persons are examined, the number of available                          
gastroenterologists would be insufficient to perform so many procedures.        
                                                                                
We have developed a new technique, called virtual colonoscopy (VC), as          
an alternative to screening diagnostic colonoscopy (DC). The procedure          
consists of cleansing a patient's colon, inflating the colon with air,          
scanning the abdomen with helical computed tomography (CT), and                 
generating a rapid sequence of three-dimensional (3D) images of the             
colon by means of virtual reality computer technology.  Although VC             
makes possible the visualization of 3D images of the colon in a manner          
similar to that of DC, a correct diagnosis depends upon a physician's           
ability to identify small and sometimes subtle polyps within hundreds           
of 3D images.  The absence of visual cues that normally occur with DC           
makes VC interpretation tedious and susceptible to error.                       
                                                                                
With support from a National Science Foundation (NSF) grant, we have            
developed a computer-assisted polyp detection (CAPD) system that                
calculates areas of abnormal colon wall thickness in helical CT image           
data in order to highlight potential polyps in the 3D images.  A                
physician ultimately determines if each detected lesion represents a            
true abnormality.  Although we have found CAPD to be sensitive for              
finding subtle abnormalities, poor specificity can be attributed to             
several obstacles, including imprecise image segmentation, limited              
feature analysis, and suboptimal bowel preparation prior to helical CT          
scanning.  With these challenges in mind, we propose research to perfect        
CAPD. Our specific aims are as follows: 1. To develop an image                  
segmentation algorithm that accurately isolates the colon from helical          
CT image data; 2. To improve our polyp detection algorithm with expanded        
feature analysis and artificial intelligence methods; 3. To optimize            
bowel preparation with digital subtraction of opacified feces and               
controlled gas distention; and 4. To validate the accuracy of VC, with          
the modifications achieved in the stated aims, by comparing the results         
of VC and DC in 200 patients undergoing usual-care colonoscopy.                 
                                                                                
If VC with CAPD proves accurate and efficient in the diagnosis of               
colorectal polyps, it could evolve into a simple laboratory test,               
thereby meeting the demand for worldwide colorectal cancer screening.           
 biomedical equipment development; clinical research; colon neoplasms; colon polyp; computer assisted diagnosis; computer simulation; diagnosis design /evaluation; endoscopy; gastrointestinal imaging /visualization; human subject; image enhancement; neoplasm /cancer diagnosis IMPROVING VIRTUAL COLONOSCOPY WITH COMPUTER DETECTION","Colorectal carcinoma is the second leading cause of cancer deaths in the        
United States today.  In an effort to reduce mortality, Congress                
recently included a provision in the Balanced Budget Act of 1997 to             
support screening colonoscopy as a means for early detection and removal        
of colorectal polyps, the precursors to cancer.  In this country alone,         
more than 68 million people are eligible for colorectal screening, but          
the majority are unlikely to comply with screening recommendations              
because of the costs, risks, discomfort, and inconvenience associated           
with traditional endoscopy.  Furthermore, even if a small fraction of           
eligible persons are examined, the number of available                          
gastroenterologists would be insufficient to perform so many procedures.        
                                                                                
We have developed a new technique, called virtual colonoscopy (VC), as          
an alternative to screening diagnostic colonoscopy (DC). The procedure          
consists of cleansing a patient's colon, inflating the colon with air,          
scanning the abdomen with helical computed tomography (CT), and                 
generating a rapid sequence of three-dimensional (3D) images of the             
colon by means of virtual reality computer technology.  Although VC             
makes possible the visualization of 3D images of the colon in a manner          
similar to that of DC, a correct diagnosis depends upon a physician's           
ability to identify small and sometimes subtle polyps within hundreds           
of 3D images.  The absence of visual cues that normally occur with DC           
makes VC interpretation tedious and susceptible to error.                       
                                                                                
With support from a National Science Foundation (NSF) grant, we have            
developed a computer-assisted polyp detection (CAPD) system that                
calculates areas of abnormal colon wall thickness in helical CT image           
data in order to highlight potential polyps in the 3D images.  A                
physician ultimately determines if each detected lesion represents a            
true abnormality.  Although we have found CAPD to be sensitive for              
finding subtle abnormalities, poor specificity can be attributed to             
several obstacles, including imprecise image segmentation, limited              
feature analysis, and suboptimal bowel preparation prior to helical CT          
scanning.  With these challenges in mind, we propose research to perfect        
CAPD. Our specific aims are as follows: 1. To develop an image                  
segmentation algorithm that accurately isolates the colon from helical          
CT image data; 2. To improve our polyp detection algorithm with expanded        
feature analysis and artificial intelligence methods; 3. To optimize            
bowel preparation with digital subtraction of opacified feces and               
controlled gas distention; and 4. To validate the accuracy of VC, with          
the modifications achieved in the stated aims, by comparing the results         
of VC and DC in 200 patients undergoing usual-care colonoscopy.                 
                                                                                
If VC with CAPD proves accurate and efficient in the diagnosis of               
colorectal polyps, it could evolve into a simple laboratory test,               
thereby meeting the demand for worldwide colorectal cancer screening.           
",2869817,R55CA078485,['R55CA078485'],CA,https://reporter.nih.gov/project-details/2869817,R55,1998,100000,-0.12562796312042132
"This is a proposal for renewal of a Senior Scientist Award.  Both               
theoretical and experimental research is proposed on several topics in          
learning and motivation: (a) Experimental and theoretical studies of            
recurrent choice; (b) complex learning, transitive  inference  in               
animals and the emergence of novel behavior and associations; (c)               
sequence learning and models for complex discrimination; (d) time               
discrimination; experimental and theoretical studies of transient and           
steady-state phenomena; (e) multiple-time scale models for behavior;            
habituation and reinforcement effects; (g) feeding dynamics; theoretical        
research of dynamic mechanisms for feeding and foraging; (h)                    
motivational dynamics, multiple time scale processes and drug addiction.        
The experimental work uses animal subjects to study the role of dynamic         
processes in learning and motivated behavior.  Our research strategy            
emphasises continual interplay between experimental and real-time               
models.  Our first objective is refine models for specific experimental         
situations; we then attempt to extend and combine these models to               
embrace as wide as possible a range of related tasks.  Our ultimate             
objective is to use real-time models as guides to understanding the role        
of the nervous system in leaning.                                               
                                                                                
The most intractable and disturbing human behavior disorders involve the        
systems for memory and learning.  The proposed work is intended to              
increase our basic understanding of how these systems work , hence aid          
in the search for means to eliminate or alleviate memory and learning           
disorders.                                                                      
 behavioral /social science research tag; choice; learning; model design /development; motivation; pigeons; psychobiology; psychological models; psychological reinforcement EXPERIMENTAL & THEORETICAL STUDIES: LEARNING MECHANISMS","This is a proposal for renewal of a Senior Scientist Award.  Both               
theoretical and experimental research is proposed on several topics in          
learning and motivation: (a) Experimental and theoretical studies of            
recurrent choice; (b) complex learning, transitive  inference  in               
animals and the emergence of novel behavior and associations; (c)               
sequence learning and models for complex discrimination; (d) time               
discrimination; experimental and theoretical studies of transient and           
steady-state phenomena; (e) multiple-time scale models for behavior;            
habituation and reinforcement effects; (g) feeding dynamics; theoretical        
research of dynamic mechanisms for feeding and foraging; (h)                    
motivational dynamics, multiple time scale processes and drug addiction.        
The experimental work uses animal subjects to study the role of dynamic         
processes in learning and motivated behavior.  Our research strategy            
emphasises continual interplay between experimental and real-time               
models.  Our first objective is refine models for specific experimental         
situations; we then attempt to extend and combine these models to               
embrace as wide as possible a range of related tasks.  Our ultimate             
objective is to use real-time models as guides to understanding the role        
of the nervous system in leaning.                                               
                                                                                
The most intractable and disturbing human behavior disorders involve the        
systems for memory and learning.  The proposed work is intended to              
increase our basic understanding of how these systems work , hence aid          
in the search for means to eliminate or alleviate memory and learning           
disorders.                                                                      
",2032795,K05MH000960,['K05MH000960'],MH,https://reporter.nih.gov/project-details/2032795,K05,1998,99144,-0.10319792255681302
"In the STARE (STructured Analysis of the REtina) project, we are                
developing a computerized image-interpreting system with hierarchical           
inferencing to measure, compare, and diagnose images of the ocular              
fundus. The system will have sufficient depth of imaging tools to be a          
resource for researchers or clinicians.                                         
                                                                                
The STARE system is designed to find objects of interest (normal                
anatomical structures and lesions) in digitized ocular fundus images and        
to use these objects to diagnose an image, to detect changes in                 
sequential images, and to make clinically useful measurements that are          
currently tedious or costly. To accomplish these difficult goals, we            
must segment and identify the objects of interest. Identified objects           
can be used to compare images, and the objects can be assembled to              
describe the image. Image interpretation incorporating expert systems           
and neural networks can provide the structure for cross-sectional               
epidemiological studies.                                                        
                                                                                
There was no established paradigm to follow to construct a system for           
image interpretation. We designed the overall structure of the process          
and determined how each task was to be accomplished. We have broken the         
project into steps, each of which has been accomplished. We are able to         
find objects of importance and correctly identify and localize them on          
a fundus coordinate system that we designed. We have created and tested         
a neural network and an expert system (INTELLEYE) to handle the                 
interpretation of an image and its contents.                                    
                                                                                
We will now improve the accuracy of each step, increase the number of           
lesions we can identify, and integrate the image analysis steps with the        
expert system to allow smooth progression from image to diagnosis and           
change detection. We will validate the usefulness and accuracy of the           
system by comparing its diagnosis and image comparison to trained               
readers of ophthalmic images.                                                   
                                                                                
The goal of this project is a system with multiple imaging tools and            
inferencing ability that can be adapted to a variety of imaging tasks.          
The outcome will be an image-interpreting system for use in clinical and        
research settings that will build annotated image databases, screen             
images of the ocular fundus for health care systems, furnish decision           
support for primary care providers, and extend the capability and               
productivity of the ophthalmologist.                                            
 artificial intelligence; bioimaging /biomedical imaging; computer system design /evaluation; diagnosis design /evaluation; digital imaging; eye fundus photography; human subject; image processing STRUCTURED ANALYSIS OF THE RETINA","In the STARE (STructured Analysis of the REtina) project, we are                
developing a computerized image-interpreting system with hierarchical           
inferencing to measure, compare, and diagnose images of the ocular              
fundus. The system will have sufficient depth of imaging tools to be a          
resource for researchers or clinicians.                                         
                                                                                
The STARE system is designed to find objects of interest (normal                
anatomical structures and lesions) in digitized ocular fundus images and        
to use these objects to diagnose an image, to detect changes in                 
sequential images, and to make clinically useful measurements that are          
currently tedious or costly. To accomplish these difficult goals, we            
must segment and identify the objects of interest. Identified objects           
can be used to compare images, and the objects can be assembled to              
describe the image. Image interpretation incorporating expert systems           
and neural networks can provide the structure for cross-sectional               
epidemiological studies.                                                        
                                                                                
There was no established paradigm to follow to construct a system for           
image interpretation. We designed the overall structure of the process          
and determined how each task was to be accomplished. We have broken the         
project into steps, each of which has been accomplished. We are able to         
find objects of importance and correctly identify and localize them on          
a fundus coordinate system that we designed. We have created and tested         
a neural network and an expert system (INTELLEYE) to handle the                 
interpretation of an image and its contents.                                    
                                                                                
We will now improve the accuracy of each step, increase the number of           
lesions we can identify, and integrate the image analysis steps with the        
expert system to allow smooth progression from image to diagnosis and           
change detection. We will validate the usefulness and accuracy of the           
system by comparing its diagnosis and image comparison to trained               
readers of ophthalmic images.                                                   
                                                                                
The goal of this project is a system with multiple imaging tools and            
inferencing ability that can be adapted to a variety of imaging tasks.          
The outcome will be an image-interpreting system for use in clinical and        
research settings that will build annotated image databases, screen             
images of the ocular fundus for health care systems, furnish decision           
support for primary care providers, and extend the capability and               
productivity of the ophthalmologist.                                            
",6130715,R01LM005759,['R01LM005759'],LM,https://reporter.nih.gov/project-details/6130715,R01,1999,96832,-0.025212781216259854
"The goal of this project is to develop a computer algorithm that accurately predicts the tertiary fold of a protein given only its amino acid sequence and sparse experimental constraints or an imperfect secondary structure prediction. It will use an evolutionary optimization algorithm, a detailed atomic representation, and a proprietary method for deriving a protein energy function. The specific aims of Phase II are: (1) improve the representation of the problem; (2) improve the optimization algorithm; (3) parameterize a more accurate protein energy function; and (4) validate the prototype algorithm and energy function on a large sample of known protein structures and in blind structure prediction experiments. In Phase III the algorithm will be developed into commercial software products for the pharmaceutical and biotechnology industries.  The long-term goal is to help satisfy the rapidly growing demand for the accurate prediction of protein structures from sequence information. This demand arises not only from the exponential growth of the database of sequences, but also from the need to understand the structure and function of newly discovered gene products known to be involved in human disease. PROPOSED COMMERCIAL APPLICATIONS: Commercial applications include protein structure determination, structure- based drug design and protein engineering in the pharmaceutical and biotechnology industries and in basic academic research.  artificial intelligence; computer program /software; computer simulation; computer system design /evaluation; conformation; intermolecular interaction; physical model; protein folding; protein sequence; protein structure; structural biology EVOLUTIONARY ALGORITHM FOR PROTEIN STRUCTURE PREDICTION","The goal of this project is to develop a computer algorithm that accurately predicts the tertiary fold of a protein given only its amino acid sequence and sparse experimental constraints or an imperfect secondary structure prediction. It will use an evolutionary optimization algorithm, a detailed atomic representation, and a proprietary method for deriving a protein energy function. The specific aims of Phase II are: (1) improve the representation of the problem; (2) improve the optimization algorithm; (3) parameterize a more accurate protein energy function; and (4) validate the prototype algorithm and energy function on a large sample of known protein structures and in blind structure prediction experiments. In Phase III the algorithm will be developed into commercial software products for the pharmaceutical and biotechnology industries.  The long-term goal is to help satisfy the rapidly growing demand for the accurate prediction of protein structures from sequence information. This demand arises not only from the exponential growth of the database of sequences, but also from the need to understand the structure and function of newly discovered gene products known to be involved in human disease. PROPOSED COMMERCIAL APPLICATIONS: Commercial applications include protein structure determination, structure- based drug design and protein engineering in the pharmaceutical and biotechnology industries and in basic academic research. ",6019377,R44GM056593,['R44GM056593'],GM,https://reporter.nih.gov/project-details/6019377,R44,1999,181519,-0.10826961778162025
"This research proposal is for a one-year continuation of the study begun in the first two years of my NRSA postdoctoral fellowship.  The current study addresses the question of what parameters of grasp are encoded in the simple and complex spike activity of Purkinje cells.  My hypothesis is that the simple spike activity of cerebellar Purkinje cells modulates in relation to object volume, shape, and orientation, and to the force generated during the grasp.  Object properties appear to be encoded during the reach and early in the grasp of the object and force tends to be encoded late in the grasp.  In the current study, the reach component is held constant and the object to be grasped and the force to be generated are varied systematically. The monkey's hand and the object are outside monkey's field of view, removing possible visual contributions to the Purkinje cell discharge.  Multiple linear regression analysis will be used to analyze task-related discharge in relation to the force generated and to object properties such as shape, volume and orientation. An additional paradigm I propose is to compare the firing rates of Purkinje cells during the grasp task with and without visual guidance.  Two assumption made in both paradigms is that reach is constant and that hand posture will vary with the different objects.  The kinematics of the wrist and hand movements will be monitored to test these assumptions.  The kinematic data will also be analyzed using singular value decomposition for possible simplifying strategies the CNS may use in the control of grasp. The discharge activity of the population of Purkinje cells will be evaluated using singular value decomposition to identify patterns of activity that may reflect the simplifying strategies observed in the psychophysical  studies.  Macaca mulatta; brain electrical activity; cerebellar Purkinje cell; cerebellum; computational neuroscience; electrophysiology; hand; histology; limb movement; model design /development; neural information processing; neurosurgery; oscillography; psychomotor function; sensorimotor system; single cell analysis; visual feedback ENCODING REACH AND GRASP IN CEREBELLAR NEURONAL ACTIVITY","This research proposal is for a one-year continuation of the study begun in the first two years of my NRSA postdoctoral fellowship.  The current study addresses the question of what parameters of grasp are encoded in the simple and complex spike activity of Purkinje cells.  My hypothesis is that the simple spike activity of cerebellar Purkinje cells modulates in relation to object volume, shape, and orientation, and to the force generated during the grasp.  Object properties appear to be encoded during the reach and early in the grasp of the object and force tends to be encoded late in the grasp.  In the current study, the reach component is held constant and the object to be grasped and the force to be generated are varied systematically. The monkey's hand and the object are outside monkey's field of view, removing possible visual contributions to the Purkinje cell discharge.  Multiple linear regression analysis will be used to analyze task-related discharge in relation to the force generated and to object properties such as shape, volume and orientation. An additional paradigm I propose is to compare the firing rates of Purkinje cells during the grasp task with and without visual guidance.  Two assumption made in both paradigms is that reach is constant and that hand posture will vary with the different objects.  The kinematics of the wrist and hand movements will be monitored to test these assumptions.  The kinematic data will also be analyzed using singular value decomposition for possible simplifying strategies the CNS may use in the control of grasp. The discharge activity of the population of Purkinje cells will be evaluated using singular value decomposition to identify patterns of activity that may reflect the simplifying strategies observed in the psychophysical  studies. ",6062437,F32NS010491,['F32NS010491'],NS,https://reporter.nih.gov/project-details/6062437,F32,1999,38368,-0.06775972053644523
"DESCRIPTION (Adapted from the applicant's abstract):                            
                                                                                
The proposed research will build, refine, and test in operational use, a set    
of software tools designed to help support, maintain, and iteratively           
revalidate computer-based clinical guidelines as they evolve over time.  The    
project will focus on the domain of childhood immunization, and will build      
upon IMM/Serve, a childhood immunization forecasting program that takes as      
input a child's immunization history, and produces recommendations as to        
which vaccinations are due and which vaccinations should be scheduled next.     
The effort required to modify and validate such a program as the clinical       
field evolves over time is a challenging task.  It will be extremely            
important to have a robust set of tools to assist in this process.  Partial     
prototype versions of certain of these tools already exist.                     
                                                                                
1.  The project will refine and extend computer-based tools for immunization    
knowledge maintenance.  These tools will include:  a) IMM/Def, a program        
which automatically generates the rule-based logic for the most complex         
portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program         
which automatically generates a set of test cases to help test the kernel       
logic.  The project will also develop an organized set of strategies for        
immunization test case generation, and implement those strategies in the        
refined version of IMM/Test.                                                    
                                                                                
2.  The project will build a Web site to support immunization knowledge         
maintenance.                                                                    
                                                                                
3.  The project will keep a detailed record of all modifications and            
customization of the knowledge, and will represent all the variations of the    
knowledge using a standardized format such as GLIF, the Guideline               
Interchange Format being developed as a standard for exchanging guidelines      
between sites.                                                                  
                                                                                
4.  The project will link IMM/Serve to a database designed to hold              
IMM/Serve's analysis of a set of cases, so that the resulting package can be    
used as a tool to perform compliance assessment.                                
                                                                                
5.  A set of evaluation studies will be carried out to help assess the          
efficacy of the tools and to help improve their functionality.                  
 Internet; artificial intelligence; computer assisted medical decision making; computer assisted patient care; computer human interaction; computer program /software; computer system design /evaluation; human data; immunization; information systems; medical records; pediatrics TOOLS TO SUPPORT COMPUTER BASED CLINICAL GUIDELINES","DESCRIPTION (Adapted from the applicant's abstract):                            
                                                                                
The proposed research will build, refine, and test in operational use, a set    
of software tools designed to help support, maintain, and iteratively           
revalidate computer-based clinical guidelines as they evolve over time.  The    
project will focus on the domain of childhood immunization, and will build      
upon IMM/Serve, a childhood immunization forecasting program that takes as      
input a child's immunization history, and produces recommendations as to        
which vaccinations are due and which vaccinations should be scheduled next.     
The effort required to modify and validate such a program as the clinical       
field evolves over time is a challenging task.  It will be extremely            
important to have a robust set of tools to assist in this process.  Partial     
prototype versions of certain of these tools already exist.                     
                                                                                
1.  The project will refine and extend computer-based tools for immunization    
knowledge maintenance.  These tools will include:  a) IMM/Def, a program        
which automatically generates the rule-based logic for the most complex         
portion (""kernel"") of IMM/Serve's knowledge, and b) IMM/Test, a program         
which automatically generates a set of test cases to help test the kernel       
logic.  The project will also develop an organized set of strategies for        
immunization test case generation, and implement those strategies in the        
refined version of IMM/Test.                                                    
                                                                                
2.  The project will build a Web site to support immunization knowledge         
maintenance.                                                                    
                                                                                
3.  The project will keep a detailed record of all modifications and            
customization of the knowledge, and will represent all the variations of the    
knowledge using a standardized format such as GLIF, the Guideline               
Interchange Format being developed as a standard for exchanging guidelines      
between sites.                                                                  
                                                                                
4.  The project will link IMM/Serve to a database designed to hold              
IMM/Serve's analysis of a set of cases, so that the resulting package can be    
used as a tool to perform compliance assessment.                                
                                                                                
5.  A set of evaluation studies will be carried out to help assess the          
efficacy of the tools and to help improve their functionality.                  
",2897401,R01LM006682,['R01LM006682'],LM,https://reporter.nih.gov/project-details/2897401,R01,1999,308075,-0.04007349086720499
"DESCRIPTION (Taken from application abstract):  This proposed study will        
replicate and extend methodology used in earlier studies and will use           
extensive clinical data repositories, informatics tools, and expert             
practitioners for perinatal medical knowledge building.                         
                                                                                
Clinical Data Repository:  Duke University's Medical Center (DUMC) TMR (The     
Medical Record) data repository will be used for this study, and contains       
45,922 electronic medical records for both low and high-risk pregnant women     
(and their infants) who have received prenatal care at DUMC, and its            
affiliated regional clinics, between 1/1/86 and 12/3l/95.  Each patient's       
electronic data is used for clinical patient care and contains a potential      
4000 variables per record.  This volume of data requires new approaches for     
data analysis and medical decision support, since human information             
processing limitations become quickly overloaded by both an individual          
patient s data and the aggregate information collected for the perinatal        
patient population.                                                             
                                                                                
lnformatics Tools:  Informatics techniques for knowledge acquisition and        
data mining will use machine learning programs, statistical analysis, and       
domain expert input to articulate relationships between the data and            
perinatal patent outcomes.  The goal is to provide decision support for         
perinatal care providers to accurately identify patients at risk and assist     
them with modifiable preterm birth ask factors.  An expert system will use      
data-generated and verified knowledge bases to test its predictive validity     
when new patient cases are induced to the expert system.  Earlier studies       
found 53-90% predictive accuracies for an expert system prototype, as           
compared to 17-38% accuracies, reported in the literature, using current        
manual techniques.  Mapping the expert system's knowledge base terms to         
medical library resources will be explored for additional decision support.     
                                                                                
Expert Practitioner:  The perinatal expert panel will consist of the            
Principal Investigator, a Board Certified OB-Gyn Physician, and a certified     
Perinatal RN.  Each of the panel members has more than 20 years of perinatal    
experience.  Participating informatics experts are known, both nationally       
and internationally for their expertise in the field of Medical Informatics.    
 artificial intelligence; computer system design /evaluation; human data; information systems; prenatal care INFORMATICS TOOLS & MEDICAL PERINATAL KNOWLEDGE BUILDING","DESCRIPTION (Taken from application abstract):  This proposed study will        
replicate and extend methodology used in earlier studies and will use           
extensive clinical data repositories, informatics tools, and expert             
practitioners for perinatal medical knowledge building.                         
                                                                                
Clinical Data Repository:  Duke University's Medical Center (DUMC) TMR (The     
Medical Record) data repository will be used for this study, and contains       
45,922 electronic medical records for both low and high-risk pregnant women     
(and their infants) who have received prenatal care at DUMC, and its            
affiliated regional clinics, between 1/1/86 and 12/3l/95.  Each patient's       
electronic data is used for clinical patient care and contains a potential      
4000 variables per record.  This volume of data requires new approaches for     
data analysis and medical decision support, since human information             
processing limitations become quickly overloaded by both an individual          
patient s data and the aggregate information collected for the perinatal        
patient population.                                                             
                                                                                
lnformatics Tools:  Informatics techniques for knowledge acquisition and        
data mining will use machine learning programs, statistical analysis, and       
domain expert input to articulate relationships between the data and            
perinatal patent outcomes.  The goal is to provide decision support for         
perinatal care providers to accurately identify patients at risk and assist     
them with modifiable preterm birth ask factors.  An expert system will use      
data-generated and verified knowledge bases to test its predictive validity     
when new patient cases are induced to the expert system.  Earlier studies       
found 53-90% predictive accuracies for an expert system prototype, as           
compared to 17-38% accuracies, reported in the literature, using current        
manual techniques.  Mapping the expert system's knowledge base terms to         
medical library resources will be explored for additional decision support.     
                                                                                
Expert Practitioner:  The perinatal expert panel will consist of the            
Principal Investigator, a Board Certified OB-Gyn Physician, and a certified     
Perinatal RN.  Each of the panel members has more than 20 years of perinatal    
experience.  Participating informatics experts are known, both nationally       
and internationally for their expertise in the field of Medical Informatics.    
",2897391,R01LM006488,['R01LM006488'],LM,https://reporter.nih.gov/project-details/2897391,R01,1999,308857,-0.045625439806500245
"Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
 HIV infections; Internet; artificial intelligence; computer graphics /printing; computer human interaction; computer program /software; computer system design /evaluation; flow cytometry; information retrieval; information systems FACS-PENGUIN--KNOWLEDGE BASE SUPPORT FOR FLOW CYTOMETRY","Fluorescence Activated Cell Sorting (FACS) is a major source of                 
information for biomedical research and clinical practice.  Basic and           
clinical FACS studies are important to research in AIDS, cancer, and            
rheumatological diseases.  However, use of the FACS instrument is               
hampered by a need for greater skills in FACS experiment protocol               
design, instrument operation, and data analysis.  Developing a FACS             
workstation environment will facilitate FACS use by reducing the need           
for on-site human expertise.  The application of recent research and            
development in medical informatics will allow a new level of automatic          
control for complex devices, helping to move FACS technology into               
clinical research use.  We have developed the infrastructure for an             
expert workstation-PENGUIN-under the initial grant and with associated          
support from computer science and electrical engineering resources.             
Data from a genetics database server can be accessed on workstations as         
object-oriented structures.  Updates from workstations can be reflected         
in the central database.  Application development on the workstation is         
facilitated by our adoption of the X Windows interface standard.  Our           
next goal is to make the workstations into effective tools for designing        
and executing FACS protocols.  The specific aims are to: 1) Build a             
knowledge-based protocol critiquing tool which encodes the expertise of         
the staff of the Herzenberg Laboratory; 2) Develop a reagent inventory-         
control module to complement the first task: 3) Develop an experiment           
management system for the FLUOROSKAN instrument, another technology used        
in biomedical research; 4) Improve and extend the current user interface        
to provide support for FACS instrumentation; 5) Augment the HELP                
facility available during FACS operation to assist in proper execution          
of the experiment; and, 6) Refine data analysis and display techniques          
to support the increasing mass of information available from FACS               
experiments.                                                                    
",2897364,R01LM004836,['R01LM004836'],LM,https://reporter.nih.gov/project-details/2897364,R01,1999,324801,-0.13532027225902754
"Commercial automated karyotyping instruments have improved to the point where the major factor limiting throughput is the time required for operator correction of chromosome classification errors. An improvement in chromosome classification accuracy would significantly increase the value of these instruments in cytogenetics labs. The goal of this project is to develop and commercialize significantly improved chromosome measurement and classification techniques for automated karyotyping. Currently the best-performing chromosome classification approach uses Weighted Density Distribution (WDD) features [11] to quantify the banding pattern of the chromosomes. These are computed as inner products between the banding profile and a set of WDD basis functions. The particular set of 1unctions originally proposed by Granum [11,38] has come into widespread use. In Phase I we showed that better function sets exist and that our new approach can find better WDD features than the best currently used. We have an innovative wavelet-based method for generating WDD functions and a chromosome classification testbed which supports large scale classification experiments. We propose to conduct a thorough, methodical search for better performing basis functions in Phase II. Phase III will incorporate the technology into PSI's PowerGene automated karyotyping instruments. PROPOSED COMMERCIAL APPLICATIONS: When the new chromosome classification technology is qualified for routine application, it will be incorporated into PSI's Powergene products, both in new systems sold and as an upgrade to existing systems.  artificial intelligence; biomedical automation; biomedical equipment development; chromosomes; computer program /software; cytogenetics; density; genetic mapping; genetic techniques; human genetic material tag; human tissue; image processing WAVELET-BASED AUTOMATED CHROMOSOME IDENTIFICATION","Commercial automated karyotyping instruments have improved to the point where the major factor limiting throughput is the time required for operator correction of chromosome classification errors. An improvement in chromosome classification accuracy would significantly increase the value of these instruments in cytogenetics labs. The goal of this project is to develop and commercialize significantly improved chromosome measurement and classification techniques for automated karyotyping. Currently the best-performing chromosome classification approach uses Weighted Density Distribution (WDD) features [11] to quantify the banding pattern of the chromosomes. These are computed as inner products between the banding profile and a set of WDD basis functions. The particular set of 1unctions originally proposed by Granum [11,38] has come into widespread use. In Phase I we showed that better function sets exist and that our new approach can find better WDD features than the best currently used. We have an innovative wavelet-based method for generating WDD functions and a chromosome classification testbed which supports large scale classification experiments. We propose to conduct a thorough, methodical search for better performing basis functions in Phase II. Phase III will incorporate the technology into PSI's PowerGene automated karyotyping instruments. PROPOSED COMMERCIAL APPLICATIONS: When the new chromosome classification technology is qualified for routine application, it will be incorporated into PSI's Powergene products, both in new systems sold and as an upgrade to existing systems. ",6016521,R44CA076896,['R44CA076896'],CA,https://reporter.nih.gov/project-details/6016521,R44,1999,337429,-0.04355516736949109
"Automated screening of Pap smear slides is challenging due the high             
processing and data transfer requirements placed on the processing              
engine. These requirements can be significantly reduced by processing           
the images at the image plane of the camera, and reading out only the           
relevant data, which results in lower cost and higher performance               
systems. Image sensors with smarts or computational capability at each          
pixel can be advantageously used in the application to build extremely          
compact and low-cost automated screening systems. Morphological                 
filtering algorithms have been shown to be effective at detecting object        
size and shape, which are distinguishing features in diagnostic                 
microscopy. The goal of this research is to design, simulate and                
fabricate a CMOS chip with morphological filtering circuits at each             
pixel, which will allow detecting suspicious cells in a Pap Smear at            
more than 1000 frames/second. In Phase I, Bosonics will (1) determine           
the desired imager's technical capabilities for a compact microscope            
mountable smart camera (2) design candidate morphological filtering             
architectures and circuits; (3) simulate the morphological algorithms           
with realistic circuits models; and (4) design and fabricate a 5x5              
imager demonstration chip in CMOS.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The chips produced under this program will lower the cost of machine            
vision systems by providing an integrated detector/processing function          
as well as increasing performance by reducing the output bandwidth              
requirements. The detector arrays developed under this program will have        
wide application in automatic target recognition, machine vision for            
automated manufacturing, medical diagnostic imaging, and remote sensing         
and surveillance.                                                               
 artificial intelligence; bioimaging /biomedical imaging; biomedical automation; biomedical equipment development; cervical /vaginal smear; computer simulation; computer system design /evaluation; cytology; image processing; morphology; photomicrography CAMERA TO DETECT REGIONS OF INTEREST IN A PAP SMEAR","Automated screening of Pap smear slides is challenging due the high             
processing and data transfer requirements placed on the processing              
engine. These requirements can be significantly reduced by processing           
the images at the image plane of the camera, and reading out only the           
relevant data, which results in lower cost and higher performance               
systems. Image sensors with smarts or computational capability at each          
pixel can be advantageously used in the application to build extremely          
compact and low-cost automated screening systems. Morphological                 
filtering algorithms have been shown to be effective at detecting object        
size and shape, which are distinguishing features in diagnostic                 
microscopy. The goal of this research is to design, simulate and                
fabricate a CMOS chip with morphological filtering circuits at each             
pixel, which will allow detecting suspicious cells in a Pap Smear at            
more than 1000 frames/second. In Phase I, Bosonics will (1) determine           
the desired imager's technical capabilities for a compact microscope            
mountable smart camera (2) design candidate morphological filtering             
architectures and circuits; (3) simulate the morphological algorithms           
with realistic circuits models; and (4) design and fabricate a 5x5              
imager demonstration chip in CMOS.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The chips produced under this program will lower the cost of machine            
vision systems by providing an integrated detector/processing function          
as well as increasing performance by reducing the output bandwidth              
requirements. The detector arrays developed under this program will have        
wide application in automatic target recognition, machine vision for            
automated manufacturing, medical diagnostic imaging, and remote sensing         
and surveillance.                                                               
",6012563,R43CA079295,['R43CA079295'],CA,https://reporter.nih.gov/project-details/6012563,R43,1999,700,-0.09770154842228262
"DESCRIPTION (Adapted from applicant's abstract): The ""closed-loop               
artificial pancreas,"" a device that would measure glucose level and             
deliver insulin automatically as needed, has been an elusive goal in the        
treatment of diabetes. There are three essential components: the blood          
glucose sensor, linking algorithms and the delivery system. For the first       
time, a viable sensor and a proven delivery system are now available for        
research. The broad goal of this clinical research proposal is to complete      
the studies needed to link the sensor to the delivery system, paving the        
way for a functional closed-loop artificial pancreas. First, we will make       
a detailed analysis of sensor signal as it reflects glucose level in            
normal and diabetic humans. Second, we will study the precise                   
pharmacokinetics of insulin delivery by external and implantable insulin        
pumps. Third, analysis of these two data sets will provide the basis for        
algorithms that link the sensor signal to insulin delivery. A formal            
safety analysis will evaluate the safety features needed in a closed loop       
device. In the last year of the project, the entire system will be tested       
and fine-tuned. This project takes advantage of our relatively extensive        
investigational experience with mechanical insulin delivery pumps in            
people with diabetes, and the recent availability, for research, of a           
subcutaneously placed, glucose oxidase-based continuous glucose sensor.         
The investigators have established experienced with clinical research in        
diabetes, and the resources of an excellent General Clinical Research           
Center. The co-investigators have extensive experience with mathematical        
modeling of biologic systems. There is a close working relationship             
between the research team and the manufacturer of the sensor and pumps, as      
reflected by the Interactive Research Project Grant collaboration, and by       
a long-standing history of collaboration. It is essential to emphasize          
that we do not anticipate completion of a manufacturable, clinically            
usable, commercially viable artificial pancreas within the time-frame of        
this work. Rather, we aim to complete the basic studies and modeling            
analyses that would form the basis of such a system, and demonstrate the        
feasibility of linking the sensor to the delivery device. If these studies      
and these trials were successful, they would be a major step towards            
development of a clinically useful close-loop artificial pancreas.              
 artificial endocrine pancreas; artificial intelligence; biosensor device; clinical research; drug delivery systems; glucose metabolism; human subject; insulin; insulin dependent diabetes mellitus; medical implant science; pharmacokinetics CLINICAL RESEARCH TOWARD CLOSED LOOP INSULIN DELIVERY","DESCRIPTION (Adapted from applicant's abstract): The ""closed-loop               
artificial pancreas,"" a device that would measure glucose level and             
deliver insulin automatically as needed, has been an elusive goal in the        
treatment of diabetes. There are three essential components: the blood          
glucose sensor, linking algorithms and the delivery system. For the first       
time, a viable sensor and a proven delivery system are now available for        
research. The broad goal of this clinical research proposal is to complete      
the studies needed to link the sensor to the delivery system, paving the        
way for a functional closed-loop artificial pancreas. First, we will make       
a detailed analysis of sensor signal as it reflects glucose level in            
normal and diabetic humans. Second, we will study the precise                   
pharmacokinetics of insulin delivery by external and implantable insulin        
pumps. Third, analysis of these two data sets will provide the basis for        
algorithms that link the sensor signal to insulin delivery. A formal            
safety analysis will evaluate the safety features needed in a closed loop       
device. In the last year of the project, the entire system will be tested       
and fine-tuned. This project takes advantage of our relatively extensive        
investigational experience with mechanical insulin delivery pumps in            
people with diabetes, and the recent availability, for research, of a           
subcutaneously placed, glucose oxidase-based continuous glucose sensor.         
The investigators have established experienced with clinical research in        
diabetes, and the resources of an excellent General Clinical Research           
Center. The co-investigators have extensive experience with mathematical        
modeling of biologic systems. There is a close working relationship             
between the research team and the manufacturer of the sensor and pumps, as      
reflected by the Interactive Research Project Grant collaboration, and by       
a long-standing history of collaboration. It is essential to emphasize          
that we do not anticipate completion of a manufacturable, clinically            
usable, commercially viable artificial pancreas within the time-frame of        
this work. Rather, we aim to complete the basic studies and modeling            
analyses that would form the basis of such a system, and demonstrate the        
feasibility of linking the sensor to the delivery device. If these studies      
and these trials were successful, they would be a major step towards            
development of a clinically useful close-loop artificial pancreas.              
",2906356,R01DK055132,['R01DK055132'],DK,https://reporter.nih.gov/project-details/2906356,R01,1999,244132,-0.03614615070612801
 DVD /CD ROM; artificial intelligence; cancer risk; cervical /vaginal smear; cervix neoplasms; computer assisted medical decision making; computer human interaction; computer program /software; computer system design /evaluation; diagnosis design /evaluation; female; interactive multimedia; neoplasm /cancer diagnosis; statistics /biometry CERVICAL CANCER SCREENING DECISION SUPPORT SYSTEM,,6017635,R43CA083491,['R43CA083491'],CA,https://reporter.nih.gov/project-details/6017635,R43,1999,95619,-0.10923222931862933
"This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
 adenosine; anesthetics; artificial intelligence; blood pressure; cardiovascular agents; computer simulation; drug tolerance; heart rate; model design /development; neurotransmitter metabolism; nicotine; pharmacokinetics; pulmonary respiration MODELS FOR BIOLOGICAL DATA RELEVANT TO HEALTH CARE","This grant proposal is focused on developing new mathematical and               
statistical models to describe biological systems. Models to represent,         
help to understand, predict future behavior, and control biological             
systems are becoming more and more important and of widespread use in           
different fields related to biology and health care. Complex mathematical       
models are needed to model the complicated interactions between the             
physiological functions of biological systems, and to model the effect of       
interventions (e.g. therapy) on these functions. The specific aims of this      
grant focus on three areas of research. 1. Develop and investigate              
statistical models for biological population data. Biological data are          
always collected from some population of different individuals, and are         
often highly variable. This is mostly due to variability of physiological       
functions between individuals, and to measurement error. Statistical            
models are needed to deal with the complex structure of population data. I      
will (I) introduce a general methodology based on the use of sophisticated      
heteroscedastic statistical models, which does not explicitly formulate a       
model for interindividual variability but promises to be fast, efficient        
and unbiased; and (ii) investigate the performance of existing population       
models using realistic simulations including model misspecification. 2.         
Develop semi-mechanistic compartmental models. I focus on three main            
problems: (i) the development and investigation a new general class of          
compartmental pharmacokinetics""'pharmacodynamic (PK/PD) models, (ii) the        
development of semi-mechanistic black-box compartmental models to deal          
with non-linear PK systems, (iii) the development of the technology to          
apply well established semi-mechanistic linear black-box models to the          
purpose of PK control. 3. Develop new multivariate dynamic models. The          
main problem addressed is how to represent a system where multiple inputs       
(drugs) and multiple interrelated responses are measured. I propose             
different classes of models to do so based on spline networks and               
eventually neural networks. The proposed models can incorporate a               
compartmental sub-structure to easily deal with kinetics. Continuous and        
discrete time versions of the models are considered. The statistical and        
mathematical models introduced in the grant have widespread application to      
a variety of biological fields. However specific areas, directly linked to      
health care issues, are selected for active research and application of         
the proposed models. These areas correspond to experimental situations          
where the models proposed in the grant are particularly needed (nonlinear       
and multivariate dynamic), and represent continuations of already               
established collaborations with leading scientists. They include: computer      
control of ultra-short acting anaesthetic drugs administration,                 
pharmacokinetics/pharmacodynamic of short-acting anesthetics,                   
pharmacodynamic of nicotine and nicotine tolerance development, adenosine       
kinetics and metabolism and their relationship to adenosine                     
pharmacodynamic effects, modeling of cardiovascular drugs effects on            
pharmacy dynamic responses (heart rate, blood pressure, and breathing           
variability) sampled at high rates.                                             
",6019004,R29GM051197,['R29GM051197'],GM,https://reporter.nih.gov/project-details/6019004,R29,1999,114592,-0.09889363111004608
"Most experiments evaluating the effect of treatments on living organisms involve comparison between the responses of a group of individuals given the treatment and those of a group not treated (or given an alternative treatment). A frequent requirement in such studies is random assignment of individuals to treatment groups. The goal of this project is to develop an Interactive Registration and Randomization System (IRRS) providing comprehensive randomization functions to researchers, targeted particularly to clinical trials, but also usable for other types of randomized experiments. The proposed product will provide cost-effective, user-friendly access to all of the functionality necessary for professional quality, auditable registration and randomization. It will support all of the randomization procedures currently in use, accommodate double-masked, single-masked, and unmasked designs, and allow flexibility in study design. It will support simultaneous access by multiple users, simultaneous management of multiple studies, and support for multiple languages in the user interface. It will support access by touch-tone phone, or from any PC with Internet access and a standard web browser, or by dial-up modem over the switched phone network. All modules will be formally validated to ensure compliance with FDA requirements for software validation. The aims of this grant are to finalize system requirements; design the architecture of the system; develop functional specifications; develop prototypes of the permuted block randomization algorithm; develop prototypes of the phone and web user interfaces; and use the prototypes to demonstrate the feasibility and merits of the system. PROPOSED COMMERCIAL APPLICATIONS: The software to be developed under this grant will permit pharmaceutical companies, NIH grant recipients, universities, and others conducting randomized experiments to quickly and inexpensively create professional- quality computer systems for patient registration and randomization, using both telephony and web-based user interfaces. The product will provide a cost-effective alternative to the ""home-grown,"" single-use systems currently in use, offering significant savings for the thousands of randomized trials initiated each year (e.g., over 9,000 per year in human medical research alone).  Internet; artificial intelligence; clinical trials; computer program /software; computer system design /evaluation; interactive multimedia; patient /disease registry; statistics /biometry; telecommunications VOICE RESPONSE/INTERNET REGISTRATION AND RANDOM SYSTEM","Most experiments evaluating the effect of treatments on living organisms involve comparison between the responses of a group of individuals given the treatment and those of a group not treated (or given an alternative treatment). A frequent requirement in such studies is random assignment of individuals to treatment groups. The goal of this project is to develop an Interactive Registration and Randomization System (IRRS) providing comprehensive randomization functions to researchers, targeted particularly to clinical trials, but also usable for other types of randomized experiments. The proposed product will provide cost-effective, user-friendly access to all of the functionality necessary for professional quality, auditable registration and randomization. It will support all of the randomization procedures currently in use, accommodate double-masked, single-masked, and unmasked designs, and allow flexibility in study design. It will support simultaneous access by multiple users, simultaneous management of multiple studies, and support for multiple languages in the user interface. It will support access by touch-tone phone, or from any PC with Internet access and a standard web browser, or by dial-up modem over the switched phone network. All modules will be formally validated to ensure compliance with FDA requirements for software validation. The aims of this grant are to finalize system requirements; design the architecture of the system; develop functional specifications; develop prototypes of the permuted block randomization algorithm; develop prototypes of the phone and web user interfaces; and use the prototypes to demonstrate the feasibility and merits of the system. PROPOSED COMMERCIAL APPLICATIONS: The software to be developed under this grant will permit pharmaceutical companies, NIH grant recipients, universities, and others conducting randomized experiments to quickly and inexpensively create professional- quality computer systems for patient registration and randomization, using both telephony and web-based user interfaces. The product will provide a cost-effective alternative to the ""home-grown,"" single-use systems currently in use, offering significant savings for the thousands of randomized trials initiated each year (e.g., over 9,000 per year in human medical research alone). ",2869997,R43RR014168,['R43RR014168'],RR,https://reporter.nih.gov/project-details/2869997,R43,1999,88245,-0.0993259542455452
"The long-term goal of Phase I and Phase II is to create a series of bilingual (Spanish/English), bicultural, interactive CD-ROMs dealing with issues related to menopause and aging, with specific emphasis on Hispanic women. This proposal will result in a CD-ROM about Hormone Replacement Therapy (HRT) that takes 15-3O minutes to use and will test its value in a controlled study. Through an interactive decision-tree that specifies individual risk factors and lifestyle preferences, the CD-ROM will help users identify their options, including HRT, lifestyle changes such as exercise and diet, and other alternatives. Besides providing information, the CD-ROM will be designed to improve decision-making and increase women's confidence in their ability to make informed choices. Although it may increase the risk of cancer, HRT relieves short-term menopausal symptoms, and appears to offer long-term benefits of reduced risk for heart disease, stroke and osteoporosis. Over 40 million women will pass through menopause in the next two decades. Studies have shown that Hispanic women are less familiar with HRT and less likely to use it than non-Hispanic white women. In addition, Hispanas approach medical care with their own set of cultural values. Interactive media offer personalization, enhanced learning through multiple sensory input, and a cost-effective method of providing information in a managed care environment or busy individual practice. This is an innovative application of existing technology to provide patient education. Possible topics for Phase II include osteoporosis, heart disease, depression, and stroke. PROPOSED COMMERCIAL APPLICATIONS: Educational media is a $3B market. Print and video materials on HRT and menopause are available, but little exists as interactive media, in Spanish, and/or is culturally-relevant to Hispanics. This CD-ROM and related topics in Phase II can be sold to HMOs, clinics, and doctors' offices through direct sales; posted on the Internet; or distributed through medical media and pharmaceutical companies, medical associations, and Hispanic and/or women's groups.  DVD /CD ROM; Hispanic Americans; aging; computer assisted medical decision making; computer program /software; computer system design /evaluation; disease /disorder proneness /risk; education evaluation /planning; estrogens; female; health education; hormone therapy; interactive multimedia; menopause; progestins; women's health INTERACTIVE MEDIA FOR WOMEN CONSIDERING HORMONE THERAPY","The long-term goal of Phase I and Phase II is to create a series of bilingual (Spanish/English), bicultural, interactive CD-ROMs dealing with issues related to menopause and aging, with specific emphasis on Hispanic women. This proposal will result in a CD-ROM about Hormone Replacement Therapy (HRT) that takes 15-3O minutes to use and will test its value in a controlled study. Through an interactive decision-tree that specifies individual risk factors and lifestyle preferences, the CD-ROM will help users identify their options, including HRT, lifestyle changes such as exercise and diet, and other alternatives. Besides providing information, the CD-ROM will be designed to improve decision-making and increase women's confidence in their ability to make informed choices. Although it may increase the risk of cancer, HRT relieves short-term menopausal symptoms, and appears to offer long-term benefits of reduced risk for heart disease, stroke and osteoporosis. Over 40 million women will pass through menopause in the next two decades. Studies have shown that Hispanic women are less familiar with HRT and less likely to use it than non-Hispanic white women. In addition, Hispanas approach medical care with their own set of cultural values. Interactive media offer personalization, enhanced learning through multiple sensory input, and a cost-effective method of providing information in a managed care environment or busy individual practice. This is an innovative application of existing technology to provide patient education. Possible topics for Phase II include osteoporosis, heart disease, depression, and stroke. PROPOSED COMMERCIAL APPLICATIONS: Educational media is a $3B market. Print and video materials on HRT and menopause are available, but little exists as interactive media, in Spanish, and/or is culturally-relevant to Hispanics. This CD-ROM and related topics in Phase II can be sold to HMOs, clinics, and doctors' offices through direct sales; posted on the Internet; or distributed through medical media and pharmaceutical companies, medical associations, and Hispanic and/or women's groups. ",2869789,R43AG017016,['R43AG017016'],AG,https://reporter.nih.gov/project-details/2869789,R43,1999,97007,-0.0798091472167197
DESCRIPTION: (adapted from the investigator's abstract) This application outlines plans to develop artificial neural networks that will identify open reading frames in genomic sequence information. The investigators list eleven specific tasks that will train and design multilayer perceptron networks using an evolutionary optimization technique in a suitable user-friendly format. The investigators predict that these networks will provide a more effective computer-based system for automatic pattern recognition that will be useful in genome sequencing projects. PROPOSED COMMERCIAL APPLICATION: The commercial application will focus an development of software for gene recognition in unannotated DNA sequences.  artificial intelligence; computer program /software; computer system design /evaluation; nucleic acid sequence; nucleic acid structure; open reading frames ID OF FUNCTIONAL/STRUCTURAL ELEMENTS IN DNA SEQUENCES,DESCRIPTION: (adapted from the investigator's abstract) This application outlines plans to develop artificial neural networks that will identify open reading frames in genomic sequence information. The investigators list eleven specific tasks that will train and design multilayer perceptron networks using an evolutionary optimization technique in a suitable user-friendly format. The investigators predict that these networks will provide a more effective computer-based system for automatic pattern recognition that will be useful in genome sequencing projects. PROPOSED COMMERCIAL APPLICATION: The commercial application will focus an development of software for gene recognition in unannotated DNA sequences. ,2867212,R43HG002004,['R43HG002004'],HG,https://reporter.nih.gov/project-details/2867212,R43,1999,93079,-0.021959775251469475
"DESCRIPTION (Adapted from the applicant's abstract):  Neuroblastoma is the      
most common extra cranial tumor of childhood, and cases are highly              
heterogenous with regard to clinical behavior.  Although patient groups with    
different expected survivals can be identified by clinical staging at           
diagnosis, individual clinically defined risk groups include patients with      
quite different outcomes.  Inter- and intra-stage diversity provides            
opportunities for identifying molecular genetic and biologic properties of      
tumors that are associated with treatment outcome.  Such correlations can       
identify risk groups that otherwise are not recognizable, thus aiding in the    
interpretation of clinical studies, and they can provide new criteria for       
more appropriate therapy assignment.  They may also suggest new approaches      
to therapy.  The long-term goal of the proposed studies is to develop tests     
that improve definition of risk groups so that the most appropriate and         
effective therapy can be given to each patient.                                 
                                                                                
The hypothesis of this application is that subsets of neuroblastomas can be     
identified by evaluating (1) genes that are critically involved in              
neuroblastoma growth, differentiation, and survival and (2) the ability of      
tumor cells to grow continuously in vitro.  Plans are to build upon previous    
studies of N-myc proto-oncogene expression in neuroblastoma and of the          
neurotrophins (nerve growth factor, NGF, and brain derived neurotrophic         
factor, BDNF) and their receptors (e.g., TrkAl), and of tumor cell growth in    
vitro, which demonstrated the potential importance of these markers in          
prognostication.                                                                
                                                                                
The specific aims are as follows:  (1) determine if tumor phenotype defined     
by MYCN gene amplification and expression, TrkA expression, telomerase RNA      
expression, and growth in vitro correlates with disease progression during      
or after therapy; (2) develop multivariate statistical models for predicting    
outcome based upon clinical presentation (stage and age) and laboratory data    
(tumor MYCN gene amplification, TrkA expression, telomerase RNA expression,     
growth in vitro, and histopathology); (3) determine in a pilot study if         
expression of other neurotrophins and their receptors defines risk groups.      
If so, include these in the above analyses.                                     
                                                                                
The CCG performs phase III studies in which newly diagnosed patients receive    
therapy according to risk classification, which is currently based upon         
clinical stage, age, histopathology, and N-myc gene amplification status.       
Approximately 200 patients are registered annually in studies for low,          
intermediate, and high-risk neuroblastoma.  Tumor tissues including (in some    
patients) bone marrow with tumor are obtained at diagnosis prospectively.       
Tumors are tested to determine their phenotype with regard to N-myc             
amplification, trk RNA expression level and pattern of expression, BDNF RNA     
expression, telomerase RNA expression and activity, chromosome 1p deletions,    
and tumor cell growth in vitro.  The goal is to determine if these              
laboratory test results can identify clinically important but small subsets     
of patients who may benefit from different therapy.  If preliminary studies     
indicate that specific tests are particularly important in predicting           
prognosis, large-scale studies will be conducted to better define the value     
of these new tests (e.g., their ability to identify patients who will likely    
fail treatment).                                                                
                                                                                
It is anticipated that these studies will improve prognostication and that      
they may contribute to development of novel and possibly more effective         
therapies for high-risk patients, while diminishing the risks of treatment      
for low risk patients.                                                          
 adolescence (12-20); artificial intelligence; cancer risk; child (0-11); clinical research; cooperative study; gene expression; growth factor receptors; human subject; human therapy evaluation; loss of heterozygosity; mathematical model; molecular oncology; neoplasm /cancer chemotherapy; neoplasm /cancer genetics; neuroblastoma; neurotrophic factors; pediatric neoplasm /cancer; polymerase chain reaction; protooncogene; receptor expression; telomerase; telomere; tumor suppressor genes CLINICAL CORRELATIVE STUDIES OF NEUROBLASTOMA","DESCRIPTION (Adapted from the applicant's abstract):  Neuroblastoma is the      
most common extra cranial tumor of childhood, and cases are highly              
heterogenous with regard to clinical behavior.  Although patient groups with    
different expected survivals can be identified by clinical staging at           
diagnosis, individual clinically defined risk groups include patients with      
quite different outcomes.  Inter- and intra-stage diversity provides            
opportunities for identifying molecular genetic and biologic properties of      
tumors that are associated with treatment outcome.  Such correlations can       
identify risk groups that otherwise are not recognizable, thus aiding in the    
interpretation of clinical studies, and they can provide new criteria for       
more appropriate therapy assignment.  They may also suggest new approaches      
to therapy.  The long-term goal of the proposed studies is to develop tests     
that improve definition of risk groups so that the most appropriate and         
effective therapy can be given to each patient.                                 
                                                                                
The hypothesis of this application is that subsets of neuroblastomas can be     
identified by evaluating (1) genes that are critically involved in              
neuroblastoma growth, differentiation, and survival and (2) the ability of      
tumor cells to grow continuously in vitro.  Plans are to build upon previous    
studies of N-myc proto-oncogene expression in neuroblastoma and of the          
neurotrophins (nerve growth factor, NGF, and brain derived neurotrophic         
factor, BDNF) and their receptors (e.g., TrkAl), and of tumor cell growth in    
vitro, which demonstrated the potential importance of these markers in          
prognostication.                                                                
                                                                                
The specific aims are as follows:  (1) determine if tumor phenotype defined     
by MYCN gene amplification and expression, TrkA expression, telomerase RNA      
expression, and growth in vitro correlates with disease progression during      
or after therapy; (2) develop multivariate statistical models for predicting    
outcome based upon clinical presentation (stage and age) and laboratory data    
(tumor MYCN gene amplification, TrkA expression, telomerase RNA expression,     
growth in vitro, and histopathology); (3) determine in a pilot study if         
expression of other neurotrophins and their receptors defines risk groups.      
If so, include these in the above analyses.                                     
                                                                                
The CCG performs phase III studies in which newly diagnosed patients receive    
therapy according to risk classification, which is currently based upon         
clinical stage, age, histopathology, and N-myc gene amplification status.       
Approximately 200 patients are registered annually in studies for low,          
intermediate, and high-risk neuroblastoma.  Tumor tissues including (in some    
patients) bone marrow with tumor are obtained at diagnosis prospectively.       
Tumors are tested to determine their phenotype with regard to N-myc             
amplification, trk RNA expression level and pattern of expression, BDNF RNA     
expression, telomerase RNA expression and activity, chromosome 1p deletions,    
and tumor cell growth in vitro.  The goal is to determine if these              
laboratory test results can identify clinically important but small subsets     
of patients who may benefit from different therapy.  If preliminary studies     
indicate that specific tests are particularly important in predicting           
prognosis, large-scale studies will be conducted to better define the value     
of these new tests (e.g., their ability to identify patients who will likely    
fail treatment).                                                                
                                                                                
It is anticipated that these studies will improve prognostication and that      
they may contribute to development of novel and possibly more effective         
therapies for high-risk patients, while diminishing the risks of treatment      
for low risk patients.                                                          
",2871801,R01CA060104,['R01CA060104'],CA,https://reporter.nih.gov/project-details/2871801,R01,1999,405392,-0.03336821307725751
"This project will make available to the social science, public health, and other research communities a Super Sample of data from the 1990 U.S. Census long form file. The file will comprise 15.9% of households and persons in the U.S. The National Institute for Child Health and Human Development has contracted with the Bureau of the Census to have the file created.  In Phase I, Public Data Queries, Inc., will: (1) provide access to the NICHD 1990 Super Sample to a wide audience of users; (2) make value-added improvements to the Super Sample data and documentation; (3) incorporate expert and intelligent tools in the PDQ-Explore interface to facilitate use of the Super Sample data; (4) begin to investigate the feasibility and practicality of extending the tabulation/description/extraction capabilities into a full statistical system. The project encompasses two primary tasks. The first is to make more than fifty million housing and person-level microdata records interactively accessible to researchers and other users through the PDQ- Explore information system. The second is to identify the types of user support that might be provided through expert systems and intelligent agents and to investigate the feasibility of implementing these tools with PDQ-Explore. PROPOSED COMMERCIAL APPLICATION: Distribution of census data is a marketable service. Addition of the 1990 Super Sample to the PDQ-Explore data base along with enhanced documentation, an expert system, and linkage to a statistical software system with dramatically improve access to the 1990 Super Sample and the information within it and other census long form files that may become available. The added features will also increase the marketability of PDQ-Explore.  behavioral /social science; computer data analysis; computer program /software; data management; human data; human population distribution; human population study; information systems; public health; statistics /biometry; technology /technique development DISTRIBUTION AND SUPPORT OF THE CENSUS SUPER SAMPLE","This project will make available to the social science, public health, and other research communities a Super Sample of data from the 1990 U.S. Census long form file. The file will comprise 15.9% of households and persons in the U.S. The National Institute for Child Health and Human Development has contracted with the Bureau of the Census to have the file created.  In Phase I, Public Data Queries, Inc., will: (1) provide access to the NICHD 1990 Super Sample to a wide audience of users; (2) make value-added improvements to the Super Sample data and documentation; (3) incorporate expert and intelligent tools in the PDQ-Explore interface to facilitate use of the Super Sample data; (4) begin to investigate the feasibility and practicality of extending the tabulation/description/extraction capabilities into a full statistical system. The project encompasses two primary tasks. The first is to make more than fifty million housing and person-level microdata records interactively accessible to researchers and other users through the PDQ- Explore information system. The second is to identify the types of user support that might be provided through expert systems and intelligent agents and to investigate the feasibility of implementing these tools with PDQ-Explore. PROPOSED COMMERCIAL APPLICATION: Distribution of census data is a marketable service. Addition of the 1990 Super Sample to the PDQ-Explore data base along with enhanced documentation, an expert system, and linkage to a statistical software system with dramatically improve access to the 1990 Super Sample and the information within it and other census long form files that may become available. The added features will also increase the marketability of PDQ-Explore. ",2867614,R43HD037738,['R43HD037738'],HD,https://reporter.nih.gov/project-details/2867614,R43,1999,92481,-0.02195731362119251
"DESCRIPTION (Taken from application abstract):  Reminder systems are expert     
 artificial intelligence; automated medical record system; behavioral /social science research tag; belief; computer assisted medical decision making; computer assisted patient care; computer system design /evaluation; health care facility information system; health services research tag; human data; patient care management BELIEF NETWORK BASED REMINDER SYSTEMS THAT LEARN","DESCRIPTION (Taken from application abstract):  Reminder systems are expert     
",2872989,R29LM006233,['R29LM006233'],LM,https://reporter.nih.gov/project-details/2872989,R29,1999,100760,-0.03896388799133855
"Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
 AIDS; Cryptococcus neoformans; antibody specificity; antifungal antibody; artificial intelligence; carbohydrate structure; cell wall; epitope mapping; fungal antigens; information systems; microorganism classification; nuclear magnetic resonance spectroscopy; opportunistic infections; polysaccharides CRYPTOCOCCUS NEOFORMANS--EPITOPE ANTIBODIES & STRUCTURE","Cryptococcus neoformans in a pathogenic yeast that causes life-                 
threatening meningoencephalitis in immunocompromised patients.                  
The incidence of crytococcosis has increased dramatically in                    
recent years as a consequence of the AIDS epidemic. The major                   
capsular polysaccharide, glucuronoxylomannan (GXM), is                          
serotype determinant of these organisms.  The capsular                          
polysaccharides are important contributors to the virulence of C.               
Neoformans. GXM is antiphagocytic and poorly immunogenic.  In                   
vitro, GNX inhibits leukocyte migration, enhances HIV infection                 
in human lymphocytes, and promotes L-selectin shedding from                     
neutrophils. The sensitivity and resolution of the analysis of GXM              
structure have improved due to the introduction of 1H Nuclear                   
Magnetic Resonance Spectroscopy (NMR) in one and two                            
dimensions.  A database of 1H NMR chemical shifts has been                      
established for the structural triad based on three  -(1-3)-D-                  
mannosyl residues.  The chemical shifts of the mannosyl residues                
of the various triads serve as reporter groups for the identification           
and quantitation of seven structural triads as they occur in any                
GXM. The data-chemical shifts relative intensities, and peak areas              
of the reporter groups-are being placed in a relational database                
program.  The specific aims for the next period are: (1) To use                 
the data to create a chemotyping scheme based on the quantitative               
distribution of the mannosyl triads in GXMS; (2) To use the data                
in aim one to develop a computer based neural network for the                   
rapid chemotyping of C. Neoformans; (3) To determine the                        
solution conformation of GXMs by high fields, multiple                          
dimensional NMR since the immune response to C. Neoformans is                   
intimately related to the three dimensional structure of GXM; (4)               
To determine the exact linkage dispositions of the 0-acetyl                     
substituent, and indispensable component of the conformational                  
epitope recognized by antibodies; (5) To characterize the                       
individual Factor Specific antibodies obtained by tandem-column                 
affinity chromatography using-GXM-affinity matrices; (6) To                     
determine the fine structures of the mannoproteins from C.                      
Neoformans Cap67: (7) To determine if polysaccharide is                         
covalently linked to the cell wall of C. Neoformans.  This                      
information will be used to foster more precise investigations of               
the pathology, treatment, mechanisms of virulence, and prevention               
of cryptococcosis.                                                              
",2882173,R01AI031769,['R01AI031769'],AI,https://reporter.nih.gov/project-details/2882173,R01,1999,233431,-0.04386970145405733
"Predictive models that generate estimate probabilities for medical outcomes have become widely used in health services research, in health policy, and increasingly, for the assessment of health care and for real-time decision support.  Logistic regression models for medical events are central to most probabilistic predictive clinical decision aids and are fundamental to comparative analyses of medical care based on risk-adjusted events.  In such applications, inaccurate assessment of patient risk can have significant health care and health policy implications. New computer-based modeling techniques including generalized additive models, classification trees, and neural networks may potentially capture information that regression methods may miss or misrepresent.  However, these methods use very local information in model construction and may be overfit to the sample data and thus not transport well to new settings.  In years 1-3, we investigated the relative accuracy of predictions made by these modeling methods under a variety of data structures, including the presence of outliers and missing data. For many of these data structures we found that the more ""local"" procedures frequently did not generalize to new test data as well as traditional regression methods.  However, our results suggest that as sample size and data complexity increases the performance of these procedures may substantially improved. Thus, to test these findings under more general conditions, we now propose two additional years of research to 1) rigorously assess the relative predictive performance and transportability of other new innovative modeling methods and of original hybrid model construction methods; 2) systematically investigate the relative predictive performance and model transportability of modeling methods applied to large and complex data structures; and 3) explore and assess procedures for handling outliers and missing data for classification trees and neural networks. The completion of the proposed work will result in the first systematic exploration of the factors affecting the predictive performance of the major modeling methods used to predict medical outcomes, and the comparative performance of models constructed by these methods on the extremely large data sets of the type that are becoming increasing available to researchers.  artificial intelligence; computational neuroscience; computer assisted medical decision making; computer simulation; health care facility information system; human data; information system analysis; mathematical model; model design /development; outcomes research; prognosis; statistics /biometry ASSESSING NEW MATHEMATICAL MODELS FOR MEDICAL EVENTS","Predictive models that generate estimate probabilities for medical outcomes have become widely used in health services research, in health policy, and increasingly, for the assessment of health care and for real-time decision support.  Logistic regression models for medical events are central to most probabilistic predictive clinical decision aids and are fundamental to comparative analyses of medical care based on risk-adjusted events.  In such applications, inaccurate assessment of patient risk can have significant health care and health policy implications. New computer-based modeling techniques including generalized additive models, classification trees, and neural networks may potentially capture information that regression methods may miss or misrepresent.  However, these methods use very local information in model construction and may be overfit to the sample data and thus not transport well to new settings.  In years 1-3, we investigated the relative accuracy of predictions made by these modeling methods under a variety of data structures, including the presence of outliers and missing data. For many of these data structures we found that the more ""local"" procedures frequently did not generalize to new test data as well as traditional regression methods.  However, our results suggest that as sample size and data complexity increases the performance of these procedures may substantially improved. Thus, to test these findings under more general conditions, we now propose two additional years of research to 1) rigorously assess the relative predictive performance and transportability of other new innovative modeling methods and of original hybrid model construction methods; 2) systematically investigate the relative predictive performance and model transportability of modeling methods applied to large and complex data structures; and 3) explore and assess procedures for handling outliers and missing data for classification trees and neural networks. The completion of the proposed work will result in the first systematic exploration of the factors affecting the predictive performance of the major modeling methods used to predict medical outcomes, and the comparative performance of models constructed by these methods on the extremely large data sets of the type that are becoming increasing available to researchers. ",2840557,R01LM005607,['R01LM005607'],LM,https://reporter.nih.gov/project-details/2840557,R01,1999,407380,-0.056042601713616226
"Colorectal carcinoma is the second leading cause of cancer deaths in the        
United States today.  In an effort to reduce mortality, Congress                
recently included a provision in the Balanced Budget Act of 1997 to             
support screening colonoscopy as a means for early detection and removal        
of colorectal polyps, the precursors to cancer.  In this country alone,         
more than 68 million people are eligible for colorectal screening, but          
the majority are unlikely to comply with screening recommendations              
because of the costs, risks, discomfort, and inconvenience associated           
with traditional endoscopy.  Furthermore, even if a small fraction of           
eligible persons are examined, the number of available                          
gastroenterologists would be insufficient to perform so many procedures.        
                                                                                
We have developed a new technique, called virtual colonoscopy (VC), as          
an alternative to screening diagnostic colonoscopy (DC). The procedure          
consists of cleansing a patient's colon, inflating the colon with air,          
scanning the abdomen with helical computed tomography (CT), and                 
generating a rapid sequence of three-dimensional (3D) images of the             
colon by means of virtual reality computer technology.  Although VC             
makes possible the visualization of 3D images of the colon in a manner          
similar to that of DC, a correct diagnosis depends upon a physician's           
ability to identify small and sometimes subtle polyps within hundreds           
of 3D images.  The absence of visual cues that normally occur with DC           
makes VC interpretation tedious and susceptible to error.                       
                                                                                
With support from a National Science Foundation (NSF) grant, we have            
developed a computer-assisted polyp detection (CAPD) system that                
calculates areas of abnormal colon wall thickness in helical CT image           
data in order to highlight potential polyps in the 3D images.  A                
physician ultimately determines if each detected lesion represents a            
true abnormality.  Although we have found CAPD to be sensitive for              
finding subtle abnormalities, poor specificity can be attributed to             
several obstacles, including imprecise image segmentation, limited              
feature analysis, and suboptimal bowel preparation prior to helical CT          
scanning.  With these challenges in mind, we propose research to perfect        
CAPD. Our specific aims are as follows: 1. To develop an image                  
segmentation algorithm that accurately isolates the colon from helical          
CT image data; 2. To improve our polyp detection algorithm with expanded        
feature analysis and artificial intelligence methods; 3. To optimize            
bowel preparation with digital subtraction of opacified feces and               
controlled gas distention; and 4. To validate the accuracy of VC, with          
the modifications achieved in the stated aims, by comparing the results         
of VC and DC in 200 patients undergoing usual-care colonoscopy.                 
                                                                                
If VC with CAPD proves accurate and efficient in the diagnosis of               
colorectal polyps, it could evolve into a simple laboratory test,               
thereby meeting the demand for worldwide colorectal cancer screening.           
 bioimaging /biomedical imaging; biomedical equipment development; clinical research; colon neoplasms; colon polyp; computed axial tomography; computer assisted diagnosis; computer simulation; diagnosis design /evaluation; endoscopy; gastrointestinal imaging /visualization; human subject; image enhancement; mathematical model; model design /development; neoplasm /cancer diagnosis IMPROVING VIRTUAL COLONOSCOPY WITH COMPUTER DETECTION","Colorectal carcinoma is the second leading cause of cancer deaths in the        
United States today.  In an effort to reduce mortality, Congress                
recently included a provision in the Balanced Budget Act of 1997 to             
support screening colonoscopy as a means for early detection and removal        
of colorectal polyps, the precursors to cancer.  In this country alone,         
more than 68 million people are eligible for colorectal screening, but          
the majority are unlikely to comply with screening recommendations              
because of the costs, risks, discomfort, and inconvenience associated           
with traditional endoscopy.  Furthermore, even if a small fraction of           
eligible persons are examined, the number of available                          
gastroenterologists would be insufficient to perform so many procedures.        
                                                                                
We have developed a new technique, called virtual colonoscopy (VC), as          
an alternative to screening diagnostic colonoscopy (DC). The procedure          
consists of cleansing a patient's colon, inflating the colon with air,          
scanning the abdomen with helical computed tomography (CT), and                 
generating a rapid sequence of three-dimensional (3D) images of the             
colon by means of virtual reality computer technology.  Although VC             
makes possible the visualization of 3D images of the colon in a manner          
similar to that of DC, a correct diagnosis depends upon a physician's           
ability to identify small and sometimes subtle polyps within hundreds           
of 3D images.  The absence of visual cues that normally occur with DC           
makes VC interpretation tedious and susceptible to error.                       
                                                                                
With support from a National Science Foundation (NSF) grant, we have            
developed a computer-assisted polyp detection (CAPD) system that                
calculates areas of abnormal colon wall thickness in helical CT image           
data in order to highlight potential polyps in the 3D images.  A                
physician ultimately determines if each detected lesion represents a            
true abnormality.  Although we have found CAPD to be sensitive for              
finding subtle abnormalities, poor specificity can be attributed to             
several obstacles, including imprecise image segmentation, limited              
feature analysis, and suboptimal bowel preparation prior to helical CT          
scanning.  With these challenges in mind, we propose research to perfect        
CAPD. Our specific aims are as follows: 1. To develop an image                  
segmentation algorithm that accurately isolates the colon from helical          
CT image data; 2. To improve our polyp detection algorithm with expanded        
feature analysis and artificial intelligence methods; 3. To optimize            
bowel preparation with digital subtraction of opacified feces and               
controlled gas distention; and 4. To validate the accuracy of VC, with          
the modifications achieved in the stated aims, by comparing the results         
of VC and DC in 200 patients undergoing usual-care colonoscopy.                 
                                                                                
If VC with CAPD proves accurate and efficient in the diagnosis of               
colorectal polyps, it could evolve into a simple laboratory test,               
thereby meeting the demand for worldwide colorectal cancer screening.           
",2849551,R01CA078485,['R01CA078485'],CA,https://reporter.nih.gov/project-details/2849551,R01,1999,470758,-0.031196961029689968
"Ovarian cancer is the leading cause of death from gynecologic cancer in the US. For most patients, the disease is first diagnosed at an advanced stage, and the 5-year survival rate is low (<30%). Despite incremental improvement in chemotherapy, the cure rate has not improved significantly in the past decades. The dramatic difference in long-term survival between patients with local disease (80-90%) and those with distant metastases (5- 20%) suggests the need for a non-invasive, yet effective test applicable to at-risk population groups to detect ovarian cancer in early stages. Building upon prior research in differentiating malignant from benign ovarian masses, this project seeks to apply artificial neural network (ANN) technology to the problem of screening for early-stage ovarian cancer based a variety of serum markers and other clinical inputs. Phase I will constitute a pilot project that assesses feasibility by (l) assembling existing data from collaborating organizations, (2) analyzing the predictive value of relevant biomarkers, (3) developing a preliminary ANN, and (4) validating the ANN using independent test data. If successful in Phase I, Phase II activities will be proposed to develop a production version of the screening system and initiate broad-scale validation through multiple clinical studies. PROPOSED COMMERCIAL APPLICATIONS: ANN software capable of detecting early-stage ovarian cancer with sufficient improvement in specificity, sensitivity, and predictive value over alternative techniques would have clear commercial value in screening high-risk populations. Horus presently offers as a commercial product an Internet based clinical information processing service, called ProstAsure, developed using ANN technology, for the detection of prostate cancer.  artificial intelligence; biomarker; computer assisted diagnosis; computer program /software; computer system design /evaluation; diagnosis design /evaluation; early diagnosis; human data; neoplasm /cancer diagnosis; ovary neoplasms ARTIFICIAL NEURAL NETWORK SOFTWARE FOR EARLY DETECTION O","Ovarian cancer is the leading cause of death from gynecologic cancer in the US. For most patients, the disease is first diagnosed at an advanced stage, and the 5-year survival rate is low (<30%). Despite incremental improvement in chemotherapy, the cure rate has not improved significantly in the past decades. The dramatic difference in long-term survival between patients with local disease (80-90%) and those with distant metastases (5- 20%) suggests the need for a non-invasive, yet effective test applicable to at-risk population groups to detect ovarian cancer in early stages. Building upon prior research in differentiating malignant from benign ovarian masses, this project seeks to apply artificial neural network (ANN) technology to the problem of screening for early-stage ovarian cancer based a variety of serum markers and other clinical inputs. Phase I will constitute a pilot project that assesses feasibility by (l) assembling existing data from collaborating organizations, (2) analyzing the predictive value of relevant biomarkers, (3) developing a preliminary ANN, and (4) validating the ANN using independent test data. If successful in Phase I, Phase II activities will be proposed to develop a production version of the screening system and initiate broad-scale validation through multiple clinical studies. PROPOSED COMMERCIAL APPLICATIONS: ANN software capable of detecting early-stage ovarian cancer with sufficient improvement in specificity, sensitivity, and predictive value over alternative techniques would have clear commercial value in screening high-risk populations. Horus presently offers as a commercial product an Internet based clinical information processing service, called ProstAsure, developed using ANN technology, for the detection of prostate cancer. ",2784815,R43CA080459,['R43CA080459'],CA,https://reporter.nih.gov/project-details/2784815,R43,1999,87291,-0.08332251982492558
"The development of software programs for the interpretation of Lyme Western Blots is proposed.  The programs will analyze Western Blot band patterns scanned by a digital camera and interpret the test result as positive or negative. Two parallel, competing approaches will be taken: one based on statistical analysis of band data and the other on a neural network model. The statistical program will incorporate the CDC/ASTPHLD interpretive criteria for Lyme Western Blots, and will offer greater objectivity than a human interpreter. In contrast, the neural network program is capable of ""learning"" and improving its performance, and will develop its own criteria for interpretation through the analysis of large numbers of positive and negative samples. The neural network thus promises to surpass human accuracy in Lyme Western Blot interpretation, and provides a platform with which to develop interpretive capability for other diagnostic western blots. The program will be implemented on Immunetics' CodaVision(TM) analyzer, a fully automated instrument capable of performing 32 Western Blot assays in 30 minutes. This software program will resolve the issues of subjectivity, reader-to--reader inconsistency and interassay variability in the interpretation of Lyme Western Blots which have constrained their use as routine clinical tests. PROPOSED COMMERCIAL APPLICATIONS: The proposed software for interpretation of Lyme Western Blot tests will open this highly accurate technique to routine clinical laboratories which depend on automation to perform sophisticated tests. Offered together with Immunetics' CodaVision instrument, it will meet the growing demand for accurate Lyme disease testing. As the first such fully automated system on the market, it is anticipated that it will rapidly become the commercial standard for Lyme disease testing.  Borrelia; Lyme disease; artificial intelligence; communicable disease diagnosis; computer assisted diagnosis; computer data analysis; computer program /software; computer system design /evaluation; diagnosis design /evaluation; digital imaging; human tissue; statistics /biometry; western blottings LYME WESTERN BLOT INTERPRETIVE SOFTWARE","The development of software programs for the interpretation of Lyme Western Blots is proposed.  The programs will analyze Western Blot band patterns scanned by a digital camera and interpret the test result as positive or negative. Two parallel, competing approaches will be taken: one based on statistical analysis of band data and the other on a neural network model. The statistical program will incorporate the CDC/ASTPHLD interpretive criteria for Lyme Western Blots, and will offer greater objectivity than a human interpreter. In contrast, the neural network program is capable of ""learning"" and improving its performance, and will develop its own criteria for interpretation through the analysis of large numbers of positive and negative samples. The neural network thus promises to surpass human accuracy in Lyme Western Blot interpretation, and provides a platform with which to develop interpretive capability for other diagnostic western blots. The program will be implemented on Immunetics' CodaVision(TM) analyzer, a fully automated instrument capable of performing 32 Western Blot assays in 30 minutes. This software program will resolve the issues of subjectivity, reader-to--reader inconsistency and interassay variability in the interpretation of Lyme Western Blots which have constrained their use as routine clinical tests. PROPOSED COMMERCIAL APPLICATIONS: The proposed software for interpretation of Lyme Western Blot tests will open this highly accurate technique to routine clinical laboratories which depend on automation to perform sophisticated tests. Offered together with Immunetics' CodaVision instrument, it will meet the growing demand for accurate Lyme disease testing. As the first such fully automated system on the market, it is anticipated that it will rapidly become the commercial standard for Lyme disease testing. ",2784254,R43AI044541,['R43AI044541'],AI,https://reporter.nih.gov/project-details/2784254,R43,1999,93458,-0.16288905405315293
"This proposed research will conduct a randomized trials to test an innovative and interactive and proactive secondary prevention program for college students to reduce high risk alcohol behaviors and minimize alcohol related problems. High volume episodic or ""binge"" drinking is a major problem on most college campuses. But most students do not perceive their high risk alcohol behaviors as problematic, and few are prepared to change those behaviors. Effective interventions are needed that can impact on an entire population of students at risk for alcohol problems. Such interventions should be theory based and capable of being delivered at a low cost to large segments of the at-risk population. The proposed study is an evaluation of the efficacy of a computerized expert system intervention designed to reduce alcohol problem behaviors, and is based on the conceptual framework of the Transtheoretical Model. Over the past decade researchers at the University of Rhode Island (URI) have developed and tested self-help prevention interventions using expert system technology that provides individualized feedback on some of the most powerful variables known to influence behavior change. These interactive interventions are matched to an individual's stage of motivational readiness to change, and have demonstrated unprecedented impacts in a series of large scale clinical trials of smoking cessation and other health behaviors. This study focuses on students during a high risk transition period-the freshman and sophomore years, and will target drinkers and second year URI students will be assessed by telephone to determine eligibility. 1,420 subjects are expected to participate. A three group randomized controlled design will a brief intervention condition that provides three report expert system feedback reports with two control conditions; a minimal assessment post-test only control and an assessment only condition. All groups will be assessed at 12, 18 and 24 months. In addition, the intervention and the assessment only control will be assessed at baseline, 3 and 6 months. Multiple measures of drinking rates, alcohol behaviors, and alcohol-related problems will be the primary outcomes for the efficacy evaluation. This project is designed to be rapidly disseminable so that results may be used in public health efforts to decrease the incidence of, and problems associated with, high risk drinking among college students.  alcoholic beverage consumption; alcoholism /alcohol abuse education; alcoholism /alcohol abuse prevention; artificial intelligence; behavior modification; behavioral /social science research tag; clinical research; computer assisted instruction; computer assisted patient care; high risk behavior /lifestyle; human subject; questionnaires; self care; substance abuse related behavior; university student; young adult human (21-34) A PROACTIVE INDIVIDUALIZED PROGRAM FOR COLLEGE DRINKERS","This proposed research will conduct a randomized trials to test an innovative and interactive and proactive secondary prevention program for college students to reduce high risk alcohol behaviors and minimize alcohol related problems. High volume episodic or ""binge"" drinking is a major problem on most college campuses. But most students do not perceive their high risk alcohol behaviors as problematic, and few are prepared to change those behaviors. Effective interventions are needed that can impact on an entire population of students at risk for alcohol problems. Such interventions should be theory based and capable of being delivered at a low cost to large segments of the at-risk population. The proposed study is an evaluation of the efficacy of a computerized expert system intervention designed to reduce alcohol problem behaviors, and is based on the conceptual framework of the Transtheoretical Model. Over the past decade researchers at the University of Rhode Island (URI) have developed and tested self-help prevention interventions using expert system technology that provides individualized feedback on some of the most powerful variables known to influence behavior change. These interactive interventions are matched to an individual's stage of motivational readiness to change, and have demonstrated unprecedented impacts in a series of large scale clinical trials of smoking cessation and other health behaviors. This study focuses on students during a high risk transition period-the freshman and sophomore years, and will target drinkers and second year URI students will be assessed by telephone to determine eligibility. 1,420 subjects are expected to participate. A three group randomized controlled design will a brief intervention condition that provides three report expert system feedback reports with two control conditions; a minimal assessment post-test only control and an assessment only condition. All groups will be assessed at 12, 18 and 24 months. In addition, the intervention and the assessment only control will be assessed at baseline, 3 and 6 months. Multiple measures of drinking rates, alcohol behaviors, and alcohol-related problems will be the primary outcomes for the efficacy evaluation. This project is designed to be rapidly disseminable so that results may be used in public health efforts to decrease the incidence of, and problems associated with, high risk drinking among college students. ",2907302,R01AA012068,['R01AA012068'],AA,https://reporter.nih.gov/project-details/2907302,R01,1999,328580,-0.04059576706968617
"DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be      
taken to advance our understanding of the genetic and associated co-            
variates affecting the inheritance of complex diseases. To that end, this       
proposal will develop improved quantitative methods to detect genetic           
factors contributing to increased susceptibility to complex disorders and       
implement these methods in software for distribution to the research            
community.                                                                      
                                                                                
The methods will concentrate on the use of classification techniques            
applied to allele sharing data and other risk factors which affect the          
trait. Allele sharing methods for mapping genes will be extended to             
include the classification methods known as latent class models, cluster        
analysis, and artificial neural networks, as well as a novel use of             
logistic regression Co-variates such as gender, parental diagnosis, or          
other concomitant factors will be systematically studied through                
applications to both stimulated and existing data sets. An additional goal      
is to determine the optimal distribution of relative pairs (e.g. siblings,      
first cousins) for these methods. Of great importance to this proposal is       
the development of well-documented, user-friendly software and                  
documentation which will be distributed to the scientific community via         
the Internet. Existing software developed by the PI will be extensively         
expanded for latent class models. Existing cluster analysis software will       
be modified and combined for ease of use.                                       
                                                                                
This proposal consists of theoretical exploration, computer simulation,         
data analysis, and software development. First, solutions of theoretical        
questions relating to classification techniques will be pursued; second,        
adaptation of computer programs to implement the analytic methods, and          
investigation into alternative research strategies will be accomplished.        
The new strategies will be applied to stimulated data, and finally, to          
existing data sets of pedigrees in which a complex trait has been               
diagnosed. Findings from this research may contribute to the ability to         
locate susceptibility loci in complex traits and to the clarification of        
those etiological mechanisms responsible for susceptibility.                    
 alleles; analytical method; artificial intelligence; biomedical resource; computer program /software; computer simulation; data collection methodology /evaluation; disease /disorder classification; disease /disorder etiology; family genetics; gene environment interaction; gene expression; genetic disorder; genetic disorder diagnosis; genetic mapping; genetic markers; genetic susceptibility; human data; mathematical model; model design /development; quantitative trait loci; statistics /biometry CLASSIFICATION METHODS FOR DETECTING DISEASE LOCI","DESCRIPTION (Adapted from the Investigator's Abstract): Bold steps must be      
taken to advance our understanding of the genetic and associated co-            
variates affecting the inheritance of complex diseases. To that end, this       
proposal will develop improved quantitative methods to detect genetic           
factors contributing to increased susceptibility to complex disorders and       
implement these methods in software for distribution to the research            
community.                                                                      
                                                                                
The methods will concentrate on the use of classification techniques            
applied to allele sharing data and other risk factors which affect the          
trait. Allele sharing methods for mapping genes will be extended to             
include the classification methods known as latent class models, cluster        
analysis, and artificial neural networks, as well as a novel use of             
logistic regression Co-variates such as gender, parental diagnosis, or          
other concomitant factors will be systematically studied through                
applications to both stimulated and existing data sets. An additional goal      
is to determine the optimal distribution of relative pairs (e.g. siblings,      
first cousins) for these methods. Of great importance to this proposal is       
the development of well-documented, user-friendly software and                  
documentation which will be distributed to the scientific community via         
the Internet. Existing software developed by the PI will be extensively         
expanded for latent class models. Existing cluster analysis software will       
be modified and combined for ease of use.                                       
                                                                                
This proposal consists of theoretical exploration, computer simulation,         
data analysis, and software development. First, solutions of theoretical        
questions relating to classification techniques will be pursued; second,        
adaptation of computer programs to implement the analytic methods, and          
investigation into alternative research strategies will be accomplished.        
The new strategies will be applied to stimulated data, and finally, to          
existing data sets of pedigrees in which a complex trait has been               
diagnosed. Findings from this research may contribute to the ability to         
locate susceptibility loci in complex traits and to the clarification of        
those etiological mechanisms responsible for susceptibility.                    
",2894288,R01AA012239,['R01AA012239'],AA,https://reporter.nih.gov/project-details/2894288,R01,1999,175004,-0.04948810711854036
"The brain is at risk of serious cerebrovascular insult during the               
400,000 cardiopulmonary bypass (CPB) surgeries performed annually in the        
United States.  There is strong clinical evidence that over two-thirds          
of patients exhibit neurologic or neuropsychologic (NP) Postoperative           
deficits caused by emboli passing to the brain during surgery.  There           
is a critical need for a device that can not only detect emboli, but            
also classify them as to type.  Classification is critical for                  
correlating neurological deficits with emboli type, determining the             
source of the emboli, and subsequently changing surgical procedures             
and/or administering neuroprotective agents to minimize brain injury.           
Continuous-wave Doppler ultrasound equipment can detect emboli, but             
cannot provide the information needed to classify emboli composition.           
In Phase I, ORINCON and the Bowman Gray School of Medicine utilized             
broadband pulse-echo ultrasound and an artificial neural network to             
demonstrate significant classification capability on in-vitro data.  In         
Phase II, we will refine and integrate components from Phase I into a           
PC-based system capable of accurate, real-time emboli detection and             
classification in both extracorporeal pump circuits and the carotid             
artery.  Extensive in-vitro and in-vivo data will be collected to permit        
refinement and thorough evaluation of classification capabilities.              
Clinical studies will be performed to correlate neuropsychologic                
deficits with embolus composition.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is immediate commercial potential for a low-cost (less than 25K),         
real-time automated emboli detection and classification system.  The            
market includes manufacturers of cardiopulmonary pump circuit devices           
(Medtronics, Pall, Cobe, Sarns), hundreds of medical centers performing         
cardiopulmonary bypass, and producers of neuroprotective drugs (Astra,          
Bayer, Sterling).  A letter from Medtronics expressing this commercial          
potential is enclosed.                                                          
 artificial intelligence; bioimaging /biomedical imaging; brain injury; cardiovascular disorder diagnosis; computational neuroscience; computer system design /evaluation; diagnosis design /evaluation; dogs; embolism; extracorporeal circulation EMBOLI CLASSIFICATION SYSTEM TO REDUCE BRAIN INJURY","The brain is at risk of serious cerebrovascular insult during the               
400,000 cardiopulmonary bypass (CPB) surgeries performed annually in the        
United States.  There is strong clinical evidence that over two-thirds          
of patients exhibit neurologic or neuropsychologic (NP) Postoperative           
deficits caused by emboli passing to the brain during surgery.  There           
is a critical need for a device that can not only detect emboli, but            
also classify them as to type.  Classification is critical for                  
correlating neurological deficits with emboli type, determining the             
source of the emboli, and subsequently changing surgical procedures             
and/or administering neuroprotective agents to minimize brain injury.           
Continuous-wave Doppler ultrasound equipment can detect emboli, but             
cannot provide the information needed to classify emboli composition.           
In Phase I, ORINCON and the Bowman Gray School of Medicine utilized             
broadband pulse-echo ultrasound and an artificial neural network to             
demonstrate significant classification capability on in-vitro data.  In         
Phase II, we will refine and integrate components from Phase I into a           
PC-based system capable of accurate, real-time emboli detection and             
classification in both extracorporeal pump circuits and the carotid             
artery.  Extensive in-vitro and in-vivo data will be collected to permit        
refinement and thorough evaluation of classification capabilities.              
Clinical studies will be performed to correlate neuropsychologic                
deficits with embolus composition.                                              
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
There is immediate commercial potential for a low-cost (less than 25K),         
real-time automated emboli detection and classification system.  The            
market includes manufacturers of cardiopulmonary pump circuit devices           
(Medtronics, Pall, Cobe, Sarns), hundreds of medical centers performing         
cardiopulmonary bypass, and producers of neuroprotective drugs (Astra,          
Bayer, Sterling).  A letter from Medtronics expressing this commercial          
potential is enclosed.                                                          
",2891987,R44NS034208,['R44NS034208'],NS,https://reporter.nih.gov/project-details/2891987,R44,1999,316660,-0.029356469708609
"DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
 artificial intelligence; behavioral /social science research tag; bioimaging /biomedical imaging; computer program /software; computer system design /evaluation; digital imaging; emotions; face expression; human subject; image processing; interpersonal relations; statistics /biometry; videotape /videodisc; visual tracking FACIAL EXPRESSION ANALYSIS BY IMAGE PROCESSING","DESCRIPTION (Applicant's Abstract):  Facial expression communicates             
information about emotional response and plays a critical role in the           
regulation of interpersonal behavior.  Current human-observer based methods     
for measuring facial expression are labor intensive, qualitative, and           
difficult to standardize across laboratories and over time.  To make            
feasible more rigorous, quantitative measurement of facial expression in        
diverse applications, we formed an interdisciplinary research group which       
covers expertise in facial expression analysis and image processing.  In the    
funding period, we developed and demonstrated the first version of an           
automated system for measuring facial expression in digitized images.  The      
system can discriminate nine combinations of FACS action units in the upper     
and lower face, quantity the timing and topography of action unit intensity     
in the brow region; and geometrically normalize image sequences within a        
range of plus or minus 20 degrees of out of-plane.                              
                                                                                
In the competing renewal, we will increase the number of action unit            
combinations that are recognized, implement convergent methods of               
quantifying action unit intensity, increase the generalizability of action      
unit estimation to a wider range of image orientations, test facial image       
processing (FIP) in image sequences from directed facial action tasks and       
laboratory studies of emotion regulation, and facilitate the integration of     
FIP into existing data management and statistical analysis software for use     
by behavioral science researchers and clinicians.  With these goals             
completed, FIP will eliminate the need for human observers in coding facial     
expression, promote standardize measurement, make possible the collection       
and processing of larger, more representative data sets, and open new areas     
of investigation and clinical application.                                      
",2890579,R01MH051435,['R01MH051435'],MH,https://reporter.nih.gov/project-details/2890579,R01,1999,227040,-0.018024114430931458
"Multimodal functional brain imaging software will be developed to               
estimate and visualize the estimated spatial extent and time course of          
brain activity by combining information from magnetic resonance imaging         
(MRI) with electroencephalography (EEG) and/or magnetoencephalography           
(MEG).  Structural information from MRI will be combined with                   
extracranial EEG and/or MEG measurements through algorithms developed           
to segment the MR images and to represent scalp, skull, and brain               
boundaries as computational objects.  This structural information may           
then be used to improve the spatial accuracy and resolution of existing         
EEG and MEG source estimation algorithms, while supporting millisecond          
temporal resolution.  The software will comprise a PC/Windows-based             
program suite for analysis and display.  The methods will be verified           
both with simulated data and with physiological data.                           
                                                                                
The algorithms and software may be used to study both normal brain              
function, such as measurements in cognitive neuroscience which may be           
studied with evoked response/event related potentials or spontaneous            
EEG, and in diseases of the brain, such as epilepsy, where precise              
spatial and temporal resolution may be of value for diagnosis and               
presurgical evaluation.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The techniques which we propose are a non-invasive, non-radiological and        
relatively low cost addition to existing EEG, MEG and MRI systems, and          
provides information which is not currently available from these systems        
independently.  The resulting software will have direct application in          
clinical and cognitive neuroscience research.  If clinical value is             
demonstrated, systems based on this methodology may find applications           
in the areas of psychiatry, neurology and psychology.                           
 artificial intelligence; bioimaging /biomedical imaging; brain electrical activity; computer program /software; computer system design /evaluation; electroencephalography; functional magnetic resonance imaging; human data; image enhancement; image processing; magnetic resonance imaging; magnetoencephalography; positron emission tomography MULTIMODAL (MRI/EEG/MEG) IMAGING SOFTWARE","Multimodal functional brain imaging software will be developed to               
estimate and visualize the estimated spatial extent and time course of          
brain activity by combining information from magnetic resonance imaging         
(MRI) with electroencephalography (EEG) and/or magnetoencephalography           
(MEG).  Structural information from MRI will be combined with                   
extracranial EEG and/or MEG measurements through algorithms developed           
to segment the MR images and to represent scalp, skull, and brain               
boundaries as computational objects.  This structural information may           
then be used to improve the spatial accuracy and resolution of existing         
EEG and MEG source estimation algorithms, while supporting millisecond          
temporal resolution.  The software will comprise a PC/Windows-based             
program suite for analysis and display.  The methods will be verified           
both with simulated data and with physiological data.                           
                                                                                
The algorithms and software may be used to study both normal brain              
function, such as measurements in cognitive neuroscience which may be           
studied with evoked response/event related potentials or spontaneous            
EEG, and in diseases of the brain, such as epilepsy, where precise              
spatial and temporal resolution may be of value for diagnosis and               
presurgical evaluation.                                                         
                                                                                
PROPOSED COMMERCIAL APPLICATION                                                 
The techniques which we propose are a non-invasive, non-radiological and        
relatively low cost addition to existing EEG, MEG and MRI systems, and          
provides information which is not currently available from these systems        
independently.  The resulting software will have direct application in          
clinical and cognitive neuroscience research.  If clinical value is             
demonstrated, systems based on this methodology may find applications           
in the areas of psychiatry, neurology and psychology.                           
",2890817,R44MH055915,['R44MH055915'],MH,https://reporter.nih.gov/project-details/2890817,R44,1999,359147,-0.033997430628076884
"The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: i) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
i) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotherapeutically induced mutants         
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary' relationships and events, provide                
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
 DNA directed DNA polymerase; DNA directed RNA polymerase; DNA topoisomerases; Mononegavirales; Paramyxovirus; RNA directed DNA polymerase; Rhabdoviridae; bacterial proteins; biochemical evolution; computer assisted sequence analysis; genetic recombination; integrase; method development; nuclease; protein sequence; ribonuclease III; virus DNA; virus RNA; virus protein COMPUTER BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION","The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: i) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
i) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotherapeutically induced mutants         
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary' relationships and events, provide                
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
",6134247,K04AI001277,['K04AI001277'],AI,https://reporter.nih.gov/project-details/6134247,K04,1999,63885,-0.040645604002232154
"The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: 1) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
1) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotheraputically induced mutants          
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary relationships and events, provide                 
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
 DNA replication; Mononegavirales; RNA biosynthesis; biochemical evolution; computer assisted sequence analysis; computer program /software; nucleic acid sequence; virus genetics; virus protein COMPUTER BASED SEQUENCE ANALYSIS AND RNA VIRUS EVOLUTION","The goal of the proposed research is the analysis of biological sequence        
data to address the molecular mechanisms of evolution and the origin(s)         
of all viruses and related genetic elements. Phylogenetic trees will            
provide a framework for the mapping of cell and tissue tropism,                 
pathogenicity and virulence, modes of transmission and geographical             
distributions, and many other higher order characteristics of viruses.          
The specific aims of proposed analytical studies are: 1) determining            
functionally equivalent networks and frequency of exchange among and            
between retroid elements, and their potential cellular homologues,              
including new studies on 300 retroviral env proteins; 2) inferring              
functionally important regions of all proteins of paramyxo-, rhabdo- and        
filoviruses, (with privileged access to new Ebola sequences), and Borna         
Disease virus, (including potential BDV sequences from schizophrenic            
patients); and 3) the analysis of the dUTPase gene, as a model system,          
to address issues relevant to the structure, function and evolution of          
duplicated sequences, and potential horizontal transfer among and between       
host and viral genomes. The specific aims of the technical studies are:         
1) evaluation of stochastic production model approaches for generation          
of multiple alignments, detection of recombination, and calculation of          
evolutionary distances; and 2) development and testing of new and               
existing methods for historical reconstruction of functionally equivalent       
networks.                                                                       
                                                                                
RNA viruses (e.g. HIV, or Ebola) are the major causative agents of human,       
animal and plant viral diseases world wide. The heterogeneous nature of         
RNA populations makes it difficult to develop effective, anti-viral             
agents. The sequence database is now large enough to conduct comparative        
studies on natural variants versus chemotheraputically induced mutants          
for several retroviral proteins. This model study will provide new              
information on the nature of selected mutations which will be useful in         
future anti-viral drug development.                                             
                                                                                
Computational analysis of primary sequence data is an area of intense           
interest in biology, mathematics, statistics and systems science. In the        
last few years new approaches to problem solving and classification, such       
as machine learning, neural networks, genetic algorithms, and stochastic        
production models or, ""intelligent systems"" as they are referred to             
collectively, have become available. Unfortunately most biologists are          
unaware of these developments. Application of these methods to real data        
remains unexplored. The proposed studies will go a long way in rectifying       
this gap in technological utilization. These studies will continue to           
define important evolutionary relationships and events, provide                 
biologically informative sequence relationships for bench-marking new           
software, and contribute new information relevant to the structure and          
function of viral proteins suggesting new directions in laboratory              
experimentation. Strategies and techniques developed for the analysis of        
highly divergent genomes can also be applied to the study of the wealth         
of sequence information generated under the auspices of the Human Genome        
Project.                                                                        
",6134233,R01AI028309,['R01AI028309'],AI,https://reporter.nih.gov/project-details/6134233,R01,1999,152194,-0.0035272658928653462
"DESCRIPTION:(Adapted from the Investigator's Abstract) The goal of this         
grant is to determine if and when cancer risks can be estimated by              
establishing record-linkages between statewide cancer surveillance systems      
and occupational cohorts.  More specifically, the aims of this study are to:    
(1) determine the feasibility of utilizing statewide cancer surveillance        
systems in the evaluation of cancer incidence within occupational cohorts;      
(2) compare and contrast the relative merits of standardized incidence          
ratios (SIR) with standardized mortality ratios (SMR) as determined from        
cancer surveillance systems incidence data and death certificate mortality      
data, respectively; and (3) provide recommendations concerning how and when     
statewide cancer surveillance systems should be utilized in the evaluation      
of occupational cohorts.                                                        
                                                                                
SMR and SIR estimates will be calculated and compared for three occupational    
cohorts (e.g., Highway Maintenance, 3M, Conwed).  SMR analyses have already     
been completed for the Highway Maintenance and 3M cohorts; a mortality          
update and SMR analysis will be conducted for the Conwed cohort (1988-1995).    
Cancer morbidity information, for the SIR analysis, will be determined by       
linking the three cohorts with the Minnesota Cancer Surveillance System         
(MCSS).  Residency status will be required before person-years can be           
calculated, however, because inclusion in the MCSS is restricted to             
Minnesota residents.  Linkages to other data sets will be used to determine     
the Minnesota residency status.  Sensitivity analyses will be used to           
evaluate confounding and follow-up bias.  Standardized mortality ratios will    
be compared to standardized incidence ratios for the Highway Maintenance,       
3M, and Conwed cohorts.  Finally, this study will evaluate the utility and      
limitations of cancer surveillance systems as a tool for occupational cancer    
research; recommendations for its use will be developed.                        
 artificial intelligence; cancer risk; clinical research; computer data analysis; data collection methodology /evaluation; environment related neoplasm /cancer; human data; human mortality; human subject; neoplasm /cancer epidemiology; occupational disease /disorder; occupational hazard; occupational health /safety; vital statistics OCCUPATIONAL CANCER SURVEILLANCE THROUGH RECORD LINKAGE","DESCRIPTION:(Adapted from the Investigator's Abstract) The goal of this         
grant is to determine if and when cancer risks can be estimated by              
establishing record-linkages between statewide cancer surveillance systems      
and occupational cohorts.  More specifically, the aims of this study are to:    
(1) determine the feasibility of utilizing statewide cancer surveillance        
systems in the evaluation of cancer incidence within occupational cohorts;      
(2) compare and contrast the relative merits of standardized incidence          
ratios (SIR) with standardized mortality ratios (SMR) as determined from        
cancer surveillance systems incidence data and death certificate mortality      
data, respectively; and (3) provide recommendations concerning how and when     
statewide cancer surveillance systems should be utilized in the evaluation      
of occupational cohorts.                                                        
                                                                                
SMR and SIR estimates will be calculated and compared for three occupational    
cohorts (e.g., Highway Maintenance, 3M, Conwed).  SMR analyses have already     
been completed for the Highway Maintenance and 3M cohorts; a mortality          
update and SMR analysis will be conducted for the Conwed cohort (1988-1995).    
Cancer morbidity information, for the SIR analysis, will be determined by       
linking the three cohorts with the Minnesota Cancer Surveillance System         
(MCSS).  Residency status will be required before person-years can be           
calculated, however, because inclusion in the MCSS is restricted to             
Minnesota residents.  Linkages to other data sets will be used to determine     
the Minnesota residency status.  Sensitivity analyses will be used to           
evaluate confounding and follow-up bias.  Standardized mortality ratios will    
be compared to standardized incidence ratios for the Highway Maintenance,       
3M, and Conwed cohorts.  Finally, this study will evaluate the utility and      
limitations of cancer surveillance systems as a tool for occupational cancer    
research; recommendations for its use will be developed.                        
",6071508,K01OH000161,['K01OH000161'],OH,https://reporter.nih.gov/project-details/6071508,K01,1999,54000,-0.06634009470812134
"DESCRIPTION:  (Adapted from the Investigator's Abstract) This project is        
directly concerned with the learning and motivational processes underlying      
ethanol-seeking behavior.  The long-term goal is to improve our                 
understanding of the behavioral and neurobiological process that contribute     
to the etiology, maintenance, elimination and relapse of alcoholism.  The       
general experimental strategy involves study of ethanol's motivational          
effects in oral self-administration and place conditioning tasks using          
animals.  Special emphasis will be placed on the learning that results from     
the predictive relationship between environmental stimuli and exposure to       
ethanol rewarding or aversive effects.  The central organizing hypothesis of    
this research is that ethanol-predictive stimuli have a direct impact on        
ethanol's motivational effects and that they are importantly involved in        
motivating or directing ethanol-seeking behavior, including the phenomenon      
of relapse after extinction or abstinence.  One set of proposed experiments     
will examine effects of ethanol-predictive stimuli on conditioned               
hyperthermia and barpressing in a signaled self-administration procedure        
using rats and mice.  Variables of interest include:  Sucrose concentration,    
CS-ethanol overlap, ethanol access duration and conditioned reinforcement.      
The second set of studies will determine effects of ethanol-predictive          
stimuli on conditioned place preference and aversion in rats and mice.          
Variables of interest include:  dose, number of trials, CS-ethanol interval,    
genotype, and route of administration.  The final series of experiments will    
study the role of ethanol-predictive stimuli in extinction, relapse and         
relapse prevention.  In addition to improving our understanding of              
ethanol-predictive stimuli, these studies will shed new light on apparent       
species differences in ethanol s motivational effects, and on findings that     
ethanol-predictive stimuli can acquire opposing motivational effects within     
the same species.  A better understanding of these issues is essential for      
using these animal models to study neurobiological and genetic contributions    
to alcoholism.  This project should help focus future research on the           
neurobiological mechanisms of ethanol-seeking behavior, aid in the              
development of beneficial treatments for alcohol abuse, and facilitate          
identification of effective relapse prevention strategies.  These studies       
could prove to be especially useful in the evaluation of putative               
pharmacotherapies intended to reduce alcohol craving and in the design of       
behavioral interventions to decrease ethanol-seeking behavior.                  
 avoidance behavior; behavioral /social science research tag; body temperature regulation; conditioning; craving; ethanol; information seeking behavior; laboratory mouse; laboratory rat; learning; motivation; operant conditionings; pharmacogenetics; preference; psychological models; psychological reinforcement; reinforcer; self medication MODULATION OF ALCOHOL REINFORCEMENT","DESCRIPTION:  (Adapted from the Investigator's Abstract) This project is        
directly concerned with the learning and motivational processes underlying      
ethanol-seeking behavior.  The long-term goal is to improve our                 
understanding of the behavioral and neurobiological process that contribute     
to the etiology, maintenance, elimination and relapse of alcoholism.  The       
general experimental strategy involves study of ethanol's motivational          
effects in oral self-administration and place conditioning tasks using          
animals.  Special emphasis will be placed on the learning that results from     
the predictive relationship between environmental stimuli and exposure to       
ethanol rewarding or aversive effects.  The central organizing hypothesis of    
this research is that ethanol-predictive stimuli have a direct impact on        
ethanol's motivational effects and that they are importantly involved in        
motivating or directing ethanol-seeking behavior, including the phenomenon      
of relapse after extinction or abstinence.  One set of proposed experiments     
will examine effects of ethanol-predictive stimuli on conditioned               
hyperthermia and barpressing in a signaled self-administration procedure        
using rats and mice.  Variables of interest include:  Sucrose concentration,    
CS-ethanol overlap, ethanol access duration and conditioned reinforcement.      
The second set of studies will determine effects of ethanol-predictive          
stimuli on conditioned place preference and aversion in rats and mice.          
Variables of interest include:  dose, number of trials, CS-ethanol interval,    
genotype, and route of administration.  The final series of experiments will    
study the role of ethanol-predictive stimuli in extinction, relapse and         
relapse prevention.  In addition to improving our understanding of              
ethanol-predictive stimuli, these studies will shed new light on apparent       
species differences in ethanol s motivational effects, and on findings that     
ethanol-predictive stimuli can acquire opposing motivational effects within     
the same species.  A better understanding of these issues is essential for      
using these animal models to study neurobiological and genetic contributions    
to alcoholism.  This project should help focus future research on the           
neurobiological mechanisms of ethanol-seeking behavior, aid in the              
development of beneficial treatments for alcohol abuse, and facilitate          
identification of effective relapse prevention strategies.  These studies       
could prove to be especially useful in the evaluation of putative               
pharmacotherapies intended to reduce alcohol craving and in the design of       
behavioral interventions to decrease ethanol-seeking behavior.                  
",2894009,R37AA007702,['R37AA007702'],AA,https://reporter.nih.gov/project-details/2894009,R37,1999,244297,0.019498607242339434
"We propose building a 21 pinhole non-rotating SPECT camera, which will acquire views simultaneously, that will be able to do studies either cannot be done or cannot be performed consistently and reliably on current rotational SPECT cameras, which acquires views sequentially.  These studies include tomographic first pass, stress studies, and gated cardiac studies. Tomographic first pass studies allow more accurate quantitative measurements (EFs), and allow wall and valve abnormalities to be located more precisely. Gated cardiac studies can be done more reliably when a posteriori beat selection is done, which the list mode acquisition of the 21 pinhole camera allows.  Multiple studies can then be made from a single acquisition to study different beta types. The high precision necessary in moving heavy heads adds to the cost of SPECT cameras.  Eliminating the movement should substantially reduce the cost, making nuclear cardiac procedures more cost effective. Cost effectiveness can also be increased using a simultaneous dual isotope (Tc-99m & T1-201) protocol.  This has the benefit of requiring only a single session to get both stress and rest images.  This also makes the procedure more comfortable and convenient to the patient and repositioning artifacts are eliminated.  Our 21 pinhole system corrects the downscatter from Tc-99m into the T1-201 window by retaining the entire energy spectrum and using it to estimate the downscatter component. PROPOSED COMMERCIAL APPLICATION:  The 21 pinhole SPECT system will be used by cardiologists to perform the following studies:  First pass studies; Gated studies (both blood pool and perfusion); stress gated studies; and Perfusion studies.  All these studies can be done tomographically, which current systems cannot do for first pass and stress studies.  This will be a dedicated cardiac system.  artificial intelligence; bioimaging /biomedical imaging; biomedical equipment development; diagnosis design /evaluation; heart disorder diagnosis; heart imaging /visualization /scanning; nuclear medicine; phantom model; radiation detector; radionuclide imaging /scanning; single photon emission computed tomography; technetium 21 PINHOLE, NONROTATIONAL CARDIAC SPECT SYSTEM","We propose building a 21 pinhole non-rotating SPECT camera, which will acquire views simultaneously, that will be able to do studies either cannot be done or cannot be performed consistently and reliably on current rotational SPECT cameras, which acquires views sequentially.  These studies include tomographic first pass, stress studies, and gated cardiac studies. Tomographic first pass studies allow more accurate quantitative measurements (EFs), and allow wall and valve abnormalities to be located more precisely. Gated cardiac studies can be done more reliably when a posteriori beat selection is done, which the list mode acquisition of the 21 pinhole camera allows.  Multiple studies can then be made from a single acquisition to study different beta types. The high precision necessary in moving heavy heads adds to the cost of SPECT cameras.  Eliminating the movement should substantially reduce the cost, making nuclear cardiac procedures more cost effective. Cost effectiveness can also be increased using a simultaneous dual isotope (Tc-99m & T1-201) protocol.  This has the benefit of requiring only a single session to get both stress and rest images.  This also makes the procedure more comfortable and convenient to the patient and repositioning artifacts are eliminated.  Our 21 pinhole system corrects the downscatter from Tc-99m into the T1-201 window by retaining the entire energy spectrum and using it to estimate the downscatter component. PROPOSED COMMERCIAL APPLICATION:  The 21 pinhole SPECT system will be used by cardiologists to perform the following studies:  First pass studies; Gated studies (both blood pool and perfusion); stress gated studies; and Perfusion studies.  All these studies can be done tomographically, which current systems cannot do for first pass and stress studies.  This will be a dedicated cardiac system. ",2867150,R43CA081759,['R43CA081759'],CA,https://reporter.nih.gov/project-details/2867150,R43,1999,135740,-0.05192527914054365
"DESCRIPTION (adapted from applicant's abstract):  A central issue in            
behavioral neuroscience is how alterations in neural pathways mediate the       
durable behavior changes involved in learning.  Taste aversion conditioning     
is an excellent model for studying the neural changes involved in learning      
because this conditioning can occur in a single trial, despite lengthy          
delays between conditioned and unconditioned stimuli.  The proposed studies     
are based on the identification of a cellular correlate of the behavioral       
expression of a conditioned taste aversion, namely, cells in the nucleus of     
the solitary tract.  C-fos induction occurs in cells in the nucleus of the      
solitary tract in response to a taste made aversive by conditioning, but not    
in response to the same taste prior to conditioning or to a taste (quinine)     
which is innately aversive.  Proposed studies will combine this cellular        
measure with behavioral assessment to further assess the cellular c-fos         
response and its reliability as a marker of learning.  Studies will also        
examine the functional importance of cells in the nucleus of the solitary       
tract which display c-fos induction during expression of this learning,         
using asymmetrical lesion techniques.  Additionally, studies will continue      
to define the forebrain pathways critical to this learning, with a focus on     
ipsilateral connections between the amygdala and the nucleus of the solitary    
tract which appear necessary for cellular, as well as behavioral,               
manifestations of this learning.  The role of insular cortex will also be       
examined.  Finally, studies will identify the targets of activated modified     
behavioral response to a taste after conditioning.  By defining the neural      
pathways and cell types involved in CTA learning, this project provide the      
groundwork for eventually characterizing the plastic changes within and         
between cells which underlie this learning.                                     
 amygdala; behavior test; behavioral /social science research tag; biomarker; brain subcortex; conditioning; experimental brain lesion; gene expression; histochemistry /cytochemistry; in situ hybridization; laboratory rat; learning; negative reinforcements; neuroanatomy; prosencephalon; protooncogene; solitary tract nucleus; taste NEURAL MEDIATION OF CONDITIONED TASTE AVERSIONS","DESCRIPTION (adapted from applicant's abstract):  A central issue in            
behavioral neuroscience is how alterations in neural pathways mediate the       
durable behavior changes involved in learning.  Taste aversion conditioning     
is an excellent model for studying the neural changes involved in learning      
because this conditioning can occur in a single trial, despite lengthy          
delays between conditioned and unconditioned stimuli.  The proposed studies     
are based on the identification of a cellular correlate of the behavioral       
expression of a conditioned taste aversion, namely, cells in the nucleus of     
the solitary tract.  C-fos induction occurs in cells in the nucleus of the      
solitary tract in response to a taste made aversive by conditioning, but not    
in response to the same taste prior to conditioning or to a taste (quinine)     
which is innately aversive.  Proposed studies will combine this cellular        
measure with behavioral assessment to further assess the cellular c-fos         
response and its reliability as a marker of learning.  Studies will also        
examine the functional importance of cells in the nucleus of the solitary       
tract which display c-fos induction during expression of this learning,         
using asymmetrical lesion techniques.  Additionally, studies will continue      
to define the forebrain pathways critical to this learning, with a focus on     
ipsilateral connections between the amygdala and the nucleus of the solitary    
tract which appear necessary for cellular, as well as behavioral,               
manifestations of this learning.  The role of insular cortex will also be       
examined.  Finally, studies will identify the targets of activated modified     
behavioral response to a taste after conditioning.  By defining the neural      
pathways and cell types involved in CTA learning, this project provide the      
groundwork for eventually characterizing the plastic changes within and         
between cells which underlie this learning.                                     
",2858219,R01NS037040,['R01NS037040'],NS,https://reporter.nih.gov/project-details/2858219,R01,1999,126411,-5.0046344393969534e-05
"
 DESCRIPTION (adapted from investigator's abstract): The proposed studies           
 examine the neuropsychological processes underlying the deficits in priming        
 exhibited by patients with Alzheimer's disease (AD). Although AD patient's         
 priming deficits have often been attributed to a breakdown in the organization     
 of semantic memory, recent evidence suggests that other factors, such as           
 inefficient semantic encoding, and deficient attentional or arousal processes      
 may play significant roles. A determination of the particular processes            
 underlying the AD patient's priming deficits may lead to important information     
 about the necessary and sufficient conditions for the occurrence of the priming    
 phenomenon, about the neural substrates mediating priming, and about the nature    
 of the neuropsychological deficits associated with AD. The first set of            
 experiments examines the status of arousal and selective attentional processes     
 in AD patients using a series of simple, choice, and covert orienting of           
 attention reaction time tasks. The second set of experiments examines the          
 dynamics of activation within AD patient's semantic network using tests of         
 direct, indirect and summation semantic priming as well as tests of retrieval      
 from semantic memory. A third set of experiments examines the effects of           
 manipulating arousal and attention on the priming performance of AD patients. A    
 fourth set of experiments examines the relative contributions of                   
 perceptually-based and conceptually based semantic information to AD patient's     
 priming performance. These studies are intended to shed light on the               
 neuropsychological deficits associated with AD and enhance a deeper                
 understanding of priming phenomena and their relevance to clinical                 
 applications.                                                                      
                                                                                    
 Alzheimer's disease; arousal; attention; behavioral /social science research tag; clinical research; cues; human subject; memory; memory disorders; neuropsychological tests; neuropsychology; psycholinguistics; psychomotor reaction time; semantics AROUSAL ATTENTION AND PRIMING IN ALZHEIMERS DISEASE","
 DESCRIPTION (adapted from investigator's abstract): The proposed studies           
 examine the neuropsychological processes underlying the deficits in priming        
 exhibited by patients with Alzheimer's disease (AD). Although AD patient's         
 priming deficits have often been attributed to a breakdown in the organization     
 of semantic memory, recent evidence suggests that other factors, such as           
 inefficient semantic encoding, and deficient attentional or arousal processes      
 may play significant roles. A determination of the particular processes            
 underlying the AD patient's priming deficits may lead to important information     
 about the necessary and sufficient conditions for the occurrence of the priming    
 phenomenon, about the neural substrates mediating priming, and about the nature    
 of the neuropsychological deficits associated with AD. The first set of            
 experiments examines the status of arousal and selective attentional processes     
 in AD patients using a series of simple, choice, and covert orienting of           
 attention reaction time tasks. The second set of experiments examines the          
 dynamics of activation within AD patient's semantic network using tests of         
 direct, indirect and summation semantic priming as well as tests of retrieval      
 from semantic memory. A third set of experiments examines the effects of           
 manipulating arousal and attention on the priming performance of AD patients. A    
 fourth set of experiments examines the relative contributions of                   
 perceptually-based and conceptually based semantic information to AD patient's     
 priming performance. These studies are intended to shed light on the               
 neuropsychological deficits associated with AD and enhance a deeper                
 understanding of priming phenomena and their relevance to clinical                 
 applications.                                                                      
                                                                                    
",2855861,R01AG015375,['R01AG015375'],AG,https://reporter.nih.gov/project-details/2855861,R01,1999,118280,-0.025508474731551593
"
 DESCRIPTION: (Adapted from the Investigator's Abstract) The proposed research                                                                 
 has the goal of developing and validating interval psychometric scales of          
 visual function limitations and vision disabilities. These interval scales will    
 be developed using Rasch probablistic measurement models applied to ordinal        
 patient rating responses to individual questions. Once developed and validated,    
 these scales will be independent of the particular assessment used, as long as     
 the instrument is calibrated to the scale. The significance of the proposed        
 research is that it will provide a means of estimating measurements of latent      
 functional ability variables for individual patients with visual impairments.      
 In the future, these measurements can be used for parametric studies,              
 epidemiological studies, and clinical outcome studies. The proposed research       
 will identify the number and nature of functional ability scales. It will          
 determine the dependence of those scales on the diagnosis of visual system         
 disorder, the type of visual impairment, the existence of co-morbidities, and      
 the patient's history of rehabilitation. Existing visual function instruments      
 (NEI-VFQ, VF-14, ADVS, and VAQ) and two general function instrument's              
 individual items will be evaluated with respect to scales. To estimate the         
 scales, a large set of specific cognitive and motor activities (e.g., writing a    
 check) will be classified according to functional domain (reading, fine and        
 gross visual-motor, visual information processing [e.g., recognition,              
 localization, orientation], or mobility). In telephone interviews, low vision      
 patients will be asked to rate the difficulty of performing each activity.         
 Rasch analysis will be used to test the hypothesis that there is a global          
 functional ability scale and to test the validity of the a priori visual           
 function domains. Principal component analysis of response residuals will be       
 used to evaluate the dimensionality of visual function limitations. Patients       
 also will be asked to rate the difficulty of achieving specific activity goals     
 (e.g., cook a meal, manage personal finances) and Rasch analysis will be used      
 to estimate a vision disability scale. Item ordering and item intervals on the     
 scales and scale validity will be compared across diagnostic groups (AMD,          
 glaucoma, diabetic retinopathy, RP, CVA, and anterior segment disorders) and       
 for different types of visual impairments (e.g., acuity loss and contracted        
 visual fields). Person measures of functional ability will be evaluated as a       
 function of severity of visual impairments (visual acuity, contrast                
 sensitivity, visual fields, dark adaptation, color vision). Determining if the     
 NEI-VFQ, VF-14, ADVS, VAQ, SF-36, and SIP can be calibrated to common scales       
 will test the hypothesis that there is a common functional ability variable(s).    
                                                                                    
 blindness; clinical research; color visions; dark adaptation; eye disorder diagnosis; functional ability; human subject; interview; psychometrics; rehabilitation; statistics /biometry; vision disorders; visual fields; visual perception VISION DISABILITIES IN LOW VISION","
 DESCRIPTION: (Adapted from the Investigator's Abstract) The proposed research                                                                 
 has the goal of developing and validating interval psychometric scales of          
 visual function limitations and vision disabilities. These interval scales will    
 be developed using Rasch probablistic measurement models applied to ordinal        
 patient rating responses to individual questions. Once developed and validated,    
 these scales will be independent of the particular assessment used, as long as     
 the instrument is calibrated to the scale. The significance of the proposed        
 research is that it will provide a means of estimating measurements of latent      
 functional ability variables for individual patients with visual impairments.      
 In the future, these measurements can be used for parametric studies,              
 epidemiological studies, and clinical outcome studies. The proposed research       
 will identify the number and nature of functional ability scales. It will          
 determine the dependence of those scales on the diagnosis of visual system         
 disorder, the type of visual impairment, the existence of co-morbidities, and      
 the patient's history of rehabilitation. Existing visual function instruments      
 (NEI-VFQ, VF-14, ADVS, and VAQ) and two general function instrument's              
 individual items will be evaluated with respect to scales. To estimate the         
 scales, a large set of specific cognitive and motor activities (e.g., writing a    
 check) will be classified according to functional domain (reading, fine and        
 gross visual-motor, visual information processing [e.g., recognition,              
 localization, orientation], or mobility). In telephone interviews, low vision      
 patients will be asked to rate the difficulty of performing each activity.         
 Rasch analysis will be used to test the hypothesis that there is a global          
 functional ability scale and to test the validity of the a priori visual           
 function domains. Principal component analysis of response residuals will be       
 used to evaluate the dimensionality of visual function limitations. Patients       
 also will be asked to rate the difficulty of achieving specific activity goals     
 (e.g., cook a meal, manage personal finances) and Rasch analysis will be used      
 to estimate a vision disability scale. Item ordering and item intervals on the     
 scales and scale validity will be compared across diagnostic groups (AMD,          
 glaucoma, diabetic retinopathy, RP, CVA, and anterior segment disorders) and       
 for different types of visual impairments (e.g., acuity loss and contracted        
 visual fields). Person measures of functional ability will be evaluated as a       
 function of severity of visual impairments (visual acuity, contrast                
 sensitivity, visual fields, dark adaptation, color vision). Determining if the     
 NEI-VFQ, VF-14, ADVS, VAQ, SF-36, and SIP can be calibrated to common scales       
 will test the hypothesis that there is a common functional ability variable(s).    
                                                                                    
",2911020,R01EY012045,['R01EY012045'],EY,https://reporter.nih.gov/project-details/2911020,R01,1999,263540,-0.07768352122422495
 artificial intelligence; computer program /software; disease /disorder etiology; mathematical model; meta analysis; method development; statistics /biometry TRINOMIAL BIVARIATE NEURAL NETWORKS FOR GIS ANALYSIS,,2907864,R41CA073131,['R41CA073131'],CA,https://reporter.nih.gov/project-details/2907864,R41,1999,42436,-0.05931451748393117
"DESCRIPTION (Taken from application abstract):  We propose to develop           
automated techniques to facilitate classification and pattern recognition in    
biomedical data sets.  These techniques will involve development of novel       
neural network architectures, as well as formulation of principles governing    
their creation and explanation of results.  Specifically, as a solution to      
the problem of recognizing infrequent categories, we will develop               
hierarchical and sequential systems of feedforward neural networks that make    
use of information such as (a) prior knowledge of the domain, and/or (b)        
natural clusters defined by clustering or unsupervised learning methods to      
develop intermediate classification goals and utilize a divide-and-conquer      
approach to complex classification problems.  Additionally, we will develop     
generic tools for pre-processing input data by making transformations of        
original data, reducing dimensionality, and producing training and test sets    
suitable for cross-validation and bootstrap.  We will build tools for           
evaluating results that measure calibration, resolution, importance of          
variables, and comparisons between different models.  Furthermore, we will      
develop standardized interfaces for certain existing classification models.     
We will use a component-based architecture to build our neural network and      
write interfaces to existing classification models (e.g., regression trees,     
logistic regression models) so that they can be interchanged in a               
user-friendly manner.  We will use our preprocessing modules to prepare data    
to be entered in a variety of classification models.  The results will be       
evaluated in isolation, and later combined to test the hypothesis that the      
combined system performs better in real biomedical data sets in terms of        
calibration, resolution, and explanatory power.                                 
                                                                                
This research will (a) quantify improvement in performance when a               
classification problem is broken down into subproblems in a systematic way,     
(b) quantify the advantages of combining different types of classifiers,        
create a library of reusable neural network classification models, data         
pre-processing, and evaluation tools that use standardized interfaces, and      
(d) foster dissemination of classification models and the use of                
pre-processing and evaluation tools by making them available to other           
researchers through the World-Wide-Web.  We will test four hypotheses:  (1)     
Combinations of different modalities of classifiers perform significantly       
better than isolated models.  (2) Hierarchical and sequential neural            
networks perform better than standard neural networks.  (3) Unsupervised        
models can decompose a problem for hierarchical or sequential neural            
networks better than models that use prior knowledge.  (4) It is possible to    
build a Classification Tool Kit composed of data pre-processing modules,        
classification models, and evaluation modules in which components are           
independent, reusable, and interchangeable.                                     
 Internet; artificial intelligence; classification; computer assisted medical decision making; computer system design /evaluation; mathematical model; model design /development COMPONENT BASED TOOLS FOR CONNECTIONIST CLASSIFICATION","DESCRIPTION (Taken from application abstract):  We propose to develop           
automated techniques to facilitate classification and pattern recognition in    
biomedical data sets.  These techniques will involve development of novel       
neural network architectures, as well as formulation of principles governing    
their creation and explanation of results.  Specifically, as a solution to      
the problem of recognizing infrequent categories, we will develop               
hierarchical and sequential systems of feedforward neural networks that make    
use of information such as (a) prior knowledge of the domain, and/or (b)        
natural clusters defined by clustering or unsupervised learning methods to      
develop intermediate classification goals and utilize a divide-and-conquer      
approach to complex classification problems.  Additionally, we will develop     
generic tools for pre-processing input data by making transformations of        
original data, reducing dimensionality, and producing training and test sets    
suitable for cross-validation and bootstrap.  We will build tools for           
evaluating results that measure calibration, resolution, importance of          
variables, and comparisons between different models.  Furthermore, we will      
develop standardized interfaces for certain existing classification models.     
We will use a component-based architecture to build our neural network and      
write interfaces to existing classification models (e.g., regression trees,     
logistic regression models) so that they can be interchanged in a               
user-friendly manner.  We will use our preprocessing modules to prepare data    
to be entered in a variety of classification models.  The results will be       
evaluated in isolation, and later combined to test the hypothesis that the      
combined system performs better in real biomedical data sets in terms of        
calibration, resolution, and explanatory power.                                 
                                                                                
This research will (a) quantify improvement in performance when a               
classification problem is broken down into subproblems in a systematic way,     
(b) quantify the advantages of combining different types of classifiers,        
create a library of reusable neural network classification models, data         
pre-processing, and evaluation tools that use standardized interfaces, and      
(d) foster dissemination of classification models and the use of                
pre-processing and evaluation tools by making them available to other           
researchers through the World-Wide-Web.  We will test four hypotheses:  (1)     
Combinations of different modalities of classifiers perform significantly       
better than isolated models.  (2) Hierarchical and sequential neural            
networks perform better than standard neural networks.  (3) Unsupervised        
models can decompose a problem for hierarchical or sequential neural            
networks better than models that use prior knowledge.  (4) It is possible to    
build a Classification Tool Kit composed of data pre-processing modules,        
classification models, and evaluation modules in which components are           
independent, reusable, and interchangeable.                                     
",2897393,R01LM006538,['R01LM006538'],LM,https://reporter.nih.gov/project-details/2897393,R01,1999,249197,-0.21032967831357133
"The PI is a Neurophysiologist and Pediatric Neurosurgeon who will be the        
Associate Director of Center V (Behavioral and Neurobiology Research), with     
special responsibility for Neurobiology programs, at the Children""s             
Research Institute.  This RCA will permit the PI to devote nearly full time     
to research.                                                                    
                                                                                
Short term goals are to test the hypothesis that neuronal ensembles have        
nonlinear deterministic properties.  If so, they will 1) have activity that     
can be characterized and controlled through unstable periodic orbits, 2)        
hen noise driven will exhibit stochastic resonance, and 3) because of           
coupling will exhibit generalized (nonlinear) synchrony and emergence.          
Long-term goals are to achieve a better understanding of neuronal network       
and brain behavior, and to develop novel methods of treating dynamical          
diseases.                                                                       
                                                                                
The research project will involve theoretical work on the detection of          
unstable orbits in In Vitro brain slices and human epileptic foci.  Such        
orbit information forms a novel method of characterizing the deterministic      
properties of complex systems despite nonstationarity, and can be used to       
control those systems.  Nonlinear systems also optimize their response to       
weak signals in the presence of noise - stochastic resonance.  We will          
define the statistical mechanics of stochastic resonance through                
simultaneous measurements of single neuron and neuronal ensemble activity.      
Since neuronal ensembles may demonstrate nonlinear generalized synchrony,       
we will quantify spatio-temporal generalized synchrony through dual             
simultaneous single cell recordings as a function of separation in a            
neuronal network.  Both unstable orbit detection and generalized synchrony      
will be used to define the emergence of nonlinear behaviors in neuronal         
ensembles.                                                                      
                                                                                
The results of this research will fundamentally alter the way that neuronal     
dynamics can be characterized and controlled, will provide a means to deal      
with neuronal nonstationarity, will further explain the role of noise in        
the nervous system, and may provide a novel approach for the control of         
pathological neuronal ensembles in dynamical diseases such as epilepsy,         
spasticity, and tremor.                                                         
 artificial intelligence; brain electrical activity; computational neuroscience; electric field; electroencephalography; extracellular; hippocampus; human data; intracellular; laboratory rat; microelectrodes; neural information processing; neuroanatomy; neurons; neurosurgery; noise; statistics /biometry NONLINEAR DYNAMICS OF NEURONAL ENSEMBLES","The PI is a Neurophysiologist and Pediatric Neurosurgeon who will be the        
Associate Director of Center V (Behavioral and Neurobiology Research), with     
special responsibility for Neurobiology programs, at the Children""s             
Research Institute.  This RCA will permit the PI to devote nearly full time     
to research.                                                                    
                                                                                
Short term goals are to test the hypothesis that neuronal ensembles have        
nonlinear deterministic properties.  If so, they will 1) have activity that     
can be characterized and controlled through unstable periodic orbits, 2)        
hen noise driven will exhibit stochastic resonance, and 3) because of           
coupling will exhibit generalized (nonlinear) synchrony and emergence.          
Long-term goals are to achieve a better understanding of neuronal network       
and brain behavior, and to develop novel methods of treating dynamical          
diseases.                                                                       
                                                                                
The research project will involve theoretical work on the detection of          
unstable orbits in In Vitro brain slices and human epileptic foci.  Such        
orbit information forms a novel method of characterizing the deterministic      
properties of complex systems despite nonstationarity, and can be used to       
control those systems.  Nonlinear systems also optimize their response to       
weak signals in the presence of noise - stochastic resonance.  We will          
define the statistical mechanics of stochastic resonance through                
simultaneous measurements of single neuron and neuronal ensemble activity.      
Since neuronal ensembles may demonstrate nonlinear generalized synchrony,       
we will quantify spatio-temporal generalized synchrony through dual             
simultaneous single cell recordings as a function of separation in a            
neuronal network.  Both unstable orbit detection and generalized synchrony      
will be used to define the emergence of nonlinear behaviors in neuronal         
ensembles.                                                                      
                                                                                
The results of this research will fundamentally alter the way that neuronal     
dynamics can be characterized and controlled, will provide a means to deal      
with neuronal nonstationarity, will further explain the role of noise in        
the nervous system, and may provide a novel approach for the control of         
pathological neuronal ensembles in dynamical diseases such as epilepsy,         
spasticity, and tremor.                                                         
",2889914,K02MH001493,['K02MH001493'],MH,https://reporter.nih.gov/project-details/2889914,K02,1999,99063,-0.036363405166281
"DESCRIPTION (Adapted from applicant's abstract):  This K02 application          
out-lines research and career development plans to investigate the role of      
sleep regulation in the developmental psychobiology of affective disorders.     
The first goal is to extend a well-established line of research based on        
measures of EEG sleep and cortisol in child and adolescent depression by        
investigating:  (1) abnormalities in sleep and cortisol regulation focusing     
on the sleep-onset transition; (2) normal maturational changes in sleep and     
cortisol regulation relevant to these abnormalities; and (3) the predictive     
validity of sleep and cortisol abnormalities in longitudinal clinical           
follow-up.                                                                      
                                                                                
The second goal is to further develop and investigate a larger developmental    
model of sleep regulation.  The model emphasizes close links between sleep      
regulation and neurobehavioral systems involved in the regulation of affect     
and arousal which are modulated in regions of prefrontal cortex (PFC).          
Matur-ational changes in PFC-subcortical circuits influencing sleep,            
arousal, and affect are hypothesized to contribute to sleep changes             
associated with depression, particularly near the transition from               
wakefulness into sleep.  Based on this model, predictions are made regarding    
sleep changes in the development of affective disorders and effects of sleep    
deprivation of affective regulation.  The long-term goal of this work is to     
understand mechanisms of dysregulation which may lead to more effective         
treatment of early onset affective disorders.                                   
                                                                                
Career development activities to support these goals are described.  Further    
advancing this line of investigation will require increased knowledge and       
skills in four areas:  the development of affect regulation; the relevant       
neurocircuitry and its development; more direct measures of the neural          
systems of interest; and statistics.  The candidate will pursue these goals     
through course work, focused readings, and supervised learning experiences      
and collab-orations with scientists locally and nationally.                     
 adolescence (12-20); arousal; clinical research; corticotropin releasing factor; cortisol; developmental neurobiology; disease /disorder onset; electroencephalography; frontal lobe /cortex; human puberty; human subject; longitudinal human study; magnetic resonance imaging; major depression; middle childhood (6-11); model design /development; neuroendocrine system; psychobiology; sleep; sleep deprivation; wakefulness DEVELOPMENT PSYCHOBIOLOGY OF SLEEP AND DEPRESSION","DESCRIPTION (Adapted from applicant's abstract):  This K02 application          
out-lines research and career development plans to investigate the role of      
sleep regulation in the developmental psychobiology of affective disorders.     
The first goal is to extend a well-established line of research based on        
measures of EEG sleep and cortisol in child and adolescent depression by        
investigating:  (1) abnormalities in sleep and cortisol regulation focusing     
on the sleep-onset transition; (2) normal maturational changes in sleep and     
cortisol regulation relevant to these abnormalities; and (3) the predictive     
validity of sleep and cortisol abnormalities in longitudinal clinical           
follow-up.                                                                      
                                                                                
The second goal is to further develop and investigate a larger developmental    
model of sleep regulation.  The model emphasizes close links between sleep      
regulation and neurobehavioral systems involved in the regulation of affect     
and arousal which are modulated in regions of prefrontal cortex (PFC).          
Matur-ational changes in PFC-subcortical circuits influencing sleep,            
arousal, and affect are hypothesized to contribute to sleep changes             
associated with depression, particularly near the transition from               
wakefulness into sleep.  Based on this model, predictions are made regarding    
sleep changes in the development of affective disorders and effects of sleep    
deprivation of affective regulation.  The long-term goal of this work is to     
understand mechanisms of dysregulation which may lead to more effective         
treatment of early onset affective disorders.                                   
                                                                                
Career development activities to support these goals are described.  Further    
advancing this line of investigation will require increased knowledge and       
skills in four areas:  the development of affect regulation; the relevant       
neurocircuitry and its development; more direct measures of the neural          
systems of interest; and statistics.  The candidate will pursue these goals     
through course work, focused readings, and supervised learning experiences      
and collab-orations with scientists locally and nationally.                     
",2889868,K02MH001362,['K02MH001362'],MH,https://reporter.nih.gov/project-details/2889868,K02,1999,98658,-0.10339831716942133
"DESCRIPTION: (Applicant's abstract) The feasibility of a Drowsiness             
Monitoring Device (DMD) to detect EEG indices of drowsiness in real-            
time, was demonstrated during Phase I with an analytical model correctly        
classifying 97.1 percent of sleep episodes and 94.3 percent of awake            
epochs in 20 sleep-deprived subjects. The model employs discriminant            
function analysis (DFA) to characterize and classify one-sec epochs,            
validated against a combination of visual scoring by polysomnographers          
and/or a behavioral measure. Algorithms to detect artifacts in real-time        
(EMG, 60-Hz and gross body/eye movements) were developed. This                  
classification accuracy represents a significant advancement over               
previously reported models and confirms the feasibility of                      
distinguishing EEG characteristics of sleep and waking on a second-by-          
second basis. Phase II will implement a three-level DFA classification          
system to further refine the model, adding sub-class states for                 
vigilance and drowsiness/sleep to improve system accuracy. The multi-           
dimensional time-series DFA analyses will correlate EEG parameters with         
behavioral measures of driving performance to provide quantitative              
predictions of performance decrements associate with sleep onset.               
The model will be validated using a population with demographics                
consistent with the target market for the DMD (e.g., truck drivers)             
during Phase II. In addition, the system will be evaluated with the             
introduction of commonly used legal drugs (caffeine, nicotine, and cold         
medications) to determine the robustness of the model.                          
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The DMD provides three levels of user safety. When sleep onset is               
approaching, the DMD will initiate a verbal warning alarm that must be          
turned off by the user. Alternatively, the DMD can provide verbal               
feedback to ensure the user maintains high levels of alertness during           
activities that require sustained vigilance. The user can also select           
the option for the DMD to recommend the optimal time to take a short            
nap, monitor the length of the nap and awaken the user at the                   
appropriate time.                                                               
                                                                                
Currently, more that 10% of the U.S. workforce or an estimated 20               
million people are engaged in night sift work. The transportation               
industry, including airline, railroad, marine and highway transportation        
companies, is the nation's third largest employer of shift workers. Long        
haul truck drivers, in particular, are vulnerable to sleepiness because         
they drive through the night, in most cases unaccompanied, and generally        
sleep less than 6 hours per day at irregular intervals. In addition, an         
estimated 6 million Americans suffer from chronic sleep disorders which         
make them vulnerable to fatigue in the workplace.                               
 artificial intelligence; attention; biomedical equipment development; clinical biomedical equipment; computer data analysis; computer program /software; computer system design /evaluation; computer system hardware; electroencephalography; human subject; patient monitoring device; polysomnography; portable biomedical equipment; sleep; wakefulness PORTABLE DROWSINESS MONITORING DEVICE","DESCRIPTION: (Applicant's abstract) The feasibility of a Drowsiness             
Monitoring Device (DMD) to detect EEG indices of drowsiness in real-            
time, was demonstrated during Phase I with an analytical model correctly        
classifying 97.1 percent of sleep episodes and 94.3 percent of awake            
epochs in 20 sleep-deprived subjects. The model employs discriminant            
function analysis (DFA) to characterize and classify one-sec epochs,            
validated against a combination of visual scoring by polysomnographers          
and/or a behavioral measure. Algorithms to detect artifacts in real-time        
(EMG, 60-Hz and gross body/eye movements) were developed. This                  
classification accuracy represents a significant advancement over               
previously reported models and confirms the feasibility of                      
distinguishing EEG characteristics of sleep and waking on a second-by-          
second basis. Phase II will implement a three-level DFA classification          
system to further refine the model, adding sub-class states for                 
vigilance and drowsiness/sleep to improve system accuracy. The multi-           
dimensional time-series DFA analyses will correlate EEG parameters with         
behavioral measures of driving performance to provide quantitative              
predictions of performance decrements associate with sleep onset.               
The model will be validated using a population with demographics                
consistent with the target market for the DMD (e.g., truck drivers)             
during Phase II. In addition, the system will be evaluated with the             
introduction of commonly used legal drugs (caffeine, nicotine, and cold         
medications) to determine the robustness of the model.                          
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The DMD provides three levels of user safety. When sleep onset is               
approaching, the DMD will initiate a verbal warning alarm that must be          
turned off by the user. Alternatively, the DMD can provide verbal               
feedback to ensure the user maintains high levels of alertness during           
activities that require sustained vigilance. The user can also select           
the option for the DMD to recommend the optimal time to take a short            
nap, monitor the length of the nap and awaken the user at the                   
appropriate time.                                                               
                                                                                
Currently, more that 10% of the U.S. workforce or an estimated 20               
million people are engaged in night sift work. The transportation               
industry, including airline, railroad, marine and highway transportation        
companies, is the nation's third largest employer of shift workers. Long        
haul truck drivers, in particular, are vulnerable to sleepiness because         
they drive through the night, in most cases unaccompanied, and generally        
sleep less than 6 hours per day at irregular intervals. In addition, an         
estimated 6 million Americans suffer from chronic sleep disorders which         
make them vulnerable to fatigue in the workplace.                               
",2794305,R44NS035387,['R44NS035387'],NS,https://reporter.nih.gov/project-details/2794305,R44,1999,342050,-0.0001339163026085335
"Asthma is a highly prevalent disease with a significant morbidity               
leading to emergency room visits, hospitalizations and absenteeism from         
work and school. Two major factors contributing to asthma morbidity (and        
mortality) are failure to detect asthma decompensation and use of               
inappropriate treatment strategies. Asthma severity can be assessed by          
clinical symptoms and peak flow measurements. There are internationally         
accepted guidelines for asthma management. However, rapid translation           
of asthma symptoms into a change in therapy is frequently impeded by the        
patient's inability to understand self management, reluctance to contact        
the health care provider, or delays in feedback from the provider to the        
patient. The objective of the proposal is to develop and clinically             
evaluate a new Automated System for Telephonic Home Management of               
Asthma.                                                                         
                                                                                
We describe the design and alpha testing of a dynamic relational                
database (DRdb) suitable for use either in a isolated PC or in a                
distributed computer system to support daily reporting and facilitate           
all provider monitoring. The resulting DRdb accepts information about           
the patient, their specific diagnoses and treatment regimens. Each DRdb         
is accessible on-line, linked through LAN or WAN and captures the               
patient's symptoms and measurements at the moment when they are                 
reported. Because the patient's history is electronically available, the        
DRdb can dynamically support the health professional to relay                   
instructions immediately. Generally these instructions are pre-defined          
by the patient's doctor for the reported symptom set. Automation of             
these procedures may be possible.                                               
                                                                                
Specifically, we are developing a custom telephone interface, conducting        
a clinical trial in both adult and pediatric populations in which asthma        
management is under constant physician direction and where asthma               
intervention is (I) computer-assisted, (II) computer-mediated, or (III)         
according to conventional methods (Control Group). We measure end-points        
relevant to disease managers(costs), to health care providers(time) and         
to asthmatic patients(respiratory outcomes). The goals of the Asthma            
system described in this proposal are to save time (efficacy), and              
increase patient and health care provider satisfaction (attractiveness).        
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The commercial application of this grant may be attractive to Health            
Maintenance Organizations, Medicare and Medicaid as well as those who           
see the need for cost containment.                                              
 adult human (21+); artificial intelligence; asthma; biomedical automation; child (0-11); clinical research; clinical trials; computer assisted patient care; computer program /software; computer system design /evaluation; home health care; human subject; human therapy evaluation; informatics; patient care management; pediatrics; self care; telecommunications; telemedicine AUTOMATED SYSTEM FOR TELEPHONIC MGMT OF ASTHMA","Asthma is a highly prevalent disease with a significant morbidity               
leading to emergency room visits, hospitalizations and absenteeism from         
work and school. Two major factors contributing to asthma morbidity (and        
mortality) are failure to detect asthma decompensation and use of               
inappropriate treatment strategies. Asthma severity can be assessed by          
clinical symptoms and peak flow measurements. There are internationally         
accepted guidelines for asthma management. However, rapid translation           
of asthma symptoms into a change in therapy is frequently impeded by the        
patient's inability to understand self management, reluctance to contact        
the health care provider, or delays in feedback from the provider to the        
patient. The objective of the proposal is to develop and clinically             
evaluate a new Automated System for Telephonic Home Management of               
Asthma.                                                                         
                                                                                
We describe the design and alpha testing of a dynamic relational                
database (DRdb) suitable for use either in a isolated PC or in a                
distributed computer system to support daily reporting and facilitate           
all provider monitoring. The resulting DRdb accepts information about           
the patient, their specific diagnoses and treatment regimens. Each DRdb         
is accessible on-line, linked through LAN or WAN and captures the               
patient's symptoms and measurements at the moment when they are                 
reported. Because the patient's history is electronically available, the        
DRdb can dynamically support the health professional to relay                   
instructions immediately. Generally these instructions are pre-defined          
by the patient's doctor for the reported symptom set. Automation of             
these procedures may be possible.                                               
                                                                                
Specifically, we are developing a custom telephone interface, conducting        
a clinical trial in both adult and pediatric populations in which asthma        
management is under constant physician direction and where asthma               
intervention is (I) computer-assisted, (II) computer-mediated, or (III)         
according to conventional methods (Control Group). We measure end-points        
relevant to disease managers(costs), to health care providers(time) and         
to asthmatic patients(respiratory outcomes). The goals of the Asthma            
system described in this proposal are to save time (efficacy), and              
increase patient and health care provider satisfaction (attractiveness).        
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
The commercial application of this grant may be attractive to Health            
Maintenance Organizations, Medicare and Medicaid as well as those who           
see the need for cost containment.                                              
",2777211,R44HL059024,['R44HL059024'],HL,https://reporter.nih.gov/project-details/2777211,R44,1999,370671,-0.0070158855468125435
"Recently, recording high-resolution Electroencephalograms (EEGs) from           
a large number of electrodes has become a clear trend in both brain             
research and clinical diagnosis.  However, the current EEG data                 
acquisition systems store the collected data in a form that has never           
changed since digital EEG emerged about 30 years ago.  As a result, the         
size of the output data file increases enormously as the number of              
recording channels increases, causing various problems including high           
costs in data analysis, database management, archiving, and transmission        
through the internet.                                                           
                                                                                
This proposal seeks to solve this problem through fundamental research          
on data compression specifically for EEG data, but applicable to other          
physiological data as well.  Our key approach is based on the                   
application of advanced mathematical and signal processing technologies         
to this critical problem.  We will develop and optimize a variable              
sampling technique which eliminates redundant data samples using spline         
interpolation and wavelet transformation.  We will also investigate             
lossless data compression algorithms that possess two important                 
features: 1) any part of the data within the compressed file can be read        
without having to decompress the entire file, and 2) the compressed data        
can be transmitted and presented in coarse or fine resolutions as               
needed.  We expect that, using both variable sampling and lossless              
compression, the EEG file size can be reduced by approximately 70               
percent.                                                                        
 Internet; artificial intelligence; bioengineering /biomedical engineering; bioimaging /biomedical imaging; clinical biomedical equipment; clinical research; computer data analysis; computer system design /evaluation; data collection methodology /evaluation; data management; digital imaging; electroencephalography; human data; human subject; informatics; technology /technique development MULTICHANNEL EEG DATA COMPRESSION","Recently, recording high-resolution Electroencephalograms (EEGs) from           
a large number of electrodes has become a clear trend in both brain             
research and clinical diagnosis.  However, the current EEG data                 
acquisition systems store the collected data in a form that has never           
changed since digital EEG emerged about 30 years ago.  As a result, the         
size of the output data file increases enormously as the number of              
recording channels increases, causing various problems including high           
costs in data analysis, database management, archiving, and transmission        
through the internet.                                                           
                                                                                
This proposal seeks to solve this problem through fundamental research          
on data compression specifically for EEG data, but applicable to other          
physiological data as well.  Our key approach is based on the                   
application of advanced mathematical and signal processing technologies         
to this critical problem.  We will develop and optimize a variable              
sampling technique which eliminates redundant data samples using spline         
interpolation and wavelet transformation.  We will also investigate             
lossless data compression algorithms that possess two important                 
features: 1) any part of the data within the compressed file can be read        
without having to decompress the entire file, and 2) the compressed data        
can be transmitted and presented in coarse or fine resolutions as               
needed.  We expect that, using both variable sampling and lossless              
compression, the EEG file size can be reduced by approximately 70               
percent.                                                                        
",2830118,R01NS038494,['R01NS038494'],NS,https://reporter.nih.gov/project-details/2830118,R01,1999,167453,-0.2000902687151525
"Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution.  artificial intelligence; bioimaging /biomedical imaging; computer simulation; computer system design /evaluation; electron crystallography; electron density; molecular biology information system; physical model; protein structure; structural biology PATTERN RECOGNITION IN MACROMOLECULAR CRYSTALLOGRAPHY","Computer algorithms based on pattern recognition are being used in many areas of science and technology to assist the scientist in solving complex, time-consuming, and often tedious real-world problems.  The basic premise is to train a computer to efficiently identify a known pattern in an unknown dataset.  This needle-in-a-haystack approach is being used in the area of genomics, where there are already several examples of very powerful computational pattern recognition approaches available for searching new sequences for structural motifs, similarities to other proteins and DNA, and predicting secondary structure, based solely on the DNA or amino acid sequence.  We believe that macromolecular crystallography can also benefit from the application of pattern recognition to the often daunting task of fitting atoms into an electron density map.  The fact that electron density maps are three-dimensional images provides an additional challenge to this technology in that the procedures we are developing in order to find matching patterns must be rotation invariant.  To test the validity of our hypothesis we will complete the following aims: 1) we will develop a set of rotation invariant features that can characterize the patterns in regions of an electron density map, 2) we will determine the optimal size of feature regions and the size and type of structural database required to find similar regions of electron density capable of accurately determining structures, and 3) we will develop a methodology to synthesize matched regions to produce coherent local and global models of protein structure. If these goals can be met, we will investigate the feasibility of incorporating knowledge-based methods, neural networks, and other AI techniques to augment the interpretation of structures from electron density maps.  In addition, we will attempt to extend this methodology to produce initial structures for electron density maps that are either of poor quality and/or low resolution. ",2835580,R21GM059398,['R21GM059398'],GM,https://reporter.nih.gov/project-details/2835580,R21,1999,101500,-0.21733966860243722
"
Over the past five years, new genomics technologies and DNA sequence information have driven unprecedented increases in functional genetic data. Academic and pharmaceutical researchers will soon have to understand and integrate millions of interrelations between genes, drugs and phenotypes. The changing scale of molecular genetic data suggests that a more digitized and computable means of representing and using functional information is needed in both academic and industry. This grant decribes the GeneNet system.
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE artificial intelligence; chemical information system; computer graphics /printing; computer system design /evaluation; drug discovery /isolation; genetic registry /resource /referral center; information system analysis; molecular biology information system GENENET-A KNOWLEDGE BASE OF  FUNCTIONAL GENETIC DATA","
Over the past five years, new genomics technologies and DNA sequence information have driven unprecedented increases in functional genetic data. Academic and pharmaceutical researchers will soon have to understand and integrate millions of interrelations between genes, drugs and phenotypes. The changing scale of molecular genetic data suggests that a more digitized and computable means of representing and using functional information is needed in both academic and industry. This grant decribes the GeneNet system.
PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE",6017746,R43LM006878,['R43LM006878'],LM,https://reporter.nih.gov/project-details/6017746,R43,1999,201880,-0.019733054290804478
"DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
 adolescence (12-20); aging; audiometry; auditory discrimination; behavioral /social science research tag; binaural hearing; human middle age (35-64); human old age (65+); human subject; longitudinal human study; noise; perception; psychoacoustics; sensorineural hearing loss; space perception; speech; speech recognition; young adult human (21-34) SPEECH PERCEPTION UNDER NONOPTIMAL CONDITIONS IN AGING","DESCRIPTION: (Adapted From The  Applicant's Abstract.) Impairment  of the       
ability to understand conversation  under difficult listening conditions,       
such  as  in highly reverberant rooms  or in  gatherings where several          
persons are  talking simultaneously,  affects a substantial portion  of         
elderly individuals.  This impairment may vary in severity, but only in         
very few cases can it be overcome  by the use  of currently available           
prosthetic devices.  Attempts to alleviate  this  impairment  have been         
impeded by  the fact that  neither the precise  characteristics of  the         
intact  process in  the young, nor the causes  of its  breakdown in the         
old, are currently well understood.                                             
                                                                                
The proposed research represents a continuation of work aimed at                
investigating  the ability of  both elderly and young individuals to            
understand speech under  non-optimal  listening conditions,  i.e.,              
perceptual separation  of a  speech  target from simultaneously ongoing         
irrelevant ""noise"".  The research  has two  main  objectives: (1) to            
investigate,  in elderly and  in young listeners, the  perceptual               
processes (in  particular,  spatial resolution and resolution of temporal       
fluctuations) which  play a role  in the separation of  simultaneously          
presented relevant and irrelevant  auditory signals; and (2) to study           
a group  of elderly individuals over a  five year period, in order to           
detect initial or  progressive deterioration of the ability to separate         
simultaneous signals and  to determine the correlates of this                   
deterioration.                                                                  
                                                                                
These objectives  will be achieved by  testing selected groups of elderly       
and  young individuals on standard  and non-standard audiological tests         
as  well as  psychophysical  tests.  Spatial hearing  will be assessed          
in a simulated  free field.  Multidimensional  auditory performance             
profiles of subjects  will  be  defined  through  principal  component          
analysis  and other  multivariate  statistical methods.                         
                                                                                
The  major scientific  significance of the  proposed  study is  that it         
will  provide a more precise  definition of auditory temporal and               
spatial processes  that allow  for the perceptual separation  of speech         
and  background noise and  will also identify precise auditory processes        
affected by aging.  The clinical  significance of  the study is that  it        
will establish a  multidimensional data  base of auditory  capabilities         
in  elderly individuals with  mild to  moderate  sensorineural  hearing         
loss,  and  may identify  auditory processes which, if  impaired, will          
help predict  impending deterioration of  speech understanding  under           
non-optimal  listening conditions.  This  work will have an impact on a         
wide-spread impairment of verbal communication in the elderly.                  
",6012548,R01AG007998,['R01AG007998'],AG,https://reporter.nih.gov/project-details/6012548,R01,1999,38048,-0.13923806352653845
"This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
 artificial intelligence; behavioral /social science research tag; biomechanics; computed axial tomography; computer program /software; computer simulation; electrical measurement; human subject; magnetic resonance imaging; medical rehabilitation related tag; psychoacoustics; respiratory airflow measurement; respiratory imaging /visualization; speech; speech synthesizers; vocal cords; vocalization; voice SIMULATION OF VOICE QUALITIES IN SPEECH","This research addresses one of the remaining challenges in speech science,      
high quality speech simulation.  We define speech simulation as a form of       
speech synthesis in which the movement of air and tissue is under               
experimental control, rather than the resulting acoustic signal.  From the      
early days of speech synthesis nearly a half century ago, the expectation       
has always been that a better representation of the laws of physics of air      
and tissue in motion would produce better synthesis. Although this              
expectation still exists today, the payoff has been slow, primarily             
because there are few data sets from which to build theoretical                 
generalizations. In this proposal, the principal investigator and his           
colleagues draw upon experience gained with simulation of the phonatory         
processes to include the entire vocal tract in sentence-level speech            
production.  The first phase will be to obtain naturalness in speech            
quality that is comparable to formant synthesis by modeling a few specific      
speakers from whom extensive data sets will be available.  The second           
phase will be to develop scaling and modification rules that will allow         
the voice of a given speaker to be transformed into a different age,            
gender, emotion, and voice quality. The transformation will also include        
induced or corrected voice and speech disorders.  The idea of voice             
transformation (conversion) is not new, but the attempt to do it all in         
the articulatory domain is relatively untried.  The results will have           
practical and theoretical impact on the development of assistive devices        
for voice/speech impaired populations, for surgery performed on the larynx      
and upper respiratory tract, and for speech training and rehabilitation.        
",6030194,R01DC002532,['R01DC002532'],DC,https://reporter.nih.gov/project-details/6030194,R01,1999,238912,-0.020115242840058323
"The overall objective of our proposal is to design and implement a general      
deconvolution tool in the SAAM II software system. Deconvolution is a           
method used to estimate input into a system, such as the rate of                
appearance of a drug in plasma after an oral administration or the              
secretion rate of a gland, since these input fluxes are usually not             
directly measurable in vivo. A tool to estimate these exogenous and             
endogenous fluxes inputs is essential for a quantitative understanding of       
the kinetics of drugs, hormones and substrates in the body in both healthy      
and disease states.                                                             
                                                                                
A novel nonparametric stochastic deconvolution approach has been developed      
under the aegis of an R-01 grant from BTP/NCRR which is able to cope with       
the peculiarities of pharmacokinetic/physiological systems. New numerical       
algorithms based on a singular value decomposition strategy permit the          
solution of high-dimensional deconvolution problems. The theory and             
algorithms have been prototyped and tested. The purpose of our proposed         
project is to implement the deconvolution tool in the SAAM II software          
system, and to make it easily available to the user via the graphical use       
interface.                                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
SAAM II is a powerful, general purpose simulation software package widely       
used in pharmaceutical and medical research. Many investigators using the       
software want to be able to estimate inputs into the systems they are           
studying under various experimental inputs. Deconvolution is a tool used        
to do this. Making this tool easily available should have a considerable        
impact on our current and potential user communities.                           
 chemical kinetics; computer graphics /printing; computer program /software; computer simulation; computer system design /evaluation; mathematical model DECONVOLUTION MODELING TOOL FOR SAAM II","The overall objective of our proposal is to design and implement a general      
deconvolution tool in the SAAM II software system. Deconvolution is a           
method used to estimate input into a system, such as the rate of                
appearance of a drug in plasma after an oral administration or the              
secretion rate of a gland, since these input fluxes are usually not             
directly measurable in vivo. A tool to estimate these exogenous and             
endogenous fluxes inputs is essential for a quantitative understanding of       
the kinetics of drugs, hormones and substrates in the body in both healthy      
and disease states.                                                             
                                                                                
A novel nonparametric stochastic deconvolution approach has been developed      
under the aegis of an R-01 grant from BTP/NCRR which is able to cope with       
the peculiarities of pharmacokinetic/physiological systems. New numerical       
algorithms based on a singular value decomposition strategy permit the          
solution of high-dimensional deconvolution problems. The theory and             
algorithms have been prototyped and tested. The purpose of our proposed         
project is to implement the deconvolution tool in the SAAM II software          
system, and to make it easily available to the user via the graphical use       
interface.                                                                      
                                                                                
PROPOSED COMMERCIAL APPLICATION:                                                
SAAM II is a powerful, general purpose simulation software package widely       
used in pharmaceutical and medical research. Many investigators using the       
software want to be able to estimate inputs into the systems they are           
studying under various experimental inputs. Deconvolution is a tool used        
to do this. Making this tool easily available should have a considerable        
impact on our current and potential user communities.                           
",2713849,R43GM058324,['R43GM058324'],GM,https://reporter.nih.gov/project-details/2713849,R43,1999,94837,-0.08815402713569
"DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed        
to  develop adaptive imaging algorithms and instrumentation to compensate       
for  tissue-induced ultrasonic image degradation.  Theoretical and              
simulation  studies are proposed to optimize the accuracy, stability, and       
speed of  adaptive algorithms and to explore the impact of transducer           
design on adaptive  imaging.  In addition, the applicants proposed to           
collect high-quality tissue  echo data and through-transmission data to         
investigate the nature of tissue- induced image degradation.  The               
adaptive imaging techniques would be implemented in real-time on an             
advanced engineering prototype scanner.  Synthetic receive aperture (SRA)       
techniques, combined with adaptive imaging,  would be used to address           
1000 and 2000 element two dimensional (2-D) arrays  and to form very high       
resolution images.  Specialized analog multiplexers and  other hardware         
would be constructed for this system.  Clinical trials would  evaluate          
the performance of the adaptive/SRA system in imaging breast lesions  and       
breast microcalcifications, and in renal and adrenal gland imaging              
studies.  The applicants hypothesized that the proposed techniques and          
system  would markedly improve ultrasonic image quality in a wide variety       
of clinical  applications.                                                      
 adrenal disorder; artificial intelligence; bioimaging /biomedical imaging; calcification; computer simulation; computer system design /evaluation; endocrine disorder diagnosis; human subject; image processing; kidney disorder diagnosis; mammary disorder; nephrolithiasis; reproductive system disorder diagnosis; ultrasonography ADAPTIVE ULTRASONIC IMAGING","DESCRIPTION (Adapted from Applicant's Abstract): The applicants proposed        
to  develop adaptive imaging algorithms and instrumentation to compensate       
for  tissue-induced ultrasonic image degradation.  Theoretical and              
simulation  studies are proposed to optimize the accuracy, stability, and       
speed of  adaptive algorithms and to explore the impact of transducer           
design on adaptive  imaging.  In addition, the applicants proposed to           
collect high-quality tissue  echo data and through-transmission data to         
investigate the nature of tissue- induced image degradation.  The               
adaptive imaging techniques would be implemented in real-time on an             
advanced engineering prototype scanner.  Synthetic receive aperture (SRA)       
techniques, combined with adaptive imaging,  would be used to address           
1000 and 2000 element two dimensional (2-D) arrays  and to form very high       
resolution images.  Specialized analog multiplexers and  other hardware         
would be constructed for this system.  Clinical trials would  evaluate          
the performance of the adaptive/SRA system in imaging breast lesions  and       
breast microcalcifications, and in renal and adrenal gland imaging              
studies.  The applicants hypothesized that the proposed techniques and          
system  would markedly improve ultrasonic image quality in a wide variety       
of clinical  applications.                                                      
",2837619,R01CA043334,['R01CA043334'],CA,https://reporter.nih.gov/project-details/2837619,R01,1999,235325,-0.10486311673106692
"DESCRIPTION:  The candidate, Dr. Adrian Casillas, is a general internist        
with board certification in clinical immunology and is an Assistant             
Professor of Clinical Immunology and Allergy at UCLA.  He has previous          
laboratory-based research experience in immunology and biochemistry.  He has    
also performed some preliminary studies the project sponsor which have not      
yet been published.                                                             
                                                                                
The sponsor, Dr. Ron Stevens, is Professor of Microbiology and Immunology       
and of Education.  He has been using computer-based tools for medical           
education since the late 1980's, and lists several publications that            
describe neural network techniques he has developed.                            
                                                                                
The candidate's career development plan includes a combination of didactic      
training and research that apply computer-based patient simulations and         
neural network strategies to the domain of childhood asthma.  Didactic work     
will include basic programming, data basis, networks, and statistics.           
                                                                                
The proposed research plan has three specific aims.  In the first aim, the      
candidate proposes to build on an existing set of computer-based clinical       
problem solving cases to construct a comprehensive set of cases that can be     
used to determine the comprehension of childhood asthma across a broad set      
of audiences.  The second aim is to employ artificial neural networks to        
evaluate performance on the computerized cases.  The third aim involves         
implementing the computer-based cases and the neural network performance        
model in three settings:  physicians in training, nursing and nurse             
practitioner students, and asthma patients in the community.  To a lesser       
extent physician experts will be evaluated with the system.  To achieve         
these aims the candidate has garnered the help of faculty in the School of      
Nursing, the Department of Medicine, and prominent members of the               
educational community in Los Angeles.  The candidate will have access to        
equipment, supplies, and technical support from the sponsor's laboratory.       
 artificial intelligence; asthma; case history; clinical research; health care model; health services research tag; human subject; information systems; model design /development; statistics /biometry MODELING THE UNDERSTANDING OF CHILDHOOD ASTHMA","DESCRIPTION:  The candidate, Dr. Adrian Casillas, is a general internist        
with board certification in clinical immunology and is an Assistant             
Professor of Clinical Immunology and Allergy at UCLA.  He has previous          
laboratory-based research experience in immunology and biochemistry.  He has    
also performed some preliminary studies the project sponsor which have not      
yet been published.                                                             
                                                                                
The sponsor, Dr. Ron Stevens, is Professor of Microbiology and Immunology       
and of Education.  He has been using computer-based tools for medical           
education since the late 1980's, and lists several publications that            
describe neural network techniques he has developed.                            
                                                                                
The candidate's career development plan includes a combination of didactic      
training and research that apply computer-based patient simulations and         
neural network strategies to the domain of childhood asthma.  Didactic work     
will include basic programming, data basis, networks, and statistics.           
                                                                                
The proposed research plan has three specific aims.  In the first aim, the      
candidate proposes to build on an existing set of computer-based clinical       
problem solving cases to construct a comprehensive set of cases that can be     
used to determine the comprehension of childhood asthma across a broad set      
of audiences.  The second aim is to employ artificial neural networks to        
evaluate performance on the computerized cases.  The third aim involves         
implementing the computer-based cases and the neural network performance        
model in three settings:  physicians in training, nursing and nurse             
practitioner students, and asthma patients in the community.  To a lesser       
extent physician experts will be evaluated with the system.  To achieve         
these aims the candidate has garnered the help of faculty in the School of      
Nursing, the Department of Medicine, and prominent members of the               
educational community in Los Angeles.  The candidate will have access to        
equipment, supplies, and technical support from the sponsor's laboratory.       
",2886124,K08AI001560,['K08AI001560'],AI,https://reporter.nih.gov/project-details/2886124,K08,1999,73980,-0.0717158680951384
"DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating            
Characteristic (ROC) analysis is recognized widely as the best way of           
measuring and specifying the accuracies of diagnostic procedures, because it    
is able to distinguish between actual differences in discrimination             
capacity, on one hand, and apparent differences that are due only to            
decision-threshold effects, on the other.  Key methodological needs remain      
to be satisfied before ROC analysis can address all of the practically          
important situations that arise in diagnostic applications, however.  This      
project employs signal detection theory and computer simulation to address      
several of those needs, by:  (1) refining and continuing distribution of        
software developed previously by the applicants for fitting ROC curves and      
for testing the statistical significance of differences between ROC curve       
estimates; (2) developing and evaluating new algorithms for ROC                 
curve-Fitting and statistical testing, based on their recently-developed        
""proper"" binormal model, that should provide more meaningful results in         
experimental situations that involve small samples of cases; (3)                
investigating the usefulness of a form of ROC methodology that is based on      
mixture distributions in order to rduce the need for diagnostic truth in ROC    
experiments; (4) investigating the effect of case-saple difficulty on the       
statistical power tests for differences between ROC curves, in order to         
determine the optimal difficulty of cases that shouldbe studied on rank         
diagnostic systems; and (5) developing methods for training artificial          
neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC          
analysis and signal detection theory.                                           
 artificial intelligence; computer assisted diagnosis; computer system design /evaluation; method development; statistics /biometry NEW ROC METHODOLOGY TO ASSESS DIAGNOSTIC ACCURACY","DESCRIPTION (Adapted from Applicant's Abstract):  Receiver Operating            
Characteristic (ROC) analysis is recognized widely as the best way of           
measuring and specifying the accuracies of diagnostic procedures, because it    
is able to distinguish between actual differences in discrimination             
capacity, on one hand, and apparent differences that are due only to            
decision-threshold effects, on the other.  Key methodological needs remain      
to be satisfied before ROC analysis can address all of the practically          
important situations that arise in diagnostic applications, however.  This      
project employs signal detection theory and computer simulation to address      
several of those needs, by:  (1) refining and continuing distribution of        
software developed previously by the applicants for fitting ROC curves and      
for testing the statistical significance of differences between ROC curve       
estimates; (2) developing and evaluating new algorithms for ROC                 
curve-Fitting and statistical testing, based on their recently-developed        
""proper"" binormal model, that should provide more meaningful results in         
experimental situations that involve small samples of cases; (3)                
investigating the usefulness of a form of ROC methodology that is based on      
mixture distributions in order to rduce the need for diagnostic truth in ROC    
experiments; (4) investigating the effect of case-saple difficulty on the       
statistical power tests for differences between ROC curves, in order to         
determine the optimal difficulty of cases that shouldbe studied on rank         
diagnostic systems; and (5) developing methods for training artificial          
neural networks (ANNs) to maximize diagnostic accuracy in terms of ROC          
analysis and signal detection theory.                                           
",2910432,R01GM057622,['R01GM057622'],GM,https://reporter.nih.gov/project-details/2910432,R01,1999,213180,-0.021674438235324885
 X ray crystallography; artificial intelligence; chemical structure; computer system design /evaluation; crystallization; method development ARTIFICIAL INTELLIGENCE METHODS FOR CRYSTALIZATION,,6033407,R01RR014477,['R01RR014477'],RR,https://reporter.nih.gov/project-details/6033407,R01,1999,225001,-0.1810789344680487
"Despite advances in pharrnacotherapy for hypertension, as many as 75% of hypertensive patients' blood pressure is poorly controlled. Uncontrolled hypertension is associated with increased morbidity and mortality and is largely attributed to failure to adhere with antihypertensive medications. Nonadherence to medication regimens is a long-standing phenomenon that increases health care utilization and costs. Existing research demonstrates that a theoretical framework is needed to advance our ability to design effective interventions. Preliminary research on the application of the Transtheoretical Model (TTM) to medication adherence suggests that the model has strong explanatory power in this area and has the potential to guide intervention development. Phase l of this Fast-Track proposal involves the development and feasibility test of a stage-matched, individualized, interactive intervention based on the TTM. Feasibility includes proactively recruiting 70% of eligible participants (n=300) to participate in a randomized trial of an adherence expert system intervention and evaluating acceptability of the intervention materials. Computerized expert system interventions and stage-matched manuals delivered proactively to entire populations can have unprecedented impacts on adherence, offering far greater availability at a low cost. Applying the TTM to intervention development will provide the theoretical and individualized framework that experts in the area have indicated is necessary. PROPOSED COMMERCIAL APPLICATIONS: Up to 75% (37.5 million) of the 50 million Americans afflicted with hypertension fail to adhere to their medication regimen. Nonadherence with antihypertensives, which increases utilization of outpatient, inpatient, and emergency services, contributes a large proportion of drug-related morbidity and mortality the annual costs of which exceed $76 billion. Effective and cost-effective population-based adherence expert systems can reduce morbidity, mortality, health care utilization, and related costs, resulting in tremendous commercial potential.  antihypertensive agents; artificial intelligence; clinical research; computer assisted patient care; computer system design /evaluation; health care service utilization; human subject; hypertension; interactive multimedia; patient monitoring device; questionnaires; therapy compliance AN EXPERT SYSTEM TO IMPROVE MEDICATION ADHERENCE","Despite advances in pharrnacotherapy for hypertension, as many as 75% of hypertensive patients' blood pressure is poorly controlled. Uncontrolled hypertension is associated with increased morbidity and mortality and is largely attributed to failure to adhere with antihypertensive medications. Nonadherence to medication regimens is a long-standing phenomenon that increases health care utilization and costs. Existing research demonstrates that a theoretical framework is needed to advance our ability to design effective interventions. Preliminary research on the application of the Transtheoretical Model (TTM) to medication adherence suggests that the model has strong explanatory power in this area and has the potential to guide intervention development. Phase l of this Fast-Track proposal involves the development and feasibility test of a stage-matched, individualized, interactive intervention based on the TTM. Feasibility includes proactively recruiting 70% of eligible participants (n=300) to participate in a randomized trial of an adherence expert system intervention and evaluating acceptability of the intervention materials. Computerized expert system interventions and stage-matched manuals delivered proactively to entire populations can have unprecedented impacts on adherence, offering far greater availability at a low cost. Applying the TTM to intervention development will provide the theoretical and individualized framework that experts in the area have indicated is necessary. PROPOSED COMMERCIAL APPLICATIONS: Up to 75% (37.5 million) of the 50 million Americans afflicted with hypertension fail to adhere to their medication regimen. Nonadherence with antihypertensives, which increases utilization of outpatient, inpatient, and emergency services, contributes a large proportion of drug-related morbidity and mortality the annual costs of which exceed $76 billion. Effective and cost-effective population-based adherence expert systems can reduce morbidity, mortality, health care utilization, and related costs, resulting in tremendous commercial potential. ",6222056,R44HL063481,['R44HL063481'],HL,https://reporter.nih.gov/project-details/6222056,R44,1999,84443,-0.01758273935659231
"The proposed work offers enhancements to the MagicMarker software developed in Phase I.  This software offers a new methodology for the display and analysis of long-term EEG records.  Epochs of similar activity are grouped into segments and then states via two-pass hierarchial clustering.  This results in clearly differentiated background, paroxysmal activity and patient state transitions.  The underlying EEG  is always available via hyper- links so that artifacts can be distinguished from ""real EEG."" The Phase II work adds classification abilities (intelligence) to the Phase I software.  Proposed are an expert-level seizure detector, an ICU abnormality detector and a user-defined activity detector.  The effort includes the development of a large library of carefully analyzed and annotated prolonged EEG studies. Newborns, older children and adults will be included ensuring robust algorithms for all age groups. The proposed software greatly reduces staff requirements for long-term monitoring through intelligent notification (visual, audible and dial-up pager) of interesting events.  This, in addition to the ability to monitor patients ""away from the lab,"" provides more frequent patient checks and improved clinical outcomes. PROPOSED COMMERCIAL APPLICATION The proposed software would be a valuable addition to any digital EEG because of the great timesavings it provides for both neurologists and EEG technicians.  The software will be marketed along with current Persyst products (Insight, SpikeDetector and Prism), which support and are sold by virtually every major DEEG manufacturer.  Internet; artificial intelligence; bioimaging /biomedical imaging; computer data analysis; computer program /software; computer system design /evaluation; electroencephalography; generalized seizures; human data; image processing; intensive care CLUSTERING AND HYPER-LINKING OF LONG-TERM EEGS","The proposed work offers enhancements to the MagicMarker software developed in Phase I.  This software offers a new methodology for the display and analysis of long-term EEG records.  Epochs of similar activity are grouped into segments and then states via two-pass hierarchial clustering.  This results in clearly differentiated background, paroxysmal activity and patient state transitions.  The underlying EEG  is always available via hyper- links so that artifacts can be distinguished from ""real EEG."" The Phase II work adds classification abilities (intelligence) to the Phase I software.  Proposed are an expert-level seizure detector, an ICU abnormality detector and a user-defined activity detector.  The effort includes the development of a large library of carefully analyzed and annotated prolonged EEG studies. Newborns, older children and adults will be included ensuring robust algorithms for all age groups. The proposed software greatly reduces staff requirements for long-term monitoring through intelligent notification (visual, audible and dial-up pager) of interesting events.  This, in addition to the ability to monitor patients ""away from the lab,"" provides more frequent patient checks and improved clinical outcomes. PROPOSED COMMERCIAL APPLICATION The proposed software would be a valuable addition to any digital EEG because of the great timesavings it provides for both neurologists and EEG technicians.  The software will be marketed along with current Persyst products (Insight, SpikeDetector and Prism), which support and are sold by virtually every major DEEG manufacturer. ",6017354,R44MH055895,['R44MH055895'],MH,https://reporter.nih.gov/project-details/6017354,R44,1999,395302,-0.02334764993912163
"The scope of the proposed study is to develop and test new statistical procedures for age-period-cohort (APC) analysis of cancer incidence and mortality rates. Guided by an expanding literature on statistical methods and experience in statistical modeling and analyses for APC studies, this research plan addresses the development of models that aim to resolve the identifiability problem. A new statistical methodology is developed through eigen-analysis, principal component analysis, shrinkage models and reduce models with bias correction methods. These models address the problem of estimation and bias in log-linear regression models with exact collinearity between the effects of age, period and cohort in APC analysis. A unique estimator is identified, which generates accurate trend estimation with minimal bias or no bias. These new statistical models provide estimates of the age, period and cohort effects of cancer incidence and mortality, which exploit fully cancer summary rate data arising from the Surveillance, Epidemiology, and End Results (SEER) and other cancer registries. These APC models also permit estimation of future cancer rates. The methodology allows for trend estimation, construction of confidence intervals, bias correction and forecasting. The performance of the proposed methods will be evaluated with simulated and real cancer incidence and mortality rate data. The methodology will be applied to several studies on cancer incidence and mortality rates, including one study on mortality rate of cervical cancer in Ontario, six studies on incidence rates of breast cancer, prostate cancer, male and female colon cancer, male and female lung cancer in Connecticut. The methodology will also be applied to studies on national mortality rates of various types of cancer in different ethnic groups, such as White, Black, Asian/Pacific islanders and Hispanics, to investigate the differences between these groups as outlined in the Healthy People 2000. The ultimate goal of the investigation is to assess trend estimation and forecasting in APC analyses. The methods proposed in this investigation have immediate application to these studies, and offer an array of promising techniques for use in APC analysis of cancer incidence and mortality.  African American; Asian Americans; Hispanic Americans; breast neoplasms; caucasian American; cervix neoplasms; clinical research; colon neoplasms; human data; human mortality; lung neoplasms; mathematical model; model design /development; neoplasm /cancer epidemiology; prostate neoplasms; statistics /biometry STATISTICAL ANALYSIS OF CANCER INCIDENCE","The scope of the proposed study is to develop and test new statistical procedures for age-period-cohort (APC) analysis of cancer incidence and mortality rates. Guided by an expanding literature on statistical methods and experience in statistical modeling and analyses for APC studies, this research plan addresses the development of models that aim to resolve the identifiability problem. A new statistical methodology is developed through eigen-analysis, principal component analysis, shrinkage models and reduce models with bias correction methods. These models address the problem of estimation and bias in log-linear regression models with exact collinearity between the effects of age, period and cohort in APC analysis. A unique estimator is identified, which generates accurate trend estimation with minimal bias or no bias. These new statistical models provide estimates of the age, period and cohort effects of cancer incidence and mortality, which exploit fully cancer summary rate data arising from the Surveillance, Epidemiology, and End Results (SEER) and other cancer registries. These APC models also permit estimation of future cancer rates. The methodology allows for trend estimation, construction of confidence intervals, bias correction and forecasting. The performance of the proposed methods will be evaluated with simulated and real cancer incidence and mortality rate data. The methodology will be applied to several studies on cancer incidence and mortality rates, including one study on mortality rate of cervical cancer in Ontario, six studies on incidence rates of breast cancer, prostate cancer, male and female colon cancer, male and female lung cancer in Connecticut. The methodology will also be applied to studies on national mortality rates of various types of cancer in different ethnic groups, such as White, Black, Asian/Pacific islanders and Hispanics, to investigate the differences between these groups as outlined in the Healthy People 2000. The ultimate goal of the investigation is to assess trend estimation and forecasting in APC analyses. The methods proposed in this investigation have immediate application to these studies, and offer an array of promising techniques for use in APC analysis of cancer incidence and mortality. ",2904830,R03CA083010,['R03CA083010'],CA,https://reporter.nih.gov/project-details/2904830,R03,1999,64300,-0.016554631297549496
"DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling      
of cognition, it is important to have well-justified criteria for choosing      
among differing explanations (i.e., models) of observed data.  This project     
investigates those criteria as well as their instantiation in five model        
selection methods.                                                              
                                                                                
Two lines of research will be undertaken.  In the first, a thorough             
investigation of model complexity will be conducted.  Comprehensive             
simulations re intended to determine complexity's contribution to model fit     
and to model selection.  An analytical solution will also be sought with the    
hope of quantifying model complexity.                                           
                                                                                
The second line of work examines the utility of each of the five selection      
methods in choosing among models in three topic areas in cognitive              
psychology (information integration, categorization, connectionist              
modeling), the end goal being to identify their merits and shortcomings.        
                                                                                
Findings should provide a better understanding of model selection than          
currently available and serve as a useful guide for researchers comparing       
the suitability of quantitative models of cognition.                            
                                                                                
 artificial intelligence; choice; cognition; computer simulation; information dissemination; mathematical model; psychometrics SELECTING AMONG MATHEMATICAL MODELS OF COGNITION","DESCRIPTION (Adapted from Applicant's Abstract):  In mathematical modeling      
of cognition, it is important to have well-justified criteria for choosing      
among differing explanations (i.e., models) of observed data.  This project     
investigates those criteria as well as their instantiation in five model        
selection methods.                                                              
                                                                                
Two lines of research will be undertaken.  In the first, a thorough             
investigation of model complexity will be conducted.  Comprehensive             
simulations re intended to determine complexity's contribution to model fit     
and to model selection.  An analytical solution will also be sought with the    
hope of quantifying model complexity.                                           
                                                                                
The second line of work examines the utility of each of the five selection      
methods in choosing among models in three topic areas in cognitive              
psychology (information integration, categorization, connectionist              
modeling), the end goal being to identify their merits and shortcomings.        
                                                                                
Findings should provide a better understanding of model selection than          
currently available and serve as a useful guide for researchers comparing       
the suitability of quantitative models of cognition.                            
                                                                                
",2891003,R01MH057472,['R01MH057472'],MH,https://reporter.nih.gov/project-details/2891003,R01,1999,75078,-0.024871539071547696
"
 DESCRIPTION: (Adapted from the Investigator's Abstract) Audio-Spatial GUIs for     
 blind people. The PI proposes to use an existing system that records and           
 recognizes hand gestures and movements (KnowWare), to enable blind users to use    
 GUIs. In particular the proposed system will provide access to the spatial         
 format of the Windows operating system, applications and internet browsers. It     
 will also permit blind programmers to design GUIs as well.                         
                                                                                    
 In the proposed system the user's hands rest upon the VIDEODESK which has a        
 known size grid and are viewed by a ceiling-mounted video camera. The image of     
 the hands is analyzed by specialized processors and the locations of the           
 fingers are reported to the computer. The virtual graphic interface is defined     
 on the desk's surface and the computer generates audio feedback to tell the        
 user what their finger are ""touching."" One hand will be used for pointing          
 within windows and the other for selecting from the menus. Voice input will        
 permit additional commands.                                                        
                                                                                    
 For phase I preliminary GUI interfaces will be developed and a tool for            
 creating graphic contents that will operate under windows will be implemented.     
 Blind subjects will be used to evaluate the functionality and usefulness of the    
 developed tools.                                                                   
 PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                   
 artificial intelligence; blindness; computer graphics /printing; computer human interaction; computer program /software; computer system design /evaluation; fingers; human data; limb movement; sound KNOWWARE: AUDIO-SPATIAL GUIS FOR BLIND PEOPLE","
 DESCRIPTION: (Adapted from the Investigator's Abstract) Audio-Spatial GUIs for     
 blind people. The PI proposes to use an existing system that records and           
 recognizes hand gestures and movements (KnowWare), to enable blind users to use    
 GUIs. In particular the proposed system will provide access to the spatial         
 format of the Windows operating system, applications and internet browsers. It     
 will also permit blind programmers to design GUIs as well.                         
                                                                                    
 In the proposed system the user's hands rest upon the VIDEODESK which has a        
 known size grid and are viewed by a ceiling-mounted video camera. The image of     
 the hands is analyzed by specialized processors and the locations of the           
 fingers are reported to the computer. The virtual graphic interface is defined     
 on the desk's surface and the computer generates audio feedback to tell the        
 user what their finger are ""touching."" One hand will be used for pointing          
 within windows and the other for selecting from the menus. Voice input will        
 permit additional commands.                                                        
                                                                                    
 For phase I preliminary GUI interfaces will be developed and a tool for            
 creating graphic contents that will operate under windows will be implemented.     
 Blind subjects will be used to evaluate the functionality and usefulness of the    
 developed tools.                                                                   
 PROPOSED COMMERCIAL APPLICATION: NOT AVAILABLE                                                                                   
",6017026,R43EY012759,['R43EY012759'],EY,https://reporter.nih.gov/project-details/6017026,R43,1999,94929,-0.0656963826095117
